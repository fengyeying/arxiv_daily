<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 23 Feb 24  to  Mon 26 Feb 24, announced Tue, 27 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item612">Cross-lists</a></li>
<li><a href="#item688">Replacements</a></li>
</ul>
<small>[ total of 1206 entries:  <b>1-1206</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 27 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15511" title="Abstract">arXiv:2402.15511</a> [<a href="/pdf/2402.15511" title="Download PDF">pdf</a>, <a href="/ps/2402.15511" title="Download PostScript">ps</a>, <a href="/format/2402.15511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sixth International Workshop on Languages for Modelling Variability  (MODEVAR 2024)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galasso-Carbonnel%2C+J">Jessie Galasso-Carbonnel</a>, 
<a href="/search/cs?searchtype=author&query=Sundermann%2C+C">Chico Sundermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">This is the proceedings of the Sixth International Workshop on Languages for
Modelling Variability (MODEVAR 2024) which was held at Bern, Switzerland,
February 06th 2024.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15513" title="Abstract">arXiv:2402.15513</a> [<a href="/pdf/2402.15513" title="Download PDF">pdf</a>, <a href="/format/2402.15513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Generalizability of Physiological Characteristics of  Anxiety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+E">Emily Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mohammad Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Matari%C4%87%2C+M+J">Maja J. Matari&#x107;</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), 2023, pp. 4848-4855
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Recent works have demonstrated the effectiveness of machine learning (ML)
techniques in detecting anxiety and stress using physiological signals, but it
is unclear whether ML models are learning physiological features specific to
stress. To address this ambiguity, we evaluated the generalizability of
physiological features that have been shown to be correlated with anxiety and
stress to high-arousal emotions. Specifically, we examine features extracted
from electrocardiogram (ECG) and electrodermal (EDA) signals from the following
three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect
Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE)
dataset. We aim to understand whether these features are specific to anxiety or
general to other high-arousal emotions through a statistical regression
analysis, in addition to a within-corpus, cross-corpus, and
leave-one-corpus-out cross-validation across instances of stress and arousal.
We used the following classifiers: Support Vector Machines, LightGBM, Random
Forest, XGBoost, and an ensemble of the aforementioned models. We found that
models trained on an arousal dataset perform relatively well on a previously
unseen stress dataset, and vice versa. Our experimental results suggest that
the evaluated models may be identifying emotional arousal instead of stress.
This work is the first cross-corpus evaluation across stress and arousal from
ECG and EDA signals, contributing new findings about the generalizability of
stress detection.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15514" title="Abstract">arXiv:2402.15514</a> [<a href="/pdf/2402.15514" title="Download PDF">pdf</a>, <a href="/ps/2402.15514" title="Download PostScript">ps</a>, <a href="/format/2402.15514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Scale Generative AI Text Applied to Sports and Music
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baughman%2C+A">Aaron Baughman</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+S">Stephen Hammer</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rahul Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Akay%2C+G">Gozde Akay</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+E">Eduardo Morales</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+T">Tony Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We address the problem of scaling up the production of media content,
including commentary and personalized news stories, for large-scale sports and
music events worldwide. Our approach relies on generative AI models to
transform a large volume of multimodal data (e.g., videos, articles, real-time
scoring feeds, statistics, and fact sheets) into coherent and fluent text.
Based on this approach, we introduce, for the first time, an AI commentary
system, which was deployed to produce automated narrations for highlight
packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same
vein, our solution was extended to create personalized content for ESPN Fantasy
Football and stories about music artists for the Grammy awards. These
applications were built using a common software architecture achieved a 15x
speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our
work was successfully deployed at the aforementioned events, supporting 90
million fans around the world with 8 billion page views, continuously pushing
the bounds on what is possible at the intersection of sports, entertainment,
and AI.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15515" title="Abstract">arXiv:2402.15515</a> [<a href="/pdf/2402.15515" title="Download PDF">pdf</a>, <a href="/ps/2402.15515" title="Download PostScript">ps</a>, <a href="/format/2402.15515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasibility of Identifying Factors Related to Alzheimer&#x27;s Disease and  Related Dementia in Real-World Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yu-neng Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Serena Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Quantitative Methods (q-bio.QM); Applications (stat.AP)

</div>
<p class="mathjax">A comprehensive view of factors associated with AD/ADRD will significantly
aid in studies to develop new treatments for AD/ADRD and identify high-risk
populations and patients for prevention efforts. In our study, we summarized
the risk factors for AD/ADRD by reviewing existing meta-analyses and review
articles on risk and preventive factors for AD/ADRD. In total, we extracted 477
risk factors in 10 categories from 537 studies. We constructed an interactive
knowledge map to disseminate our study results. Most of the risk factors are
accessible from structured Electronic Health Records (EHRs), and clinical
narratives show promise as information sources. However, evaluating genomic
risk factors using RWD remains a challenge, as genetic testing for AD/ADRD is
still not a common practice and is poorly documented in both structured and
unstructured EHRs. Considering the constantly evolving research on AD/ADRD risk
factors, literature mining via NLP methods offers a solution to automatically
update our knowledge map.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15516" title="Abstract">arXiv:2402.15516</a> [<a href="/pdf/2402.15516" title="Download PDF">pdf</a>, <a href="/format/2402.15516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haocheng Liu</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Baoueb%2C+T">Teysir Baoueb</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Fontaine%2C+M">Mathieu Fontaine</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a> (MERL), 
<a href="/search/cs?searchtype=author&query=Richard%2C+G">Gael Richard</a> (IP Paris, LTCI, IDS, S2A)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Acoustics, Speech and Signal
  Processing, Apr 2024, Seoul (Korea), South Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Diffusion models are receiving a growing interest for a variety of signal
generation tasks such as speech or music synthesis. WaveGrad, for example, is a
successful diffusion model that conditionally uses the mel spectrogram to guide
a diffusion process for the generation of high-fidelity audio. However, such
models face important challenges concerning the noise diffusion process for
training and inference, and they have difficulty generating high-quality speech
for speakers that were not seen during training. With the aim of minimizing the
conditioning error and increasing the efficiency of the noise diffusion
process, we propose in this paper a new scheme called GLA-Grad, which consists
in introducing a phase recovery algorithm such as the Griffin-Lim algorithm
(GLA) at each step of the regular diffusion process. Furthermore, it can be
directly applied to an already-trained waveform generation model, without
additional training or fine-tuning. We show that our algorithm outperforms
state-of-the-art diffusion models for speech generation, especially when
generating speech for a previously unseen target speaker.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15518" title="Abstract">arXiv:2402.15518</a> [<a href="/pdf/2402.15518" title="Download PDF">pdf</a>, <a href="/format/2402.15518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beware of Words: Evaluating the Lexical Richness of Conversational Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+G">Gonzalo Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+J+A">Jos&#xe9; Alberto Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+J">Javier Conde</a>, 
<a href="/search/cs?searchtype=author&query=Reviriego%2C+P">Pedro Reviriego</a>, 
<a href="/search/cs?searchtype=author&query=Merino%2C+E">Elena Merino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The performance of conversational Large Language Models (LLMs) in general,
and of ChatGPT in particular, is currently being evaluated on many different
tasks, from logical reasoning or maths to answering questions on a myriad of
topics. Instead, much less attention is being devoted to the study of the
linguistic features of the texts generated by these LLMs. This is surprising
since LLMs are models for language, and understanding how they use the language
is important. Indeed, conversational LLMs are poised to have a significant
impact on the evolution of languages as they may eventually dominate the
creation of new text. This means that for example, if conversational LLMs do
not use a word it may become less and less frequent and eventually stop being
used altogether. Therefore, evaluating the linguistic features of the text they
produce and how those depend on the model parameters is the first step toward
understanding the potential impact of conversational LLMs on the evolution of
languages. In this paper, we consider the evaluation of the lexical richness of
the text generated by LLMs and how it depends on the model parameters. A
methodology is presented and used to conduct a comprehensive evaluation of
lexical richness using ChatGPT as a case study. The results show how lexical
richness depends on the version of ChatGPT and some of its parameters, such as
the presence penalty, or on the role assigned to the model. The dataset and
tools used in our analysis are released under open licenses with the goal of
drawing the much-needed attention to the evaluation of the linguistic features
of LLM-generated text.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15521" title="Abstract">arXiv:2402.15521</a> [<a href="/pdf/2402.15521" title="Download PDF">pdf</a>, <a href="/format/2402.15521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HKD-SHO: A hybrid smart home system based on knowledge-based and  data-driven services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+M">Mingming Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Najm%2C+E">Elie Najm</a>, 
<a href="/search/cs?searchtype=author&query=Sharrock%2C+R">R&#xe9;mi Sharrock</a>, 
<a href="/search/cs?searchtype=author&query=Traverson%2C+B">Bruno Traverson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> keywords: Hybrid System, Knowledge Representation, Reinforcement Learning, Services, Smart Home
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A smart home is realized by setting up various services. Several methods have
been proposed to create smart home services, which can be divided into
knowledge-based and data-driven approaches. However, knowledge-based approaches
usually require manual input from the inhabitant, which can be complicated if
the physical phenomena of the concerned environment states are complex, and the
inhabitant does not know how to adjust related actuators to achieve the target
values of the states monitored by services. Moreover, machine learning-based
data-driven approaches that we are interested in are like black boxes and
cannot show the inhabitant in which situations certain services proposed
certain actuators' states. To solve these problems, we propose a hybrid system
called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart
HOme system), where knowledge-based and machine learning-based data-driven
services are profitably integrated. The principal advantage is that it inherits
the explicability of knowledge-based services and the dynamism of data-driven
services. We compare HKD-SHO with several systems for creating dynamic smart
home services, and the results show the better performance of HKD-SHO.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15522" title="Abstract">arXiv:2402.15522</a> [<a href="/pdf/2402.15522" title="Download PDF">pdf</a>, <a href="/ps/2402.15522" title="Download PostScript">ps</a>, <a href="/format/2402.15522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IntSat: Integer Linear Programming by Conflict-Driven  Constraint-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nieuwenhuis%2C+R">Robert Nieuwenhuis</a>, 
<a href="/search/cs?searchtype=author&query=Oliveras%2C+A">Albert Oliveras</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez-Carbonell%2C+E">Enric Rodriguez-Carbonell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages. This is the Author's Original Manuscript of the journal version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">State-of-the-art SAT solvers are nowadays able to handle huge real-world
instances. The key to this success is the so-called Conflict-Driven
Clause-Learning (CDCL) scheme, which encompasses a number of techniques that
exploit the conflicts that are encountered during the search for a solution. In
this article we extend these techniques to Integer Linear Programming (ILP),
where variables may take general integer values instead of purely binary ones,
constraints are more expressive than just propositional clauses, and there may
be an objective function to optimise. We explain how these methods can be
implemented efficiently, and discuss possible improvements. Our work is backed
with a basic implementation that shows that, even in this far less mature
stage, our techniques are already a useful complement to the state of the art
in ILP solving.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15524" title="Abstract">arXiv:2402.15524</a> [<a href="/pdf/2402.15524" title="Download PDF">pdf</a>, <a href="/format/2402.15524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lymperopoulos%2C+P">Panagiotis Lymperopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liping Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a
common problem in infeasibility analysis of over-constrained systems. However,
because of the exponential search space of the problem, enumerating MUSes is
extremely time-consuming in real applications. In this work, we propose to
prune formulas using a learned model to speed up MUS enumeration. We represent
formulas as graphs and then develop a graph-based learning model to predict
which part of the formula should be pruned. Importantly, our algorithm does not
require data labeling by only checking the satisfiability of pruned formulas.
It does not even require training data from the target application because it
extrapolates to data with different distributions. In our experiments we
combine our algorithm with existing MUS enumerators and validate its
effectiveness in multiple benchmarks including a set of real-world problems
outside our training distribution. The experiment results show that our method
significantly accelerates MUS enumeration on average on these benchmark
problems.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15525" title="Abstract">arXiv:2402.15525</a> [<a href="/pdf/2402.15525" title="Download PDF">pdf</a>, <a href="/format/2402.15525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting misinformation through Framing Theory: the Frame Element-based  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frederick%2C+R">Rebecca Frederick</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinglong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+W">William Wong</a>, 
<a href="/search/cs?searchtype=author&query=Rupar%2C+V">Verica Rupar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weihua Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Quan Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper, we delve into the rapidly evolving challenge of misinformation
detection, with a specific focus on the nuanced manipulation of narrative
frames - an under-explored area within the AI community. The potential for
Generative AI models to generate misleading narratives underscores the urgency
of this problem. Drawing from communication and framing theories, we posit that
the presentation or 'framing' of accurate information can dramatically alter
its interpretation, potentially leading to misinformation. We highlight this
issue through real-world examples, demonstrating how shifts in narrative frames
can transmute fact-based information into misinformation. To tackle this
challenge, we propose an innovative approach leveraging the power of
pre-trained Large Language Models and deep neural networks to detect
misinformation originating from accurate facts portrayed under different
frames. These advanced AI techniques offer unprecedented capabilities in
identifying complex patterns within unstructured data critical for examining
the subtleties of narrative frames. The objective of this paper is to bridge a
significant research gap in the AI domain, providing valuable insights and
methodologies for tackling framing-induced misinformation, thus contributing to
the advancement of responsible and trustworthy AI technologies. Several
experiments are intensively conducted and experimental results explicitly
demonstrate the various impact of elements of framing theory proving the
rationale of applying framing theory to increase the performance in
misinformation detection.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15526" title="Abstract">arXiv:2402.15526</a> [<a href="/pdf/2402.15526" title="Download PDF">pdf</a>, <a href="/format/2402.15526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Specificity: An Iteratively Refining Method for Eliciting  Knowledge from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kaiwen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Li Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) exhibit remarkable generative capabilities,
enabling the generation of valuable information. Despite these advancements,
previous research found that LLMs sometimes struggle with adhering to specific
constraints (e.g., in specific place or at specific time), at times even
overlooking them, which leads to responses that are either too generic or not
fully satisfactory. Existing approaches attempted to address this issue by
decomposing or rewriting input instructions, yet they fall short in adequately
emphasizing specific constraints and in unlocking the underlying knowledge
(e.g., programming within the context of software development). In response,
this paper proposes a simple yet effective method named Chain-of-Specificity
(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the
input instructions, unlocks knowledge within LLMs, and refines responses.
Experiments conducted on publicly available and self-build complex datasets
demonstrate that CoS outperforms existing methods in enhancing generated
content especially for the specificity. Besides, as the number of specific
constraints increase, other baselines falter, while CoS still performs well.
Moreover, we show that distilling responses generated by CoS effectively
enhances the ability of smaller models to follow the constrained instructions.
Resources of this paper will be released for further research.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15527" title="Abstract">arXiv:2402.15527</a> [<a href="/pdf/2402.15527" title="Download PDF">pdf</a>, <a href="/format/2402.15527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PCA-Bench: Evaluating Multimodal Large Language Models in  Perception-Cognition-Action Chain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shuhuai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiangdi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and Data released at <a href="https://github.com/pkunlp-icler/PCA-EVAL.">this https URL</a> Leaderboard at: <a href="https://docs.qq.com/sheet/DVUd4WUpGRHRqUnNV.">this https URL</a> This article supersedes its workshop version arxiv: <a href="/abs/2310.02071">2310.02071</a>. arXiv admin note: text overlap with <a href="/abs/2310.02071">arXiv:2310.02071</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15533" title="Abstract">arXiv:2402.15533</a> [<a href="/pdf/2402.15533" title="Download PDF">pdf</a>, <a href="/format/2402.15533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetries of Service: Interdependence and Synchronicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daw%2C+A">Andrew Daw</a>, 
<a href="/search/cs?searchtype=author&query=Yom-Tov%2C+G+B">Galit B. Yom-Tov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Performance (cs.PF); Probability (math.PR)

</div>
<p class="mathjax">On many dimensions, services can be seen to exist along spectra measuring the
degree of interaction between customer and agent. For instance, every
interaction features some number of contributions by each of the two sides,
creating a spectrum of interdependence. Additionally, each interaction is
further characterized by the pacing of these contributions, implying a spectrum
of synchronicity. Where a service falls on such spectra can simply be a
consequence of its design, but it can also be a function of its state. As
broadly evidenced empirically, an agent with several concurrent interactions
will be slowed in each individual interaction, altering the service's
synchronicity. Here, we study a Hawkes cluster model of the customer-agent
interaction, which we show captures both of these service (a)symmetries. We
find insightful connections to behavioral operations, such as proving the
occurrence of non-monotonic performance (e.g., inverted-U throughput) from
concurrency-driven asynchrony. Hence, we can prescribe the agent's optimal
concurrency level. Furthermore, we show how the service design dictates the
efficacy of these operational improvements, proving that the
concurrency-optimized throughput is itself non-monotonic as a function of the
interdependence. In what may be of independent interest methodologically, we
establish an interpretable decomposition for Hawkes clusters via probabilistic
combinatorics.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15537" title="Abstract">arXiv:2402.15537</a> [<a href="/pdf/2402.15537" title="Download PDF">pdf</a>, <a href="/format/2402.15537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Performance of ChatGPT for Spam Email Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shijing Si</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yugui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiawen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wosik%2C+J">Jedrek Wosik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report and analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Email continues to be a pivotal and extensively utilized communication medium
within professional and commercial domains. Nonetheless, the prevalence of spam
emails poses a significant challenge for users, disrupting their daily routines
and diminishing productivity. Consequently, accurately identifying and
filtering spam based on content has become crucial for cybersecurity. Recent
advancements in natural language processing, particularly with large language
models like ChatGPT, have shown remarkable performance in tasks such as
question answering and text generation. However, its potential in spam
identification remains underexplored. To fill in the gap, this study attempts
to evaluate ChatGPT's capabilities for spam identification in both English and
Chinese email datasets. We employ ChatGPT for spam email detection using
in-context learning, which requires a prompt instruction and a few
demonstrations. We also investigate how the training example size affects the
performance of ChatGPT. For comparison, we also implement five popular
benchmark methods, including naive Bayes, support vector machines (SVM),
logistic regression (LR), feedforward dense neural networks (DNN), and BERT
classifiers. Though extensive experiments, the performance of ChatGPT is
significantly worse than deep supervised learning methods in the large English
dataset, while it presents superior performance on the low-resourced Chinese
dataset, even outperforming BERT in this case.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15538" title="Abstract">arXiv:2402.15538</a> [<a href="/pdf/2402.15538" title="Download PDF">pdf</a>, <a href="/format/2402.15538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentLite: A Lightweight Library for Building and Advancing  Task-Oriented LLM Agent System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weiran Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Juntao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Choubey%2C+P+K">Prafulla K. Choubey</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jason Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+S">Shelby Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint. Library is available at <a href="https://github.com/SalesforceAIResearch/AgentLite">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The booming success of LLMs initiates rapid development in LLM agents. Though
the foundation of an LLM agent is the generative model, it is critical to
devise the optimal reasoning strategies and agent architectures. Accordingly,
LLM agent research advances from the simple chain-of-thought prompting to more
complex ReAct and Reflection reasoning strategy; agent architecture also
evolves from single agent generation to multi-agent conversation, as well as
multi-LLM multi-agent group chat. However, with the existing intricate
frameworks and libraries, creating and evaluating new reasoning strategies and
agent architectures has become a complex challenge, which hinders research
investigation into LLM agents. Thus, we open-source a new AI agent library,
AgentLite, which simplifies this process by offering a lightweight,
user-friendly platform for innovating LLM agent reasoning, architectures, and
applications with ease. AgentLite is a task-oriented framework designed to
enhance the ability of agents to break down tasks and facilitate the
development of multi-agent systems. Furthermore, we introduce multiple
practical applications developed with AgentLite to demonstrate its convenience
and flexibility. Get started now at:
\url{https://github.com/SalesforceAIResearch/AgentLite}.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15542" title="Abstract">arXiv:2402.15542</a> [<a href="/pdf/2402.15542" title="Download PDF">pdf</a>, <a href="/format/2402.15542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine  Learning Use Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbst%2C+S">Sabrina Herbst</a>, 
<a href="/search/cs?searchtype=author&query=De+Maio%2C+V">Vincenzo De Maio</a>, 
<a href="/search/cs?searchtype=author&query=Brandic%2C+I">Ivona Brandic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the advent of the Post-Moore era, the scientific community is faced with
the challenge of addressing the demands of current data-intensive machine
learning applications, which are the cornerstone of urgent analytics in
distributed computing. Quantum machine learning could be a solution for the
increasing demand of urgent analytics, providing potential theoretical speedups
and increased space efficiency. However, challenges such as (1) the encoding of
data from the classical to the quantum domain, (2) hyperparameter tuning, and
(3) the integration of quantum hardware into a distributed computing continuum
limit the adoption of quantum machine learning for urgent analytics. In this
work, we investigate the use of Edge computing for the integration of quantum
machine learning into a distributed computing continuum, identifying the main
challenges and possible solutions. Furthermore, exploring the data encoding and
hyperparameter tuning challenges, we present preliminary results for quantum
machine learning analytics on an IoT scenario.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15546" title="Abstract">arXiv:2402.15546</a> [<a href="/pdf/2402.15546" title="Download PDF">pdf</a>, <a href="/format/2402.15546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent  Pathfinding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huijie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Berto%2C+F">Federico Berto</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zihan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+C">Chuanbo Hua</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kyuree Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Extended Abstract in Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Large-scale multi-agent pathfinding (MAPF) presents significant challenges in
several areas. As systems grow in complexity with a multitude of autonomous
agents operating simultaneously, efficient and collision-free coordination
becomes paramount. Traditional algorithms often fall short in scalability,
especially in intricate scenarios. Reinforcement Learning (RL) has shown
potential to address the intricacies of MAPF; however, it has also been shown
to struggle with scalability, demanding intricate implementation, lengthy
training, and often exhibiting unstable convergence, limiting its practical
application. In this paper, we introduce Heuristics-Informed Multi-Agent
Pathfinding (HiMAP), a novel scalable approach that employs imitation learning
with heuristic guidance in a decentralized manner. We train on small-scale
instances using a heuristic policy as a teacher that maps each single agent
observation information to an action probability distribution. During
pathfinding, we adopt several inference techniques to improve performance. With
a simple training scheme and implementation, HiMAP demonstrates competitive
results in terms of success rate and scalability in the field of
imitation-learning-only MAPF, showing the potential of imitation-learning-only
MAPF equipped with inference techniques.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15552" title="Abstract">arXiv:2402.15552</a> [<a href="/pdf/2402.15552" title="Download PDF">pdf</a>, <a href="/format/2402.15552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morphological Symmetries in Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ordo%C3%B1ez-Apraez%2C+D">Daniel Ordo&#xf1;ez-Apraez</a>, 
<a href="/search/cs?searchtype=author&query=Turrisi%2C+G">Giulio Turrisi</a>, 
<a href="/search/cs?searchtype=author&query=Kostic%2C+V">Vladimir Kostic</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+M">Mario Martin</a>, 
<a href="/search/cs?searchtype=author&query=Agudo%2C+A">Antonio Agudo</a>, 
<a href="/search/cs?searchtype=author&query=Moreno-Noguer%2C+F">Francesc Moreno-Noguer</a>, 
<a href="/search/cs?searchtype=author&query=Pontil%2C+M">Massimiliano Pontil</a>, 
<a href="/search/cs?searchtype=author&query=Semini%2C+C">Claudio Semini</a>, 
<a href="/search/cs?searchtype=author&query=Mastalli%2C+C">Carlos Mastalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We present a comprehensive framework for studying and leveraging
morphological symmetries in robotic systems. These are intrinsic properties of
the robot's morphology, frequently observed in animal biology and robotics,
which stem from the replication of kinematic structures and the symmetrical
distribution of mass. We illustrate how these symmetries extend to the robot's
state space and both proprioceptive and exteroceptive sensor measurements,
resulting in the equivariance of the robot's equations of motion and optimal
control policies. Thus, we recognize morphological symmetries as a relevant and
previously unexplored physics-informed geometric prior, with significant
implications for both data-driven and analytical methods used in modeling,
control, estimation and design in robotics. For data-driven methods, we
demonstrate that morphological symmetries can enhance the sample efficiency and
generalization of machine learning models through data augmentation, or by
applying equivariant/invariant constraints on the model's architecture. In the
context of analytical methods, we employ abstract harmonic analysis to
decompose the robot's dynamics into a superposition of lower-dimensional,
independent dynamics. We substantiate our claims with both synthetic and
real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we
introduce the repository MorphoSymm to facilitate the practical use of the
theory and applications outlined in this work.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15554" title="Abstract">arXiv:2402.15554</a> [<a href="/pdf/2402.15554" title="Download PDF">pdf</a>, <a href="/ps/2402.15554" title="Download PostScript">ps</a>, <a href="/format/2402.15554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The LC Method: A parallelizable numerical method for approximating the  roots of single-variable polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alba-Cuellar%2C+D">Daniel Alba-Cuellar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PDF document with 270 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Complex Variables (math.CV)

</div>
<p class="mathjax">The LC method described in this work seeks to approximate the roots of
polynomial equations in one variable. This book allows you to explore the LC
method, which uses geometric structures of Lines L and Circumferences C in the
plane of complex numbers, based on polynomial coefficients. These structures
depend on the inclination angle of a line with fixed point that seeks to
contain one of the roots; they are associated with an error measure that
indicates the degree of proximity to that root, without knowing a priori its
location.
<br />Using a computer with parallel processing capabilities, it is feasible to
construct several of these geometric structures at the same time, varying the
inclination angle of the lines with fixed point, in order to obtain an error
measure map, with which it is possible to identify, approximately, the location
of all polynomial roots.
<br />To show how the LC method works, this book includes numerical examples for
quadratic, cubic, and quartic polynomials, and also for polynomials of degree
greater than or equal to 5; this book also includes R programs that allow you
to reproduce the results of the examples on a typical personal computer; these
R programs use vectorization of operations instead of loops, which can be seen
as a basic and accessible form of parallel processing.
<br />This book, in the end, invites us to explore beyond the basic ideas and
concepts described here, motivating the development of a more efficient and
complete computational implementation of the LC method.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15555" title="Abstract">arXiv:2402.15555</a> [<a href="/pdf/2402.15555" title="Download PDF">pdf</a>, <a href="/format/2402.15555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Networks Always Grok and Here is Why
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Humayun%2C+A+I">Ahmed Imtiaz Humayun</a>, 
<a href="/search/cs?searchtype=author&query=Balestriero%2C+R">Randall Balestriero</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R">Richard Baraniuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://bit.ly/grok-adversarial.">this https URL</a> Pages 20, Figures 28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Grokking, or delayed generalization, is a phenomenon where generalization in
a deep neural network (DNN) occurs long after achieving near zero training
error. Previous studies have reported the occurrence of grokking in specific
controlled settings, such as DNNs initialized with large-norm parameters or
transformers trained on algorithmic datasets. We demonstrate that grokking is
actually much more widespread and materializes in a wide range of practical
settings, such as training of a convolutional neural network (CNN) on CIFAR10
or a Resnet on Imagenette. We introduce the new concept of delayed robustness,
whereby a DNN groks adversarial examples and becomes robust, long after
interpolation and/or generalization. We develop an analytical explanation for
the emergence of both delayed generalization and delayed robustness based on a
new measure of the local complexity of a DNN's input-output mapping. Our local
complexity measures the density of the so-called 'linear regions' (aka, spline
partition regions) that tile the DNN input space, and serves as a utile
progress measure for training. We provide the first evidence that for
classification problems, the linear regions undergo a phase transition during
training whereafter they migrate away from the training samples (making the DNN
mapping smoother there) and towards the decision boundary (making the DNN
mapping less smooth there). Grokking occurs post phase transition as a robust
partition of the input space emerges thanks to the linearization of the DNN
mapping around the training points. Website: https://bit.ly/grok-adversarial
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15561" title="Abstract">arXiv:2402.15561</a> [<a href="/pdf/2402.15561" title="Download PDF">pdf</a>, <a href="/format/2402.15561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Multivariate Adaptive Regression Splines for Ensuring Equity and  Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghighat%2C+P">Parian Haghighat</a>, 
<a href="/search/cs?searchtype=author&query=G%27andara%2C+D">Denisa G&#x27;andara</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+L">Lulu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Anahideh%2C+H">Hadis Anahideh</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 38th Annual AAAI Conference on Artificial Intelligence, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Predictive analytics is widely used in various domains, including education,
to inform decision-making and improve outcomes. However, many predictive models
are proprietary and inaccessible for evaluation or modification by researchers
and practitioners, limiting their accountability and ethical design. Moreover,
predictive models are often opaque and incomprehensible to the officials who
use them, reducing their trust and utility. Furthermore, predictive models may
introduce or exacerbate bias and inequity, as they have done in many sectors of
society. Therefore, there is a need for transparent, interpretable, and fair
predictive models that can be easily adopted and adapted by different
stakeholders. In this paper, we propose a fair predictive model based on
multivariate adaptive regression splines(MARS) that incorporates fairness
measures in the learning process. MARS is a non-parametric regression model
that performs feature selection, handles non-linear relationships, generates
interpretable decision rules, and derives optimal splitting criteria on the
variables. Specifically, we integrate fairness into the knot optimization
algorithm and provide theoretical and empirical evidence of how it results in a
fair knot placement. We apply our fairMARS model to real-world data and
demonstrate its effectiveness in terms of accuracy and equity. Our paper
contributes to the advancement of responsible and ethical predictive analytics
for social good.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15567" title="Abstract">arXiv:2402.15567</a> [<a href="/pdf/2402.15567" title="Download PDF">pdf</a>, <a href="/format/2402.15567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Policies with Hilbert Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seohong Park</a>, 
<a href="/search/cs?searchtype=author&query=Kreiman%2C+T">Tobias Kreiman</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Unsupervised and self-supervised objectives, such as next token prediction,
have enabled pre-training generalist models from large amounts of unlabeled
data. In reinforcement learning (RL), however, finding a truly general and
scalable unsupervised pre-training objective for generalist policies from
offline data remains a major open question. While a number of methods have been
proposed to enable generic self-supervised RL, based on principles such as
goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such
methods remain limited in terms of either the diversity of the discovered
behaviors, the need for high-quality demonstration data, or the lack of a clear
prompting or adaptation mechanism for downstream tasks. In this work, we
propose a novel unsupervised framework to pre-train generalist policies that
capture diverse, optimal, long-horizon behaviors from unlabeled offline data
such that they can be quickly adapted to any arbitrary new tasks in a zero-shot
manner. Our key insight is to learn a structured representation that preserves
the temporal structure of the underlying environment, and then to span this
learned latent space with directional movements, which enables various
zero-shot policy "prompting" schemes for downstream tasks. Through our
experiments on simulated robotic locomotion and manipulation benchmarks, we
show that our unsupervised policies can solve goal-conditioned and general RL
tasks in a zero-shot fashion, even often outperforming prior methods designed
specifically for each setting. Our code and videos are available at
https://seohong.me/projects/hilp/
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15570" title="Abstract">arXiv:2402.15570</a> [<a href="/pdf/2402.15570" title="Download PDF">pdf</a>, <a href="/format/2402.15570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Adversarial Attacks on Language Models In One GPU Minute
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadasivan%2C+V+S">Vinu Sankar Sadasivan</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Shoumik Saha</a>, 
<a href="/search/cs?searchtype=author&query=Sriramanan%2C+G">Gaurang Sriramanan</a>, 
<a href="/search/cs?searchtype=author&query=Kattakinda%2C+P">Priyatham Kattakinda</a>, 
<a href="/search/cs?searchtype=author&query=Chegini%2C+A">Atoosa Chegini</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we introduce a novel class of fast, beam search-based
adversarial attack (BEAST) for Language Models (LMs). BEAST employs
interpretable parameters, enabling attackers to balance between attack speed,
success rate, and the readability of adversarial prompts. The computational
efficiency of BEAST facilitates us to investigate its applications on LMs for
jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free
targeted attack can jailbreak aligned LMs with high attack success rates within
one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute
with a success rate of 89% when compared to a gradient-based baseline that
takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000
48GB GPU. Additionally, we discover a unique outcome wherein our untargeted
attack induces hallucinations in LM chatbots. Through human evaluations, we
find that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more
incorrect outputs when compared to LM outputs in the absence of our attack. We
also learn that 22% of the time, BEAST causes Vicuna to generate outputs that
are not relevant to the original prompt. Further, we use BEAST to generate
adversarial prompts in a few seconds that can boost the performance of existing
membership inference attacks for LMs. We believe that our fast attack, BEAST,
has the potential to accelerate research in LM security and privacy. Our
codebase is publicly available at https://github.com/vinusankars/BEAST.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15571" title="Abstract">arXiv:2402.15571</a> [<a href="/pdf/2402.15571" title="Download PDF">pdf</a>, <a href="/format/2402.15571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Convos: Capturing Agendas and Emotions on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaumik%2C+A">Ankita Bhaumik</a>, 
<a href="/search/cs?searchtype=author&query=Sa%2C+N">Ning Sa</a>, 
<a href="/search/cs?searchtype=author&query=Katsios%2C+G">Gregorios Katsios</a>, 
<a href="/search/cs?searchtype=author&query=Strzalkowski%2C+T">Tomek Strzalkowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Social media platforms are popular tools for disseminating targeted
information during major public events like elections or pandemics. Systematic
analysis of the message traffic can provide valuable insights into prevailing
opinions and social dynamics among different segments of the population. We are
specifically interested in influence spread, and in particular whether more
deliberate influence operations can be detected. However, filtering out the
essential messages with telltale influence indicators from the extensive and
often chaotic social media traffic is a major challenge. In this paper we
present a novel approach to extract influence indicators from messages
circulating among groups of users discussing particular topics. We build upon
the concept of a convo to identify influential authors who are actively
promoting some particular agenda around that topic within the group. We focus
on two influence indicators: the (control of) agenda and the use of emotional
language.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15572" title="Abstract">arXiv:2402.15572</a> [<a href="/pdf/2402.15572" title="Download PDF">pdf</a>, <a href="/format/2402.15572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Explainable Object-induced Model through Uncertainty for  Automated Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">Shihong Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yue Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaowei Jia</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Na Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 2024 ACM / IEEE International Conference on Human-Robot Interaction (HRI '24), March 11--14, 2024, Boulder, CO, USA. ACM, New York, NY, USA, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">The rapid evolution of automated vehicles (AVs) has the potential to provide
safer, more efficient, and comfortable travel options. However, these systems
face challenges regarding reliability in complex driving scenarios. Recent
explainable AV architectures neglect crucial information related to inherent
uncertainties while providing explanations for actions. To overcome such
challenges, our study builds upon the "object-induced" model approach that
prioritizes the role of objects in scenes for decision-making and integrates
uncertainty assessment into the decision-making process using an evidential
deep learning paradigm with a Beta prior. Additionally, we explore several
advanced training strategies guided by uncertainty, including
uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA
dataset, our findings underscore that the model, through these enhancements,
not only offers a clearer comprehension of AV decisions and their underlying
reasoning but also surpasses existing baselines across a broad range of
scenarios.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15578" title="Abstract">arXiv:2402.15578</a> [<a href="/pdf/2402.15578" title="Download PDF">pdf</a>, <a href="/format/2402.15578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Pre-Training for Table Structure Recognition Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">ShengYun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramaniyan%2C+R">Rajarajeswari Balasubramaniyan</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI'24 Workshop on Scientific Document Understanding Oral. arXiv admin note: text overlap with <a href="/abs/2311.05565">arXiv:2311.05565</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Table structure recognition (TSR) aims to convert tabular images into a
machine-readable format. Although hybrid convolutional neural network
(CNN)-transformer architecture is widely used in existing approaches, linear
projection transformer has outperformed the hybrid architecture in numerous
vision tasks due to its simplicity and efficiency. However, existing research
has demonstrated that a direct replacement of CNN backbone with linear
projection leads to a marked performance drop. In this work, we resolve the
issue by proposing a self-supervised pre-training (SSP) method for TSR
transformers. We discover that the performance gap between the linear
projection transformer and the hybrid CNN-transformer can be mitigated by SSP
of the visual encoder in the TSR model. We conducted reproducible ablation
studies and open-sourced our code at https://github.com/poloclub/unitable to
enhance transparency, inspire innovations, and facilitate fair comparisons in
our domain as tables are a promising modality for representation learning.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15579" title="Abstract">arXiv:2402.15579</a> [<a href="/pdf/2402.15579" title="Download PDF">pdf</a>, <a href="/format/2402.15579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CI w/o TN: Context Injection without Task Name for Procedure Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinjie Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper explores the challenge of procedure planning in instructional
videos, which involves creating goal-directed plans based on visual start and
goal observations from videos. Previous research has tackled this problem with
gradually weaker training supervision, from heavy intermediate visual
observations or language instructions to task class supervision. However, with
the advent of large language models, even given only the task name, these
models can produce a detailed plan. In this study, we propose a much weaker
setting without task name as supervision, which is not currently solvable by
existing large language models since they require good prompts with sufficient
information. Specifically, we hypothesize that previous intermediate
supervisions can serve as context information, and we use captions of visual
start and goal observations as a much cheaper form of supervision. This
approach greatly reduces the labeling cost since the captions can be easily
obtained by large pre-trained vision-language foundation models. Technically,
we apply BLIP to generate captions as supervision to train the context feature
with contrastive learning loss. Afterward, the context feature is fed into the
generator to aid in plan generation. Our experiments on two datasets with
varying scales demonstrate that our model can achieve comparable performance on
multiple metrics, which validates our hypothesis.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15580" title="Abstract">arXiv:2402.15580</a> [<a href="/pdf/2402.15580" title="Download PDF">pdf</a>, <a href="/format/2402.15580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CharacterMixer: Rig-Aware Interpolation of 3D Characters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xiao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+R">Rao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ritchie%2C+D">Daniel Ritchie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">We present CharacterMixer, a system for blending two rigged 3D characters
with different mesh and skeleton topologies while maintaining a rig throughout
interpolation. CharacterMixer also enables interpolation during motion for such
characters, a novel feature. Interpolation is an important shape editing
operation, but prior methods have limitations when applied to rigged
characters: they either ignore the rig (making interpolated characters no
longer posable) or use a fixed rig and mesh topology. To handle different mesh
topologies, CharacterMixer uses a signed distance field (SDF) representation of
character shapes, with one SDF per bone. To handle different skeleton
topologies, it computes a hierarchical correspondence between source and target
character skeletons and interpolates the SDFs of corresponding bones. This
correspondence also allows the creation of a single "unified skeleton" for
posing and animating interpolated characters. We show that CharacterMixer
produces qualitatively better interpolation results than two state-of-the-art
methods while preserving a rig throughout interpolation.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15583" title="Abstract">arXiv:2402.15583</a> [<a href="/pdf/2402.15583" title="Download PDF">pdf</a>, <a href="/format/2402.15583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation  Learning of Vision-based Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yichen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongge Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+G+P">Gregory P. Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+J">Yong Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wolff%2C+E+M">Eric M. Wolff</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yuning Chai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the lack of depth cues in images, multi-frame inputs are important for
the success of vision-based perception, prediction, and planning in autonomous
driving. Observations from different angles enable the recovery of 3D object
states from 2D image inputs if we can identify the same instance in different
input frames. However, the dynamic nature of autonomous driving scenes leads to
significant changes in the appearance and shape of each instance captured by
the camera at different time steps. To this end, we propose a novel contrastive
learning algorithm, Cohere3D, to learn coherent instance representations in a
long-term input sequence robust to the change in distance and perspective. The
learned representation aids in instance-level correspondence across multiple
input frames in downstream tasks. In the pretraining stage, the raw point
clouds from LiDAR sensors are utilized to construct the long-term temporal
correspondence for each instance, which serves as guidance for the extraction
of instance-level representation from the vision-based bird's eye-view (BEV)
feature map. Cohere3D encourages a consistent representation for the same
instance at different frames but distinguishes between representations of
different instances. We evaluate our algorithm by finetuning the pretrained
model on various downstream perception, prediction, and planning tasks. Results
show a notable improvement in both data efficiency and task performance.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15584" title="Abstract">arXiv:2402.15584</a> [<a href="/pdf/2402.15584" title="Download PDF">pdf</a>, <a href="/format/2402.15584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Space Models for Event Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zubi%C4%87%2C+N">Nikola Zubi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Gehrig%2C+M">Mathias Gehrig</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Today, state-of-the-art deep neural networks that process event-camera data
first convert a temporal window of events into dense, grid-like input
representations. As such, they exhibit poor generalizability when deployed at
higher inference frequencies (i.e., smaller temporal windows) than the ones
they were trained on. We address this challenge by introducing state-space
models (SSMs) with learnable timescale parameters to event-based vision. This
design adapts to varying frequencies without the need to retrain the network at
different frequencies. Additionally, we investigate two strategies to
counteract aliasing effects when deploying the model at higher frequencies. We
comprehensively evaluate our approach against existing methods based on RNN and
Transformer architectures across various benchmarks, including Gen1 and 1 Mpx
event camera datasets. Our results demonstrate that SSM-based models train 33%
faster and also exhibit minimal performance degradation when tested at higher
frequencies than the training input. Traditional RNN and Transformer models
exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31
mAP, highlighting the effectiveness of SSMs in event-based vision tasks.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15586" title="Abstract">arXiv:2402.15586</a> [<a href="/pdf/2402.15586" title="Download PDF">pdf</a>, <a href="/format/2402.15586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Adversarial Robustness Using Heterogeneous Teachers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jieren Deng</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+A">Aaron Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+R">Rigel Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Rathbun%2C+E">Ethan Rathbun</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+J">Jinbo Bi</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+K">Kaleel Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Aguiar%2C+D">Derek Aguiar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Achieving resiliency against adversarial attacks is necessary prior to
deploying neural network classifiers in domains where misclassification incurs
substantial costs, e.g., self-driving cars or medical imaging. Recent work has
demonstrated that robustness can be transferred from an adversarially trained
teacher to a student model using knowledge distillation. However, current
methods perform distillation using a single adversarial and vanilla teacher and
consider homogeneous architectures (i.e., residual networks) that are
susceptible to misclassify examples from similar adversarial subspaces. In this
work, we develop a defense framework against adversarial attacks by distilling
adversarial robustness using heterogeneous teachers (DARHT). In DARHT, the
student model explicitly represents teacher logits in a student-teacher feature
map and leverages multiple teachers that exhibit low adversarial example
transferability (i.e., exhibit high performance on dissimilar adversarial
examples). Experiments on classification tasks in both white-box and black-box
scenarios demonstrate that DARHT achieves state-of-the-art clean and robust
accuracies when compared to competing adversarial training and distillation
methods in the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Comparisons
with homogeneous and heterogeneous teacher sets suggest that leveraging
teachers with low adversarial example transferability increases student model
robustness.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15587" title="Abstract">arXiv:2402.15587</a> [<a href="/pdf/2402.15587" title="Download PDF">pdf</a>, <a href="/ps/2402.15587" title="Download PostScript">ps</a>, <a href="/format/2402.15587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study of Shape Modeling Against Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Cheng Long</a>, 
<a href="/search/cs?searchtype=author&query=Barbu%2C+A">Adrian Barbu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures, International Conference on Image Processing (ICIP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Shape modeling is a challenging task with many potential applications in
computer vision and medical imaging. There are many shape modeling methods in
the literature, each with its advantages and applications. However, many shape
modeling methods have difficulties handling shapes that have missing pieces or
outliers. In this regard, this paper introduces shape denoising, a fundamental
problem in shape modeling that lies at the core of many computer vision and
medical imaging applications and has not received enough attention in the
literature. The paper introduces six types of noise that can be used to perturb
shapes as well as an objective measure for the noise level and for comparing
methods on their shape denoising capabilities. Finally, the paper evaluates
seven methods capable of accomplishing this task, of which six are based on
deep learning, including some generative models.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15589" title="Abstract">arXiv:2402.15589</a> [<a href="/pdf/2402.15589" title="Download PDF">pdf</a>, <a href="/format/2402.15589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives  of Scholarly Manuscripts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santu%2C+S+K+K">Shubhra Kanti Karmaker Santu</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S+K">Sanjeev Kumar Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+N">Naman Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Knipper%2C+A">Alex Knipper</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Souvika Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Salvador%2C+J">John Salvador</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+Y">Yash Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Guttikonda%2C+S">Sri Guttikonda</a>, 
<a href="/search/cs?searchtype=author&query=Akter%2C+M">Mousumi Akter</a>, 
<a href="/search/cs?searchtype=author&query=Freestone%2C+M">Matthew Freestone</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M+C">Matthew C. Williams Jr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">One of the most important yet onerous tasks in the academic peer-reviewing
process is composing meta-reviews, which involves understanding the core
contributions, strengths, and weaknesses of a scholarly manuscript based on
peer-review narratives from multiple experts and then summarizing those
multiple experts' perspectives into a concise holistic overview. Given the
latest major developments in generative AI, especially Large Language Models
(LLMs), it is very compelling to rigorously study the utility of LLMs in
generating such meta-reviews in an academic peer-review setting. In this paper,
we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and
PaLM2, to automatically generate meta-reviews by prompting them with different
types/levels of prompts based on the recently proposed TELeR taxonomy. Finally,
we perform a detailed qualitative study of the meta-reviews generated by the
LLMs and summarize our findings and recommendations for prompting LLMs for this
complex task.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15591" title="Abstract">arXiv:2402.15591</a> [<a href="/pdf/2402.15591" title="Download PDF">pdf</a>, <a href="/format/2402.15591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecWizard: A Toolkit for Conversational Recommendation with Modular,  Portable Models and Interactive User Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Laud%2C+T">Tanmay Laud</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihang He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinshuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhouhang Xie</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhankui He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI'24 Demo Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a new Python toolkit called RecWizard for Conversational
Recommender Systems (CRS). RecWizard offers support for development of models
and interactive user interface, drawing from the best practices of the
Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive
and Large Language Models (LLMs)-friendly, to streamline the learning process
and reduce the additional effort for CRS research. For more comprehensive
information about RecWizard, please check our GitHub
https://github.com/McAuley-Lab/RecWizard.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15594" title="Abstract">arXiv:2402.15594</a> [<a href="/pdf/2402.15594" title="Download PDF">pdf</a>, <a href="/format/2402.15594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model  Improves End-to-End ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jintao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yingbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zeineldeen%2C+M">Mohammad Zeineldeen</a>, 
<a href="/search/cs?searchtype=author&query=Tuske%2C+Z">Zoltan Tuske</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, alternating weak triphone/BPE alignment supervision is
proposed to improve end-to-end model training. Towards this end, triphone and
BPE alignments are extracted using a pre-existing hybrid ASR system. Then,
regularization effect is obtained by cross-entropy based intermediate auxiliary
losses computed on such alignments at a mid-layer representation of the encoder
for triphone alignments and at the encoder for BPE alignments. Weak supervision
is achieved through strong label smoothing with parameter of 0.5. Experimental
results on TED-LIUM 2 indicate that either triphone or BPE alignment based weak
supervision improves ASR performance over standard CTC auxiliary loss.
Moreover, their combination lowers the word error rate further. We also
investigate the alternation of the two auxiliary tasks during model training,
and additional performance gain is observed. Overall, the proposed techniques
result in over 10% relative error rate reduction over a CTC-regularized
baseline system.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15598" title="Abstract">arXiv:2402.15598</a> [<a href="/pdf/2402.15598" title="Download PDF">pdf</a>, <a href="/format/2402.15598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSet SimCLR: Self-supervised deep sets for improved pathology  representation learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torpey%2C+D">David Torpey</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Richard Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Often, applications of self-supervised learning to 3D medical data opt to use
3D variants of successful 2D network architectures. Although promising
approaches, they are significantly more computationally demanding to train, and
thus reduce the widespread applicability of these methods away from those with
modest computational resources. Thus, in this paper, we aim to improve standard
2D SSL algorithms by modelling the inherent 3D nature of these datasets
implicitly. We propose two variants that build upon a strong baseline model and
show that both of these variants often outperform the baseline in a variety of
downstream tasks. Importantly, in contrast to previous works in both 2D and 3D
approaches for 3D medical data, both of our proposals introduce negligible
additional overhead over the baseline, improving the democratisation of these
approaches for medical applications.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15601" title="Abstract">arXiv:2402.15601</a> [<a href="/pdf/2402.15601" title="Download PDF">pdf</a>, <a href="/format/2402.15601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Bounds for Compositions of Piecewise Affine Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Glunt%2C+J+J">Jonah J. Glunt</a>, 
<a href="/search/eess?searchtype=author&query=Siefert%2C+J+A">Jacob A. Siefert</a>, 
<a href="/search/eess?searchtype=author&query=Thompson%2C+A+F">Andrew F. Thompson</a>, 
<a href="/search/eess?searchtype=author&query=Pangborn%2C+H+C">Herschel C. Pangborn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Nonlinear expressions are often approximated by piecewise affine (PWA)
functions to simplify analysis or reduce computational costs. To reduce
computational complexity, multivariate functions can be represented as
compositions of functions with one or two inputs, which can be approximated
individually. This paper provides efficient methods to generate PWA
approximations of nonlinear functions via functional decomposition. The key
contributions focus on intelligent placement of breakpoints for PWA
approximations without requiring optimization, and on bounding the error of PWA
compositions as a function of the error tolerance for each component of that
composition. The proposed methods are used to systematically construct a PWA
approximation for a complicated function, either to within a desired error
tolerance or to a given level of complexity.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15603" title="Abstract">arXiv:2402.15603</a> [<a href="/pdf/2402.15603" title="Download PDF">pdf</a>, <a href="/ps/2402.15603" title="Download PostScript">ps</a>, <a href="/format/2402.15603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Fair Binary Classifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghoukasian%2C+H">Hrad Ghoukasian</a>, 
<a href="/search/cs?searchtype=author&query=Asoodeh%2C+S">Shahab Asoodeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we investigate binary classification under the constraints of
both differential privacy and fairness. We first propose an algorithm based on
the decoupling technique for learning a classifier with only fairness
guarantee. This algorithm takes in classifiers trained on different demographic
groups and generates a single classifier satisfying statistical parity. We then
refine this algorithm to incorporate differential privacy. The performance of
the final algorithm is rigorously examined in terms of privacy, fairness, and
utility guarantees. Empirical evaluations conducted on the Adult and Credit
Card datasets illustrate that our algorithm outperforms the state-of-the-art in
terms of fairness guarantees, while maintaining the same level of privacy and
utility.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15604" title="Abstract">arXiv:2402.15604</a> [<a href="/pdf/2402.15604" title="Download PDF">pdf</a>, <a href="/format/2402.15604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal-Reaching Trajectory Design Near Danger with Piecewise Affine  Reach-avoid Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+L+K">Long Kiu Chung</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonsuhk Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chuizheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Kousik%2C+S">Shreyas Kousik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to the work. This work has been submitted for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Autonomous mobile robots must maintain safety, but should not sacrifice
performance, leading to the classical reach-avoid problem. This paper seeks to
compute trajectory plans for which a robot is guaranteed to reach a goal and
avoid obstacles in the specific near-danger case that the obstacles and goal
are near each other. The proposed method builds off of a common approach of
using a simplified planning model to generate plans, which are then tracked
using a high-fidelity tracking model and controller. Existing safe planning
approaches use reachability analysis to overapproximate the error between these
models, but this introduces additional numerical approximation error and
thereby conservativeness that prevents goal-reaching. The present work instead
proposes a Piecewise Affine Reach-avoid Computation (PARC) method to tightly
approximate the reachable set of the planning model. With PARC, the main source
of conservativeness is the model mismatch, which can be mitigated by careful
controller and planning model design. The utility of this method is
demonstrated through extensive numerical experiments in which PARC outperforms
state-of-the-art reach-avoid methods in near-danger goal-reaching. Furthermore,
in a simulated demonstration, PARC enables the generation of provably-safe
extreme vehicle dynamics drift parking maneuvers.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15605" title="Abstract">arXiv:2402.15605</a> [<a href="/pdf/2402.15605" title="Download PDF">pdf</a>, <a href="/format/2402.15605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do You See What I See? A Qualitative Study Eliciting High-Level  Visualization Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quadri%2C+G+J">Ghulam Jilani Quadri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A+Z">Arran Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhehao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Adorno%2C+J">Jennifer Adorno</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Szafir%2C+D+A">Danielle Albers Szafir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in ACM CHI 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Designers often create visualizations to achieve specific high-level
analytical or communication goals. These goals require people to naturally
extract complex, contextualized, and interconnected patterns in data. While
limited prior work has studied general high-level interpretation, prevailing
perceptual studies of visualization effectiveness primarily focus on isolated,
predefined, low-level tasks, such as estimating statistical quantities. This
study more holistically explores visualization interpretation to examine the
alignment between designers' communicative goals and what their audience sees
in a visualization, which we refer to as their comprehension. We found that
statistics people effectively estimate from visualizations in classical
graphical perception studies may differ from the patterns people intuitively
comprehend in a visualization. We conducted a qualitative study on three types
of visualizations -- line graphs, bar graphs, and scatterplots -- to
investigate the high-level patterns people naturally draw from a visualization.
Participants described a series of graphs using natural language and
think-aloud protocols. We found that comprehension varies with a range of
factors, including graph complexity and data distribution. Specifically, 1) a
visualization's stated objective often does not align with people's
comprehension, 2) results from traditional experiments may not predict the
knowledge people build with a graph, and 3) chart type alone is insufficient to
predict the information people extract from a graph. Our study confirms the
importance of defining visualization effectiveness from multiple perspectives
to assess and inform visualization practices.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15607" title="Abstract">arXiv:2402.15607</a> [<a href="/pdf/2402.15607" title="Download PDF">pdf</a>, <a href="/format/2402.15607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Nonlinear Transformers for Efficient In-Context Learning: A  Theoretical Learning and Generalization Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongkang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Songtao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xiaodong Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformer-based large language models have displayed impressive in-context
learning capabilities, where a pre-trained model can handle new tasks without
fine-tuning by simply augmenting the query with some input-output examples from
that task. Despite the empirical success, the mechanics of how to train a
Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive
due to the technical challenges of analyzing the nonconvex training problems
resulting from the nonlinear self-attention and nonlinear activation in
Transformers. To the best of our knowledge, this paper provides the first
theoretical analysis of the training dynamics of Transformers with nonlinear
self-attention and nonlinear MLP, together with the ICL generalization
capability of the resulting model. Focusing on a group of binary classification
tasks, we train Transformers using data from a subset of these tasks and
quantify the impact of various factors on the ICL generalization performance on
the remaining unseen tasks with and without data distribution shifts. We also
analyze how different components in the learned Transformers contribute to the
ICL performance. Furthermore, we provide the first theoretical analysis of how
model pruning affects the ICL performance and prove that proper magnitude-based
pruning can have a minimal impact on ICL while reducing inference costs. These
theoretical findings are justified through numerical experiments.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15608" title="Abstract">arXiv:2402.15608</a> [<a href="/pdf/2402.15608" title="Download PDF">pdf</a>, <a href="/ps/2402.15608" title="Download PostScript">ps</a>, <a href="/format/2402.15608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-Based Completions Sequencing for Well Performance  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J+W">Jinglang W. Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+A">Anh Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Mabadeje%2C+A+O">Ademide O. Mabadeje</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Mejia%2C+J+L">Jose L. Hernandez-Mejia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Establishing accurate field development parameters to optimize long-term oil
production takes time and effort due to the complexity of oil well development,
and the uncertainty in estimating long-term well production. Traditionally, oil
and gas companies use simulation software that are inherently computationally
expensive to forecast production. Thus, machine learning approaches are
recently utilized in literature as an efficient alternative to optimize well
developments by enhancing completion conditions. The primary goal of this
project is to develop effective machine-learning models that can integrate the
effects of multidimensional predictive variables (i.e., completion conditions)
to predict 12-Month Cumulative Production accurately.
<br />Three predictive regression machine learning models are implemented for
predicting 12-month cumulative oil production: Random Forest, Gradient
Boosting, and Long Short-Term Memory Models. All three models yielded
cumulative production predictions with root mean squared error (RMSE ) values
ranging from 7.35 to 20.01 thousand barrels of oil. Although we hypothesized
that all models would yield accurate predictions, the results indicated a
crucial need for further refinement to create reliable and rational predictive
tools in the subsurface. While this study did not produce optimal models for
completion sequencing to maximize long-term production, we established that
machine learning models alone are not self-sufficient for problems of this
nature. Hence, there is potential for significant improvement, including
comprehensive feature engineering, and a recommendation of exploring the use of
hybrid or surrogate models (i.e., coupling physics reduced models and machine
learning models), to ascertain significant contribution to the progress of
completion sequencing workflows.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15609" title="Abstract">arXiv:2402.15609</a> [<a href="/pdf/2402.15609" title="Download PDF">pdf</a>, <a href="/ps/2402.15609" title="Download PostScript">ps</a>, <a href="/format/2402.15609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration and Exploitation in Consumer Automation: Visualizing IoT  Interactions with Topological Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Novak%2C+T+P">Thomas P. Novak</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+D+L">Donna L. Hoffman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages, 6 figures, Web Appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This article proposes a method to uncover opportunities for exploitation and
exploration from consumer IoT interaction data. We develop a unique
decomposition of cosine similarity that quantifies exploitation through
functional similarity of interactions, exploration through cross-capacity
similarity of counterfactual interactions, and differentiation of the two
opportunities through within-similarity. We propose a topological data analysis
method that incorporates these components of similarity and provides for their
visualization. Functionally similar automations reveal exploitation
opportunities for substitutes-in-use or complements-in-use, while exploration
opportunities extend functionality into new use cases. This data-driven
approach provides marketers with a powerful capability to discover
possibilities for refining existing automation features while exploring new
innovations. More generally, our approach can aid marketing efforts to balance
these strategic opportunities in high technology contexts.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15610" title="Abstract">arXiv:2402.15610</a> [<a href="/pdf/2402.15610" title="Download PDF">pdf</a>, <a href="/format/2402.15610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective &quot;Selective Prediction&quot;: Reducing Unnecessary Abstention in  Vision-Language Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+T">Tanmay Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K+R">Khyathi Raghavi Chandu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Prior work on selective prediction minimizes incorrect predictions from
vision-language models (VLMs) by allowing them to abstain from answering when
uncertain. However, when deploying a vision-language system with low tolerance
for inaccurate predictions, selective prediction may be over-cautious and
abstain too frequently, even on many correct predictions. We introduce
ReCoVERR, an inference-time algorithm to reduce the over-abstention of a
selective vision-language system without decreasing prediction accuracy. When
the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries
to find relevant clues in the image that provide additional evidence for the
prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects
high-confidence evidences, and if enough evidence confirms the prediction the
system makes a prediction instead of abstaining. ReCoVERR enables two VLMs,
BLIP2 and InstructBLIP, to answer up to 20% more questions on the A-OKVQA task
than vanilla selective prediction without decreasing system accuracy, thus
improving overall system reliability.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15613" title="Abstract">arXiv:2402.15613</a> [<a href="/pdf/2402.15613" title="Download PDF">pdf</a>, <a href="/format/2402.15613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient Active Learning in NLP via Pretrained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vysogorets%2C+A">Artem Vysogorets</a>, 
<a href="/search/cs?searchtype=author&query=Gopal%2C+A">Achintya Gopal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Fine-tuning Large Language Models (LLMs) is now a common approach for text
classification in a wide range of applications. When labeled documents are
scarce, active learning helps save annotation efforts but requires retraining
of massive models on each acquisition iteration. We drastically expedite this
process by using pretrained representations of LLMs within the active learning
loop and, once the desired amount of labeled data is acquired, fine-tuning that
or even a different pretrained LLM on this labeled data to achieve the best
performance. As verified on common text classification benchmarks with
pretrained BERT and RoBERTa as the backbone, our strategy yields similar
performance to fine-tuning all the way through the active learning loop but is
orders of magnitude less computationally expensive. The data acquired with our
procedure generalizes across pretrained networks, allowing flexibility in
choosing the final model or updating it as newer versions get released.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15617" title="Abstract">arXiv:2402.15617</a> [<a href="/pdf/2402.15617" title="Download PDF">pdf</a>, <a href="/ps/2402.15617" title="Download PostScript">ps</a>, <a href="/format/2402.15617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning-Based Approaches for Enhancing Security and  Resilience in Smart Control: A Survey on Attack and Defense Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Reinforcement Learning (RL), one of the core paradigms in machine learning,
learns to make decisions based on real-world experiences. This approach has
significantly advanced AI applications across various domains, notably in smart
grid optimization and smart home automation. However, the proliferation of RL
in these critical sectors has also exposed them to sophisticated adversarial
attacks that target the underlying neural network policies, compromising system
integrity. Given the pivotal role of RL in enhancing the efficiency and
sustainability of smart grids and the personalized convenience in smart homes,
ensuring the security of these systems is paramount. This paper aims to bolster
the resilience of RL frameworks within these specific contexts, addressing the
unique challenges posed by the intricate and potentially adversarial
environments of smart grids and smart homes. We provide a thorough review of
the latest adversarial RL threats and outline effective defense strategies
tailored to safeguard these applications. Our comparative analysis sheds light
on the nuances of adversarial tactics against RL-driven smart systems and
evaluates the defense mechanisms, focusing on their innovative contributions,
limitations, and the compromises they entail. By concentrating on the smart
grid and smart home scenarios, this survey equips ML developers and researchers
with the insights needed to secure RL applications against emerging threats,
ensuring their reliability and safety in our increasingly connected world.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15623" title="Abstract">arXiv:2402.15623</a> [<a href="/pdf/2402.15623" title="Download PDF">pdf</a>, <a href="/format/2402.15623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Based User Profiles for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Joyce Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yijia Dai</a>, 
<a href="/search/cs?searchtype=author&query=Joachims%2C+T">Thorsten Joachims</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (4 in appendix), 22 tables/figures (16 in appendix). Accepted to LLM-IGS@WSDM2024 workshop, now sharing this slightly updated revision version with workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Most conventional recommendation methods (e.g., matrix factorization)
represent user profiles as high-dimensional vectors. Unfortunately, these
vectors lack interpretability and steerability, and often perform poorly in
cold-start settings. To address these shortcomings, we explore the use of user
profiles that are represented as human-readable text. We propose the
Language-based Factorization Model (LFM), which is essentially an
encoder/decoder model where both the encoder and the decoder are large language
models (LLMs). The encoder LLM generates a compact natural-language profile of
the user's interests from the user's rating history. The decoder LLM uses this
summary profile to complete predictive downstream tasks. We evaluate our LFM
approach on the MovieLens dataset, comparing it against matrix factorization
and an LLM model that directly predicts from the user's rating history. In
cold-start settings, we find that our method can have higher accuracy than
matrix factorization. Furthermore, we find that generating a compact and
human-readable summary often performs comparably with or better than direct LLM
prediction, while enjoying better interpretability and shorter model input
length. Our results motivate a number of future research directions and
potential improvements.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15627" title="Abstract">arXiv:2402.15627</a> [<a href="/pdf/2402.15627" title="Download PDF">pdf</a>, <a href="/format/2402.15627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MegaScale: Scaling Large Language Model Training to More Than 10,000  GPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haibin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yinmin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yanghua Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Cong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Nong%2C+S">Shibiao Nong</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yulu Jia</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sun He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zhihao Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qi Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shipeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Ding Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Yiyao Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haohan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haoran Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+P">Pengfei Nie</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Leqi Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sida Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Liang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zherui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaoying Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jianxi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We present the design, implementation and engineering experience in building
and deploying MegaScale, a production system for training large language models
(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale
brings unprecedented challenges to training efficiency and stability. We take a
full-stack approach that co-designs the algorithmic and system components
across model block and optimizer design, computation and communication
overlapping, operator optimization, data pipeline, and network performance
tuning. Maintaining high efficiency throughout the training process (i.e.,
stability) is an important consideration in production given the long extent of
LLM training jobs. Many hard stability issues only emerge at large scale, and
in-depth observability is the key to address them. We develop a set of
diagnosis tools to monitor system components and events deep in the stack,
identify root causes, and derive effective techniques to achieve fault
tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs
Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the
MFU by 1.34x compared to Megatron-LM. We share our operational experience in
identifying and fixing failures and stragglers. We hope by articulating the
problems and sharing our experience from a systems perspective, this work can
inspire future LLM systems research.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15631" title="Abstract">arXiv:2402.15631</a> [<a href="/pdf/2402.15631" title="Download PDF">pdf</a>, <a href="/format/2402.15631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Self-Endorsement Improves Factuality and Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Ante Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linfeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lifeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+H">Haitao Mi</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinsong Su</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work studies improving large language model (LLM) generations at
inference time by mitigating fact-conflicting hallucinations. Particularly, we
propose a self-endorsement framework that leverages the fine-grained fact-level
comparisons across multiple sampled responses. Compared with prior ensemble
methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level
selection, our approach can better alleviate hallucinations, especially for
longform generation tasks. Our approach can broadly benefit smaller and
open-source LLMs as it mainly conducts simple content-based comparisons.
Experiments on Biographies show that our method can effectively improve the
factuality of generations with simple and intuitive prompts across different
scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K
demonstrate the potential of self-endorsement for broader application.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15632" title="Abstract">arXiv:2402.15632</a> [<a href="/pdf/2402.15632" title="Download PDF">pdf</a>, <a href="/format/2402.15632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statically Inferring Usage Bounds for Infrastructure as Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+F">Feitong Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A">Aryana Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Cito%2C+J">J&#xfc;rgen Cito</a>, 
<a href="/search/cs?searchtype=author&query=Santolucito%2C+M">Mark Santolucito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Infrastructure as Code (IaC) has enabled cloud customers to have more agility
in creating and modifying complex deployments of cloud-provisioned resources.
By writing a configuration in IaC languages such as CloudFormation, users can
declaratively specify their infrastructure and CloudFormation will handle the
creation of the resources. However, understanding the complexity of IaC
deployments has emerged as an unsolved issue. In particular, estimating the
cost of an IaC deployment requires estimating the future usage and pricing
models of every cloud resource in the deployment. Gaining transparency into
predicted usage/costs is a leading challenge in cloud management. Existing work
either relies on historical usage metrics to predict cost or on coarse-grain
static analysis that ignores interactions between resources. Our key insight is
that the topology of an IaC deployment imposes constraints on the usage of each
resource, and we can formalize and automate the reasoning on constraints by
using an SMT solver. This allows customers to have formal guarantees on the
bounds of their cloud usage. We propose a tool for fine-grained static usage
analysis that works by modeling the inter-resource interactions in an IaC
deployment as a set of SMT constraints, and evaluate our tool on a benchmark of
over 1000 real world IaC configurations.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15634" title="Abstract">arXiv:2402.15634</a> [<a href="/pdf/2402.15634" title="Download PDF">pdf</a>, <a href="/format/2402.15634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sense-Then-Train: A Novel Beam Training Design for Near-Field MIMO  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A novel sense-then-train (STT) scheme is proposed for beam training in
near-field multiple-input multiple-output (MIMO) systems. Compared to
conventional codebook-based schemes, the proposed STT scheme is capable of not
only addressing the complex spherical-wave propagation but also effectively
exploiting the additional degrees-of-freedoms (DoFs). The STT scheme is
tailored for both single-beam and multi-beam cases. 1) For the single-beam
case, the STT scheme first utilizes a sensing phase to estimate a
low-dimensional representation of the near-field MIMO channel in the wavenumber
domain. Then, in the subsequent training phase, an online learning algorithm is
proposed to obtain the optimal beam pair without predefined codebooks or
training datasets. 2) For the multi-beam case, based on the single-beam STT, a
Gram-Schmidt method is further utilized to guarantee the orthogonality between
beams in the training phase. Numerical results unveil that 1) the proposed STT
scheme can significantly enhance the beam training performance in the near
field compared to the conventional far-field codebook-based schemes, and 2) the
proposed STT scheme can perform fast and low-complexity beam training, while
achieving a near-optimal performance without full channel state information in
both cases.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15635" title="Abstract">arXiv:2402.15635</a> [<a href="/pdf/2402.15635" title="Download PDF">pdf</a>, <a href="/format/2402.15635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bagged Deep Image Prior for Recovering Images in the Presence of Speckle  Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhewen Hou</a>, 
<a href="/search/cs?searchtype=author&query=Metzler%2C+C+A">Christopher A. Metzler</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+A">Arian Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Jalali%2C+S">Shirin Jalali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">We investigate both the theoretical and algorithmic aspects of
likelihood-based methods for recovering a complex-valued signal from multiple
sets of measurements, referred to as looks, affected by speckle
(multiplicative) noise. Our theoretical contributions include establishing the
first existing theoretical upper bound on the Mean Squared Error (MSE) of the
maximum likelihood estimator under the deep image prior hypothesis. Our
theoretical results capture the dependence of MSE upon the number of parameters
in the deep image prior, the number of looks, the signal dimension, and the
number of measurements per look. On the algorithmic side, we introduce the
concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with
projected gradient descent. Furthermore, we show how employing Newton-Schulz
algorithm for calculating matrix inverses within the iterations of PGD reduces
the computational complexity of the algorithm. We will show that this method
achieves the state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15636" title="Abstract">arXiv:2402.15636</a> [<a href="/pdf/2402.15636" title="Download PDF">pdf</a>, <a href="/format/2402.15636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth and Sparse Latent Dynamics in Operator Learning with Jerk  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Mowlavi%2C+S">Saviz Mowlavi</a>, 
<a href="/search/cs?searchtype=author&query=Benosman%2C+M">Mouhacine Benosman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Mathematical Physics (math-ph); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Spatiotemporal modeling is critical for understanding complex systems across
various scientific and engineering disciplines, but governing equations are
often not fully known or computationally intractable due to inherent system
complexity. Data-driven reduced-order models (ROMs) offer a promising approach
for fast and accurate spatiotemporal forecasting by computing solutions in a
compressed latent space. However, these models often neglect temporal
correlations between consecutive snapshots when constructing the latent space,
leading to suboptimal compression, jagged latent trajectories, and limited
extrapolation ability over time. To address these issues, this paper introduces
a continuous operator learning framework that incorporates jerk regularization
into the learning of the compressed latent space. This jerk regularization
promotes smoothness and sparsity of latent space dynamics, which not only
yields enhanced accuracy and convergence speed but also helps identify
intrinsic latent space coordinates. Consisting of an implicit neural
representation (INR)-based autoencoder and a neural ODE latent dynamics model,
the framework allows for inference at any desired spatial or temporal
resolution. The effectiveness of this framework is demonstrated through a
two-dimensional unsteady flow problem governed by the Navier-Stokes equations,
highlighting its potential to expedite high-fidelity simulations in various
scientific and engineering applications.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15637" title="Abstract">arXiv:2402.15637</a> [<a href="/pdf/2402.15637" title="Download PDF">pdf</a>, <a href="/format/2402.15637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Order Sensitivity of In-Context Demonstration Examples in  Causal Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yanzheng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hanqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning has become a popular paradigm in natural language
processing. However, its performance can be significantly influenced by the
order of in-context demonstration examples. In this paper, we found that causal
language models (CausalLMs) are more sensitive to this order compared to prefix
language models (PrefixLMs). We attribute this phenomenon to the
auto-regressive attention masks within CausalLMs, which restrict each token
from accessing information from subsequent tokens. This results in different
receptive fields for samples at different positions, thereby leading to
representation disparities across positions. To tackle this challenge, we
introduce an unsupervised fine-tuning method, termed the Information-Augmented
and Consistency-Enhanced approach. This approach utilizes contrastive learning
to align representations of in-context examples across different positions and
introduces a consistency loss to ensure similar representations for inputs with
different permutations. This enhances the model's predictive consistency across
permutations. Experimental results on four benchmarks suggest that our proposed
method can reduce the sensitivity to the order of in-context examples and
exhibit robust generalizability, particularly when demonstrations are sourced
from a pool different from that used in the training phase, or when the number
of in-context examples differs from what is used during training.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15638" title="Abstract">arXiv:2402.15638</a> [<a href="/pdf/2402.15638" title="Download PDF">pdf</a>, <a href="/format/2402.15638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Resource Allocation in Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ban%2C+H">Hao Ban</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaiyi Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">By jointly learning multiple tasks, multi-task learning (MTL) can leverage
the shared knowledge across tasks, resulting in improved data efficiency and
generalization performance. However, a major challenge in MTL lies in the
presence of conflicting gradients, which can hinder the fair optimization of
some tasks and subsequently impede MTL's ability to achieve better overall
performance. Inspired by fair resource allocation in communication networks, we
formulate the optimization of MTL as a utility maximization problem, where the
loss decreases across tasks are maximized under different fairness
measurements. To solve this problem, we propose FairGrad, a novel MTL
optimization method. FairGrad not only enables flexible emphasis on certain
tasks but also achieves a theoretical convergence guarantee. Extensive
experiments demonstrate that our method can achieve state-of-the-art
performance among gradient manipulation methods on a suite of multi-task
benchmarks in supervised learning and reinforcement learning. Furthermore, we
incorporate the idea of $\alpha$-fairness into loss functions of various MTL
methods. Extensive empirical studies demonstrate that their performance can be
significantly enhanced. Code is provided at
\url{https://github.com/OptMN-Lab/fairgrad}.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15643" title="Abstract">arXiv:2402.15643</a> [<a href="/pdf/2402.15643" title="Download PDF">pdf</a>, <a href="/format/2402.15643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artful Path to Healing: Using Machine Learning for Visual Art  Recommendation to Prevent and Reduce Post-Intensive Care
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yilma%2C+B+A">Bereket A. Yilma</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C+M">Chan Mi Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cupchik%2C+G+C">Gerald C. Cupchik</a>, 
<a href="/search/cs?searchtype=author&query=Leiva%2C+L+A">Luis A. Leiva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Staying in the intensive care unit (ICU) is often traumatic, leading to
post-intensive care syndrome (PICS), which encompasses physical, psychological,
and cognitive impairments. Currently, there are limited interventions available
for PICS. Studies indicate that exposure to visual art may help address the
psychological aspects of PICS and be more effective if it is personalized. We
develop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to
enable personalized therapeutic visual art experiences for post-ICU patients.
We investigate four state-of-the-art VA RecSys engines, evaluating the
relevance of their recommendations for therapeutic purposes compared to
expert-curated recommendations. We conduct an expert pilot test and a
large-scale user study (n=150) to assess the appropriateness and effectiveness
of these recommendations. Our results suggest all recommendations enhance
temporal affective states. Visual and multimodal VA RecSys engines compare
favourably with expert-curated recommendations, indicating their potential to
support the delivery of personalized art therapy for PICS prevention and
treatment.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15645" title="Abstract">arXiv:2402.15645</a> [<a href="/pdf/2402.15645" title="Download PDF">pdf</a>, <a href="/format/2402.15645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimization based limiter for enforcing positivity in a  semi-implicit discontinuous Galerkin scheme for compressible Navier-Stokes  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider an optimization based limiter for enforcing positivity of
internal energy in a semi-implicit scheme for solving gas dynamics equations.
With Strang splitting, the compressible Navier-Stokes system is splitted into
the compressible Euler equations, solved by the positivity-preserving
Runge-Kutta discontinuous Galerkin (DG) method, and the parabolic subproblem,
solved by Crank-Nicolson method with interior penalty DG method. Such a scheme
is at most second order accurate in time, high order accurate in space,
conservative, and preserves positivity of density. To further enforce the
positivity of internal energy, we impose an optimization based limiter for the
total energy variable to post process DG polynomial cell averages. The
optimization based limiter can be efficiently implemented by the popular first
order convex optimization algorithms such as the Douglas-Rachford splitting
method if using the optimal algorithm parameters. Numerical tests suggest that
the DG method with $\mathbb{Q}^k$ basis and the optimization-based limiter is
robust for demanding low pressure problems such as high speed flows.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15648" title="Abstract">arXiv:2402.15648</a> [<a href="/pdf/2402.15648" title="Download PDF">pdf</a>, <a href="/format/2402.15648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MambaIR: A Simple Baseline for Image Restoration with State-Space Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinmin Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+T">Tao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Z">Zhihao Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xudong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent years have witnessed great progress in image restoration thanks to the
advancements in modern deep neural networks e.g. Convolutional Neural Network
and Transformer. However, existing restoration backbones are usually limited
due to the inherent local reductive bias or quadratic computational complexity.
Recently, Selective Structured State Space Model e.g., Mamba, has shown great
potential for long-range dependencies modeling with linear complexity, but it
is still under-explored in low-level computer vision. In this work, we
introduce a simple but strong benchmark model, named MambaIR, for image
restoration. In detail, we propose the Residual State Space Block as the core
component, which employs convolution and channel attention to enhance the
capabilities of the vanilla Mamba. In this way, our MambaIR takes advantage of
local patch recurrence prior as well as channel interaction to produce
restoration-specific feature representation. Extensive experiments demonstrate
the superiority of our method, for example, MambaIR outperforms
Transformer-based baseline SwinIR by up to 0.36dB, using similar computational
cost but with a global receptive field. Code is available at
\url{https://github.com/csguoh/MambaIR}.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15650" title="Abstract">arXiv:2402.15650</a> [<a href="/pdf/2402.15650" title="Download PDF">pdf</a>, <a href="/ps/2402.15650" title="Download PostScript">ps</a>, <a href="/format/2402.15650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Constraint Safe RL with Objective Suppression for Safety-Critical  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Booher%2C+J">Jonathan Booher</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Petiushko%2C+A">Aleksandr Petiushko</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Animesh Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Safe reinforcement learning tasks with multiple constraints are a challenging
domain despite being very common in the real world. To address this challenge,
we propose Objective Suppression, a novel method that adaptively suppresses the
task reward maximizing objectives according to a safety critic. We benchmark
Objective Suppression in two multi-constraint safety domains, including an
autonomous driving domain where any incorrect behavior can lead to disastrous
consequences. Empirically, we demonstrate that our proposed method, when
combined with existing safe RL algorithms, can match the task reward achieved
by our baselines with significantly fewer constraint violations.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15651" title="Abstract">arXiv:2402.15651</a> [<a href="/pdf/2402.15651" title="Download PDF">pdf</a>, <a href="/format/2402.15651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Physics-Based and Data-Driven Modeling of Vascular Bifurcation  Pressure Differences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rubio%2C+N+L">Natalia L. Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Pegolotti%2C+L">Luca Pegolotti</a>, 
<a href="/search/cs?searchtype=author&query=Pfaller%2C+M+R">Martin R. Pfaller</a>, 
<a href="/search/cs?searchtype=author&query=Darve%2C+E+F">Eric F. Darve</a>, 
<a href="/search/cs?searchtype=author&query=Marsden%2C+A+L">Alison L. Marsden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Reduced-order models (ROMs) allow for the simulation of blood flow in
patient-specific vasculatures without the high computational cost and wait time
associated with traditional computational fluid dynamics (CFD) models.
Unfortunately, due to the simplifications made in their formulations, ROMs can
suffer from significantly reduced accuracy. One common simplifying assumption
is the continuity of static or total pressure over vascular junctions. In many
cases, this assumption has been shown to introduce significant error. We
propose a model to account for this pressure difference, with the ultimate goal
of increasing the accuracy of cardiovascular ROMs. Our model successfully uses
a structure common in existing ROMs in conjunction with machine-learning
techniques to predict the pressure difference over a vascular bifurcation. We
analyze the performance of our model on steady and transient flows, testing it
on three bifurcation cohorts representing three different bifurcation geometric
types. We also compare the efficacy of different machine-learning techniques
and two different model modalities.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15653" title="Abstract">arXiv:2402.15653</a> [<a href="/pdf/2402.15653" title="Download PDF">pdf</a>, <a href="/format/2402.15653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yanqi Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dazhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaitai Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While convolutional neural networks (CNNs) have achieved success in computer
vision tasks, it is vulnerable to backdoor attacks. Such attacks could mislead
the victim model to make attacker-chosen prediction with a specific trigger
pattern. Until now, the trigger injection of existing attacks is mainly limited
to spatial domain. Recent works take advantage of perceptual properties of
planting specific patterns in the frequency domain, which only reflect
indistinguishable pixel-wise perturbations in pixel domain. However, in the
black-box setup, the inaccessibility of training process often renders more
complex trigger designs. Existing frequency attacks simply handcraft the
magnitude of spectrum, introducing anomaly frequency disparities between clean
and poisoned data and taking risks of being removed by image processing
operations (such as lossy compression and filtering). In this paper, we propose
a robust low-frequency black-box backdoor attack (LFBA), which minimally
perturbs low-frequency components of frequency spectrum and maintains the
perceptual similarity in spatial space simultaneously. The key insight of our
attack restrict the search for the optimal trigger to low-frequency region that
can achieve high attack effectiveness, robustness against image transformation
defenses and stealthiness in dual space. We utilize simulated annealing (SA), a
form of evolutionary algorithm, to optimize the properties of frequency trigger
including the number of manipulated frequency bands and the perturbation of
each frequency component, without relying on the knowledge from the victim
classifier. Extensive experiments on real-world datasets verify the
effectiveness and robustness of LFBA against image processing operations and
the state-of-the-art backdoor defenses, as well as its inherent stealthiness in
both spatial and frequency space, making it resilient against frequency
inspection.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15654" title="Abstract">arXiv:2402.15654</a> [<a href="/pdf/2402.15654" title="Download PDF">pdf</a>, <a href="/format/2402.15654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+S">Sadaf Ghaffari</a>, 
<a href="/search/cs?searchtype=author&query=Krishnaswamy%2C+N">Nikhil Krishnaswamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures, Proceedings of AAAI Spring Symposium: Empowering Machine Learning and Large Language Models with Domain and Commonsense Knowledge (MAKE). AAAI (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we present an exploration of LLMs' abilities to problem solve
with physical reasoning in situated environments. We construct a simple
simulated environment and demonstrate examples of where, in a zero-shot
setting, both text and multimodal LLMs display atomic world knowledge about
various objects but fail to compose this knowledge in correct solutions for an
object manipulation and placement task. We also use BLIP, a vision-language
model trained with more sophisticated cross-modal attention, to identify cases
relevant to object physical properties that that model fails to ground.
Finally, we present a procedure for discovering the relevant properties of
objects in the environment and propose a method to distill this knowledge back
into the LLM.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15655" title="Abstract">arXiv:2402.15655</a> [<a href="/pdf/2402.15655" title="Download PDF">pdf</a>, <a href="/format/2402.15655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contact Complexity in Customer Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pi%2C+S">Shu-Ting Pi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Michael Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics for Online Marketplaces
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Customers who reach out for customer service support may face a range of
issues that vary in complexity. Routing high-complexity contacts to junior
agents can lead to multiple transfers or repeated contacts, while directing
low-complexity contacts to senior agents can strain their capacity to assist
customers who need professional help. To tackle this, a machine learning model
that accurately predicts the complexity of customer issues is highly desirable.
However, defining the complexity of a contact is a difficult task as it is a
highly abstract concept. While consensus-based data annotation by experienced
agents is a possible solution, it is time-consuming and costly. To overcome
these challenges, we have developed a novel machine learning approach to define
contact complexity. Instead of relying on human annotation, we trained an AI
expert model to mimic the behavior of agents and evaluate each contact's
complexity based on how the AI expert responds. If the AI expert is uncertain
or lacks the skills to comprehend the contact transcript, it is considered a
high-complexity contact. Our method has proven to be reliable, scalable, and
cost-effective based on the collected data.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15656" title="Abstract">arXiv:2402.15656</a> [<a href="/pdf/2402.15656" title="Download PDF">pdf</a>, <a href="/format/2402.15656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Semilinear Neural Operators : A Unified Recursive Framework For  Prediction And Data Assimilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ashutosh Singh</a>, 
<a href="/search/cs?searchtype=author&query=Borsoi%2C+R+A">Ricardo Augusto Borsoi</a>, 
<a href="/search/cs?searchtype=author&query=Erdogmus%2C+D">Deniz Erdogmus</a>, 
<a href="/search/cs?searchtype=author&query=Imbiriba%2C+T">Tales Imbiriba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Recent advances in the theory of Neural Operators (NOs) have enabled fast and
accurate computation of the solutions to complex systems described by partial
differential equations (PDEs). Despite their great success, current NO-based
solutions face important challenges when dealing with spatio-temporal PDEs over
long time scales. Specifically, the current theory of NOs does not present a
systematic framework to perform data assimilation and efficiently correct the
evolution of PDE solutions over time based on sparsely sampled noisy
measurements. In this paper, we propose a learning-based state-space approach
to compute the solution operators to infinite-dimensional semilinear PDEs.
Exploiting the structure of semilinear PDEs and the theory of nonlinear
observers in function spaces, we develop a flexible recursive method that
allows for both prediction and data assimilation by combining prediction and
correction operations. The proposed framework is capable of producing fast and
accurate predictions over long time horizons, dealing with irregularly sampled
noisy measurements to correct the solution, and benefits from the decoupling
between the spatial and temporal dynamics of this class of PDEs. We show
through experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de
Vries equations that the proposed model is robust to noise and can leverage
arbitrary amounts of measurements to correct its prediction over a long time
horizon with little computational overhead.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15659" title="Abstract">arXiv:2402.15659</a> [<a href="/pdf/2402.15659" title="Download PDF">pdf</a>, <a href="/format/2402.15659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepLight: Reconstructing High-Resolution Observations of Nighttime  Light With Multi-Modal Remote Sensing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lixian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Runmin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengxuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Juepeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haohuan Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Nighttime light (NTL) remote sensing observation serves as a unique proxy for
quantitatively assessing progress toward meeting a series of Sustainable
Development Goals (SDGs), such as poverty estimation, urban sustainable
development, and carbon emission. However, existing NTL observations often
suffer from pervasive degradation and inconsistency, limiting their utility for
computing the indicators defined by the SDGs. In this study, we propose a novel
approach to reconstruct high-resolution NTL images using multi-modal remote
sensing data. To support this research endeavor, we introduce DeepLightMD, a
comprehensive dataset comprising data from five heterogeneous sensors, offering
fine spatial resolution and rich spectral information at a national scale.
Additionally, we present DeepLightSR, a calibration-aware method for building
bridges between spatially heterogeneous modality data in the multi-modality
super-resolution. DeepLightSR integrates calibration-aware alignment, an
auxiliary-to-main multi-modality fusion, and an auxiliary-embedded refinement
to effectively address spatial heterogeneity, fuse diversely representative
features, and enhance performance in $8\times$ super-resolution (SR) tasks.
Extensive experiments demonstrate the superiority of DeepLightSR over 8
competing methods, as evidenced by improvements in PSNR (2.01 dB $ \sim $ 13.25
dB) and PIQE (0.49 $ \sim $ 9.32). Our findings underscore the practical
significance of our proposed dataset and model in reconstructing
high-resolution NTL data, supporting efficiently and quantitatively assessing
the SDG progress.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15662" title="Abstract">arXiv:2402.15662</a> [<a href="/pdf/2402.15662" title="Download PDF">pdf</a>, <a href="/format/2402.15662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GiMeFive: Towards Interpretable Facial Emotion Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiawen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kawka%2C+L">Leah Kawka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep convolutional neural networks have been shown to successfully recognize
facial emotions for the past years in the realm of computer vision. However,
the existing detection approaches are not always reliable or explainable, we
here propose our model GiMeFive with interpretations, i.e., via layer
activations and gradient-weighted class activation mapping. We compare against
the state-of-the-art methods to classify the six facial emotions. Empirical
results show that our model outperforms the previous methods in terms of
accuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated
FER GiMeFive. Furthermore, we explain our work in real-world image and video
examples, as well as real-time live camera streams. Our code and supplementary
material are available at https: //github.com/werywjw/SEP-CVDL.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15663" title="Abstract">arXiv:2402.15663</a> [<a href="/pdf/2402.15663" title="Download PDF">pdf</a>, <a href="/format/2402.15663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhaoyue Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pergola%2C+G">Gabriele Pergola</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures, accepted by EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the advent of large language models (LLMs), there has been growing
interest in exploring their potential for medical applications. This research
aims to investigate the ability of LLMs, specifically ChatGPT, in the context
of pharmacovigilance event extraction, of which the main goal is to identify
and extract adverse events or potential therapeutic events from textual medical
sources. We conduct extensive experiments to assess the performance of ChatGPT
in the pharmacovigilance event extraction task, employing various prompts and
demonstration selection strategies. The findings demonstrate that while ChatGPT
demonstrates reasonable performance with appropriate demonstration selection
strategies, it still falls short compared to fully fine-tuned small models.
Additionally, we explore the potential of leveraging ChatGPT for data
augmentation. However, our investigation reveals that the inclusion of
synthesized data into fine-tuning may lead to a decrease in performance,
possibly attributed to noise in the ChatGPT-generated labels. To mitigate this,
we explore different filtering strategies and find that, with the proper
approach, more stable performance can be achieved, although constant
improvement remains elusive.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15665" title="Abstract">arXiv:2402.15665</a> [<a href="/pdf/2402.15665" title="Download PDF">pdf</a>, <a href="/format/2402.15665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teacher-Student Learning on Complexity in Intelligent Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pi%2C+S">Shu-Ting Pi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Michael Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD 2023 Workshop on End-End Customer Journey Optimization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Customer service is often the most time-consuming aspect for e-commerce
websites, with each contact typically taking 10-15 minutes. Effectively routing
customers to appropriate agents without transfers is therefore crucial for
e-commerce success. To this end, we have developed a machine learning framework
that predicts the complexity of customer contacts and routes them to
appropriate agents accordingly. The framework consists of two parts. First, we
train a teacher model to score the complexity of a contact based on the
post-contact transcripts. Then, we use the teacher model as a data annotator to
provide labels to train a student model that predicts the complexity based on
pre-contact data only. Our experiments show that such a framework is successful
and can significantly improve customer experience. We also propose a useful
metric called complexity AUC that evaluates the effectiveness of customer
service at a statistical level.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15666" title="Abstract">arXiv:2402.15666</a> [<a href="/pdf/2402.15666" title="Download PDF">pdf</a>, <a href="/format/2402.15666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Model in Online Customer Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pi%2C+S">Shu-Ting Pi</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cheng-Ping Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuying Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Companion Proceedings of the ACM Web Conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Building machine learning models can be a time-consuming process that often
takes several months to implement in typical business scenarios. To ensure
consistent model performance and account for variations in data distribution,
regular retraining is necessary. This paper introduces a solution for improving
online customer service in e-commerce by presenting a universal model for
predict-ing labels based on customer questions, without requiring training. Our
novel approach involves using machine learning techniques to tag customer
questions in transcripts and create a repository of questions and corresponding
labels. When a customer requests assistance, an information retrieval model
searches the repository for similar questions, and statistical analysis is used
to predict the corresponding label. By eliminating the need for individual
model training and maintenance, our approach reduces both the model development
cycle and costs. The repository only requires periodic updating to maintain
accuracy.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15668" title="Abstract">arXiv:2402.15668</a> [<a href="/pdf/2402.15668" title="Download PDF">pdf</a>, <a href="/format/2402.15668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel,  and Local Computation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalirrooyfard%2C+M">Mina Dalirrooyfard</a>, 
<a href="/search/cs?searchtype=author&query=Makarychev%2C+K">Konstantin Makarychev</a>, 
<a href="/search/cs?searchtype=author&query=Mitrovi%C4%87%2C+S">Slobodan Mitrovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given a graph with positive and negative edge labels, the correlation
clustering problem aims to cluster the nodes so to minimize the total number of
between-cluster positive and within-cluster negative edges. This problem has
many applications in data mining, particularly in unsupervised learning.
Inspired by the prevalence of large graphs and constantly changing data in
modern applications, we study correlation clustering in dynamic, parallel
(MPC), and local computation (LCA) settings. We design an approach that
improves state-of-the-art runtime complexities in all these settings. In
particular, we provide the first fully dynamic algorithm that runs in an
expected amortized constant time, without any dependence on the graph size.
Moreover, our algorithm essentially matches the approximation guarantee of the
celebrated Pivot algorithm.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15670" title="Abstract">arXiv:2402.15670</a> [<a href="/pdf/2402.15670" title="Download PDF">pdf</a>, <a href="/format/2402.15670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mathematical model for simultaneous personnel shift planning and  unrelated parallel machine scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khadivi%2C+M">Maziyar Khadivi</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+M">Mostafa Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=Charter%2C+T">Todd Charter</a>, 
<a href="/search/cs?searchtype=author&query=Najjaran%2C+H">Homayoun Najjaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">This paper addresses a production scheduling problem derived from an
industrial use case, focusing on unrelated parallel machine scheduling with the
personnel availability constraint. The proposed model optimizes the production
plan over a multi-period scheduling horizon, accommodating variations in
personnel shift hours within each time period. It assumes shared personnel
among machines, with one personnel required per machine for setup and
supervision during job processing. Available personnel are fewer than the
machines, thus limiting the number of machines that can operate in parallel.
The model aims to minimize the total production time considering
machine-dependent processing times and sequence-dependent setup times. The
model handles practical scenarios like machine eligibility constraints and
production time windows. A Mixed Integer Linear Programming (MILP) model is
introduced to formulate the problem, taking into account both continuous and
district variables. A two-step solution approach enhances computational speed,
first maximizing accepted jobs and then minimizing production time. Validation
with synthetic problem instances and a real industrial case study of a food
processing plant demonstrates the performance of the model and its usefulness
in personnel shift planning. The findings offer valuable insights for practical
managerial decision-making in the context of production scheduling.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15671" title="Abstract">arXiv:2402.15671</a> [<a href="/pdf/2402.15671" title="Download PDF">pdf</a>, <a href="/ps/2402.15671" title="Download PostScript">ps</a>, <a href="/format/2402.15671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Participatory and Social Justice-Oriented Measure of  Human-Robot Trust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korpan%2C+R">Raj Korpan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 Workshop on Trust in HRI at the ACM/IEEE International Conference on Human Robot Interaction (HRI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Many measures of human-robot trust have proliferated across the HRI research
literature because each attempts to capture the factors that impact trust
despite its many dimensions. None of the previous trust measures, however,
address the systems of inequity and structures of power present in HRI research
or attempt to counteract the systematic biases and potential harms caused by
HRI systems. This position paper proposes a participatory and social
justice-oriented approach for the design and evaluation of a trust measure.
This proposed process would iteratively co-design the trust measure with the
community for whom the HRI system is being created. The process would
prioritize that community's needs and unique circumstances to produce a trust
measure that accurately reflects the factors that impact their trust in a
robot.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15674" title="Abstract">arXiv:2402.15674</a> [<a href="/pdf/2402.15674" title="Download PDF">pdf</a>, <a href="/format/2402.15674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formally Verified C Code Generation from Hybrid Communicating Sequential  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zekun Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+B">Bohua Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+N">Naijun Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Hybrid Communicating Sequential Processes (HCSP) is a formal model for hybrid
systems, including primitives for evolution along an ordinary differential
equation (ODE), communication, and parallel composition. Code generation is
needed to convert HCSP models into code that can be executed in practice, and
the correctness of this conversion is essential to ensure that the generated
code accurately reflects the formal model. In this paper, we propose a code
generation algorithm from HCSP to C with POSIX library for concurrency. The
main difficulties include how to bridge the gap between the synchronized
communication model in HCSP and the use of mutexes for synchronization in C,
and how to discretize evolution along ODEs and support interrupt of ODE
evolution by communication. To prove the correctness of code generation, we
define a formal semantics for POSIX C, and build transition system models for
both HCSP and C programs. We then define an approximate bisimulation relation
between traces of transition systems, and show that under certain robustness
conditions for HCSP, the generated C program is approximately bisimilar to the
original model. Finally, we evaluate the code generation algorithm on a
detailed model for automatic cruise control, showing its utility on real-world
examples.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15677" title="Abstract">arXiv:2402.15677</a> [<a href="/pdf/2402.15677" title="Download PDF">pdf</a>, <a href="/format/2402.15677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus seeking in diffusive multidimensional networks with a repeated  interaction pattern and time-delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vu%2C+H+H">Hoang Huy Vu</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+Q+N">Quyen Ngoc Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Van+Nguyen%2C+C">Chuong Van Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Van+Pham%2C+T">Tuynh Van Pham</a>, 
<a href="/search/eess?searchtype=author&query=Trinh%2C+M+H">Minh Hoang Trinh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures, submitted to a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper studies a consensus problem in multidimensional networks having
the same agent-to-agent interaction pattern under both intra- and cross-layer
time delays. Several conditions for the agents to globally asymptotically
achieve a consensus are derived, which involve the overall network's structure,
the local interacting pattern, and the values of the time delays. The validity
of these conditions is proved by direct eigenvalue evaluation and supported by
numerical simulations.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15678" title="Abstract">arXiv:2402.15678</a> [<a href="/pdf/2402.15678" title="Download PDF">pdf</a>, <a href="/format/2402.15678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minions: Accelerating Large Language Model Inference with Adaptive and  Collective Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hailong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xuning Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kejie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tianyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xin You</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yongjun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+Z">Zhongzhi Luan</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+D">Depei Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Large language models (LLM) have recently attracted surging interest due to
their outstanding capabilities across various domains. However, enabling
efficient LLM inference is challenging due to its autoregressive decoding that
generates tokens only one at a time. Although research works apply pruning or
quantization to speed up LLM inference, they typically require fine-tuning the
LLM, incurring significant time and economic costs. Meanwhile, speculative
decoding has been proposed to use small speculative models (SSMs) to accelerate
the inference of LLM. However, the low acceptance rate of SSM and the high
verification cost of LLM prohibit further performance improvement of inference.
In this paper, we propose Minions, an LLM inference system that accelerates LLM
inference with a collective and adaptive speculative generation. Specifically,
Minions proposes a majority-voted mechanism to leverage multiple SSMs to
jointly speculate the outputs of LLM, which improves the inference performance
without introducing prohibitive computation costs for LLM. To better trade off
the number of tokens speculated from SSM and the verification cost of LLM,
Minions proposes an adaptive mechanism to dynamically determine the optimal
speculation length of SSM, which can achieve better inference performance
across different models, datasets, and hyper-parameters. In addition, Minions
decouples the SSM decoding and LLM verification efficiently and adopts a
pipelined execution mechanism to further improve the inference performance of
LLM. By comparing with the state-of-the-art LLM inference systems, we
demonstrate that Minions can achieve higher inference throughput and lower
inference time.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15679" title="Abstract">arXiv:2402.15679</a> [<a href="/pdf/2402.15679" title="Download PDF">pdf</a>, <a href="/ps/2402.15679" title="Download PostScript">ps</a>, <a href="/format/2402.15679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Density-based Clustering with Random Projections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haochuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+N">Ninh Pham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present sDBSCAN, a scalable density-based clustering algorithm in high
dimensions with cosine distance. Utilizing the neighborhood-preserving property
of random projections, sDBSCAN can quickly identify core points and their
neighborhoods, the primary hurdle of density-based clustering. Theoretically,
sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions
with high probability. To further facilitate sDBSCAN, we present sOPTICS, a
scalable OPTICS for interactive exploration of the intrinsic clustering
structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and
Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is
significantly faster and provides higher accuracy than many other clustering
algorithms on real-world million-point data sets. On these data sets, sDBSCAN
and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand
several hours or cannot run due to memory constraints.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15680" title="Abstract">arXiv:2402.15680</a> [<a href="/pdf/2402.15680" title="Download PDF">pdf</a>, <a href="/format/2402.15680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward  Comprehensive Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Hongliang Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kay Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The rise of self-supervised learning, which operates without the need for
labeled data, has garnered significant interest within the graph learning
community. This enthusiasm has led to the development of numerous Graph
Contrastive Learning (GCL) techniques, all aiming to create a versatile graph
encoder that leverages the wealth of unlabeled data for various downstream
tasks. However, the current evaluation standards for GCL approaches are flawed
due to the need for extensive hyper-parameter tuning during pre-training and
the reliance on a single downstream task for assessment. These flaws can skew
the evaluation away from the intended goals, potentially leading to misleading
conclusions. In our paper, we thoroughly examine these shortcomings and offer
fresh perspectives on how GCL methods are affected by hyper-parameter choices
and the choice of downstream tasks for their evaluation. Additionally, we
introduce an enhanced evaluation framework designed to more accurately gauge
the effectiveness, consistency, and overall capability of GCL methods.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15681" title="Abstract">arXiv:2402.15681</a> [<a href="/pdf/2402.15681" title="Download PDF">pdf</a>, <a href="/ps/2402.15681" title="Download PostScript">ps</a>, <a href="/format/2402.15681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Study of Noncoherent Sparse Subarrays for Direction Finding Based on  Low-Rank and Sparse Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leite%2C+W">W. Leite</a>, 
<a href="/search/cs?searchtype=author&query=de+Lamare%2C+R+C">R. C. de Lamare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates the problem of noncoherent direction-of-arrival (DOA)
estimation using different sparse subarrays. In particular, we present a
Multiple Measurements Vector (MMV) model for noncoherent DOA estimation based
on a low-rank and sparse recovery optimization problem. Moreover, we develop
two different practical strategies to obtain sparse arrays and subarrays: i)
the subarrays are generated from a main sparse array geometry (Type-I sparse
array), and ii) the sparse subarrays that are directly designed and grouped
together to generate the whole sparse array (Type-II sparse array). Numerical
results demonstrate that the proposed MMV model can benefit from multiple data
records and that Type-II sparse noncoherent arrays are superior in performance
for DOA estimation
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15683" title="Abstract">arXiv:2402.15683</a> [<a href="/pdf/2402.15683" title="Download PDF">pdf</a>, <a href="/format/2402.15683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exit Ripple Effects: Understanding the Disruption of Socialization  Networks Following Employee Departures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gamba%2C+D">David Gamba</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yulin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Schoenebeck%2C+G">Grant Schoenebeck</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+D+M">Daniel M. Romero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in proceedings of the ACM Web Conference 2024 (WWW '24), May 13--17, 2024, Singapore, Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Amidst growing uncertainty and frequent restructurings, the impacts of
employee exits are becoming one of the central concerns for organizations.
Using rich communication data from a large holding company, we examine the
effects of employee departures on socialization networks among the remaining
coworkers. Specifically, we investigate how network metrics change among people
who historically interacted with departing employees. We find evidence of
``breakdown" in communication among the remaining coworkers, who tend to become
less connected with fewer interactions after their coworkers' departure. This
effect appears to be moderated by both external factors, such as periods of
high organizational stress, and internal factors, such as the characteristics
of the departing employee. At the external level, periods of high stress
correspond to greater communication breakdown; at the internal level, however,
we find patterns suggesting individuals may end up better positioned in their
networks after a network neighbor's departure. Overall, our study provides
critical insights into managing workforce changes and preserving communication
dynamics in the face of employee exits.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15687" title="Abstract">arXiv:2402.15687</a> [<a href="/pdf/2402.15687" title="Download PDF">pdf</a>, <a href="/format/2402.15687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Purpose Image Encoder DINOv2 for Medical Image Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xinrui Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuanang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+P">Pingkun Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing medical image registration algorithms rely on either dataset
specific training or local texture-based features to align images. The former
cannot be reliably implemented without large modality-specific training
datasets, while the latter lacks global semantics thus could be easily trapped
at local minima. In this paper, we present a training-free deformable image
registration method, DINO-Reg, leveraging a general purpose image encoder
DINOv2 for image feature extraction. The DINOv2 encoder was trained using the
ImageNet data containing natural images. We used the pretrained DINOv2 without
any finetuning. Our method feeds the DINOv2 encoded features into a discrete
optimizer to find the optimal deformable registration field. We conducted a
series of experiments to understand the behavior and role of such a general
purpose image encoder in the application of image registration. Combined with
handcrafted features, our method won the first place in the recent OncoReg
Challenge. To our knowledge, this is the first application of general vision
foundation models in medical image registration.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15688" title="Abstract">arXiv:2402.15688</a> [<a href="/pdf/2402.15688" title="Download PDF">pdf</a>, <a href="/format/2402.15688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor-free Clustering based on Anchor Graph Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+S">Shikun Mei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fangfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Quanxue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Anchor-based methods are a pivotal approach in handling clustering of
large-scale data. However, these methods typically entail two distinct stages:
selecting anchor points and constructing an anchor graph. This bifurcation,
along with the initialization of anchor points, significantly influences the
overall performance of the algorithm. To mitigate these issues, we introduce a
novel method termed Anchor-free Clustering based on Anchor Graph Factorization
(AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the
computation of pairwise distances between samples. This process, achievable
through straightforward optimization, circumvents the necessity for explicit
selection of anchor points. More concretely, our approach enhances the Fuzzy
k-means clustering algorithm (FKM), introducing a new manifold learning
technique that obviates the need for initializing cluster centers.
Additionally, we evolve the concept of the membership matrix between cluster
centers and samples in FKM into an anchor graph encompassing multiple anchor
points and samples. Employing Non-negative Matrix Factorization (NMF) on this
anchor graph allows for the direct derivation of cluster labels, thereby
eliminating the requirement for further post-processing steps. To solve the
method proposed, we implement an alternating optimization algorithm that
ensures convergence. Empirical evaluations on various real-world datasets
underscore the superior efficacy of our algorithm compared to traditional
approaches.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15690" title="Abstract">arXiv:2402.15690</a> [<a href="/pdf/2402.15690" title="Download PDF">pdf</a>, <a href="/format/2402.15690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foot In The Door: Understanding Large Language Model Jailbreaking via  Cognitive Psychology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenhua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baosheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+E">Enze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+Z">Zhiwen Gui</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuoyoucheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have gradually become the gateway for people to
acquire new knowledge. However, attackers can break the model's security
protection ("jail") to access restricted information, which is called
"jailbreaking." Previous studies have shown the weakness of current LLMs when
confronted with such jailbreaking attacks. Nevertheless, comprehension of the
intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak
prompts is noticeably lacking. Our research provides a psychological
explanation of the jailbreak prompts. Drawing on cognitive consistency theory,
we argue that the key to jailbreak is guiding the LLM to achieve cognitive
coordination in an erroneous direction. Further, we propose an automatic
black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.
This method progressively induces the model to answer harmful questions via
multi-step incremental prompts. We instantiated a prototype system to evaluate
the jailbreaking effectiveness on 8 advanced LLMs, yielding an average success
rate of 83.9%. This study builds a psychological perspective on the explanatory
insights into the intrinsic decision-making logic of LLMs.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15691" title="Abstract">arXiv:2402.15691</a> [<a href="/pdf/2402.15691" title="Download PDF">pdf</a>, <a href="/format/2402.15691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bodic%2C+P+L">Pierre Le Bodic</a>, 
<a href="/search/cs?searchtype=author&query=Kamp%2C+M">Michael Kamp</a>, 
<a href="/search/cs?searchtype=author&query=Boley%2C+M">Mario Boley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures, accepted at AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Gradient boosting of prediction rules is an efficient approach to learn
potentially interpretable yet accurate probabilistic models. However, actual
interpretability requires to limit the number and size of the generated rules,
and existing boosting variants are not designed for this purpose. Though
corrective boosting refits all rule weights in each iteration to minimise
prediction risk, the included rule conditions tend to be sub-optimal, because
commonly used objective functions fail to anticipate this refitting. Here, we
address this issue by a new objective function that measures the angle between
the risk gradient vector and the projection of the condition output vector onto
the orthogonal complement of the already selected conditions. This approach
correctly approximate the ideal update of adding the risk gradient itself to
the model and favours the inclusion of more general and thus shorter rules. As
we demonstrate using a wide range of prediction tasks, this significantly
improves the comprehensibility/accuracy trade-off of the fitted ensemble.
Additionally, we show how objective values for related rule conditions can be
computed incrementally to avoid any substantial computational overhead of the
new method.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15692" title="Abstract">arXiv:2402.15692</a> [<a href="/pdf/2402.15692" title="Download PDF">pdf</a>, <a href="/format/2402.15692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing for Human Operations on the Moon: Challenges and Opportunities  of Navigational HUD Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bensch%2C+L">Leonie Bensch</a>, 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+T">Tommy Nilsson</a>, 
<a href="/search/cs?searchtype=author&query=Wulkop%2C+J">Jan Wulkop</a>, 
<a href="/search/cs?searchtype=author&query=de+Medeiros%2C+P">Paul de Medeiros</a>, 
<a href="/search/cs?searchtype=author&query=Herzberger%2C+N+D">Nicolas Daniel Herzberger</a>, 
<a href="/search/cs?searchtype=author&query=Preutenborbeck%2C+M">Michael Preutenborbeck</a>, 
<a href="/search/cs?searchtype=author&query=Gerndt%2C+A">Andreas Gerndt</a>, 
<a href="/search/cs?searchtype=author&query=Flemisch%2C+F">Frank Flemisch</a>, 
<a href="/search/cs?searchtype=author&query=Dufresne%2C+F">Florian Dufresne</a>, 
<a href="/search/cs?searchtype=author&query=Albuquerque%2C+G">Georgia Albuquerque</a>, 
<a href="/search/cs?searchtype=author&query=Cowley%2C+A">Aidan Cowley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Future crewed missions to the Moon will face significant environmental and
operational challenges, posing risks to the safety and performance of
astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs)
have proven effective in providing intuitive navigational support on Earth, the
design of novel human-spaceflight solutions typically relies on costly and
time-consuming analogue deployments, leaving the potential use of lunar HUDs
largely under-explored. This paper explores an alternative approach by
simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR)
representation of the lunar environment. In evaluating these concepts with
astronauts and other aerospace experts (n=25), our mixed methods study
demonstrates the efficacy of simulated analogues in facilitating rapid design
assessments of early-stage HUD solutions. We illustrate this by elaborating key
design challenges and guidelines for future lunar HUDs. In reflecting on the
limitations of our approach, we propose directions for future design
exploration of human-machine interfaces for the Moon.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15693" title="Abstract">arXiv:2402.15693</a> [<a href="/pdf/2402.15693" title="Download PDF">pdf</a>, <a href="/ps/2402.15693" title="Download PostScript">ps</a>, <a href="/format/2402.15693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Photolithography Control System : A Case Study For Cyber-Physical System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Youbao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">Huijie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 10 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Photolithography control system (PCS) is an extremely complex distributed
control system, which is composed of dozens of networked microprocessors,
hundreds of actuators, hundreds of thousands of sensors, and millions of lines
of code. Cyber-physical system (CPS), which deeply merges computation with
physical processes together, copes with complex system from a higher level of
abstraction. PCS is a representative CPS. This work points out that thinking
under the framework of CPS, which includes holistic perspective, model-based
design, hardware/software co-design and continuous integration, could solve the
issues presented in the current PCS. Although the traditional embedded system
approach and the CPS approach would be coexisting in the PCS for a long time,
the CPS approach is definitely the future of the PCS development.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15694" title="Abstract">arXiv:2402.15694</a> [<a href="/pdf/2402.15694" title="Download PDF">pdf</a>, <a href="/format/2402.15694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence  for Operational Assessments in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dufresne%2C+F">Florian Dufresne</a>, 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+T">Tommy Nilsson</a>, 
<a href="/search/cs?searchtype=author&query=Gorisse%2C+G">Geoffrey Gorisse</a>, 
<a href="/search/cs?searchtype=author&query=Guerra%2C+E">Enrico Guerra</a>, 
<a href="/search/cs?searchtype=author&query=Zenner%2C+A">Andr&#xe9; Zenner</a>, 
<a href="/search/cs?searchtype=author&query=Christmann%2C+O">Olivier Christmann</a>, 
<a href="/search/cs?searchtype=author&query=Bensch%2C+L">Leonie Bensch</a>, 
<a href="/search/cs?searchtype=author&query=Callus%2C+N+A">Nikolai Anton Callus</a>, 
<a href="/search/cs?searchtype=author&query=Cowley%2C+A">Aidan Cowley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI'24, May 11-16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Space agencies are in the process of drawing up carefully thought-out
Concepts of Operations (ConOps) for future human missions on the Moon. These
are typically assessed and validated through costly and logistically demanding
analogue field studies. While interactive simulations in Virtual Reality (VR)
offer a comparatively cost-effective alternative, they have faced criticism for
lacking the fidelity of real-world deployments. This paper explores the
applicability of passive haptic interfaces in bridging the gap between
simulated and real-world ConOps assessments. Leveraging passive haptic props
(equipment mockup and astronaut gloves), we virtually recreated the Apollo 12
mission procedure and assessed it with experienced astronauts and other space
experts. Quantitative and qualitative findings indicate that haptics increased
presence and embodiment, thus improving perceived simulation fidelity and
validity of user reflections. We conclude by discussing the potential role of
passive haptic modalities in facilitating early-stage ConOps assessments for
human endeavours on the Moon and beyond.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15695" title="Abstract">arXiv:2402.15695</a> [<a href="/pdf/2402.15695" title="Download PDF">pdf</a>, <a href="/ps/2402.15695" title="Download PostScript">ps</a>, <a href="/format/2402.15695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applied User Research in Virtual Reality: Tools, Methods, and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bensch%2C+L">Leonie Bensch</a>, 
<a href="/search/cs?searchtype=author&query=Casini%2C+A">Andrea Casini</a>, 
<a href="/search/cs?searchtype=author&query=Cowley%2C+A">Aidan Cowley</a>, 
<a href="/search/cs?searchtype=author&query=Dufresne%2C+F">Florian Dufresne</a>, 
<a href="/search/cs?searchtype=author&query=Guerra%2C+E">Enrico Guerra</a>, 
<a href="/search/cs?searchtype=author&query=de+Medeiros%2C+P">Paul de Medeiros</a>, 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+T">Tommy Nilsson</a>, 
<a href="/search/cs?searchtype=author&query=Rometsch%2C+F">Flavie Rometsch</a>, 
<a href="/search/cs?searchtype=author&query=Treuer%2C+A">Andreas Treuer</a>, 
<a href="/search/cs?searchtype=author&query=Vock%2C+A">Anna Vock</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">This chapter explores the practice of conducting user research studies and
design assessments in virtual reality (VR). An overview of key VR hardware and
software tools is provided, including game engines, such as Unity and Unreal
Engine. Qualitative and quantitative research methods, along with their various
synergies with VR, are likewise discussed, and some of the challenges
associated with VR, such as limited sensory stimulation, are reflected upon. VR
is proving particularly useful in the context of space systems development,
where its utilisation offers a cost-effective and secure method for simulating
extraterrestrial environments, allowing for rapid prototyping and evaluation of
innovative concepts under representative operational conditions. To illustrate
this, we present a case study detailing the application of VR to aid aerospace
engineers testing their ideas with end-users and stakeholders during early
design stages of the European Space Agency's (ESA) prospective Argonaut lunar
lander. This case study demonstrates the effectiveness of VR simulations in
gathering important feedback concerning the operability of the Argonaut lander
in poor lighting conditions as well as surfacing relevant ergonomics
considerations and constraints. The chapter concludes by discussing the
strengths and weaknesses associated with VR-based user studies and proposes
future research directions, emphasising the necessity for novel VR interfaces
to overcome existing technical limitations.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15700" title="Abstract">arXiv:2402.15700</a> [<a href="/pdf/2402.15700" title="Download PDF">pdf</a>, <a href="/format/2402.15700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoRelation: Boosting Automatic ICD Coding Through Contextualized Code  Relation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+A">Aofei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-Coling 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Automatic International Classification of Diseases (ICD) coding plays a
crucial role in the extraction of relevant information from clinical notes for
proper recording and billing. One of the most important directions for boosting
the performance of automatic ICD coding is modeling ICD code relations.
However, current methods insufficiently model the intricate relationships among
ICD codes and often overlook the importance of context in clinical notes. In
this paper, we propose a novel approach, a contextualized and flexible
framework, to enhance the learning of ICD code representations. Our approach,
unlike existing methods, employs a dependent learning paradigm that considers
the context of clinical notes in modeling all possible code relations. We
evaluate our approach on six public ICD coding datasets and the experimental
results demonstrate the effectiveness of our approach compared to
state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15703" title="Abstract">arXiv:2402.15703</a> [<a href="/pdf/2402.15703" title="Download PDF">pdf</a>, <a href="/format/2402.15703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Offline Decision Making Possible with Only Few Samples? Reliable  Decisions in Data-Starved Bandits via Trust Region Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuexiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zanette%2C+A">Andrea Zanette</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from
a dataset that contains just a single sample for each arm? Surprisingly, in
this work, we demonstrate that even in such a data-starved setting it may still
be possible to find a policy competitive with the optimal one. This paves the
way to reliable decision-making in settings where critical decisions must be
made by relying only on a handful of samples.
<br />Our analysis reveals that \emph{stochastic policies can be substantially
better} than deterministic ones for offline decision-making. Focusing on
offline multi-armed bandits, we design an algorithm called Trust Region of
Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different
from the predominant value-based lower confidence bound approach. Its design is
enabled by localization laws, critical radii, and relative pessimism. We prove
that its sample complexity is comparable to that of LCB on minimax problems
while being substantially lower on problems with very few samples.
<br />Finally, we consider an application to offline reinforcement learning in the
special case where the logging policies are known.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15708" title="Abstract">arXiv:2402.15708</a> [<a href="/pdf/2402.15708" title="Download PDF">pdf</a>, <a href="/format/2402.15708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Augmentation by Decoding Semantics from Brain Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Ziyi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+J">Jingtao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Lioma%2C+C">Christina Lioma</a>, 
<a href="/search/cs?searchtype=author&query=Ruotsalo%2C+T">Tuukka Ruotsalo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Query augmentation is a crucial technique for refining semantically imprecise
queries. Traditionally, query augmentation relies on extracting information
from initially retrieved, potentially relevant documents. If the quality of the
initially retrieved documents is low, then the effectiveness of query
augmentation would be limited as well. We propose Brain-Aug, which enhances a
query by incorporating semantic information decoded from brain signals.
BrainAug generates the continuation of the original query with a prompt
constructed with brain signal information and a ranking-oriented inference
approach. Experimental results on fMRI (functional magnetic resonance imaging)
datasets show that Brain-Aug produces semantically more accurate queries,
leading to improved document ranking performance. Such improvement brought by
brain signals is particularly notable for ambiguous queries.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15710" title="Abstract">arXiv:2402.15710</a> [<a href="/pdf/2402.15710" title="Download PDF">pdf</a>, <a href="/format/2402.15710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Statistical Analysis of Wasserstein Autoencoders for Intrinsically  Low-dimensional Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saptarshi Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the twelfth International Conference on Learning Representations (ICLR'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Variational Autoencoders (VAEs) have gained significant popularity among
researchers as a powerful tool for understanding unknown distributions based on
limited samples. This popularity stems partly from their impressive performance
and partly from their ability to provide meaningful feature representations in
the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to
not only improve model efficiency but also interpretability. However, there has
been limited focus on analyzing their statistical guarantees. The matter is
further complicated by the fact that the data distributions to which WAEs are
applied - such as natural images - are often presumed to possess an underlying
low-dimensional structure within a high-dimensional feature space, which
current theory does not adequately account for, rendering known bounds
inefficient. To bridge the gap between the theory and practice of WAEs, in this
paper, we show that WAEs can learn the data distributions when the network
architectures are properly chosen. We show that the convergence rates of the
expected excess risk in the number of samples for WAEs are independent of the
high feature dimension, instead relying only on the intrinsic dimension of the
data distribution.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15713" title="Abstract">arXiv:2402.15713</a> [<a href="/pdf/2402.15713" title="Download PDF">pdf</a>, <a href="/format/2402.15713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Pre-trained Language Models Better Continual Few-Shot Relation  Extractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shengkun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiale Han</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bo Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as COLING2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Continual Few-shot Relation Extraction (CFRE) is a practical problem that
requires the model to continuously learn novel relations while avoiding
forgetting old ones with few labeled training data. The primary challenges are
catastrophic forgetting and overfitting. This paper harnesses prompt learning
to explore the implicit capabilities of pre-trained language models to address
the above two challenges, thereby making language models better continual
few-shot relation extractors. Specifically, we propose a Contrastive Prompt
Learning framework, which designs prompt representation to acquire more
generalized knowledge that can be easily adapted to old and new categories, and
margin-based contrastive learning to focus more on hard samples, therefore
alleviating catastrophic forgetting and overfitting issues. To further remedy
overfitting in low-resource scenarios, we introduce an effective memory
augmentation strategy that employs well-crafted prompts to guide ChatGPT in
generating diverse samples. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin and significantly
mitigates catastrophic forgetting and overfitting in low-resource scenarios.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15715" title="Abstract">arXiv:2402.15715</a> [<a href="/pdf/2402.15715" title="Download PDF">pdf</a>, <a href="/format/2402.15715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator Learning: Algorithms and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kovachki%2C+N+B">Nikola B. Kovachki</a>, 
<a href="/search/cs?searchtype=author&query=Lanthaler%2C+S">Samuel Lanthaler</a>, 
<a href="/search/cs?searchtype=author&query=Stuart%2C+A+M">Andrew M. Stuart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Operator learning refers to the application of ideas from machine learning to
approximate (typically nonlinear) operators mapping between Banach spaces of
functions. Such operators often arise from physical models expressed in terms
of partial differential equations (PDEs). In this context, such approximate
operators hold great potential as efficient surrogate models to complement
traditional numerical methods in many-query tasks. Being data-driven, they also
enable model discovery when a mathematical description in terms of a PDE is not
available. This review focuses primarily on neural operators, built on the
success of deep neural networks in the approximation of functions defined on
finite dimensional Euclidean spaces. Empirically, neural operators have shown
success in a variety of applications, but our theoretical understanding remains
incomplete. This review article summarizes recent progress and the current
state of our theoretical understanding of neural operators, focusing on an
approximation theoretic point of view.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15719" title="Abstract">arXiv:2402.15719</a> [<a href="/pdf/2402.15719" title="Download PDF">pdf</a>, <a href="/format/2402.15719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;It Is Hard to Remove from My Eye&quot;: Design Makeup Residue Visualization  System for Chinese Traditional Opera (Xiqu) Performers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zeyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shihan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingming Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Chinese traditional opera (Xiqu) performers often experience skin problems
due to the long-term use of heavy-metal-laden face paints. To explore the
current skincare challenges encountered by Xiqu performers, we conducted an
online survey (N=136) and semi-structured interviews (N=15) as a formative
study. We found that incomplete makeup removal is the leading cause of
human-induced skin problems, especially the difficulty in removing eye makeup.
Therefore, we proposed EyeVis, a prototype that can visualize the residual eye
makeup and record the time make-up was worn by Xiqu performers. We conducted a
7-day deployment study (N=12) to evaluate EyeVis. Results indicate that EyeVis
helps to increase Xiqu performers' awareness about removing makeup, as well as
boosting their confidence and security in skincare. Overall, this work also
provides implications for studying the work of people who wear makeup on a
daily basis, and helps to promote and preserve the intangible cultural heritage
of practitioners.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15721" title="Abstract">arXiv:2402.15721</a> [<a href="/pdf/2402.15721" title="Download PDF">pdf</a>, <a href="/format/2402.15721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hal-Eval: A Universal and Fine-grained Hallucination Evaluation  Framework for Large Vision Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Mengfan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Hongrui Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Vision Language Models exhibit remarkable capabilities but struggle
with hallucinations inconsistencies between images and their descriptions.
Previous hallucination evaluation studies on LVLMs have identified
hallucinations in terms of objects, attributes, and relations but overlooked
complex hallucinations that create an entire narrative around a fictional
entity. In this paper, we introduce a refined taxonomy of hallucinations,
featuring a new category: Event Hallucination. We then utilize advanced LLMs to
generate and filter fine grained hallucinatory data consisting of various types
of hallucinations, with a particular focus on event hallucinations, laying the
groundwork for integrating discriminative and generative evaluation methods
within our universal evaluation framework. The proposed benchmark distinctively
assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it
a reliable and comprehensive tool for gauging LVLMs efficacy in handling
hallucinations. We will release our code and data.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15723" title="Abstract">arXiv:2402.15723</a> [<a href="/pdf/2402.15723" title="Download PDF">pdf</a>, <a href="/format/2402.15723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision  People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery  Mechanism, and AR-based Search Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Zhitong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zeyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingming Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Parcel lockers have become an increasingly prevalent last-mile delivery
method. Yet, a recent study revealed its accessibility challenges to blind and
low-vision people (BLV). Informed by the study, we designed FetchAid, a
standalone intelligent mobile app assisting BLV in using a parcel locker in
real-time by integrating computer vision and augmented reality (AR)
technologies. FetchAid first uses a deep network to detect the user's fingertip
and relevant buttons on the touch screen of the parcel locker to guide the user
to reveal and scan the QR code to open the target compartment door and then
guide the user to reach the door safely with AR-based context-aware audio
feedback. Moreover, FetchAid provides an error-recovery mechanism and real-time
feedback to keep the user on track. We show that FetchAid substantially
improved task accomplishment and efficiency, and reduced frustration and
overall effort in a study with 12 BLV participants, regardless of their vision
conditions and previous experience.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15726" title="Abstract">arXiv:2402.15726</a> [<a href="/pdf/2402.15726" title="Download PDF">pdf</a>, <a href="/format/2402.15726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIPose: Category-Level Object Pose Estimation with Pre-trained  Vision-Language Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+R">Ronghao Dang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guangliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+S">Shaolong Shu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Feng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most of existing category-level object pose estimation methods devote to
learning the object category information from point cloud modality. However,
the scale of 3D datasets is limited due to the high cost of 3D data collection
and annotation. Consequently, the category features extracted from these
limited point cloud samples may not be comprehensive. This motivates us to
investigate whether we can draw on knowledge of other modalities to obtain
category information. Inspired by this motivation, we propose CLIPose, a novel
6D pose framework that employs the pre-trained vision-language model to develop
better learning of object category information, which can fully leverage
abundant semantic knowledge in image and text modalities. To make the 3D
encoder learn category-specific features more efficiently, we align
representations of three modalities in feature space via multi-modal
contrastive learning. In addition to exploiting the pre-trained knowledge of
the CLIP's model, we also expect it to be more sensitive with pose parameters.
Therefore, we introduce a prompt tuning approach to fine-tune image encoder
while we incorporate rotations and translations information in the text
descriptions. CLIPose achieves state-of-the-art performance on two mainstream
benchmark datasets, REAL275 and CAMERA25, and runs in real-time during
inference (40FPS).
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15727" title="Abstract">arXiv:2402.15727</a> [<a href="/pdf/2402.15727" title="Download PDF">pdf</a>, <a href="/format/2402.15727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A  Vision Paper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Daoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a vision paper on defending against LLM jailbreaks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Jailbreaking is an emerging adversarial attack that bypasses the safety
alignment deployed in off-the-shelf large language models (LLMs). A
considerable amount of research exists proposing more effective jailbreak
attacks, including the recent Greedy Coordinate Gradient (GCG) attack,
jailbreak template-based attacks such as using "Do-Anything-Now" (DAN), and
multilingual jailbreak. In contrast, the defensive side has been relatively
less explored. This paper proposes a lightweight yet practical defense called
SELFDEFEND, which can defend against all existing jailbreak attacks with
minimal delay for jailbreak prompts and negligible delay for normal user
prompts. Our key insight is that regardless of the kind of jailbreak strategies
employed, they eventually need to include a harmful prompt (e.g., "how to make
a bomb") in the prompt sent to LLMs, and we found that existing LLMs can
effectively recognize such harmful prompts that violate their safety policies.
Based on this insight, we design a shadow stack that concurrently checks
whether a harmful prompt exists in the user prompt and triggers a checkpoint in
the normal stack once a token of "No" or a harmful prompt is output. The latter
could also generate an explainable LLM response to adversarial prompts. We
demonstrate our idea of SELFDEFEND works in various jailbreak scenarios through
manual analysis in GPT-3.5/4. We also list three future directions to further
enhance SELFDEFEND.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15728" title="Abstract">arXiv:2402.15728</a> [<a href="/pdf/2402.15728" title="Download PDF">pdf</a>, <a href="/ps/2402.15728" title="Download PostScript">ps</a>, <a href="/format/2402.15728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Implementation of Low-Cost Electric Vehicles (Evs)  Supercharger: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rahman%2C+M+K">Md Khaledur Rahman</a>, 
<a href="/search/eess?searchtype=author&query=Tanvir%2C+F+A">Faysal Amin Tanvir</a>, 
<a href="/search/eess?searchtype=author&query=Islam%2C+M+S">Md Saiful Islam</a>, 
<a href="/search/eess?searchtype=author&query=Ahsan%2C+M+S">Md Shameem Ahsan</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+M">Manam Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This article presents a probabilistic modeling method utilizing smart meter
data and an innovative agent-based simulator for electric vehicles (EVs). The
aim is to assess the effects of different cost-driven EV charging strategies on
the power distribution network (PDN). We investigate the effects of a 40% EV
adoption on three parts of Frederiksberg's low voltage distribution network
(LVDN), a densely urbanized municipality in Denmark. Our findings indicate that
cable and transformer overloading especially pose a challenge. However, the
impact of EVs varies significantly between each LVDN area and charging
scenario. Across scenarios and LVDNs, the share of cables facing congestion
ranges between 5% and 60%. It is also revealed that time-of-use (ToU)-based and
single-day cost-minimized charging could be beneficial for LVDNs with moderate
EV adoption rates. In contrast, multiple-day optimization will likely lead to
severe congestion, as such strategies concentrate demand on a single day that
would otherwise be distributed over several days, thus raising concerns about
how to prevent it. The broader implications of our research suggest that,
despite initial worries primarily centered on congestion due to unregulated
charging during peak hours, a transition to cost-based smart charging,
propelled by an increasing awareness of time-dependent electricity prices, may
lead to a significant rise in charging synchronization, bringing about
undesirable consequences for the power distribution network (PDN).
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15729" title="Abstract">arXiv:2402.15729</a> [<a href="/pdf/2402.15729" title="Download PDF">pdf</a>, <a href="/format/2402.15729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Do Humans Write Code? Large Models Do It the Same Way Too
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Long Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">Large Language Models (LLMs) often make errors when performing numerical
calculations. In contrast to traditional chain-of-thought reasoning, the
program-of-thoughts approach involves generating executable code to solve
problems. By executing this code, it achieves more precise results. Using
generated executable code instead of natural language can reduce computational
errors. However, we observe that when LLMs solve mathematical problems using
code, they tend to generate more incorrect reasoning than when using natural
language. To address this issue, we propose Human-Think Language (HTL), a
straightforward yet highly efficient approach inspired by human coding
practices. The approach first generates problem-solving methods described in
the natural language by the model, then converts them into code, mirroring the
process where people think through the logic in natural language before writing
it as code. Additionally, it utilizes the Proximal Policy Optimization (PPO)
algorithm, enabling it to provide feedback to itself based on the correctness
of mathematical answers, much like humans do. Finally, we introduce a
focus-attention mechanism that masks the question segment, enhancing its
reliance on natural language inference solutions during code generation. We
conduct our experiments without introducing any additional information, and the
results across five mathematical calculation datasets showcase the
effectiveness of our approach. Notably, on the NumGLUE dataset, the
LlaMA-2-7B-based model achieves a superior performance rate (75.1%) compared to
the previous best performance with the LlaMA-2-70B model (74.4%).
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15730" title="Abstract">arXiv:2402.15730</a> [<a href="/pdf/2402.15730" title="Download PDF">pdf</a>, <a href="/format/2402.15730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Missingness in Time-series Electronic Health Records for  Individualized Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosheh%2C+G+O">Ghadeer O. Ghosheh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tingting Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the widespread of machine learning models for healthcare applications,
there is increased interest in building applications for personalized medicine.
Despite the plethora of proposed research for personalized medicine, very few
focus on representing missingness and learning from the missingness patterns in
time-series Electronic Health Records (EHR) data. The lack of focus on
missingness representation in an individualized way limits the full utilization
of machine learning applications towards true personalization. In this brief
communication, we highlight new insights into patterns of missingness with
real-world examples and implications of missingness in EHRs. The insights in
this work aim to bridge the gap between theoretical assumptions and practical
observations in real-world EHRs. We hope this work will open new doors for
exploring directions for better representation in predictive modelling for true
personalization.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15731" title="Abstract">arXiv:2402.15731</a> [<a href="/pdf/2402.15731" title="Download PDF">pdf</a>, <a href="/ps/2402.15731" title="Download PostScript">ps</a>, <a href="/format/2402.15731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering in Dynamic Environments: A Framework for Benchmark Dataset  Generation With Heterogeneous Changes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yazdani%2C+D">Danial Yazdani</a>, 
<a href="/search/cs?searchtype=author&query=Branke%2C+J">Juergen Branke</a>, 
<a href="/search/cs?searchtype=author&query=Khorshidi%2C+M+S">Mohammad Sadegh Khorshidi</a>, 
<a href="/search/cs?searchtype=author&query=Omidvar%2C+M+N">Mohammad Nabi Omidvar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaodong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gandomi%2C+A+H">Amir H. Gandomi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xin Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Clustering in dynamic environments is of increasing importance, with broad
applications ranging from real-time data analysis and online unsupervised
learning to dynamic facility location problems. While meta-heuristics have
shown promising effectiveness in static clustering tasks, their application for
tracking optimal clustering solutions or robust clustering over time in dynamic
environments remains largely underexplored. This is partly due to a lack of
dynamic datasets with diverse, controllable, and realistic dynamic
characteristics, hindering systematic performance evaluations of clustering
algorithms in various dynamic scenarios. This deficiency leads to a gap in our
understanding and capability to effectively design algorithms for clustering in
dynamic environments. To bridge this gap, this paper introduces the Dynamic
Dataset Generator (DDG). DDG features multiple dynamic Gaussian components
integrated with a range of heterogeneous, local, and global changes. These
changes vary in spatial and temporal severity, patterns, and domain of
influence, providing a comprehensive tool for simulating a wide range of
dynamic scenarios.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15733" title="Abstract">arXiv:2402.15733</a> [<a href="/pdf/2402.15733" title="Download PDF">pdf</a>, <a href="/ps/2402.15733" title="Download PostScript">ps</a>, <a href="/format/2402.15733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for  Arabic Characters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darwish%2C+H">Hazem Darwish</a>, 
<a href="/search/cs?searchtype=author&query=Malah%2C+A+A">Abdalrahman Al Malah</a>, 
<a href="/search/cs?searchtype=author&query=Jallad%2C+K+A">Khloud Al Jallad</a>, 
<a href="/search/cs?searchtype=author&query=Ghneim%2C+N">Nada Ghneim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Brain-Computer-Interface (BCI) has been a hot research topic in the last few
years that could help paralyzed people in their lives. Several researches were
done to classify electroencephalography (EEG) signals automatically into
English characters and words. Arabic language is one of the most used languages
around the world. However, to the best of our knowledge, there is no dataset
for Arabic characters EEG signals. In this paper, we have created an EEG
dataset for Arabic characters and named it ArEEG_Chars. Moreover, several
experiments were done on ArEEG_Chars using deep learning. Best results were
achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be
public for researchers.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15734" title="Abstract">arXiv:2402.15734</a> [<a href="/pdf/2402.15734" title="Download PDF">pdf</a>, <a href="/format/2402.15734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Efficient Operator Learning via Unsupervised Pretraining and  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wuyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jialin Song</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Shashank Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+D">Dmitriy Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent years have witnessed the promise of coupling machine learning methods
and physical domain-specific insight for solving scientific problems based on
partial differential equations (PDEs). However, being data-intensive, these
methods still require a large amount of PDE data. This reintroduces the need
for expensive numerical PDE solutions, partially undermining the original goal
of avoiding these expensive simulations. In this work, seeking data efficiency,
we design unsupervised pretraining and in-context learning methods for PDE
operator learning. To reduce the need for training data with simulated
solutions, we pretrain neural operators on unlabeled PDE data using
reconstruction-based proxy tasks. To improve out-of-distribution performance,
we further assist neural operators in flexibly leveraging in-context learning
methods, without incurring extra training costs or designs. Extensive empirical
evaluations on a diverse set of PDEs demonstrate that our method is highly
data-efficient, more generalizable, and even outperforms conventional
vision-pretrained models.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15738" title="Abstract">arXiv:2402.15738</a> [<a href="/pdf/2402.15738" title="Download PDF">pdf</a>, <a href="/format/2402.15738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xinhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guanzhong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+D+E">Daniel E. Quevedo</a>, 
<a href="/search/cs?searchtype=author&query=Murguia%2C+C">Carlos Murguia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hailong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Networked systems are increasingly the target of cyberattacks that exploit
vulnerabilities within digital communications, embedded hardware, and software.
Arguably, the simplest class of attacks -- and often the first type before
launching destructive integrity attacks -- are eavesdropping attacks, which aim
to infer information by collecting system data and exploiting it for malicious
purposes. A key technology of networked systems is state estimation, which
leverages sensing and actuation data and first-principles models to enable
trajectory planning, real-time monitoring, and control. However, state
estimation can also be exploited by eavesdroppers to identify models and
reconstruct states with the aim of, e.g., launching integrity (stealthy)
attacks and inferring sensitive information. It is therefore crucial to protect
disclosed system data to avoid an accurate state estimation by eavesdroppers.
This survey presents a comprehensive review of existing literature on
privacy-preserving state estimation methods, while also identifying potential
limitations and research gaps. Our primary focus revolves around three types of
methods: cryptography, data perturbation, and transmission scheduling, with
particular emphasis on Kalman-like filters. Within these categories, we delve
into the concepts of homomorphic encryption and differential privacy, which
have been extensively investigated in recent years in the context of
privacy-preserving state estimation. Finally, we shed light on several
technical and fundamental challenges surrounding current methods and propose
potential directions for future research.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15739" title="Abstract">arXiv:2402.15739</a> [<a href="/pdf/2402.15739" title="Download PDF">pdf</a>, <a href="/format/2402.15739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jedra%2C+Y">Yassir Jedra</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9veillard%2C+W">William R&#xe9;veillard</a>, 
<a href="/search/cs?searchtype=author&query=Stojanovic%2C+S">Stefan Stojanovic</a>, 
<a href="/search/cs?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study contextual bandits with low-rank structure where, in each round, if
the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner
observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward
matrix. Successive contexts are generated randomly in an i.i.d. manner and are
revealed to the learner. For such bandits, we present efficient algorithms for
policy evaluation, best policy identification and regret minimization. For
policy evaluation and best policy identification, we show that our algorithms
are nearly minimax optimal. For instance, the number of samples required to
return an $\varepsilon$-optimal policy with probability at least $1-\delta$
typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret
minimization algorithm enjoys minimax guarantees scaling as
$r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the
proposed algorithms consist of two phases: they first leverage spectral methods
to estimate the left and right singular subspaces of the low-rank reward
matrix. We show that these estimates enjoy tight error guarantees in the
two-to-infinity norm. This in turn allows us to reformulate our problems as a
misspecified linear bandit problem with dimension roughly $r(m+n)$ and
misspecification controlled by the subspace recovery error, as well as to
design the second phase of our algorithms efficiently.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15745" title="Abstract">arXiv:2402.15745</a> [<a href="/pdf/2402.15745" title="Download PDF">pdf</a>, <a href="/format/2402.15745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yi Zong</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The Large Vision-Language Models (LVLMs) have demonstrated great abilities in
image perception and language understanding. However, existing multimodal
benchmarks focus on primary perception abilities and commonsense knowledge
which are insufficient to reflect the comprehensive capabilities of LVLMs. We
propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance
Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as
diagrams, function graphs, maps and photos. GAOKAO-MM derives from native
Chinese context and sets human-level requirements for the model's abilities,
including perception, understanding, knowledge and reasoning. We evaluate 10
LVLMs and find that the accuracies of all of them are lower than 50%, with
GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking
in the top three positions. The results of our multi-dimension analysis
indicate that LVLMs have moderate distance towards Artificial General
Intelligence (AGI) and provide insights facilitating the development of
multilingual LVLMs.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15746" title="Abstract">arXiv:2402.15746</a> [<a href="/pdf/2402.15746" title="Download PDF">pdf</a>, <a href="/format/2402.15746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Director: An Automatic Framework for Dynamic Visual  Composition using ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sixiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+J">Jingyang Huo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://sixiaozheng.github.io/IntelligentDirector/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">With the rise of short video platforms represented by TikTok, the trend of
users expressing their creativity through photos and videos has increased
dramatically. However, ordinary users lack the professional skills to produce
high-quality videos using professional creation software. To meet the demand
for intelligent and user-friendly video creation tools, we propose the Dynamic
Visual Composition (DVC) task, an interesting and challenging task that aims to
automatically integrate various media elements based on user requirements and
create storytelling videos. We propose an Intelligent Director framework,
utilizing LENS to generate descriptions for images and video frames and
combining ChatGPT to generate coherent captions while recommending appropriate
music names. Then, the best-matched music is obtained through music retrieval.
Then, materials such as captions, images, videos, and music are integrated to
seamlessly synthesize the video. Finally, we apply AnimeGANv2 for style
transfer. We construct UCF101-DVC and Personal Album datasets and verified the
effectiveness of our framework in solving DVC through qualitative and
quantitative comparisons, along with user studies, demonstrating its
substantial potential.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15749" title="Abstract">arXiv:2402.15749</a> [<a href="/pdf/2402.15749" title="Download PDF">pdf</a>, <a href="/format/2402.15749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A summary of the routing algorithm and their optimization,performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xunchi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This essay provides a comprehensive analysis of the optimization and
performance evaluation of various routing algorithms within the context of
computer networks. Routing algorithms are critical for determining the most
efficient path for data transmission between nodes in a network. The
efficiency, reliability, and scalability of a network heavily rely on the
choice and optimization of its routing algorithm. This paper begins with an
overview of fundamental routing strategies, including shortest path, flooding,
distance vector, and link state algorithms, and extends to more sophisticated
techniques.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15750" title="Abstract">arXiv:2402.15750</a> [<a href="/pdf/2402.15750" title="Download PDF">pdf</a>, <a href="/format/2402.15750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design, Implementation and Analysis of a Compressed Sensing  Photoacoustic Projection Imaging System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Matthias Ye</a>, 
<a href="/search/cs?searchtype=author&query=Felbermayer%2C+K">Karoline Felbermayer</a>, 
<a href="/search/cs?searchtype=author&query=Hinterleitner%2C+F">Florian Hinterleitner</a>, 
<a href="/search/cs?searchtype=author&query=Burgholzer%2C+P">Peter Burgholzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Numerical Analysis (math.NA); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Significance: Compressed sensing (CS) uses special measurement designs
combined with powerful mathematical algorithms to reduce the amount of data to
be collected while maintaining image quality. This is relevant to almost any
imaging modality, and in this paper we focus on CS in photoacoustic projection
imaging (PAPI) with integrating line detectors (ILDs).
<br />Aim: Our previous research involved rather general CS measurements, where
each ILD can contribute to any measurement. In the real world, however, the
design of CS measurements is subject to practical constraints. In this
research, we aim at a CS-PAPI system where each measurement involves only a
subset of ILDs, and which can be implemented in a cost-effective manner.
<br />Approach: We extend the existing PAPI with a self-developed CS unit. The
system provides structured CS matrices for which the existing recovery theory
cannot be applied directly. A random search strategy is applied to select the
CS measurement matrix within this class for which we obtain exact sparse
recovery.
<br />Results: We implement a CS PAPI system for a compression factor of $4:3$,
where specific measurements are made on separate groups of 16 ILDs. We
algorithmically design optimal CS measurements that have proven sparse CS
capabilities. Numerical experiments are used to support our results.
<br />Conclusions: CS with proven sparse recovery capabilities can be integrated
into PAPI, and numerical results support this setup. Future work will focus on
applying it to experimental data and utilizing data-driven approaches to
enhance the compression factor and generalize the signal class.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15751" title="Abstract">arXiv:2402.15751</a> [<a href="/pdf/2402.15751" title="Download PDF">pdf</a>, <a href="/format/2402.15751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM  Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zirui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chaoyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">While fine-tuning large language models (LLMs) for specific tasks often
yields impressive results, it comes at the cost of memory inefficiency due to
back-propagation in gradient-based training. Memory-efficient Zeroth-order
(MeZO) optimizers, recently proposed to address this issue, only require
forward passes during training, making them more memory-friendly. However, the
quality of gradient estimates in zeroth order optimization often depends on the
data dimensionality, potentially explaining why MeZO still exhibits significant
performance drops compared to standard fine-tuning across various tasks.
Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper
introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization
approach that applies ZO only to a carefully chosen subset of parameters. We
propose a simple yet effective parameter selection scheme that yields
significant performance gains with Sparse-MeZO. Additionally, we develop a
memory-optimized implementation for sparse masking, ensuring the algorithm
requires only inference-level memory consumption, allowing Sparse-MeZO to
fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that
Sparse-MeZO consistently improves both performance and convergence speed over
MeZO without any overhead. For example, it achieves a 9\% absolute accuracy
improvement and 3.5x speedup over MeZO on the RTE task.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15754" title="Abstract">arXiv:2402.15754</a> [<a href="/pdf/2402.15754" title="Download PDF">pdf</a>, <a href="/format/2402.15754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical  Criteria Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haizhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Feng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have emerged as a promising alternative to
expensive human evaluations. However, the alignment and coverage of LLM-based
evaluations are often limited by the scope and potential bias of the evaluation
prompts and criteria. To address this challenge, we propose HD-Eval, a novel
framework that iteratively aligns LLM-based evaluators with human preference
via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the
evaluation mindset of human experts and enhances the alignment of LLM-based
evaluators by decomposing a given evaluation task into finer-grained criteria,
aggregating them according to estimated human preferences, pruning
insignificant criteria with attribution, and further decomposing significant
criteria. By integrating these steps within an iterative alignment training
process, we obtain a hierarchical decomposition of criteria that
comprehensively captures aspects of natural language at multiple levels of
granularity. Implemented as a white box, the human preference-guided aggregator
is efficient to train and more explainable than relying solely on prompting,
and its independence from model parameters makes it applicable to closed-source
LLMs. Extensive experiments on three evaluation domains demonstrate the
superiority of HD-Eval in further aligning state-of-the-art evaluators and
providing deeper insights into the explanation of evaluation results and the
task itself.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15755" title="Abstract">arXiv:2402.15755</a> [<a href="/pdf/2402.15755" title="Download PDF">pdf</a>, <a href="/ps/2402.15755" title="Download PostScript">ps</a>, <a href="/format/2402.15755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dental Severity Assessment through Few-shot Learning and SBERT  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Mohammad Dehghani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dental diseases have a significant impact on a considerable portion of the
population, leading to various health issues that can detrimentally affect
individuals' overall well-being. The integration of automated systems in oral
healthcare has become increasingly crucial. Machine learning approaches offer a
viable solution to address challenges such as diagnostic difficulties,
inefficiencies, and errors in oral disease diagnosis. These methods prove
particularly useful when physicians struggle to predict or diagnose diseases at
their early stages. In this study, thirteen different machine learning, deep
learning, and large language models were employed to determine the severity
level of oral health issues based on radiologists' reports. The results
revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model
outperformed all other models across various experiments, achieving an
impressive accuracy of 94.1% as the best result. Consequently, this model
exhibits promise as a reliable tool for evaluating the severity of oral
diseases, enabling patients to receive more effective treatment and aiding
healthcare professionals in making informed decisions regarding resource
allocation and the management of high-risk patients.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15756" title="Abstract">arXiv:2402.15756</a> [<a href="/pdf/2402.15756" title="Download PDF">pdf</a>, <a href="/format/2402.15756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models  Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lingji Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Conventional tracking paradigm takes in instantaneous measurements such as
range and bearing, and produces object tracks across time. In applications such
as autonomous driving, lidar measurements in the form of point clouds are
usually passed through a "virtual sensor" realized by a deep learning model, to
produce "measurements" such as bounding boxes, which are in turn ingested by a
tracking module to produce object tracks. Very often multiple lidar sweeps are
accumulated in a buffer to merge and become the input to the virtual sensor. We
argue in this paper that such an input already contains temporal information,
and therefore the virtual sensor output should also contain temporal
information, not just instantaneous values for the time corresponding to the
end of the buffer. In particular, we present the deep learning model called
MULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,
a pair of bounding boxes at both the end time and the beginning time of the
input buffer. This is achieved with fairly straightforward changes in commonly
used lidar detection models, and with only marginal extra processing, but the
resulting symmetry is satisfying. Such paired detections make it possible not
only to construct rudimentary trackers fairly easily, but also to construct
more sophisticated trackers that can exploit the extra information conveyed by
the pair and be robust to choices of motion models and object birth/death
models. We have conducted preliminary training and experimentation using Waymo
Open Dataset, which shows the efficacy of our proposed method.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15757" title="Abstract">arXiv:2402.15757</a> [<a href="/pdf/2402.15757" title="Download PDF">pdf</a>, <a href="/format/2402.15757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch Active Learning of Reward Functions from Human Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C4%B1y%C4%B1k%2C+E">Erdem B&#x131;y&#x131;k</a>, 
<a href="/search/cs?searchtype=author&query=Anari%2C+N">Nima Anari</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ACM Transactions on Human-Robot Interaction (THRI). 27 pages, 12 figures, 2 tables. arXiv admin note: text overlap with <a href="/abs/1810.04303">arXiv:1810.04303</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Data generation and labeling are often expensive in robot learning.
Preference-based learning is a concept that enables reliable labeling by
querying users with preference questions. Active querying methods are commonly
employed in preference-based learning to generate more informative data at the
expense of parallelization and computation time. In this paper, we develop a
set of novel algorithms, batch active preference-based learning methods, that
enable efficient learning of reward functions using as few data samples as
possible while still having short query generation times and also retaining
parallelizability. We introduce a method based on determinantal point processes
(DPP) for active batch generation and several heuristic-based alternatives.
Finally, we present our experimental results for a variety of robotics tasks in
simulation. Our results suggest that our batch active learning algorithm
requires only a few queries that are computed in a short amount of time. We
showcase one of our algorithms in a study to learn human users' preferences.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15758" title="Abstract">arXiv:2402.15758</a> [<a href="/pdf/2402.15758" title="Download PDF">pdf</a>, <a href="/format/2402.15758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chimera: A Lossless Decoding Method for Accelerating Large Language  Models Inference by Fusing all Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziqian Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiahong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Q">Qianshi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Huiping Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable capabilities across
various tasks. However, their widespread application is hindered by the
resource-intensive decoding process. To address this challenge, current
approaches have incorporated additional decoding heads to enable parallel
prediction of multiple subsequent tokens, thereby achieving inference
acceleration. Nevertheless, the accuracy of these decoding heads falls short of
the auto-regressive decoding approach.
<br />In light of these limitations, we propose Chimera, a novel framework
specifically designed for speculative sampling. Within this framework, we
introduce a lightweight draft model that effectively utilizes previously
generated tokens to predict subsequent words. To ensure both accuracy and
efficiency, we present two strategies within the lightweight draft model.
Firstly, we focus on capturing short-range dependencies at the bottom layer.
Secondly, we leverage the readily available representations from the original
LLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera
demonstrates impressive results, achieving an average latency speedup ratio of
2.7x compared to the vanilla auto-regressive decoding approach. This highlights
the potential of our proposed framework in significantly improving the
efficiency of large language models during the decoding process.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15759" title="Abstract">arXiv:2402.15759</a> [<a href="/pdf/2402.15759" title="Download PDF">pdf</a>, <a href="/ps/2402.15759" title="Download PostScript">ps</a>, <a href="/format/2402.15759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using  GPT-4 Generated Descriptive Prompts Without Human Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zekun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dongjie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Ziyuan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Q">Qicheng Lao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Le Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study develops and evaluates a novel multimodal medical image zero-shot
segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual
annotations. TV-SAM incorporates and integrates large language model GPT-4,
Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously
generate descriptive text prompts and visual bounding box prompts from medical
images, thereby enhancing SAM for zero-shot segmentation. Comprehensive
evaluations are implemented on seven public datasets encompassing eight imaging
modalities to demonstrate that TV-SAM can effectively segment unseen targets
across various modalities without additional training, significantly
outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX
with gold standard bounding box prompts, and surpassing the state-of-the-art on
specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as
an effective multimodal medical image zero-shot segmentation algorithm,
highlighting the significant contribution of GPT-4 to zero-shot segmentation.
By integrating foundational models such as GPT-4, GLIP, and SAM, it could
enhance the capability to address complex problems in specialized domains. The
code is available at: https://github.com/JZK00/TV-SAM.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15761" title="Abstract">arXiv:2402.15761</a> [<a href="/pdf/2402.15761" title="Download PDF">pdf</a>, <a href="/format/2402.15761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Res-VMamba: Fine-Grained Food Category Visual Classification Using  Selective State Space Models with Deep Residual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chi-Sheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guan-Ying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Di Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dai-Shi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Food classification is the foundation for developing food vision tasks and
plays a key role in the burgeoning field of computational nutrition. Due to the
complexity of food requiring fine-grained classification, recent academic
research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision
Transformers (ViTs) to perform food category classification. However, to learn
fine-grained features, the CNN backbone needs additional structural design,
whereas ViT, containing the self-attention module, has increased computational
complexity. In recent months, a new Sequence State Space (S4) model, through a
Selection mechanism and computation with a Scan (S6), colloquially termed
Mamba, has demonstrated superior performance and computation efficiency
compared to the Transformer architecture. The VMamba model, which incorporates
the Mamba mechanism into image tasks (such as classification), currently
establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this
research, we introduce an academically underestimated food dataset CNFOOD-241,
and pioneer the integration of a residual learning framework within the VMamba
model to concurrently harness both global and local state features inherent in
the original VMamba architectural design. The research results show that VMamba
surpasses current SOTA models in fine-grained and food classification. The
proposed Res-VMamba further improves the classification accuracy to 79.54\%
without pretrained weight. Our findings elucidate that our proposed methodology
establishes a new benchmark for SOTA performance in food recognition on the
CNFOOD-241 dataset. The code can be obtained on GitHub:
https://github.com/ChiShengChen/ResVMamba.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15764" title="Abstract">arXiv:2402.15764</a> [<a href="/pdf/2402.15764" title="Download PDF">pdf</a>, <a href="/format/2402.15764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look Before You Leap: Problem Elaboration Prompting Improves  Mathematical Reasoning in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Haoran Liao</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jidong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shaohua Hu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hao He</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaohui Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models~(LLMs) have exhibited impressive performance across NLP
tasks. So far they still face challenges in complex reasoning tasks and can be
sensitive to input context. Despite significant efforts have been invested in
enhancing reasoning process and improving prefix-prompts robustness, the
crucial role of problem context has been overlooked. In this study, we propose
a new approach to improve the mathematical capacities of LLMs, named Problem
Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the
problem context before reasoning, thus enhancing the global context modeling
and reducing the parsing difficulties. Experiments on datasets demonstrate
promising performances on complex reasoning and indicate the beneficial impact
for ill-formed problems. For instance, with the GPT-3.5
model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy
decoding and 8.80\% improvement with self-consistency on GSM8k compared to the
standard CoT. With ChatGPT~(\texttt{turbo}) and PEP, we achieve SOTA
performances on SVAMP with 86.2\% and GSM8k with 90.98\%.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15767" title="Abstract">arXiv:2402.15767</a> [<a href="/pdf/2402.15767" title="Download PDF">pdf</a>, <a href="/ps/2402.15767" title="Download PostScript">ps</a>, <a href="/format/2402.15767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhyPlan: Compositional and Adaptive Physical Task Reasoning with  Physics-Informed Skill Networks for Robot Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vagadia%2C+H">Harshil Vagadia</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+M">Mudit Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Barnawal%2C+A">Abhinav Barnawal</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+T">Tamajit Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Tuli%2C+S">Shreshth Tuli</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souvik Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+R">Rohan Paul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given the task of positioning a ball-like object to a goal region beyond
direct reach, humans can often throw, slide, or rebound objects against the
wall to attain the goal. However, enabling robots to reason similarly is
non-trivial. Existing methods for physical reasoning are data-hungry and
struggle with complexity and uncertainty inherent in the real world. This paper
presents PhyPlan, a novel physics-informed planning framework that combines
physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search
(MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan
leverages PINNs to simulate and predict outcomes of actions in a fast and
accurate manner and uses MCTS for planning. It dynamically determines whether
to consult a PINN-based simulator (coarse but fast) or engage directly with the
actual environment (fine but slow) to determine optimal policy. Evaluation with
robots in simulated 3D environments demonstrates the ability of our approach to
solve 3D-physical reasoning tasks involving the composition of dynamic skills.
Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret
when learning novel tasks compared to state-of-the-art, (ii) it expedites skill
learning and enhances the speed of physical reasoning, (iii) it demonstrates
higher data efficiency compared to a physics un-informed approach.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15769" title="Abstract">arXiv:2402.15769</a> [<a href="/pdf/2402.15769" title="Download PDF">pdf</a>, <a href="/format/2402.15769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Importance Guided Data Augmentation for Neural-Based Code Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zeming Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaofei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cordy%2C+M">Maxime Cordy</a>, 
<a href="/search/cs?searchtype=author&query=Papadakis%2C+M">Mike Papadakis</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianjun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained code models lead the era of code intelligence. Many models have
been designed with impressive performance recently. However, one important
problem, data augmentation for code data that automatically helps developers
prepare training data lacks study in the field of code learning. In this paper,
we introduce a general data augmentation framework, GenCode, to enhance the
training of code understanding models. GenCode follows a
generation-and-selection paradigm to prepare useful training codes.
Specifically, it uses code transformation techniques to generate new code
candidates first and then selects important ones as the training data by
importance metrics. To evaluate the effectiveness of GenCode with a general
importance metric -- loss value, we conduct experiments on four code
understanding tasks (e.g., code clone detection) and three pre-trained code
models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code
augmentation method, MixCode, GenCode produces code models with 2.92% higher
accuracy and 4.90% robustness on average.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15770" title="Abstract">arXiv:2402.15770</a> [<a href="/pdf/2402.15770" title="Download PDF">pdf</a>, <a href="/format/2402.15770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for  Opportunities, Risks, and Regulatory Compliance in Commercializing Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McIntosh%2C+T+R">Timothy R. McIntosh</a>, 
<a href="/search/cs?searchtype=author&query=Susnjak%2C+T">Teo Susnjak</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Watters%2C+P">Paul Watters</a>, 
<a href="/search/cs?searchtype=author&query=Nowrozy%2C+R">Raza Nowrozy</a>, 
<a href="/search/cs?searchtype=author&query=Halgamuge%2C+M+N">Malka N. Halgamuge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study investigated the integration readiness of four predominant
cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,
COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the
opportunities, risks, and regulatory compliance when adopting Large Language
Models (LLMs), using qualitative content analysis and expert validation. Our
analysis, with both LLMs and human experts in the loop, uncovered potential for
LLM integration together with inadequacies in LLM risk oversight of those
frameworks. Comparative gap analysis has highlighted that the new ISO
42001:2023, specifically designed for Artificial Intelligence (AI) management
systems, provided most comprehensive facilitation for LLM opportunities,
whereas COBIT 2019 aligned most closely with the impending European Union AI
Act. Nonetheless, our findings suggested that all evaluated frameworks would
benefit from enhancements to more effectively and more comprehensively address
the multifaceted risks associated with LLMs, indicating a critical and
time-sensitive need for their continuous evolution. We propose integrating
human-expert-in-the-loop validation processes as crucial for enhancing
cybersecurity frameworks to support secure and compliant LLM integration, and
discuss implications for the continuous evolution of cybersecurity GRC
frameworks to support the secure integration of LLMs.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15773" title="Abstract">arXiv:2402.15773</a> [<a href="/pdf/2402.15773" title="Download PDF">pdf</a>, <a href="/format/2402.15773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance bottlenecks detection through microarchitectural sensitivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pompougnac%2C+H">Hugo Pompougnac</a>, 
<a href="/search/cs?searchtype=author&query=Dutilleul%2C+A">Alban Dutilleul</a>, 
<a href="/search/cs?searchtype=author&query=Guillon%2C+C">Christophe Guillon</a>, 
<a href="/search/cs?searchtype=author&query=Derumigny%2C+N">Nicolas Derumigny</a>, 
<a href="/search/cs?searchtype=author&query=Rastello%2C+F">Fabrice Rastello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">Modern Out-of-Order (OoO) CPUs are complex systems with many components
interleaved in non-trivial ways. Pinpointing performance bottlenecks and
understanding the underlying causes of program performance issues are critical
tasks to make the most of hardware resources.
<br />We provide an in-depth overview of performance bottlenecks in recent OoO
microarchitectures and describe the difficulties of detecting them. Techniques
that measure resources utilization can offer a good understanding of a
program's execution, but, due to the constraints inherent to Performance
Monitoring Units (PMU) of CPUs, do not provide the relevant metrics for each
use case.
<br />Another approach is to rely on a performance model to simulate the CPU
behavior. Such a model makes it possible to implement any new
microarchitecture-related metric. Within this framework, we advocate for
implementing modeled resources as parameters that can be varied at will to
reveal performance bottlenecks. This allows a generalization of bottleneck
analysis that we call sensitivity analysis.
<br />We present Gus, a novel performance analysis tool that combines the
advantages of sensitivity analysis and dynamic binary instrumentation within a
resource-centric CPU model. We evaluate the impact of sensitivity on bottleneck
analysis over a set of high-performance computing kernels.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15776" title="Abstract">arXiv:2402.15776</a> [<a href="/pdf/2402.15776" title="Download PDF">pdf</a>, <a href="/format/2402.15776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Truly No-Regret Learning in Constrained MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+A">Adrian M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Alatur%2C+P">Pragnya Alatur</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>, 
<a href="/search/cs?searchtype=author&query=Ramponi%2C+G">Giorgia Ramponi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Constrained Markov decision processes (CMDPs) are a common way to model
safety constraints in reinforcement learning. State-of-the-art methods for
efficiently solving CMDPs are based on primal-dual algorithms. For these
algorithms, all currently known regret bounds allow for error cancellations --
one can compensate for a constraint violation in one round with a strict
constraint satisfaction in another. This makes the online learning process
unsafe since it only guarantees safety for the final (mixture) policy but not
during learning. As Efroni et al. (2020) pointed out, it is an open question
whether primal-dual algorithms can provably achieve sublinear regret if we do
not allow error cancellations. In this paper, we give the first affirmative
answer. We first generalize a result on last-iterate convergence of regularized
primal-dual schemes to CMDPs with multiple constraints. Building upon this
insight, we propose a model-based primal-dual algorithm to learn in an unknown
CMDP. We prove that our algorithm achieves sublinear regret without error
cancellations.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15779" title="Abstract">arXiv:2402.15779</a> [<a href="/pdf/2402.15779" title="Download PDF">pdf</a>, <a href="/ps/2402.15779" title="Download PostScript">ps</a>, <a href="/format/2402.15779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptanalysis and improvement of multimodal data encryption by  machine-learning-based system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tolba%2C+Z">Zakaria Tolba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Doctoral thesis. Keywords: Cryptanalysis, Black-box, Deep learning, Machine learning, Ciphertext, Plaintext, Genetic algorithm, Permutation box, Substitution Box
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">With the rising popularity of the internet and the widespread use of networks
and information systems via the cloud and data centers, the privacy and
security of individuals and organizations have become extremely crucial. In
this perspective, encryption consolidates effective technologies that can
effectively fulfill these requirements by protecting public information
exchanges. To achieve these aims, the researchers used a wide assortment of
encryption algorithms to accommodate the varied requirements of this field, as
well as focusing on complex mathematical issues during their work to
substantially complicate the encrypted communication mechanism. as much as
possible to preserve personal information while significantly reducing the
possibility of attacks. Depending on how complex and distinct the requirements
established by these various applications are, the potential of trying to break
them continues to occur, and systems for evaluating and verifying the
cryptographic algorithms implemented continue to be necessary. The best
approach to analyzing an encryption algorithm is to identify a practical and
efficient technique to break it or to learn ways to detect and repair weak
aspects in algorithms, which is known as cryptanalysis. Experts in
cryptanalysis have discovered several methods for breaking the cipher, such as
discovering a critical vulnerability in mathematical equations to derive the
secret key or determining the plaintext from the ciphertext. There are various
attacks against secure cryptographic algorithms in the literature, and the
strategies and mathematical solutions widely employed empower cryptanalysts to
demonstrate their findings, identify weaknesses, and diagnose maintenance
failures in algorithms.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15780" title="Abstract">arXiv:2402.15780</a> [<a href="/pdf/2402.15780" title="Download PDF">pdf</a>, <a href="/format/2402.15780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Holding Secrets Accountable: Auditing Privacy-Preserving Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lycklama%2C+H">Hidde Lycklama</a>, 
<a href="/search/cs?searchtype=author&query=Viand%2C+A">Alexander Viand</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BCchler%2C+N">Nicolas K&#xfc;chler</a>, 
<a href="/search/cs?searchtype=author&query=Knabenhans%2C+C">Christian Knabenhans</a>, 
<a href="/search/cs?searchtype=author&query=Hithnawi%2C+A">Anwar Hithnawi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Recent advancements in privacy-preserving machine learning are paving the way
to extend the benefits of ML to highly sensitive data that, until now, have
been hard to utilize due to privacy concerns and regulatory constraints.
Simultaneously, there is a growing emphasis on enhancing the transparency and
accountability of machine learning, including the ability to audit ML
deployments. While ML auditing and PPML have both been the subjects of
intensive research, they have predominately been examined in isolation.
However, their combination is becoming increasingly important. In this work, we
introduce Arc, an MPC framework for auditing privacy-preserving machine
learning. At the core of our framework is a new protocol for efficiently
verifying MPC inputs against succinct commitments at scale. We evaluate the
performance of our framework when instantiated with our consistency protocol
and compare it to hashing-based and homomorphic-commitment-based approaches,
demonstrating that it is up to 10^4x faster and up to 10^6x more concise.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15781" title="Abstract">arXiv:2402.15781</a> [<a href="/pdf/2402.15781" title="Download PDF">pdf</a>, <a href="/ps/2402.15781" title="Download PostScript">ps</a>, <a href="/format/2402.15781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Off-Policy Multi-Step TD-Learning with Linear Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper analyzes multi-step TD-learning algorithms within the `deadly
triad' scenario, characterized by linear function approximation, off-policy
learning, and bootstrapping. In particular, we prove that n-step TD-learning
algorithms converge to a solution as the sampling horizon n increases
sufficiently. The paper is divided into two parts. In the first part, we
comprehensively examine the fundamental properties of their model-based
deterministic counterparts, including projected value iteration, gradient
descent algorithms, and the control theoretic approach, which can be viewed as
prototype deterministic algorithms whose analysis plays a pivotal role in
understanding and developing their model-free reinforcement learning
counterparts. In particular, we prove that these algorithms converge to
meaningful solutions when n is sufficiently large. Based on these findings, two
n-step TD-learning algorithms are proposed and analyzed, which can be seen as
the model-free reinforcement learning counterparts of the gradient and control
theoretic algorithms.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15784" title="Abstract">arXiv:2402.15784</a> [<a href="/pdf/2402.15784" title="Download PDF">pdf</a>, <a href="/format/2402.15784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRConStyle: Image Restoration Framework Using Contrastive Learning and  Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Dongqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+L">Liang Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, the contrastive learning paradigm has achieved remarkable success
in high-level tasks such as classification, detection, and segmentation.
However, contrastive learning applied in low-level tasks, like image
restoration, is limited, and its effectiveness is uncertain. This raises a
question: Why does the contrastive learning paradigm not yield satisfactory
results in image restoration? In this paper, we conduct in-depth analyses and
propose three guidelines to address the above question. In addition, inspired
by style transfer and based on contrastive learning, we propose a novel module
for image restoration called \textbf{ConStyle}, which can be efficiently
integrated into any U-Net structure network. By leveraging the flexibility of
ConStyle, we develop a \textbf{general restoration network} for image
restoration. ConStyle and the general restoration network together form an
image restoration framework, namely \textbf{IRConStyle}. To demonstrate the
capability and compatibility of ConStyle, we replace the general restoration
network with transformer-based, CNN-based, and MLP-based networks,
respectively. We perform extensive experiments on various image restoration
tasks, including denoising, deblurring, deraining, and dehazing. The results on
19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based
network and significantly enhance performance. For instance, ConStyle NAFNet
significantly outperforms the original NAFNet on SOTS outdoor (dehazing) and
Rain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB
with 85% fewer parameters.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15787" title="Abstract">arXiv:2402.15787</a> [<a href="/pdf/2402.15787" title="Download PDF">pdf</a>, <a href="/format/2402.15787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid Peeling of Parabolas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rote%2C+G">G&#xfc;nter Rote</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCber%2C+M">Moritz R&#xfc;ber</a>, 
<a href="/search/cs?searchtype=author&query=Saghafian%2C+M">Morteza Saghafian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Grid peeling is the process of repeatedly removing the convex hull vertices
of the grid-points that lie inside a given convex curve. It has been
conjectured that, for a more and more refined grid, grid peeling converges to a
continuous process, the affine curve-shortening flow, which deforms the curve
based on the curvature. We prove this conjecture for one class of curves,
parabolas with a vertical axis, and we determine the value of the constant
factor in the formula that relates the two processes.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15789" title="Abstract">arXiv:2402.15789</a> [<a href="/pdf/2402.15789" title="Download PDF">pdf</a>, <a href="/ps/2402.15789" title="Download PostScript">ps</a>, <a href="/format/2402.15789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Liftings of Polynomial Traces on Tetrahedra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parker%2C+C">Charles Parker</a>, 
<a href="/search/math?searchtype=author&query=S%C3%BCli%2C+E">Endre S&#xfc;li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">On the reference tetrahedron $K$, we construct, for each $k \in
\mathbb{N}_0$, a right inverse for the trace operator $u \mapsto (u,
\partial_{n} u, \ldots, \partial_{n}^k u)|_{\partial K}$. The operator is
stable as a mapping from the trace space of $W^{s, p}(K)$ to $W^{s, p}(K)$ for
all $p \in (1, \infty)$ and $s \in (k+1/p, \infty)$. Moreover, if the data is
the trace of a polynomial of degree $N \in \mathbb{N}_0$, then the resulting
lifting is a polynomial of degree $N$. One consequence of the analysis is a
novel characterization for the range of the trace operator.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15790" title="Abstract">arXiv:2402.15790</a> [<a href="/pdf/2402.15790" title="Download PDF">pdf</a>, <a href="/format/2402.15790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discretionary Lane-Change Decision and Control via Parameterized Soft  Actor-Critic for Hybrid Action Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zishun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liyao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This study focuses on a crucial task in the field of autonomous driving,
autonomous lane change. Autonomous lane change plays a pivotal role in
improving traffic flow, alleviating driver burden, and reducing the risk of
traffic accidents. However, due to the complexity and uncertainty of
lane-change scenarios, the functionality of autonomous lane change still faces
challenges. In this research, we conduct autonomous lane-change simulations
using both Deep Reinforcement Learning (DRL) and Model Predictive Control
(MPC). Specifically, we propose the Parameterized Soft Actor-Critic (PASAC)
algorithm to train a DRL-based lane-change strategy to output both discrete
lane-change decision and continuous longitudinal vehicle acceleration. We also
use MPC for lane selection based on predictive car-following costs for
different lanes. For the first time, we compare the performance of DRL and MPC
in the context of lane-change decision. Simulation results indicate that, under
the same reward/cost functions and traffic flow, both MPC and PASAC achieve a
collision rate of 0\%. PASAC demonstrates comparable performance to MPC in
terms of episodic rewards/costs and average vehicle speeds.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15795" title="Abstract">arXiv:2402.15795</a> [<a href="/pdf/2402.15795" title="Download PDF">pdf</a>, <a href="/format/2402.15795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positioning Error Impact Compensation through Data-Driven Optimization  in User-Centric Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Raza%2C+W">Waseem Raza</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+F+A">Fahd Ahmed Khan</a>, 
<a href="/search/eess?searchtype=author&query=Farooq%2C+M+U+B">Muhammad Umar Bin Farooq</a>, 
<a href="/search/eess?searchtype=author&query=Ekin%2C+S">Sabit Ekin</a>, 
<a href="/search/eess?searchtype=author&query=Imran%2C+A">Ali Imran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The performance of user-centric ultra-dense networks (UCUDNs) hinges on the
Service zone (Szone) radius, which is an elastic parameter that balances the
area spectral efficiency (ASE) and energy efficiency (EE) of the network.
Accurately determining the Szone radius requires the precise location of the
user equipment (UE) and data base stations (DBSs). Even a slight error in
reported positions of DBSs or UE will lead to an incorrect determination of
Szone radius and UE-DBS pairing, leading to degradation of the UE-DBS
communication link. To compensate for the positioning error impact and improve
the ASE and EE of the UCUDN, this work proposes a data-driven optimization and
error compensation (DD-OEC) framework. The framework comprises an additional
machine learning model that assesses the impact of residual errors and
regulates the erroneous datadriven optimization to output Szone radius,
transmit power, and DBS density values which improve network ASE and EE. The
performance of the framework is compared to a baseline scheme, which does not
employ the residual, and results demonstrate that the DD-OEC framework
outperforms the baseline, achieving up to a 23% improvement in performance.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15796" title="Abstract">arXiv:2402.15796</a> [<a href="/pdf/2402.15796" title="Download PDF">pdf</a>, <a href="/ps/2402.15796" title="Download PostScript">ps</a>, <a href="/format/2402.15796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction and application of artificial intelligence crowdsourcing  map based on multi-track GPS data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yanlin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Huan Ji</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zheng He</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xinyu Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In recent years, the rapid development of high-precision map technology
combined with artificial intelligence has ushered in a new development
opportunity in the field of intelligent vehicles. High-precision map technology
is an important guarantee for intelligent vehicles to achieve autonomous
driving. However, due to the lack of research on high-precision map technology,
it is difficult to rationally use this technology in the field of intelligent
vehicles. Therefore, relevant researchers studied a fast and effective
algorithm to generate high-precision GPS data from a large number of
low-precision GPS trajectory data fusion, and generated several key data points
to simplify the description of GPS trajectory, and realized the "crowdsourced
update" model based on a large number of social vehicles for map data
collection came into being. This kind of algorithm has the important
significance to improve the data accuracy, reduce the measurement cost and
reduce the data storage space. On this basis, this paper analyzes the
implementation form of crowdsourcing map, so as to improve the various
information data in the high-precision map according to the actual situation,
and promote the high-precision map can be reasonably applied to the intelligent
car.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15797" title="Abstract">arXiv:2402.15797</a> [<a href="/pdf/2402.15797" title="Download PDF">pdf</a>, <a href="/format/2402.15797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gait-Based Privacy Protection for Smart Wearable Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongjiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhu Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Internet of Things Journal 11, 3497 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Smart wearable devices (SWDs) collect and store sensitive daily information
of many people. Its primary method of identification is still the password
unlocking method. However, several studies have shown serious security flaws in
that method, which makes the privacy and security concerns of SWDs particularly
urgent. Gait identification is well suited for SWDs because its built-in
sensors can provide data support for identification. However, existing gait
identification methods have low accuracy and neglect to protect the privacy of
gait features. In addition, the SWD can be used as an internet of things device
for users to share data. But few studies have used gait feature-based
encryption schemes to protect the privacy of message interactions between SWDs
and other devices. In this paper, we propose a gait identification network, a
bi-directional long short-term memory network with an attention mechanism
(ABLSTM), to improve the identification accuracy and a stochastic orthogonal
transformation (SOT) scheme to protect the extracted gait features from
leakage. In the experiments, ABLSTM achieves an accuracy of 95.28%, reducing
previous error rate by 19.3%. The SOT scheme is proved to be resistant to the
chosen plaintext attack (CPA) and is 30% faster than previous methods. A
biometric-based encryption scheme is proposed to enable secure message
interactions using gait features as keys after the gait identification stage is
passed, and offers better protection of the gait features compared to previous
schemes.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15806" title="Abstract">arXiv:2402.15806</a> [<a href="/pdf/2402.15806" title="Download PDF">pdf</a>, <a href="/format/2402.15806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Visual and Semantic Consistency for Semi-supervised Text  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingkun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Biao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Minghui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yingying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene text recognition (STR) is a challenging task that requires large-scale
annotated data for training. However, collecting and labeling real text images
is expensive and time-consuming, which limits the availability of real data.
Therefore, most existing STR methods resort to synthetic data, which may
introduce domain discrepancy and degrade the performance of STR models. To
alleviate this problem, recent semi-supervised STR methods exploit unlabeled
real data by enforcing character-level consistency regularization between
weakly and strongly augmented views of the same image. However, these methods
neglect word-level consistency, which is crucial for sequence recognition
tasks. This paper proposes a novel semi-supervised learning method for STR that
incorporates word-level consistency regularization from both visual and
semantic aspects. Specifically, we devise a shortest path alignment module to
align the sequential visual features of different views and minimize their
distance. Moreover, we adopt a reinforcement learning framework to optimize the
semantic similarity of the predicted strings in the embedding space. We conduct
extensive experiments on several standard and challenging STR benchmarks and
demonstrate the superiority of our proposed method over existing
semi-supervised STR methods.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15808" title="Abstract">arXiv:2402.15808</a> [<a href="/pdf/2402.15808" title="Download PDF">pdf</a>, <a href="/format/2402.15808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Zero-Shot Detector for Multi-Armed Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Granese%2C+F">Federica Granese</a>, 
<a href="/search/cs?searchtype=author&query=Romanelli%2C+M">Marco Romanelli</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear in the 27th International Conference on Artificial Intelligence and Statistics (AISTATS), May 2nd - May 4th, 2024. arXiv admin note: substantial text overlap with <a href="/abs/2302.02216">arXiv:2302.02216</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper explores a scenario in which a malicious actor employs a
multi-armed attack strategy to manipulate data samples, offering them various
avenues to introduce noise into the dataset. Our central objective is to
protect the data by detecting any alterations to the input. We approach this
defensive strategy with utmost caution, operating in an environment where the
defender possesses significantly less information compared to the attacker.
Specifically, the defender is unable to utilize any data samples for training a
defense model or verifying the integrity of the channel. Instead, the defender
relies exclusively on a set of pre-existing detectors readily available ``off
the shelf''. To tackle this challenge, we derive an innovative
information-theoretic defense approach that optimally aggregates the decisions
made by these detectors, eliminating the need for any training data. We further
explore a practical use-case scenario for empirical evaluation, where the
attacker possesses a pre-trained classifier and launches well-known adversarial
attacks against it. Our experiments highlight the effectiveness of our proposed
solution, even in scenarios that deviate from the optimal setup.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15809" title="Abstract">arXiv:2402.15809</a> [<a href="/pdf/2402.15809" title="Download PDF">pdf</a>, <a href="/format/2402.15809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Large Language Model Agents through Action Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiteng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jing Su</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhi-Hong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Model (LLM) Agents have recently garnered increasing interest
yet they are limited in their ability to learn from trial and error, a key
element of intelligent behavior. In this work, we argue that the capacity to
learn new actions from experience is fundamental to the advancement of learning
in LLM agents. While humans naturally expand their action spaces and develop
skills through experiential learning, LLM agents typically operate within fixed
action spaces, limiting their potential for growth. To address these
challenges, our study explores open-action learning for language agents. We
introduce a framework LearnAct with an iterative learning strategy to create
and improve actions in the form of Python functions. In each iteration, LLM
revises and updates the currently available actions based on the errors
identified in unsuccessful training tasks, thereby enhancing action
effectiveness. Our experimental evaluations across Robotic Planning and
Alfworld environments reveal that after learning on a few training task
instances, our approach to open-action learning markedly improves agent
performance for the type of task (by 32 percent in AlfWorld compared to
ReAct+Reflexion, for instance) highlighting the importance of experiential
action learning in the development of more intelligent LLM agents.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15810" title="Abstract">arXiv:2402.15810</a> [<a href="/pdf/2402.15810" title="Download PDF">pdf</a>, <a href="/format/2402.15810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fanjin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shijie Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+Y">Yukuo Cen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yelin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lulu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingfei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuqing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyi Han</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Y">Yuwei An</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+W+L">Weng Lam Tam</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+K">Kun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yunhe Pang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xinyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Huihui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapid proliferation of scientific literature, versatile academic
knowledge services increasingly rely on comprehensive academic graph mining.
Despite the availability of public academic graphs, benchmarks, and datasets,
these resources often fall short in multi-aspect and fine-grained annotations,
are constrained to specific task types and domains, or lack underlying real
academic graphs. In this paper, we present OAG-Bench, a comprehensive,
multi-aspect, and fine-grained human-curated benchmark based on the Open
Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,
and 120+ experimental results to date. We propose new data annotation
strategies for certain tasks and offer a suite of data pre-processing codes,
algorithm implementations, and standardized evaluation protocols to facilitate
academic graph mining. Extensive experiments reveal that even advanced
algorithms like large language models (LLMs) encounter difficulties in
addressing key challenges in certain tasks, such as paper source tracing and
scholar profiling. We also introduce the Open Academic Graph Challenge
(OAG-Challenge) to encourage community input and sharing. We envisage that
OAG-Bench can serve as a common ground for the community to evaluate and
compare algorithms in academic graph mining, thereby accelerating algorithm
development and advancement in this field. OAG-Bench is accessible at
https://www.aminer.cn/data/.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15813" title="Abstract">arXiv:2402.15813</a> [<a href="/pdf/2402.15813" title="Download PDF">pdf</a>, <a href="/format/2402.15813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Bargaining Abilities of LLMs: A Benchmark and A  Buyer-Enhancement Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tian Xia</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+T">Tong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yibo Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The dataset AmazonHistoryPrice and our code are available at <a href="https://github.com/TianXiaSJTU/AmazonPriceHistory">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Bargaining is an important and unique part of negotiation between humans. As
LLM-driven agents learn to negotiate and act like real humans, how to evaluate
agents' bargaining abilities remains an open problem. For the first time, we
formally described the Bargaining task as an asymmetric incomplete information
game, defining the gains of the Buyer and Seller in multiple bargaining
processes. It allows us to quantitatively assess an agent's performance in the
Bargain task. We collected a real product price dataset, AmazonHistoryPrice,
and conducted evaluations of various LLM agents' bargaining abilities. We find
that playing a Buyer is much harder than a Seller, and increasing model size
can not effectively improve the Buyer's performance. To address the challenge,
we propose a novel approach called OG-Narrator that integrates a deterministic
Offer Generator to control the price range of Buyer's offers, and an LLM
Narrator to create natural language sentences for generated offers.
Experimental results show that OG-Narrator improves the buyer's deal rates from
26.67% to 88.88% and brings a ten times of multiplication of profits on all
baselines, even a model that has not been aligned.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15814" title="Abstract">arXiv:2402.15814</a> [<a href="/pdf/2402.15814" title="Download PDF">pdf</a>, <a href="/format/2402.15814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Result on the Inductive Bias of RNN Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R+S+M">Robin Shing Moon Chan</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work by Hewitt et al. (2020) provides a possible interpretation of the
empirical success of recurrent neural networks (RNNs) as language models (LMs).
<br />It shows that RNNs can efficiently represent bounded hierarchical structures
that are prevalent in human language.
<br />This suggests that RNNs' success might be linked to their ability to model
hierarchy.
<br />However, a closer inspection of Hewitt et al.'s (2020) construction shows
that it is not limited to hierarchical LMs, posing the question of what
\emph{other classes} of LMs can be efficiently represented by RNNs.
<br />To this end, we generalize their construction to show that RNNs can
efficiently represent a larger class of LMs: Those that can be represented by a
pushdown automaton with a bounded stack and a generalized stack update
function.
<br />This is analogous to an automaton that keeps a memory of a fixed number of
symbols and updates the memory with a simple update mechanism.
<br />Altogether, the efficiency in representing a diverse class of
non-hierarchical LMs posits a lack of concrete cognitive and
human-language-centered inductive biases in RNNs.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15815" title="Abstract">arXiv:2402.15815</a> [<a href="/pdf/2402.15815" title="Download PDF">pdf</a>, <a href="/ps/2402.15815" title="Download PostScript">ps</a>, <a href="/format/2402.15815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Machine Learning Model for Material Microstructure 3D  Reconstruction and Performance Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhigong Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The reconstruction of 3D microstructures from 2D slices is considered to hold
significant value in predicting the spatial structure and physical properties
of materials.The dimensional extension from 2D to 3D is viewed as a highly
challenging inverse problem from the current technological
perspective.Recently,methods based on generative adversarial networks have
garnered widespread attention.However,they are still hampered by numerous
limitations,including oversimplified models,a requirement for a substantial
number of training samples,and difficulties in achieving model convergence
during training.In light of this,a novel generative model that integrates the
multiscale properties of U-net with and the generative capabilities of GAN has
been proposed.Based on this,the innovative construction of a multi-scale
channel aggregation module,a multi-scale hierarchical feature aggregation
module and a convolutional block attention mechanism can better capture the
properties of the material microstructure and extract the image information.The
model's accuracy is further improved by combining the image regularization loss
with the Wasserstein distance loss.In addition,this study utilizes the
anisotropy index to accurately distinguish the nature of the image,which can
clearly determine the isotropy and anisotropy of the image.It is also the first
time that the generation quality of material samples from different domains is
evaluated and the performance of the model itself is compared.The experimental
results demonstrate that the present model not only shows a very high
similarity between the generated 3D structures and real samples but is also
highly consistent with real data in terms of statistical data analysis.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15817" title="Abstract">arXiv:2402.15817</a> [<a href="/pdf/2402.15817" title="Download PDF">pdf</a>, <a href="/format/2402.15817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BETA-UAV: Blockchain-based Efficient Authentication for Secure UAV  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hafeez%2C+S">Sana Hafeez</a>, 
<a href="/search/cs?searchtype=author&query=Shawky%2C+M+A">Mahmoud A. Shawky</a>, 
<a href="/search/cs?searchtype=author&query=Al-Quraan%2C+M">Mohammad Al-Quraan</a>, 
<a href="/search/cs?searchtype=author&query=Mohjazi%2C+L">Lina Mohjazi</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+A">Muhammad Ali Imran</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 22nd IEEE ICCT | 2022 IEEE 22nd International Conference on Communication Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Unmanned aerial vehicles (UAV), an emerging architecture that embodies flying
ad-hoc networks, face critical privacy and security challenges, mainly when
engaged in data-sensitive missions. Therefore, message authentication is a
crucial security feature in drone communications. This paper presents a
Blockchain-based Efficient, and Trusted Authentication scheme for UAV
communication, BETA-UAV, which exploits the inherent properties of blockchain
technology concerning memorability and is immutable to record communication
sessions via transactions using a smart contract. The smart contract in
BETA-UAV allows participants to publish and call transactions from the
blockchain network. Furthermore, transaction addresses are proof of freshness
and trustworthiness for subsequent transmissions. Furthermore, we investigated
their ability to resist active attacks, such as impersonation, replaying, and
modification. In addition, we evaluate the gas costs associated with the
functions of the smart contract by implementing a BETA-UAV on the Ethereum
public blockchain. A comparison of the computation and communication overheads
shows that the proposed approach can save significant costs over traditional
techniques.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15818" title="Abstract">arXiv:2402.15818</a> [<a href="/pdf/2402.15818" title="Download PDF">pdf</a>, <a href="/format/2402.15818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistic Intelligence in Large Language Models for Telecommunications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Tasnim Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Piovesan%2C+N">Nicola Piovesan</a>, 
<a href="/search/cs?searchtype=author&query=De+Domenico%2C+A">Antonio De Domenico</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Salimur Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have emerged as a significant advancement in the
field of Natural Language Processing (NLP), demonstrating remarkable
capabilities in language generation and other language-centric tasks. Despite
their evaluation across a multitude of analytical and reasoning tasks in
various scientific domains, a comprehensive exploration of their knowledge and
understanding within the realm of natural language tasks in the
telecommunications domain is still needed. This study, therefore, seeks to
evaluate the knowledge and understanding capabilities of LLMs within this
domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four
prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer
resources than ChatGPT, making them suitable for resource-constrained
environments. Their performance is compared with state-of-the-art, fine-tuned
models. To the best of our knowledge, this is the first work to extensively
evaluate and compare the understanding of LLMs across multiple language-centric
tasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve
performance levels comparable to the current state-of-the-art fine-tuned
models. This indicates that pretraining on extensive text corpora equips LLMs
with a degree of specialization, even within the telecommunications domain. We
also observe that no single LLM consistently outperforms others, and the
performance of different LLMs can fluctuate. Although their performance lags
behind fine-tuned models, our findings underscore the potential of LLMs as a
valuable resource for understanding various aspects of this field that lack
large annotated data.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15819" title="Abstract">arXiv:2402.15819</a> [<a href="/pdf/2402.15819" title="Download PDF">pdf</a>, <a href="/format/2402.15819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiased Model-based Interactive Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haiqin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sili Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuguang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhenghua Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing model-based interactive recommendation systems are trained by
querying a world model to capture the user preference, but learning the world
model from historical logged data will easily suffer from bias issues such as
popularity bias and sampling bias. This is why some debiased methods have been
proposed recently. However, two essential drawbacks still remain: 1) ignoring
the dynamics of the time-varying popularity results in a false reweighting of
items. 2) taking the unknown samples as negative samples in negative sampling
results in the sampling bias. To overcome these two drawbacks, we develop a
model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based
\textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In
iDMIR, for the first drawback, we devise a debiased causal world model based on
the causal mechanism of the time-varying recommendation generation process with
identification guarantees; for the second drawback, we devise a debiased
contrastive policy, which coincides with the debiased contrastive learning and
avoids sampling bias. Moreover, we demonstrate that the proposed method not
only outperforms several latest interactive recommendation algorithms but also
enjoys diverse recommendation performance.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15820" title="Abstract">arXiv:2402.15820</a> [<a href="/pdf/2402.15820" title="Download PDF">pdf</a>, <a href="/format/2402.15820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DART: Depth-Enhanced Accurate and Real-Time Background Matting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hanxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guofeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yan Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Matting with a static background, often referred to as ``Background Matting"
(BGM), has garnered significant attention within the computer vision community
due to its pivotal role in various practical applications like webcasting and
photo editing. Nevertheless, achieving highly accurate background matting
remains a formidable challenge, primarily owing to the limitations inherent in
conventional RGB images. These limitations manifest in the form of
susceptibility to varying lighting conditions and unforeseen shadows.
<br />In this paper, we leverage the rich depth information provided by the
RGB-Depth (RGB-D) cameras to enhance background matting performance in
real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm
to incorporate depth information. The resulting model's output undergoes
refinement through Bayesian inference, incorporating a background depth prior.
The posterior prediction is then translated into a "trimap," which is
subsequently fed into a state-of-the-art matting algorithm to generate more
precise alpha mattes. To ensure real-time matting capabilities, a critical
requirement for many real-world applications, we distill the backbone of our
model from a larger and more versatile BGM network. Our experiments demonstrate
the superior performance of the proposed method. Moreover, thanks to the
distillation operation, our method achieves a remarkable processing speed of 33
frames per second (fps) on a mid-range edge-computing device. This high
efficiency underscores DART's immense potential for deployment in mobile
applications}
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15821" title="Abstract">arXiv:2402.15821</a> [<a href="/pdf/2402.15821" title="Download PDF">pdf</a>, <a href="/format/2402.15821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperation and Control in Delegation Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sourbut%2C+O">Oliver Sourbut</a>, 
<a href="/search/cs?searchtype=author&query=Hammond%2C+L">Lewis Hammond</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+H">Harriet Wood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Many settings of interest involving humans and machines -- from virtual
personal assistants to autonomous vehicles -- can naturally be modelled as
principals (humans) delegating to agents (machines), which then interact with
each other on their principals' behalf. We refer to these multi-principal,
multi-agent scenarios as delegation games. In such games, there are two
important failure modes: problems of control (where an agent fails to act in
line their principal's preferences) and problems of cooperation (where the
agents fail to work well together). In this paper we formalise and analyse
these problems, further breaking them down into issues of alignment (do the
players have similar preferences?) and capabilities (how competent are the
players at satisfying those preferences?). We show -- theoretically and
empirically -- how these measures determine the principals' welfare, how they
can be estimated using limited observations, and thus how they might be used to
help us design more aligned and cooperative AI systems.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15823" title="Abstract">arXiv:2402.15823</a> [<a href="/pdf/2402.15823" title="Download PDF">pdf</a>, <a href="/format/2402.15823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-efficient Prompt Learning for 3D Point Cloud Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongcai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haoran Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Deying Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 6 tables; accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a parameter-efficient prompt tuning method, named PPT, to
adapt a large multi-modal model for 3D point cloud understanding. Existing
strategies are quite expensive in computation and storage, and depend on
time-consuming prompt engineering. We address the problems from three aspects.
Firstly, a PromptLearner module is devised to replace hand-crafted prompts with
learnable contexts to automate the prompt tuning process. Then, we lock the
pre-trained backbone instead of adopting the full fine-tuning paradigm to
substantially improve the parameter efficiency. Finally, a lightweight
PointAdapter module is arranged near target tasks to enhance prompt tuning for
3D point cloud understanding. Comprehensive experiments are conducted to
demonstrate the superior parameter and data efficiency of the proposed
method.Meanwhile, we obtain new records on 4 public datasets and multiple 3D
tasks, i.e., point cloud recognition, few-shot learning, and part segmentation.
The implementation is available at https://github.com/auniquesun/PPT.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15824" title="Abstract">arXiv:2402.15824</a> [<a href="/pdf/2402.15824" title="Download PDF">pdf</a>, <a href="/format/2402.15824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Secure Memory System for Efficient Data Protection and Access  Pattern Obfuscation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Haoran Geng</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+Y">Yuezhi Che</a>, 
<a href="/search/cs?searchtype=author&query=Dingler%2C+A">Aaron Dingler</a>, 
<a href="/search/cs?searchtype=author&query=Niemier%2C+M">Michael Niemier</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X+S">Xiaobo Sharon Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">As the reliance on secure memory environments permeates across applications,
memory encryption is used to ensure memory security. However, most effective
encryption schemes, such as the widely used AES-CTR, inherently introduce extra
overheads, including those associated with counter storage and version number
integrity checks. Moreover, encryption only protects data content, and it does
not fully address the memory access pattern leakage. While Oblivious RAM (ORAM)
aims to obscure these patterns, its high performance costs hinder practical
applications. We introduce Secure Scattered Memory (SSM), an efficient scheme
provides a comprehensive security solution that preserves the confidentiality
of data content without traditional encryption, protects access patterns, and
enables efficient integrity verification. Moving away from traditional
encryption-centric methods, SSM offers a fresh approach to protecting data
content while eliminating counter-induced overheads. Moreover, SSM is designed
to inherently obscure memory access patterns, thereby significantly enhancing
the confidentiality of memory data. In addition, SSM incorporates lightweight,
thus integrated mechanisms for integrity assurance, protecting against data
tampering. We also introduce SSM+, an extension that adapts Path ORAM to offer
even greater security guarantees for both data content and memory access
patterns, demonstrating its flexibility and efficiency. Experimental results
show that SSM incurs only a 10% performance overhead compared to non-protected
memory and offers a 15% improvement over AES-CTR mode memory protection.
Notably, SSM+ provides an 20% improvement against Path ORAM integrated with
Intel SGX under the highest security guarantees.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15826" title="Abstract">arXiv:2402.15826</a> [<a href="/pdf/2402.15826" title="Download PDF">pdf</a>, <a href="/format/2402.15826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Design for Justifiable Sequential Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukovic%2C+A">Aleksa Sukovic</a>, 
<a href="/search/cs?searchtype=author&query=Radanovic%2C+G">Goran Radanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Equipping agents with the capacity to justify made decisions using supporting
evidence represents a cornerstone of accountable decision-making. Furthermore,
ensuring that justifications are in line with human expectations and societal
norms is vital, especially in high-stakes situations such as healthcare. In
this work, we propose the use of a debate-based reward model for reinforcement
learning agents, where the outcome of a zero-sum debate game quantifies the
justifiability of a decision in a particular state. This reward model is then
used to train a justifiable policy, whose decisions can be more easily
corroborated with supporting evidence. In the debate game, two argumentative
agents take turns providing supporting evidence for two competing decisions.
Given the proposed evidence, a proxy of a human judge evaluates which decision
is better justified. We demonstrate the potential of our approach in learning
policies for prescribing and justifying treatment decisions of septic patients.
We show that augmenting the reward with the feedback signal generated by the
debate-based reward model yields policies highly favored by the judge when
compared to the policy obtained solely from the environment rewards, while
hardly sacrificing any performance. Moreover, in terms of the overall
performance and justifiability of trained policies, the debate-based feedback
is comparable to the feedback obtained from an ideal judge proxy that evaluates
decisions using the full information encoded in the state. This suggests that
the debate game outputs key information contained in states that is most
relevant for evaluating decisions, which in turn substantiates the practicality
of combining our approach with human-in-the-loop evaluations. Lastly, we
showcase that agents trained via multi-agent debate learn to propose evidence
that is resilient to refutations and closely aligns with human preferences.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15830" title="Abstract">arXiv:2402.15830</a> [<a href="/pdf/2402.15830" title="Download PDF">pdf</a>, <a href="/format/2402.15830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swarm Body: Embodied Swarm Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ichihashi%2C+S">Sosuke Ichihashi</a>, 
<a href="/search/cs?searchtype=author&query=Kuroki%2C+S">So Kuroki</a>, 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+M">Mai Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Kasaura%2C+K">Kazumi Kasaura</a>, 
<a href="/search/cs?searchtype=author&query=Hiraki%2C+T">Takefumi Hiraki</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kazutoshi Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+S">Shigeo Yoshida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Emerging Technologies (cs.ET); Robotics (cs.RO)

</div>
<p class="mathjax">The human brain's plasticity allows for the integration of artificial body
parts into the human body. Leveraging this, embodied systems realize intuitive
interactions with the environment. We introduce a novel concept: embodied swarm
robots. Swarm robots constitute a collective of robots working in harmony to
achieve a common objective, in our case, serving as functional body parts.
Embodied swarm robots can dynamically alter their shape, density, and the
correspondences between body parts and individual robots. We contribute an
investigation of the influence on embodiment of swarm robot-specific factors
derived from these characteristics, focusing on a hand. Our paper is the first
to examine these factors through virtual reality (VR) and real-world robot
studies to provide essential design considerations and applications of embodied
swarm robots. Through quantitative and qualitative analysis, we identified a
system configuration to achieve the embodiment of swarm robots.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15832" title="Abstract">arXiv:2402.15832</a> [<a href="/pdf/2402.15832" title="Download PDF">pdf</a>, <a href="/format/2402.15832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and  Eosin Whole Slide Images: An Indian cohort Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+E">Ekansh Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Uppin%2C+M+S">Megha S Uppin</a>, 
<a href="/search/cs?searchtype=author&query=Jawahar%2C+C+V">C.V. Jawahar</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+V+P">Vinod P.K</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Brain tumors represent a severe and life-threatening condition, demanding
precise diagnosis and tailored treatment strategies. This study advances
patient care with findings from rigorous multiple-instance-learning
experimentations across various feature extractors and aggregators in brain
tumor histopathology. It establishes new performance benchmarks in glioma
subtype classification across multiple datasets, including a novel dataset
focused on the Indian demographic (IPD-Brain), providing a valuable resource
for existing research. Using a ResNet-50, pretrained on histopathology
datasets, for feature extraction, combined with DTFD feature aggregator, our
approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on
TCGA-Brain dataset respectively for three-way glioma subtype classification.
Moreover, it establishes new benchmarks in grading and detecting IHC molecular
biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&amp;E stained whole
slide images for the IPD-Brain dataset. The work also highlights a significant
correlation between the model decision-making processes and the diagnostic
reasoning of pathologists, underscoring its capability to mimic professional
diagnostic procedures.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15833" title="Abstract">arXiv:2402.15833</a> [<a href="/pdf/2402.15833" title="Download PDF">pdf</a>, <a href="/format/2402.15833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Perturbation Consistency Learning for Robust Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiang%2C+Y">Yao Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Nandi%2C+S">Subhrangshu Nandi</a>, 
<a href="/search/cs?searchtype=author&query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Anoop Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Rumshisky%2C+A">Anna Rumshisky</a>, 
<a href="/search/cs?searchtype=author&query=Galstyan%2C+A">Aram Galstyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive performance on a
number of natural language processing tasks, such as question answering and
text summarization. However, their performance on sequence labeling tasks such
as intent classification and slot filling (IC-SF), which is a central component
in personal assistant systems, lags significantly behind discriminative models.
Furthermore, there is a lack of substantive research on the robustness of LLMs
to various perturbations in the input prompts. The contributions of this paper
are three-fold. First, we show that fine-tuning sufficiently large LLMs can
produce IC-SF performance comparable to discriminative models. Next, we
systematically analyze the performance deterioration of those fine-tuned models
due to three distinct yet relevant types of input perturbations - oronyms,
synonyms, and paraphrasing. Finally, we propose an efficient mitigation
approach, Prompt Perturbation Consistency Learning (PPCL), which works by
regularizing the divergence between losses from clean and perturbed samples.
Our experiments demonstrate that PPCL can recover on average 59% and 69% of the
performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the
data augmentation approach while using ten times fewer augmented data samples.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15834" title="Abstract">arXiv:2402.15834</a> [<a href="/pdf/2402.15834" title="Download PDF">pdf</a>, <a href="/format/2402.15834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree decompositions meet induced matchings: beyond Max Weight  Independent Set
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lima%2C+P+T">Paloma T. Lima</a>, 
<a href="/search/cs?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Mur%C5%A1i%C4%8D%2C+P">Peter Mur&#x161;i&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Okrasa%2C+K">Karolina Okrasa</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0torgel%2C+K">Kenny &#x160;torgel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.12315">arXiv:2209.12315</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">For a tree decomposition $\mathcal{T}$ of a graph $G$, by $\mu(\mathcal{T})$
we denote the size of a largest induced matching in $G$ all of whose edges
intersect one bag of $\mathcal{T}$. Induced matching treewidth of a graph $G$
is the minimum value of $\mu(\mathcal{T})$ over all tree decompositions
$\mathcal{T}$ of $G$. Yolov [SODA 2018] proved that Max Weight Independent Set
can be solved in polynomial time for graphs of bounded induced matching
treewidth.
<br />In this paper we explore what other problems are tractable in such classes of
graphs. As our main result, we give a polynomial-time algorithm for Min Weight
Feedback Vertex Set. We also provide some positive results concerning packing
induced subgraphs, which in particular imply a PTAS for the problem of finding
a largest induced subgraph of bounded treewidth.
<br />These results suggest that in graphs of bounded induced matching treewidth,
one could find in polynomial time a maximum-weight induced subgraph of bounded
treewidth satisfying a given CMSO$_2$ formula. We conjecture that such a result
indeed holds and prove it for graphs of bounded tree-independence number, which
form a rich and important family of subclasses of graphs of bounded induced
matching treewidth.
<br />We complement these algorithmic results with a number of complexity and
structural results concerning induced matching treewidth.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15837" title="Abstract">arXiv:2402.15837</a> [<a href="/pdf/2402.15837" title="Download PDF">pdf</a>, <a href="/format/2402.15837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An $O(n \log n)$-Time Approximation Scheme for Geometric Many-to-Many  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bandyapadhyay%2C+S">Sayan Bandyapadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jie Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In SoCG'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Geometric matching is an important topic in computational geometry and has
been extensively studied over decades. In this paper, we study a
geometric-matching problem, known as geometric many-to-many matching. In this
problem, the input is a set $S$ of $n$ colored points in $\mathbb{R}^d$, which
implicitly defines a graph $G = (S,E(S))$ where $E(S) = \{(p,q): p,q \in S
\text{ have different colors}\}$, and the goal is to compute a minimum-cost
subset $E^* \subseteq E(S)$ of edges that cover all points in $S$. Here the
cost of $E^*$ is the sum of the costs of all edges in $E^*$, where the cost of
a single edge $e$ is the Euclidean distance (or more generally, the
$L_p$-distance) between the two endpoints of $e$. Our main result is a
$(1+\varepsilon)$-approximation algorithm with an optimal running time
$O_\varepsilon(n \log n)$ for geometric many-to-many matching in any fixed
dimension, which works under any $L_p$-norm. This is the first near-linear
approximation scheme for the problem in any $d \geq 2$. Prior to this work,
only the bipartite case of geometric many-to-many matching was considered in
$\mathbb{R}^1$ and $\mathbb{R}^2$, and the best known approximation scheme in
$\mathbb{R}^2$ takes $O_\varepsilon(n^{1.5} \cdot \mathsf{poly}(\log n))$ time.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15838" title="Abstract">arXiv:2402.15838</a> [<a href="/pdf/2402.15838" title="Download PDF">pdf</a>, <a href="/format/2402.15838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Soyoung Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E">Eunbi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yireun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+H">Hyeongu Yun</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Seung-won Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">We propose ListT5, a novel reranking approach based on Fusion-in-Decoder
(FiD) that handles multiple candidate passages at both train and inference
time. We also introduce an efficient inference framework for listwise ranking
based on m-ary tournament sort with output caching. We evaluate and compare our
model on the BEIR benchmark for zero-shot retrieval task, demonstrating that
ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3
gain in the average NDCG@10 score, (2) has an efficiency comparable to
pointwise ranking models and surpasses the efficiency of previous listwise
ranking models, and (3) overcomes the lost-in-the-middle problem of previous
listwise rerankers. Our code, model checkpoints, and the evaluation framework
are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15844" title="Abstract">arXiv:2402.15844</a> [<a href="/pdf/2402.15844" title="Download PDF">pdf</a>, <a href="/format/2402.15844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BalanceDN: Load-Balancing Allocation of Interest for Fast Discovery in  Content Centric Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gunti%2C+M">Murali Gunti</a>, 
<a href="/search/cs?searchtype=author&query=Rojas-Cessa%2C+R">Roberto Rojas-Cessa</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Word Forum of Internet of Things, 5 pp., Oct. 12-27, 2023,
  Alveiro, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In Named Data Networking (NDN), data is identified by unique names instead of
IP addresses, and routers use the names of the content to forward Interest
packets towards the producers of the requested content. However, the current
content search mechanism in NDN is complex and slow. This mechanism not only
creates congestion but also hinders practical deployment due to its slowness
and cumbersome nature. To address this issue, we propose a methodology, called
BalanceDN, that distributes content through the network such that sought
content can be found quickly. BalanceDN uses a distributed allocation of
resolvers as those used by the domain name system but differs in how content is
distributed. Our approach avoids flooding the network with pending interest
requests and also eliminates the need for blind search when the location of
content is unknown. We tested our approach on ndnSIM; a simulation platform for
NDN. The results show that the proposed routing scheme utilizes far fewer
network resources compared to the NDN network when retrieving content. The
proposed scheme accomplishes this performance gain by leveraging a
load-balanced hashing mechanism to distribute and locate the name of the
content on the distributed nameserver lookup service nodes.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15849" title="Abstract">arXiv:2402.15849</a> [<a href="/pdf/2402.15849" title="Download PDF">pdf</a>, <a href="/format/2402.15849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Redistribution of Maximal Extractable Value: A Dynamic Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braga%2C+P">Pedro Braga</a>, 
<a href="/search/cs?searchtype=author&query=Chionas%2C+G">Georgios Chionas</a>, 
<a href="/search/cs?searchtype=author&query=Leonardos%2C+S">Stefanos Leonardos</a>, 
<a href="/search/cs?searchtype=author&query=Krysta%2C+P">Piotr Krysta</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Ventre%2C+C">Carmine Ventre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended abstract in the 23rd International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH); Dynamical Systems (math.DS)

</div>
<p class="mathjax">Maximal Extractable Value (MEV) has emerged as a new frontier in the design
of blockchain systems. The marriage between decentralization and finance gives
the power to block producers (a.k.a., miners) not only to select and add
transactions to the blockchain but, crucially, also to order them so as to
extract as much financial gain as possible for themselves. Whilst this price
may be unavoidable for the service provided by block producers, users of the
chain may in the long run prefer to use less predatory systems. In this paper,
we propose to make the MEV extraction rate part of the protocol design space.
Our aim is to leverage this parameter to maintain a healthy balance between
miners (who need to be compensated) and users (who need to feel encouraged to
transact). Inspired by the principles introduced by EIP-1559 for transaction
fees, we design a dynamic mechanism which updates the MEV extraction rate with
the goal of stabilizing it at a target value. We analyse the evolution of this
dynamic mechanism under various market conditions and provide formal guarantees
about its long-term performance. Our results show that even when the system
behavior is provably chaotic, the dynamics guarantee long-term liveness
(survival) and robustness of the system. The main takeaway from our work is
that the proposed system exhibits desirable behavior (near-optimal performance)
even when it operates in out of equilibrium conditions that are often met in
practice. Our work establishes, the first to our knowledge, dynamic framework
for the integral problem of MEV sharing between extractors and users.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15852" title="Abstract">arXiv:2402.15852</a> [<a href="/pdf/2402.15852" title="Download PDF">pdf</a>, <a href="/format/2402.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NaVid: Video-based VLM Plans the Next Step for Vision-and-Language  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiazhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kunyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rongtao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gengze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yicong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xiaomeng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Vision-and-Language Navigation (VLN) stands as a key research problem of
Embodied AI, aiming at enabling agents to navigate in unseen environments
following linguistic instructions. In this field, generalization is a
long-standing challenge, either to out-of-distribution scenes or from Sim to
Real. In this paper, we propose NaVid, a video-based large vision language
model (VLM), to mitigate such a generalization gap. NaVid makes the first
endeavour to showcase the capability of VLMs to achieve state-of-the-art level
navigation performance without any maps, odometer and depth inputs. Following
human instruction, NaVid only requires an on-the-fly video stream from a
monocular RGB camera equipped on the robot to output the next-step action. Our
formulation mimics how humans navigate and naturally gets rid of the problems
introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.
Moreover, our video-based approach can effectively encode the historical
observations of robots as spatio-temporal contexts for decision-making and
instruction following. We train NaVid with 550k navigation samples collected
from VLN-CE trajectories, including action-planning and instruction-reasoning
samples, along with 665k large-scale web data. Extensive experiments show that
NaVid achieves SOTA performance in simulation environments and the real world,
demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our
proposed VLM approach plans the next step for not only the navigation agents
but also this research field.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15853" title="Abstract">arXiv:2402.15853</a> [<a href="/pdf/2402.15853" title="Download PDF">pdf</a>, <a href="/format/2402.15853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via  Robust and Accurate Camouflage Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiawei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Linye Lyu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Daojing He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Adversarial camouflage is a widely used physical attack against vehicle
detectors for its superiority in multi-view attack performance. One promising
approach involves using differentiable neural renderers to facilitate
adversarial camouflage optimization through gradient back-propagation. However,
existing methods often struggle to capture environmental characteristics during
the rendering process or produce adversarial textures that can precisely map to
the target vehicle, resulting in suboptimal attack performance. Moreover, these
approaches neglect diverse weather conditions, reducing the efficacy of
generated camouflage across varying weather scenarios. To tackle these
challenges, we propose a robust and accurate camouflage generation method,
namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural
Renderer Plus (NRP), which can accurately project vehicle textures and render
images with environmental characteristics such as lighting and weather. In
addition, we integrate a multi-weather dataset for camouflage generation,
leveraging the NRP to enhance the attack robustness. Experimental results on
six popular object detectors show that RAUCA consistently outperforms existing
methods in both simulation and real-world settings.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15858" title="Abstract">arXiv:2402.15858</a> [<a href="/pdf/2402.15858" title="Download PDF">pdf</a>, <a href="/format/2402.15858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in  Computational Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yuanzhe Peng</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jieming Bian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 International Conference on Acoustics, Speech and Signal
  Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The fusion of complementary multimodal information is crucial in
computational pathology for accurate diagnostics. However, existing multimodal
learning approaches necessitate access to users' raw data, posing substantial
privacy risks. While Federated Learning (FL) serves as a privacy-preserving
alternative, it falls short in addressing the challenges posed by heterogeneous
(yet possibly overlapped) modalities data across various hospitals. To bridge
this gap, we propose a Federated Multi-Modal (FedMM) learning framework that
federatedly trains multiple single-modal feature extractors to enhance
subsequent classification performance instead of existing FL that aims to train
a unified multimodal fusion model. Any participating hospital, even with
small-scale datasets or limited devices, can leverage these federated trained
extractors to perform local downstream tasks (e.g., classification) while
ensuring data privacy. Through comprehensive evaluations of two publicly
available datasets, we demonstrate that FedMM notably outperforms two baselines
in accuracy and AUC metrics.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15861" title="Abstract">arXiv:2402.15861</a> [<a href="/pdf/2402.15861" title="Download PDF">pdf</a>, <a href="/format/2402.15861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MATHWELL: Generating Educational Math Word Problems at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christ%2C+B+R">Bryan R Christ</a>, 
<a href="/search/cs?searchtype=author&query=Kropko%2C+J">Jonathan Kropko</a>, 
<a href="/search/cs?searchtype=author&query=Hartvigsen%2C+T">Thomas Hartvigsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Math word problems are critical K-8 educational tools, but writing them is
time-consuming and requires domain expertise. We suggest that language models
can support K-8 math education by automatically generating problems at scale.
To be educational, generated problems must be 1) solvable, 2) accurate, and 3)
appropriate. Existing datasets are unlabeled for these criteria, making them
ill-suited for training problem generators. We introduce MATHWELL, a Llama-2
(70B) model iteratively finetuned to generate K-8 math word problems using data
from expert annotation. Using MATHWELL, we generate the largest English word
problem dataset to date, containing 20,490 problems. 3,484 are scored by domain
experts who find MATHWELL has a 40% higher share of problems that have
executable solutions and meet all criteria than alternatives, with 74% of its
problems with executable solutions being solvable, accurate, and appropriate.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15862" title="Abstract">arXiv:2402.15862</a> [<a href="/pdf/2402.15862" title="Download PDF">pdf</a>, <a href="/format/2402.15862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SportQA: A Benchmark for Sports Understanding in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haotian Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengbang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tracy%2C+R">Rhys Tracy</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dongdong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zezhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-fang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weining Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A deep understanding of sports, a field rich in strategic and dynamic
content, is crucial for advancing Natural Language Processing (NLP). This holds
particular significance in the context of evaluating and advancing Large
Language Models (LLMs), given the existing gap in specialized benchmarks. To
bridge this gap, we introduce SportQA, a novel benchmark specifically designed
for evaluating LLMs in the context of sports understanding. SportQA encompasses
over 70,000 multiple-choice questions across three distinct difficulty levels,
each targeting different aspects of sports knowledge from basic historical
facts to intricate, scenario-based reasoning tasks. We conducted a thorough
evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms
supplemented by chain-of-thought (CoT) prompting. Our results reveal that while
LLMs exhibit competent performance in basic sports knowledge, they struggle
with more complex, scenario-based sports reasoning, lagging behind human
expertise. The introduction of SportQA marks a significant step forward in NLP,
offering a tool for assessing and enhancing sports understanding in LLMs.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15864" title="Abstract">arXiv:2402.15864</a> [<a href="/pdf/2402.15864" title="Download PDF">pdf</a>, <a href="/format/2402.15864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Field-based Molecule Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+A">Alexandru Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Korpela%2C+D">Dani Korpela</a>, 
<a href="/search/cs?searchtype=author&query=Heinonen%2C+M">Markus Heinonen</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+Y">Yogesh Verma</a>, 
<a href="/search/cs?searchtype=author&query=Iakovlev%2C+V">Valerii Iakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+V">Vikas Garg</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A4hdesm%C3%A4ki%2C+H">Harri L&#xe4;hdesm&#xe4;ki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">This work introduces FMG, a field-based model for drug-like molecule
generation. We show how the flexibility of this method provides crucial
advantages over the prevalent, point-cloud based methods, and achieves
competitive molecular stability generation. We tackle optical isomerism
(enantiomers), a previously omitted molecular property that is crucial for drug
safety and effectiveness, and thus account for all molecular geometry aspects.
We demonstrate how previous methods are invariant to a group of transformations
that includes enantiomer pairs, leading them invariant to the molecular R and S
configurations, while our field-based generative model captures this property.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15865" title="Abstract">arXiv:2402.15865</a> [<a href="/pdf/2402.15865" title="Download PDF">pdf</a>, <a href="/format/2402.15865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Li Pang</a>, 
<a href="/search/cs?searchtype=author&query=Rui%2C+X">Xiangyu Rui</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Long Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiangyong Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Hyperspectral image (HSI) restoration aims at recovering clean images from
degraded observations and plays a vital role in downstream tasks. Existing
model-based methods have limitations in accurately modeling the complex image
characteristics with handcraft priors, and deep learning-based methods suffer
from poor generalization ability. To alleviate these issues, this paper
proposes an unsupervised HSI restoration framework with pre-trained diffusion
model (HIR-Diff), which restores the clean HSIs from the product of two
low-rank components, i.e., the reduced image and the coefficient matrix.
Specifically, the reduced image, which has a low spectral dimension, lies in
the image field and can be inferred from our improved diffusion model where a
new guidance function with total variation (TV) prior is designed to ensure
that the reduced image can be well sampled. The coefficient matrix can be
effectively pre-estimated based on singular value decomposition (SVD) and
rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise
schedule is proposed to accelerate the restoration process (about 5$\times$
acceleration for denoising) with little performance decrease. Extensive
experimental results validate the superiority of our method in both performance
and speed on a variety of HSI restoration tasks, including HSI denoising, noisy
HSI super-resolution, and noisy HSI inpainting. The code is available at
https://github.com/LiPang/HIRDiff.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15870" title="Abstract">arXiv:2402.15870</a> [<a href="/pdf/2402.15870" title="Download PDF">pdf</a>, <a href="/format/2402.15870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian  Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yangtian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yihua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xiaoyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+S">Shaohui Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaogang Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent advancements in 3D Gaussian splatting (3D-GS) have not only
facilitated real-time rendering through modern GPU rasterization pipelines but
have also attained state-of-the-art rendering quality. Nevertheless, despite
its exceptional rendering quality and performance on standard datasets, 3D-GS
frequently encounters difficulties in accurately modeling specular and
anisotropic components. This issue stems from the limited ability of spherical
harmonics (SH) to represent high-frequency information. To overcome this
challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic
spherical Gaussian (ASG) appearance field instead of SH for modeling the
view-dependent appearance of each 3D Gaussian. Additionally, we have developed
a coarse-to-fine training strategy to improve learning efficiency and eliminate
floaters caused by overfitting in real-world scenes. Our experimental results
demonstrate that our method surpasses existing approaches in terms of rendering
quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to
model scenes with specular and anisotropic components without increasing the
number of 3D Gaussians. This improvement extends the applicability of 3D GS to
handle intricate scenarios with specular and anisotropic surfaces.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15871" title="Abstract">arXiv:2402.15871</a> [<a href="/pdf/2402.15871" title="Download PDF">pdf</a>, <a href="/format/2402.15871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regular resolution effectively simulates resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buss%2C+S">Sam Buss</a>, 
<a href="/search/cs?searchtype=author&query=Yolcu%2C+E">Emre Yolcu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Regular resolution is a refinement of the resolution proof system requiring
that no variable be resolved on more than once along any path in the proof. It
is known that there exist sequences of formulas that require exponential-size
proofs in regular resolution while admitting polynomial-size proofs in
resolution. Thus, with respect to the usual notion of simulation, regular
resolution is separated from resolution. An alternative, and weaker, notion for
comparing proof systems is that of an "effective simulation," which allows the
translation of the formula along with the proof when moving between proof
systems. We prove that regular resolution is equivalent to resolution under
effective simulations. As a corollary, we recover in a black-box fashion a
recent result on the hardness of automating regular resolution.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15872" title="Abstract">arXiv:2402.15872</a> [<a href="/pdf/2402.15872" title="Download PDF">pdf</a>, <a href="/ps/2402.15872" title="Download PostScript">ps</a>, <a href="/format/2402.15872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Bayesian Persuasion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yuqi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuran Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The tension between persuasion and privacy preservation is common in
real-world settings. Online platforms should protect the privacy of web users
whose data they collect, even as they seek to disclose information about these
data to selling advertising spaces. Similarly, hospitals may share patient data
to attract research investments with the obligation to preserve patients'
privacy. To deal with these issues, we develop a framework to study Bayesian
persuasion under differential privacy constraints, where the sender must design
an optimal signaling scheme for persuasion while guaranteeing the privacy of
each agent's private information in the database. To understand how privacy
constraints affect information disclosure, we explore two perspectives within
Bayesian persuasion: one views the mechanism as releasing a posterior about the
private data, while the other views it as sending an action recommendation.
<br />The posterior-based formulation helps consider privacy-utility tradeoffs,
quantifying how the tightness of privacy constraints impacts the sender's
optimal utility. For any instance in a common utility function family and a
wide range of privacy levels, a significant constant utility gap can be found
between any two of the three conditions: $\epsilon$-differential privacy
constraint, relaxation $(\epsilon,\delta)$-differential privacy constraint, and
no privacy constraint. We further geometrically characterize optimal signaling
schemes under different types of constraints ($\epsilon$-differential privacy,
$(\epsilon,\delta)$-differential privacy and Renyi differential privacy), all
of which can be seen as finding concave hulls in constrained posterior regions.
Meanwhile, by taking the action-based view of persuasion, we provide
polynomial-time algorithms for computing optimal differentially private
signaling schemes, as long as a mild homogeneous condition is met.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15873" title="Abstract">arXiv:2402.15873</a> [<a href="/pdf/2402.15873" title="Download PDF">pdf</a>, <a href="/ps/2402.15873" title="Download PostScript">ps</a>, <a href="/format/2402.15873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box  Machine-Generated Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Ayan Datta</a>, 
<a href="/search/cs?searchtype=author&query=Chandramania%2C+A">Aryan Chandramania</a>, 
<a href="/search/cs?searchtype=author&query=Mamidi%2C+R">Radhika Mamidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This document contains the details of the authors' submission to the
proceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and
Multilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual)
and B. Detection of machine-generated text is becoming an increasingly
important task, with the advent of large language models (LLMs). In this
document, we lay out the techniques utilized for performing the same, along
with the results obtained.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15876" title="Abstract">arXiv:2402.15876</a> [<a href="/pdf/2402.15876" title="Download PDF">pdf</a>, <a href="/format/2402.15876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GoT: Decreasing DCC Queuing for CAM Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amador%2C+O">Oscar Amador</a>, 
<a href="/search/cs?searchtype=author&query=Soto%2C+I">Ignacio Soto</a>, 
<a href="/search/cs?searchtype=author&query=Urue%C3%B1a%2C+M">Manuel Urue&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+M">Maria Calderon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications Letters, 24(12), pp. 2974 - 2978, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Vehicular networks use Decentralized Congestion Control (DCC) mechanisms to
operate effectively, but this mechanism may introduce queuing delays. Freshness
of Cooperative Awareness Messages (CAMs) is critical for their usefulness. In
this letter we explore how the presence of other types of traffic additional to
CAMs, even with lower priorities, has an impact on the freshness of CAM
messages due to DCC queuing. Finally, we propose Generate-on-Time (GoT), which
is a simple mechanism that reduces DCC queuing delays for CAM messages without
introducing any downside in other performance metrics.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15883" title="Abstract">arXiv:2402.15883</a> [<a href="/pdf/2402.15883" title="Download PDF">pdf</a>, <a href="/format/2402.15883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion Encoder Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasteris%2C+S">Stephen Pasteris</a>, 
<a href="/search/cs?searchtype=author&query=Hicks%2C+C">Chris Hicks</a>, 
<a href="/search/cs?searchtype=author&query=Mavroudis%2C+V">Vasilios Mavroudis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper we present fusion encoder networks (FENs): a class of
algorithms for creating neural networks that map fixed-length sequences to
outputs. The resulting neural network has only logarithmic depth (alleviating
the degradation of data as it propagates through the network) and can process
sequences in linear time (or in logarithmic time with a linear number of
processors). The crucial property of FENs is that they learn by training a
quasi-linear number of constant-depth neural networks in parallel. The fact
that these networks are constant depth means that backpropagation works well.
We note that currently the performance of FENs is only conjectured as we are
yet to implement them.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15885" title="Abstract">arXiv:2402.15885</a> [<a href="/pdf/2402.15885" title="Download PDF">pdf</a>, <a href="/ps/2402.15885" title="Download PostScript">ps</a>, <a href="/format/2402.15885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Listings as Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sawczuk%2C+H">Hamilton Sawczuk</a>, 
<a href="/search/cs?searchtype=author&query=Gnang%2C+E">Edinah Gnang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">We propose an algebraic model of computation which formally relates symbolic
listings, complexity of Boolean functions, and low depth arithmetic circuit
complexity. In this model algorithms are arithmetic formula expressing symbolic
listings of YES instances of Boolean functions, and computation is executed via
partial differential operators. We consider the Chow rank of an arithmetic
formula as a measure of complexity and establish the Chow rank of multilinear
polynomials with totally non-overlapping monomial support. We also provide Chow
rank non-decreasing transformations from sets of graphs to sets of functional
graphs.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15893" title="Abstract">arXiv:2402.15893</a> [<a href="/pdf/2402.15893" title="Download PDF">pdf</a>, <a href="/format/2402.15893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concurrent Learning of Policy and Unknown Safety Constraints in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yifru%2C+L">Lunet Yifru</a>, 
<a href="/search/eess?searchtype=author&query=Baheri%2C+A">Ali Baheri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning (RL) has revolutionized decision-making across a wide
range of domains over the past few decades. Yet, deploying RL policies in
real-world scenarios presents the crucial challenge of ensuring safety.
Traditional safe RL approaches have predominantly focused on incorporating
predefined safety constraints into the policy learning process. However, this
reliance on predefined safety constraints poses limitations in dynamic and
unpredictable real-world settings where such constraints may not be available
or sufficiently adaptable. Bridging this gap, we propose a novel approach that
concurrently learns a safe RL control policy and identifies the unknown safety
constraint parameters of a given environment. Initializing with a parametric
signal temporal logic (pSTL) safety specification and a small initial labeled
dataset, we frame the problem as a bilevel optimization task, intricately
integrating constrained policy optimization, using a Lagrangian-variant of the
twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian
optimization for optimizing parameters for the given pSTL safety specification.
Through experimentation in comprehensive case studies, we validate the efficacy
of this approach across varying forms of environmental constraints,
consistently yielding safe RL policies with high returns. Furthermore, our
findings indicate successful learning of STL safety constraint parameters,
exhibiting a high degree of conformity with true environmental safety
constraints. The performance of our model closely mirrors that of an ideal
scenario that possesses complete prior knowledge of safety constraints,
demonstrating its proficiency in accurately identifying environmental safety
constraints and learning safe policies that adhere to those constraints.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15894" title="Abstract">arXiv:2402.15894</a> [<a href="/pdf/2402.15894" title="Download PDF">pdf</a>, <a href="/format/2402.15894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-graph Graph Matching for Coronary Artery Semantic Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhihui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+P">Pukar Baral</a>, 
<a href="/search/cs?searchtype=author&query=Esposito%2C+M">Michel Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Coronary artery disease (CAD) stands as the leading cause of death worldwide,
and invasive coronary angiography (ICA) remains the gold standard for assessing
vascular anatomical information. However, deep learning-based methods encounter
challenges in generating semantic labels for arterial segments, primarily due
to the morphological similarity between arterial branches. To address this
challenge, we model the vascular tree as a graph and propose a multi-graph
graph matching (MGM) algorithm for coronary artery semantic labeling. The MGM
algorithm assesses the similarity between arterials in multiple vascular tree
graphs, taking into account the cycle consistency between each pair of graphs.
This ensures that unannotated arterial segments are appropriately labeled by
matching them with annotated segments. Through the incorporation of anatomical
graph structure, radiomics features, and semantic mapping, the proposed MGM
model achieves an impressive accuracy of 0.9471 for coronary artery semantic
labeling. This approach presents a novel tool for coronary artery analysis
using ICA videos, offering valuable insights into vascular health and
pathology.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15895" title="Abstract">arXiv:2402.15895</a> [<a href="/pdf/2402.15895" title="Download PDF">pdf</a>, <a href="/format/2402.15895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Object Tracking by Hierarchical Visual Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jinkun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiangmiao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 10 tables, accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a new visual hierarchical representation paradigm for multi-object
tracking. It is more effective to discriminate between objects by attending to
objects' compositional visual regions and contrasting with the background
contextual information instead of sticking to only the semantic visual cue such
as bounding boxes. This compositional-semantic-contextual hierarchy is flexible
to be integrated in different appearance-based multi-object tracking methods.
We also propose an attention-based visual feature module to fuse the
hierarchical visual representations. The proposed method achieves
state-of-the-art accuracy and time efficiency among query-based methods on
multiple multi-object tracking benchmarks.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15896" title="Abstract">arXiv:2402.15896</a> [<a href="/pdf/2402.15896" title="Download PDF">pdf</a>, <a href="/format/2402.15896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Instruction Tuning with Conditional Mixture of LoRA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wenpeng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, multimodal instruction tuning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal Large Language Models (MLLMs) have demonstrated remarkable
proficiency in diverse tasks across different domains, with an increasing focus
on improving their zero-shot generalization capabilities for unseen multimodal
tasks. Multimodal instruction tuning has emerged as a successful strategy for
achieving zero-shot generalization by fine-tuning pre-trained models on diverse
multimodal tasks through instructions. As MLLMs grow in complexity and size,
the need for parameter-efficient fine-tuning methods like Low-Rank Adaption
(LoRA), which fine-tunes with a minimal set of parameters, becomes essential.
However, applying LoRA in multimodal instruction tuning presents the challenge
of task interference, which leads to performance degradation, especially when
dealing with a broad array of multimodal tasks. To address this, this paper
introduces a novel approach that integrates multimodal instruction tuning with
Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically
constructing low-rank adaptation matrices tailored to the unique demands of
each input instance, aiming to mitigate task interference. Experimental results
on various multimodal evaluation datasets indicate that MixLoRA not only
outperforms the conventional LoRA with the same or even higher ranks,
demonstrating its efficacy and adaptability in diverse multimodal tasks.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15898" title="Abstract">arXiv:2402.15898</a> [<a href="/pdf/2402.15898" title="Download PDF">pdf</a>, <a href="/format/2402.15898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-based Transductive Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%BCbotter%2C+J">Jonas H&#xfc;botter</a>, 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=Treven%2C+L">Lenart Treven</a>, 
<a href="/search/cs?searchtype=author&query=As%2C+Y">Yarden As</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.15441">arXiv:2402.15441</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We generalize active learning to address real-world settings where sampling
is restricted to an accessible region of the domain, while prediction targets
may lie outside this region. To this end, we propose ITL, short for
information-based transductive learning, an approach which samples adaptively
to maximize the information gained about specified prediction targets. We show,
under general regularity assumptions, that ITL converges uniformly to the
smallest possible uncertainty obtainable from the accessible data. We
demonstrate ITL in two key applications: Few-shot fine-tuning of large neural
networks and safe Bayesian optimization, and in both cases, ITL significantly
outperforms the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15900" title="Abstract">arXiv:2402.15900</a> [<a href="/pdf/2402.15900" title="Download PDF">pdf</a>, <a href="/format/2402.15900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dedicated Restricted Target Wake Time for Real-Time Applications in  Wi-Fi 7
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belogaev%2C+A">Andrey Belogaev</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoman Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xingfeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Blondia%2C+C">Chris Blondia</a>, 
<a href="/search/cs?searchtype=author&query=Famaey%2C+J">Jeroen Famaey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Real-time applications (RTA) tend to play a crucial role in people's everyday
life. Such applications are among the key use cases for the next generations of
wireless technologies. RTA applications are characterized by strict guaranteed
delay requirements (in the order of a few milliseconds). One of the pillars of
enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time
(R-TWT), which provides Wi-Fi stations exclusive channel access within
negotiated service periods (SPs). If each RTA data flow uses dedicated SPs for
data transmission, they are completely isolated from each other and do not
experience any contention. To ensure the satisfaction of RTA QoS requirements
while minimizing the channel airtime consumption, it is important to properly
select the R-TWT parameters, namely the duration of SPs and the period between
SPs. In this paper, we develop a mathematical model that estimates the delay
probability distribution and packet loss probability for a given set of
network, traffic and R-TWT parameters. Using this model, the access point can
select the optimal R-TWT parameters for the given QoS requirements. The high
accuracy of the model is proven by means of simulation.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15903" title="Abstract">arXiv:2402.15903</a> [<a href="/pdf/2402.15903" title="Download PDF">pdf</a>, <a href="/format/2402.15903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ESFL: Efficient Split Federated Learning over Resource-Constrained  Heterogeneous Wireless Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yiqin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haixia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuguang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+T+F">Tan F. Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Federated learning (FL) allows multiple parties (distributed devices) to
train a machine learning model without sharing raw data. How to effectively and
efficiently utilize the resources on devices and the central server is a highly
interesting yet challenging problem. In this paper, we propose an efficient
split federated learning algorithm (ESFL) to take full advantage of the
powerful computing capabilities at a central server under a split federated
learning framework with heterogeneous end devices (EDs). By splitting the model
into different submodels between the server and EDs, our approach jointly
optimizes user-side workload and server-side computing resource allocation by
considering users' heterogeneity. We formulate the whole optimization problem
as a mixed-integer non-linear program, which is an NP-hard problem, and develop
an iterative approach to obtain an approximate solution efficiently. Extensive
simulations have been conducted to validate the significantly increased
efficiency of our ESFL approach compared with standard federated learning,
split learning, and splitfed learning.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15905" title="Abstract">arXiv:2402.15905</a> [<a href="/pdf/2402.15905" title="Download PDF">pdf</a>, <a href="/format/2402.15905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustari%2C+A">Ashfiqun Mustari</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+R">Rushmia Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Tasnim%2C+A">Afsara Tasnim</a>, 
<a href="/search/cs?searchtype=author&query=Juthi%2C+J+S">Jakia Sultana Juthi</a>, 
<a href="/search/cs?searchtype=author&query=Shahariar%2C+G+M">G M Shahariar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented in 26th International Conference on Computer and Information Technology (ICCIT 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes an efficient system for classifying cervical cancer cells
using pre-trained convolutional neural networks (CNNs). We first fine-tune five
pre-trained CNNs and minimize the overall cost of misclassification by
prioritizing accuracy for certain classes that have higher associated costs or
importance. To further enhance the performance of the models, supervised
contrastive learning is included to make the models more adept at capturing
important features and patterns. Extensive experimentation are conducted to
evaluate the proposed system on the SIPaKMeD dataset. The experimental results
demonstrate the effectiveness of the developed system, achieving an accuracy of
97.29%. To make our system more trustworthy, we have employed several
explainable AI techniques to interpret how the models reached a specific
decision. The implementation of the system can be found at -
https://github.com/isha-67/CervicalCancerStudy.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15909" title="Abstract">arXiv:2402.15909</a> [<a href="/pdf/2402.15909" title="Download PDF">pdf</a>, <a href="/format/2402.15909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Droplet Analysis Using Generative Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Tan-Hanh Pham</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Kim-Doang Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Precision devices play an important role in enhancing production quality and
productivity in agricultural systems. Therefore, the optimization of these
devices is essential in precision agriculture. Recently, with the advancements
of deep learning, there have been several studies aiming to harness its
capabilities for improving spray system performance. However, the effectiveness
of these methods heavily depends on the size of the training dataset, which is
expensive and time-consuming to collect. To address the challenge of
insufficient training samples, this paper proposes an alternative solution by
generating artificial images of droplets using generative adversarial networks
(GAN). The GAN model is trained by using a small dataset captured by a
high-speed camera and capable of generating images with progressively
increasing resolution. The results demonstrate that the model can generate
high-quality images with the size of $1024\times1024$. Furthermore, this
research leverages recent advancements in computer vision and deep learning to
develop a light droplet detector using the synthetic dataset. As a result, the
detection model achieves a 16.06\% increase in mean average precision (mAP)
when utilizing the synthetic dataset. To the best of our knowledge, this work
stands as the first to employ a generative model for augmenting droplet
detection. Its significance lies not only in optimizing nozzle design for
constructing efficient spray systems but also in addressing the common
challenge of insufficient data in various precision agriculture tasks. This
work offers a critical contribution to conserving resources while striving for
optimal and sustainable agricultural practices.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15911" title="Abstract">arXiv:2402.15911</a> [<a href="/pdf/2402.15911" title="Download PDF">pdf</a>, <a href="/format/2402.15911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRP: Propagating Universal Perturbations to Attack Large Language Model  Guard-Rails
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangaokar%2C+N">Neal Mangaokar</a>, 
<a href="/search/cs?searchtype=author&query=Hooda%2C+A">Ashish Hooda</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihye Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chandrashekaran%2C+S">Shreyas Chandrashekaran</a>, 
<a href="/search/cs?searchtype=author&query=Fawaz%2C+K">Kassem Fawaz</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+A">Atul Prakash</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) are typically aligned to be harmless to humans.
Unfortunately, recent work has shown that such models are susceptible to
automated jailbreak attacks that induce them to generate harmful content. More
recent LLMs often incorporate an additional layer of defense, a Guard Model,
which is a second LLM that is designed to check and moderate the output
response of the primary LLM. Our key contribution is to show a novel attack
strategy, PRP, that is successful against several open-source (e.g., Llama 2)
and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP
leverages a two step prefix-based attack that operates by (a) constructing a
universal adversarial prefix for the Guard Model, and (b) propagating this
prefix to the response. We find that this procedure is effective across
multiple threat models, including ones in which the adversary has no access to
the Guard Model at all. Our work suggests that further advances are required on
defenses and Guard Models before they can be considered effective.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15917" title="Abstract">arXiv:2402.15917</a> [<a href="/pdf/2402.15917" title="Download PDF">pdf</a>, <a href="/format/2402.15917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Finite Element Model for Hydro-thermal Convective Flow in a Porous  Medium: Effects of Hydraulic Resistivity and Thermal Diffusivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mallikarjunaiah%2C+S+M">S. M. Mallikarjunaiah</a>, 
<a href="/search/math?searchtype=author&query=Bhatta%2C+D">Dambaru Bhatta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this article, a finite element model is implemented to analyze
hydro-thermal convective flow in a porous medium. The mathematical model
encompasses Darcy's law for incompressible fluid behavior, which is coupled
with a convection-diffusion-type energy equation to characterize the
temperature in the porous medium. The current investigation presents an
efficient, stable, and accurate finite element discretization for the
hydro-thermal convective flow model. The well-posedness of the proposed
discrete Galerkin finite element formulation is guaranteed due to the
decoupling property and the linearity of the numerical method. Computational
experiments confirm the optimal convergence rates for a manufactured solution.
Several numerical results are obtained for the variations of the hydraulic
resistivity and thermal diffusivity. In the present study, the bottom wall is
maintained at a constant higher hot temperature while side vertical walls are
thermally insulated and the top wall is maintained at a constant cold
temperature. Heat transfer rates at the heated bottom wall are presented in
terms of local Nusselt number. A linear variation in hydraulic resistivity and
a quadratic variation in thermal diffusivity show an increase in the heat
transfer rate.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15921" title="Abstract">arXiv:2402.15921</a> [<a href="/pdf/2402.15921" title="Download PDF">pdf</a>, <a href="/format/2402.15921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretraining Strategy for Neural Potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zehua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijie Li</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">We propose a mask pretraining method for Graph Neural Networks (GNNs) to
improve their performance on fitting potential energy surfaces, particularly in
water systems. GNNs are pretrained by recovering spatial information related to
masked-out atoms from molecules, then transferred and finetuned on atomic
forcefields. Through such pretraining, GNNs learn meaningful prior about
structural and underlying physical information of molecule systems that are
useful for downstream tasks. From comprehensive experiments and ablation
studies, we show that the proposed method improves the accuracy and convergence
speed compared to GNNs trained from scratch or using other pretraining
techniques such as denoising. On the other hand, our pretraining method is
suitable for both energy-centric and force-centric GNNs. This approach
showcases its potential to enhance the performance and data efficiency of GNNs
in fitting molecular force fields.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15923" title="Abstract">arXiv:2402.15923</a> [<a href="/pdf/2402.15923" title="Download PDF">pdf</a>, <a href="/format/2402.15923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Outcomes in Video Games with Long Short Term Memory Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chulajata%2C+K">Kittimate Chulajata</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Sean Wu</a>, 
<a href="/search/cs?searchtype=author&query=Scalzo%2C+F">Fabien Scalzo</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+E+S">Eun Sang Cha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 Figures, 2 Tables. Kittimate Chulajata and Sean Wu are considered co-first authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15925" title="Abstract">arXiv:2402.15925</a> [<a href="/pdf/2402.15925" title="Download PDF">pdf</a>, <a href="/format/2402.15925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiContrievers: Analysis of Dense Retrieval Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldfarb-Tarrant%2C+S">Seraphina Goldfarb-Tarrant</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pedro Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi-Yu%2C+J">Jane Dwivedi-Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+P">Patrick Lewis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Dense retrievers compress source documents into (possibly lossy) vector
representations, yet there is little analysis of what information is lost
versus preserved, and how it affects downstream tasks. We conduct the first
analysis of the information captured by dense retrievers compared to the
language models they are based on (e.g., BERT versus Contriever). We use 25
MultiBert checkpoints as randomized initialisations to train MultiContrievers,
a set of 25 contriever models. We test whether specific pieces of information
-- such as gender and occupation -- can be extracted from contriever vectors of
wikipedia-like documents. We measure this extractability via information
theoretic probing. We then examine the relationship of extractability to
performance and gender bias, as well as the sensitivity of these results to
many random initialisations and data shuffles. We find that (1) contriever
models have significantly increased extractability, but extractability usually
correlates poorly with benchmark performance 2) gender bias is present, but is
not caused by the contriever representations 3) there is high sensitivity to
both random initialisation and to data shuffle, suggesting that future
retrieval research should test across a wider spread of both.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15926" title="Abstract">arXiv:2402.15926</a> [<a href="/pdf/2402.15926" title="Download PDF">pdf</a>, <a href="/format/2402.15926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of  the Loss Improves Optimization Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingfeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Telgarsky%2C+M">Matus Telgarsky</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider gradient descent (GD) with a constant stepsize applied to
logistic regression with linearly separable data, where the constant stepsize
$\eta$ is so large that the loss initially oscillates. We show that GD exits
this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and
subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate
after $t$ additional steps. Our results imply that, given a budget of $T$
steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with
an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or
variable stepsize schedulers. Our proof technique is versatile and also handles
general classification loss functions (where exponential tails are needed for
the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the
neural tangent kernel regime, and online stochastic gradient descent (SGD) with
a large stepsize, under suitable separability conditions.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15928" title="Abstract">arXiv:2402.15928</a> [<a href="/pdf/2402.15928" title="Download PDF">pdf</a>, <a href="/format/2402.15928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing BDD Software Testing: Dynamic Scenario Re-Usability And Step  Auto-Complete For Cucumber Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mughal%2C+A+H">A. H. Mughal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, multiple code segments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY); Programming Languages (cs.PL)

</div>
<p class="mathjax">This paper presents and implements the re-usability of scenarios within
scenarios for behavior-driven development (BDD) Gherkin test scripts in the
Cucumber Java framework. Though the focus of the presented work is on scenario
re-usability through an implementation within the Cucumber BDD Java framework,
the paper also dives a little into the limitations of Cucumber single-threaded
scenario execution model. This implementation increases the modularity and
efficiency of the test suite. The paper also discusses VSCode step definition
auto-completion integration, simplifying the test script writing process. This
functionality is handy to Quality Assurance(QA) test writers, allowing instant
access to relevant step definitions. In addition, the use of these methods in a
popular continuous integration and delivery platform Jenkins as a Maven Java
project is discussed. This integration with Jenkins, facilitates for more
efficient test automation for continuous deployment scenarios. Empirical
research and practical applications reveal significant improvements in the
speed and efficiency of test writing, which is especially valuable for large
and complex software projects. Integrating these methods into traditional
sequential BDD practices paves the way towards more effective, efficient, and
sustainable test automation strategies.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15929" title="Abstract">arXiv:2402.15929</a> [<a href="/pdf/2402.15929" title="Download PDF">pdf</a>, <a href="/format/2402.15929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+I">Isha Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V+V">Vedaant V. Jain</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gagandeep Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated impressive performance on
several benchmarks. However, traditional studies do not provide formal
guarantees on the performance of LLMs. In this work, we propose a novel
certification framework for LLM, QuaCer-C, wherein we formally certify the
knowledge-comprehension capabilities of popular LLMs. Our certificates are
quantitative - they consist of high-confidence, tight bounds on the probability
that the target LLM gives the correct answer on any relevant knowledge
comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs
indicate that the knowledge comprehension capability improves with an increase
in the number of parameters and that the Mistral model is less performant than
the rest in this evaluation.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15930" title="Abstract">arXiv:2402.15930</a> [<a href="/pdf/2402.15930" title="Download PDF">pdf</a>, <a href="/ps/2402.15930" title="Download PostScript">ps</a>, <a href="/format/2402.15930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Prompting Strategies for Grammatical Error Correction Based  on Language Proficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Min Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+J">Jiexin Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+M">Mengyang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jayoung Song</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungyeul Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in LREC-COLING 2024, short paper (preprint)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The writing examples of English language learners may be different from those
of native speakers. Given that there is a significant differences in second
language (L2) learners' error types by their proficiency levels, this paper
attempts to reduce overcorrection by examining the interaction between LLM's
performance and L2 language proficiency. Our method focuses on zero-shot and
few-shot prompting and fine-tuning models for GEC for learners of English as a
foreign language based on the different proficiency. We investigate GEC results
and find that overcorrection happens primarily in advanced language learners'
writing (proficiency C) rather than proficiency A (a beginner level) and
proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot
prompting with writing examples of English learners, actually tend to exhibit
decreased recall measures. To make our claim concrete, we conduct a
comprehensive examination of GEC outcomes and their evaluation results based on
language proficiency.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15931" title="Abstract">arXiv:2402.15931</a> [<a href="/pdf/2402.15931" title="Download PDF">pdf</a>, <a href="/ps/2402.15931" title="Download PostScript">ps</a>, <a href="/format/2402.15931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frustratingly Simple Prompting-based Text Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungyeul Park</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+M">Mengyang Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a Tiny Paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper introduces a novel perspective on the automated essay scoring
(AES) task, challenging the conventional view of the ASAP dataset as a static
entity. Employing simple text denoising techniques using prompting, we explore
the dynamic potential within the dataset. While acknowledging the previous
emphasis on building regression systems, our paper underscores how making minor
changes to a dataset through text denoising can enhance the final results.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15932" title="Abstract">arXiv:2402.15932</a> [<a href="/pdf/2402.15932" title="Download PDF">pdf</a>, <a href="/format/2402.15932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A  Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Selim%2C+A">Alaa Selim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanzhu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In the rapidly evolving domain of electrical power systems, the Volt-VAR
optimization (VVO) is increasingly critical, especially with the burgeoning
integration of renewable energy sources. Traditional approaches to
learning-based VVO in expansive and dynamically changing power systems are
often hindered by computational complexities. To address this challenge, our
research presents a novel framework that harnesses the potential of Deep
Reinforcement Learning (DRL), specifically utilizing the Importance Weighted
Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform.
This framework, built upon RLlib-an industry-standard in Reinforcement
Learning-ingeniously capitalizes on the distributed computing capabilities and
advanced hyperparameter tuning offered by RAY. This design significantly
expedites the exploration and exploitation phases in the VVO solution space.
Our empirical results demonstrate that our approach not only surpasses existing
DRL methods in achieving superior reward outcomes but also manifests a
remarkable tenfold reduction in computational requirements. The integration of
our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a
novel framework that efficiently uses RAY's resources to improve system
adaptability and control. RLlib-IMPALA leverages RAY's toolkit to enhance
analytical capabilities and significantly speeds up training to become more
than 10 times faster than other state-of-the-art DRL methods.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15933" title="Abstract">arXiv:2402.15933</a> [<a href="/pdf/2402.15933" title="Download PDF">pdf</a>, <a href="/format/2402.15933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion  Approach for 3D VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+W">Wentao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in AAAI 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated
data and limited visual content diversity hampers the generalization to novel
scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and
SQA dataset). Current approaches resort supplement 3D reasoning with 2D
information. However, these methods face challenges: either they use top-down
2D views that introduce overly complex and sometimes question-irrelevant visual
clues, or they rely on globally aggregated scene/image-level representations
from 2D VLMs, losing the fine-grained vision-language correlations. To overcome
these limitations, our approach utilizes question-conditional 2D view selection
procedure, pinpointing semantically relevant 2D inputs for crucial visual
clues. We then integrate this 2D knowledge into the 3D-VQA system via a
two-branch Transformer structure. This structure, featuring a Twin-Transformer
design, compactly combines 2D and 3D modalities and captures fine-grained
correlations between modalities, allowing them mutually augmenting each other.
Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh
perspective on multi-modal transformer-based architectures for 3D-VQA.
Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets
and significantly outperforms existing solutions. Code is available at
$\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15937" title="Abstract">arXiv:2402.15937</a> [<a href="/pdf/2402.15937" title="Download PDF">pdf</a>, <a href="/format/2402.15937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolation-based immersogeometric analysis methods for multi-material  and multi-physics problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fromm%2C+J+E">Jennifer E. Fromm</a>, 
<a href="/search/math?searchtype=author&query=Wunsch%2C+N">Nils Wunsch</a>, 
<a href="/search/math?searchtype=author&query=Maute%2C+K">Kurt Maute</a>, 
<a href="/search/math?searchtype=author&query=Evans%2C+J+A">John A. Evans</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jiun-Shyan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Immersed boundary methods are high-order accurate computational tools used to
model geometrically complex problems in computational mechanics. While
traditional finite element methods require the construction of high-quality
boundary-fitted meshes, immersed boundary methods instead embed the
computational domain in a background grid. Interpolation-based immersed
boundary methods augment existing finite element software to non-invasively
implement immersed boundary capabilities through extraction. Extraction
interpolates the background basis as a linear combination of Lagrange
polynomials defined on a foreground mesh, creating an interpolated basis that
can be easily integrated by existing methods. This work extends the
interpolation-based immersed boundary method to multi-material and
multi-physics problems. Beginning from level-set descriptions of domain
geometries, Heaviside enrichment is implemented to accommodate discontinuities
in state variable fields across material interfaces. Adaptive refinement with
truncated hierarchical B-splines is used to both improve interface geometry
representations and resolve large solution gradients near interfaces.
Multi-physics problems typically involve coupled fields where each field has
unique discretization requirements. This work presents a novel discretization
method for coupled problems through the application of extraction, using a
single foreground mesh for all fields. Numerical examples illustrate optimal
convergence rates for this method in both 2D and 3D, for heat conduction,
linear elasticity, and a coupled thermo-mechanical problem. The utility of this
method is demonstrated through image-based analysis of a composite sample,
where in addition to circumventing typical meshing difficulties, this method
reduces the required degrees of freedom compared to classical boundary-fitted
finite element methods.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15938" title="Abstract">arXiv:2402.15938</a> [<a href="/pdf/2402.15938" title="Download PDF">pdf</a>, <a href="/format/2402.15938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization or Memorization: Data Contamination and Trustworthy  Evaluation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yihong Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huanyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Recent statements about the impressive capabilities of large language models
(LLMs) are usually supported by evaluating on open-access benchmarks.
Considering the vast size and wide-ranging sources of LLMs' training data, it
could explicitly or implicitly include test data, leading to LLMs being more
susceptible to data contamination. However, due to the opacity of training
data, the black-box access of models, and the rapid growth of synthetic
training data, detecting and mitigating data contamination for LLMs faces
significant challenges. In this paper, we propose CDD, which stands for
Contamination Detection via output Distribution for LLMs. CDD necessitates only
the sampled texts to detect data contamination, by identifying the peakedness
of LLM's output distribution. To mitigate the impact of data contamination in
evaluation, we also present TED: Trustworthy Evaluation via output
Distribution, based on the correction of LLM's output distribution. To
facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,
for data contamination detection and contamination mitigation evaluation tasks.
Extensive experimental results show that CDD achieves the average relative
improvements of 21.8\%-30.2\% over other contamination detection approaches in
terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect
contamination caused by the variants of test data. TED significantly mitigates
performance improvements up to 66.9\% attributed to data contamination across
24 settings and 21 contamination degrees. In real-world applications, we reveal
that ChatGPT exhibits a high potential to suffer from data contamination on
HumanEval benchmark.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15940" title="Abstract">arXiv:2402.15940</a> [<a href="/pdf/2402.15940" title="Download PDF">pdf</a>, <a href="/format/2402.15940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-performance finite elements with MFEM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrej%2C+J">Julian Andrej</a>, 
<a href="/search/cs?searchtype=author&query=Atallah%2C+N">Nabil Atallah</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4cker%2C+J">Jan-Phillip B&#xe4;cker</a>, 
<a href="/search/cs?searchtype=author&query=Camier%2C+J">John Camier</a>, 
<a href="/search/cs?searchtype=author&query=Copeland%2C+D">Dylan Copeland</a>, 
<a href="/search/cs?searchtype=author&query=Dobrev%2C+V">Veselin Dobrev</a>, 
<a href="/search/cs?searchtype=author&query=Dudouit%2C+Y">Yohann Dudouit</a>, 
<a href="/search/cs?searchtype=author&query=Duswald%2C+T">Tobias Duswald</a>, 
<a href="/search/cs?searchtype=author&query=Keith%2C+B">Brendan Keith</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dohyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kolev%2C+T">Tzanio Kolev</a>, 
<a href="/search/cs?searchtype=author&query=Lazarov%2C+B">Boyan Lazarov</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+K">Ketan Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Pazner%2C+W">Will Pazner</a>, 
<a href="/search/cs?searchtype=author&query=Petrides%2C+S">Socratis Petrides</a>, 
<a href="/search/cs?searchtype=author&query=Shiraiwa%2C+S">Syun&#x27;ichi Shiraiwa</a>, 
<a href="/search/cs?searchtype=author&query=Stowell%2C+M">Mark Stowell</a>, 
<a href="/search/cs?searchtype=author&query=Tomov%2C+V">Vladimir Tomov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The MFEM (Modular Finite Element Methods) library is a high-performance C++
library for finite element discretizations. MFEM supports numerous types of
finite element methods and is the discretization engine powering many
computational physics and engineering applications across a number of domains.
This paper describes some of the recent research and development in MFEM,
focusing on performance portability across leadership-class supercomputing
facilities, including exascale supercomputers, as well as new capabilities and
functionality, enabling a wider range of applications. Much of this work was
undertaken as part of the Department of Energy's Exascale Computing Project
(ECP) in collaboration with the Center for Efficient Exascale Discretizations
(CEED).
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15941" title="Abstract">arXiv:2402.15941</a> [<a href="/pdf/2402.15941" title="Download PDF">pdf</a>, <a href="/format/2402.15941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementing Recycling Methods for Linear Systems in Python with an  Application to Multiple Objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garcia%2C+A">Ainara Garcia</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+S">Sihong Xie</a>, 
<a href="/search/math?searchtype=author&query=Carr%2C+A">Arielle Carr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sequences of linear systems arise in the predictor-corrector method when
computing the Pareto front for multi-objective optimization. Rather than
discarding information generated when solving one system, it may be
advantageous to recycle information for subsequent systems. To accomplish this,
we seek to reduce the overall cost of computation when solving linear systems
using common recycling methods. In this work, we assessed the performance of
recycling minimum residual (RMINRES) method along with a map between
coefficient matrices. For these methods to be fully integrated into the
software used in Enouen et al. (2022), there must be working version of each in
both Python and PyTorch. Herein, we discuss the challenges we encountered and
solutions undertaken (and some ongoing) when computing efficient Python
implementations of these recycling strategies. The goal of this project was to
implement RMINRES in Python and PyTorch and add it to the established Pareto
front code to reduce computational cost. Additionally, we wanted to implement
the sparse approximate maps code in Python and PyTorch, so that it can be
parallelized in future work.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15943" title="Abstract">arXiv:2402.15943</a> [<a href="/pdf/2402.15943" title="Download PDF">pdf</a>, <a href="/ps/2402.15943" title="Download PostScript">ps</a>, <a href="/format/2402.15943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Software Engineering in the Era of Foundation Models: A  Curated Catalogue of Challenges in the Development of Trustworthy FMware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+A+E">Ahmed E. Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dayi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rajbahadur%2C+G+K">Gopi Krishnan Rajbahadur</a>, 
<a href="/search/cs?searchtype=author&query=Gallaba%2C+K">Keheliya Gallaba</a>, 
<a href="/search/cs?searchtype=author&query=Cogo%2C+F+R">Filipe R. Cogo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Thangarajah%2C+K">Kishanthan Thangarajah</a>, 
<a href="/search/cs?searchtype=author&query=Oliva%2C+G+A">Gustavo Ansaldi Oliva</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiahuei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+W+M">Wali Mohammad Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z+M">Zhen Ming Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Foundation models (FMs), such as Large Language Models (LLMs), have
revolutionized software development by enabling new use cases and business
models. We refer to software built using FMs as FMware. The unique properties
of FMware (e.g., prompts, agents, and the need for orchestration), coupled with
the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new
set of software engineering challenges. Based on our industrial experience, we
identified 10 key SE4FMware challenges that have caused enterprise FMware
development to be unproductive, costly, and risky. In this paper, we discuss
these challenges in detail and state the path for innovation that we envision.
Next, we present FMArts, which is our long-term effort towards creating a
cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we
(i) show how the unique properties of FMArts enabled us to design and develop a
complex FMware for a large customer in a timely manner and (ii) discuss the
lessons that we learned in doing so. We hope that the disclosure of the
aforementioned challenges and our associated efforts to tackle them will not
only raise awareness but also promote deeper and further discussions, knowledge
sharing, and innovative solutions across the software engineering discipline.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15944" title="Abstract">arXiv:2402.15944</a> [<a href="/pdf/2402.15944" title="Download PDF">pdf</a>, <a href="/format/2402.15944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On A Class of Greedy Sparse Recovery Algorithms -- A High Dimensional  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiuwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W+A">Wu Angela Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Sparse signal recovery deals with finding the sparest solution of an
under-determined linear system $x = Qs$. In this paper, we propose a novel
greedy approach to addressing the challenges from such a problem. Such an
approach is based on a characterization of solutions to the system, which
allows us to work on the sparse recovery in the $s$-space directly with a given
measure. With $l_2$-based measure, two OMP-type algorithms are proposed, which
significantly outperform the classical OMP algorithm in terms of recovery
accuracy while maintaining comparable computational complexity. An $l_1$-based
algorithm, denoted as $\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is
derived. Such an algorithm significantly outperforms the classical BP
algorithm. A CoSaMP-type algorithm is also proposed to further enhance the
performance of the two proposed OMP-type algorithms. The superior performance
of our proposed algorithms is demonstrated through extensive numerical
simulations using synthetic data as well as video signals, highlighting their
potential for various applications in compressed sensing and signal processing.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15945" title="Abstract">arXiv:2402.15945</a> [<a href="/pdf/2402.15945" title="Download PDF">pdf</a>, <a href="/ps/2402.15945" title="Download PostScript">ps</a>, <a href="/format/2402.15945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to  Cybersecurity Threat Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+M+A">Mohammed Abo Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes an innovative Attention-GAN framework for enhancing
cybersecurity, focusing on anomaly detection. In response to the challenges
posed by the constantly evolving nature of cyber threats, the proposed approach
aims to generate diverse and realistic synthetic attack scenarios, thereby
enriching the dataset and improving threat identification. Integrating
attention mechanisms with Generative Adversarial Networks (GANs) is a key
feature of the proposed method. The attention mechanism enhances the model's
ability to focus on relevant features, essential for detecting subtle and
complex attack patterns. In addition, GANs address the issue of data scarcity
by generating additional varied attack data, encompassing known and emerging
threats. This dual approach ensures that the system remains relevant and
effective against the continuously evolving cyberattacks. The KDD Cup and
CICIDS2017 datasets were used to validate this model, which exhibited
significant improvements in anomaly detection. It achieved an accuracy of
99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision,
recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing
complex attack patterns. This study contributes significantly to cybersecurity
by providing a scalable and adaptable solution for anomaly detection in the
face of sophisticated and dynamic cyber threats. The exploration of GANs for
data augmentation highlights a promising direction for future research,
particularly in situations where data limitations restrict the development of
cybersecurity systems. The attention-GAN framework has emerged as a pioneering
approach, setting a new benchmark for advanced cyber-defense strategies.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15951" title="Abstract">arXiv:2402.15951</a> [<a href="/pdf/2402.15951" title="Download PDF">pdf</a>, <a href="/format/2402.15951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GreenLLaMA: A Framework for Detoxification with Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khondaker%2C+M+T+I">Md Tawkat Islam Khondaker</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Lakshmanan%2C+L+V+S">Laks V. S. Lakshmanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Prior works on detoxification are scattered in the sense that they do not
cover all aspects of detoxification needed in a real-world scenario. Notably,
prior works restrict the task of developing detoxification models to only a
seen subset of platforms, leaving the question of how the models would perform
on unseen platforms unexplored. Additionally, these works do not address
non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified
without altering the meaning. We propose GreenLLaMA, the first comprehensive
end-to-end detoxification framework, which attempts to alleviate the
aforementioned limitations. We first introduce a cross-platform pseudo-parallel
corpus applying multi-step data processing and generation strategies leveraging
ChatGPT. We then train a suite of detoxification models with our cross-platform
corpus. We show that our detoxification models outperform the SoTA model
trained with human-annotated parallel corpus. We further introduce explanation
to promote transparency and trustworthiness. GreenLLaMA additionally offers a
unique paraphrase detector especially dedicated for the detoxification task to
tackle the non-detoxifiable cases. Through experimental analysis, we
demonstrate the effectiveness of our cross-platform corpus and the robustness
of GreenLLaMA against adversarial toxicity.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15952" title="Abstract">arXiv:2402.15952</a> [<a href="/pdf/2402.15952" title="Download PDF">pdf</a>, <a href="/format/2402.15952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViSTec: Video Modeling for Sports Technique Recognition and Tactical  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuchen He</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zeqing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+D">Dazhen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by AAAI-24 Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The immense popularity of racket sports has fueled substantial demand in
tactical analysis with broadcast videos. However, existing manual methods
require laborious annotation, and recent attempts leveraging video perception
models are limited to low-level annotations like ball trajectories, overlooking
tactics that necessitate an understanding of stroke techniques.
State-of-the-art action segmentation models also struggle with technique
recognition due to frequent occlusions and motion-induced blurring in racket
sports videos. To address these challenges, We propose ViSTec, a Video-based
Sports Technique recognition model inspired by human cognition that synergizes
sparse visual data with rich contextual insights. Our approach integrates a
graph to explicitly model strategic knowledge in stroke sequences and enhance
technique recognition with contextual inductive bias. A two-stage action
perception model is jointly trained to align with the contextual knowledge in
the graph. Experiments demonstrate that our method outperforms existing models
by a significant margin. Case studies with experts from the Chinese national
table tennis team validate our model's capacity to automate analysis for
technical actions and tactical strategies. More details are available at:
https://ViSTec2024.github.io/.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15953" title="Abstract">arXiv:2402.15953</a> [<a href="/pdf/2402.15953" title="Download PDF">pdf</a>, <a href="/format/2402.15953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolution and Cross-Correlation of Count Sketches Enables Fast  Cardinality Estimation of Multi-Join Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heddes%2C+M">Mike Heddes</a>, 
<a href="/search/cs?searchtype=author&query=Nunes%2C+I">Igor Nunes</a>, 
<a href="/search/cs?searchtype=author&query=Givargis%2C+T">Tony Givargis</a>, 
<a href="/search/cs?searchtype=author&query=Nicolau%2C+A">Alex Nicolau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the International Conference on Management of Data 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">With the increasing rate of data generated by critical systems, estimating
functions on streaming data has become essential. This demand has driven
numerous advancements in algorithms designed to efficiently query and analyze
one or more data streams while operating under memory constraints. The primary
challenge arises from the rapid influx of new items, requiring algorithms that
enable efficient incremental processing of streams in order to keep up. A
prominent algorithm in this domain is the AMS sketch. Originally developed to
estimate the second frequency moment of a data stream, it can also estimate the
cardinality of the equi-join between two relations. Since then, two important
advancements are the Count sketch, a method which significantly improves upon
the sketch update time, and secondly, an extension of the AMS sketch to
accommodate multi-join queries. However, combining the strengths of these
methods to maintain sketches for multi-join queries while ensuring fast update
times is a non-trivial task, and has remained an open problem for decades as
highlighted in the existing literature. In this work, we successfully address
this problem by introducing a novel sketching method which has fast updates,
even for sketches capable of accurately estimating the cardinality of complex
multi-join queries. We prove that our estimator is unbiased and has the same
error guarantees as the AMS-based method. Our experimental results confirm the
significant improvement in update time complexity, resulting in orders of
magnitude faster estimates, with equal or better estimation accuracy.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15957" title="Abstract">arXiv:2402.15957</a> [<a href="/pdf/2402.15957" title="Download PDF">pdf</a>, <a href="/format/2402.15957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+A">Anthony Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+G">Guy Tennenholtz</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chih-wei Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+Y">Yinlam Chow</a>, 
<a href="/search/cs?searchtype=author&query=B%C4%B1y%C4%B1k%2C+E">Erdem B&#x131;y&#x131;k</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to
approximate inference in environments where the latent state evolves at varying
rates. We model episode sessions - parts of the episode where the latent state
is fixed - and propose three key modifications to existing meta-RL methods:
consistency of latent information within sessions, session masking, and prior
latent conditioning. We demonstrate the importance of these modifications in
various domains, ranging from discrete Gridworld environments to
continuous-control and simulated robot assistive tasks, demonstrating that
DynaMITE-RL significantly outperforms state-of-the-art baselines in sample
efficiency and inference returns.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15958" title="Abstract">arXiv:2402.15958</a> [<a href="/pdf/2402.15958" title="Download PDF">pdf</a>, <a href="/format/2402.15958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the dynamics of three-layer neural networks: initial condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng-an Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tao Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Empirical and theoretical works show that the input weights of two-layer
neural networks, when initialized with small values, converge towards isolated
orientations. This phenomenon, referred to as condensation, indicates that the
gradient descent methods tend to spontaneously reduce the complexity of neural
networks during the training process. In this work, we elucidate the mechanisms
behind the condensation phenomena occurring in the training of three-layer
neural networks and distinguish it from the training of two-layer neural
networks. Through rigorous theoretical analysis, we establish the blow-up
property of effective dynamics and present a sufficient condition for the
occurrence of condensation, findings that are substantiated by experimental
results. Additionally, we explore the association between condensation and the
low-rank bias observed in deep matrix factorization.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15959" title="Abstract">arXiv:2402.15959</a> [<a href="/pdf/2402.15959" title="Download PDF">pdf</a>, <a href="/format/2402.15959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Image Stitching: An Adaptive Resistance Learning against  Compatible Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhiying Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Risheng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image stitching seamlessly integrates images captured from varying
perspectives into a single wide field-of-view image. Such integration not only
broadens the captured scene but also augments holistic perception in computer
vision applications. Given a pair of captured images, subtle perturbations and
distortions which go unnoticed by the human visual system tend to attack the
correspondence matching, impairing the performance of image stitching
algorithms. In light of this challenge, this paper presents the first attempt
to improve the robustness of image stitching against adversarial attacks.
Specifically, we introduce a stitching-oriented attack~(SoA), tailored to
amplify the alignment loss within overlapping regions, thereby targeting the
feature matching procedure. To establish an attack resistant model, we delve
into the robustness of stitching architecture and develop an adaptive
adversarial training~(AAT) to balance attack resistance with stitching
precision. In this way, we relieve the gap between the routine adversarial
training and benign models, ensuring resilience without quality compromise.
Comprehensive evaluation across real-world and synthetic datasets validate the
deterioration of SoA on stitching performance. Furthermore, AAT emerges as a
more robust solution against adversarial perturbations, delivering superior
stitching results. Code is available at:https://github.com/Jzy2017/TRIS.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15960" title="Abstract">arXiv:2402.15960</a> [<a href="/pdf/2402.15960" title="Download PDF">pdf</a>, <a href="/format/2402.15960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Budget-Constrained Tool Learning with Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuanhang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Despite intensive efforts devoted to tool learning, the problem of
budget-constrained tool learning, which focuses on resolving user queries
within a specific budget constraint, has been widely overlooked. This paper
proposes a novel method for budget-constrained tool learning. Our approach
involves creating a preferable plan under the budget constraint before
utilizing the tools. This plan outlines the feasible tools and the maximum
number of times they can be employed, offering a comprehensive overview of the
tool learning process for large language models. This allows them to allocate
the budget from a broader perspective. To devise the plan without incurring
significant extra costs, we suggest initially estimating the usefulness of the
candidate tools based on past experience. Subsequently, we employ dynamic
programming to formulate the plan. Experimental results demonstrate that our
method can be integrated with various tool learning methods, significantly
enhancing their effectiveness under strict budget constraints.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15961" title="Abstract">arXiv:2402.15961</a> [<a href="/pdf/2402.15961" title="Download PDF">pdf</a>, <a href="/format/2402.15961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VOLoc: Visual Place Recognition by Querying Compressed Lidar Map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xudong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongcai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yu Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Deying Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7figures, ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The availability of city-scale Lidar maps enables the potential of city-scale
place recognition using mobile cameras. However, the city-scale Lidar maps
generally need to be compressed for storage efficiency, which increases the
difficulty of direct visual place recognition in compressed Lidar maps. This
paper proposes VOLoc, an accurate and efficient visual place recognition method
that exploits geometric similarity to directly query the compressed Lidar map
via the real-time captured image sequence. In the offline phase, VOLoc
compresses the Lidar maps using a \emph{Geometry-Preserving Compressor} (GPC),
in which the compression is reversible, a crucial requirement for the
downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online
Geometric Recovery Module (GRM), which is composed of online Visual Odometry
(VO) and a point cloud optimization module, such that the local scene structure
around the camera is online recovered to build the \emph{Querying Point Cloud}
(QPC). Then the QPC is compressed by the same GPC, and is aggregated into a
global descriptor by an attention-based aggregation module, to query the
compressed Lidar map in the vector space. A transfer learning mechanism is also
proposed to improve the accuracy and the generality of the aggregation network.
Extensive evaluations show that VOLoc provides localization accuracy even
better than the Lidar-to-Lidar place recognition, setting up a new record for
utilizing the compressed Lidar map by low-end mobile cameras. The code are
publicly available at https://github.com/Master-cai/VOLoc.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15962" title="Abstract">arXiv:2402.15962</a> [<a href="/pdf/2402.15962" title="Download PDF">pdf</a>, <a href="/ps/2402.15962" title="Download PostScript">ps</a>, <a href="/format/2402.15962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical energy signatures using machine learning for operational  visibility and diagnostics in automotive manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Ankur Verma</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Seog-Chan Oh</a>, 
<a href="/search/cs?searchtype=author&query=Arinez%2C+J">Jorge Arinez</a>, 
<a href="/search/cs?searchtype=author&query=Kumara%2C+S">Soundar Kumara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Manufacturing energy consumption data contains important process signatures
required for operational visibility and diagnostics. These signatures may be of
different temporal scales, ranging from monthly to sub-second resolutions. We
introduce a hierarchical machine learning approach to identify automotive
process signatures from paint shop electricity consumption data at varying
temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and Principal Component Analysis (PCA)
combined with Logistic Regression (LR) are used for the analysis. We validate
the utility of the developed algorithms with subject matter experts for (i)
better operational visibility, and (ii) identifying energy saving
opportunities.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15966" title="Abstract">arXiv:2402.15966</a> [<a href="/pdf/2402.15966" title="Download PDF">pdf</a>, <a href="/format/2402.15966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven micromorphic mechanics for materials with strain  localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ulloa%2C+J">Jacinto Ulloa</a>, 
<a href="/search/math?searchtype=author&query=Stainier%2C+L">Laurent Stainier</a>, 
<a href="/search/math?searchtype=author&query=Ortiz%2C+M">Michael Ortiz</a>, 
<a href="/search/math?searchtype=author&query=Andrade%2C+J+E">Jos&#xe9; E. Andrade</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper explores the role of generalized continuum mechanics, and the
feasibility of model-free data-driven computing approaches thereof, in solids
undergoing failure by strain localization. Specifically, we set forth a
methodology for capturing material instabilities using data-driven mechanics
without prior information regarding the failure mode. We show numerically that,
in problems involving strain localization, the standard data-driven framework
for Cauchy/Boltzmann continua fails to capture the length scale of the
material, as expected. We address this shortcoming by formulating a generalized
data-driven framework for micromorphic continua that effectively captures both
stiffness and length-scale information, as encoded in the material data, in a
model-free manner. These properties are exhibited systematically in a
one-dimensional softening bar problem and further verified through selected
plane-strain problems.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15967" title="Abstract">arXiv:2402.15967</a> [<a href="/pdf/2402.15967" title="Download PDF">pdf</a>, <a href="/format/2402.15967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Punjabi to English speech translation using discrete units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaur%2C+P">Prabhjot Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Bush%2C+L+A+M">L. Andrew M. Bush</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weisong Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech-to-speech translation is yet to reach the same level of coverage as
text-to-text translation systems. The current speech technology is highly
limited in its coverage of over 7000 languages spoken worldwide, leaving more
than half of the population deprived of such technology and shared experiences.
With voice-assisted technology (such as social robots and speech-to-text apps)
and auditory content (such as podcasts and lectures) on the rise, ensuring that
the technology is available for all is more important than ever. Speech
translation can play a vital role in mitigating technological disparity and
creating a more inclusive society. With a motive to contribute towards speech
translation research for low-resource languages, our work presents a direct
speech-to-speech translation model for one of the Indic languages called
Punjabi to English. Additionally, we explore the performance of using a
discrete representation of speech called discrete acoustic units as input to
the Transformer-based translation model. The model, abbreviated as Unit-to-Unit
Translation (U2UT), takes a sequence of discrete units of the source language
(the language being translated from) and outputs a sequence of discrete units
of the target language (the language being translated to). Our results show
that the U2UT model performs better than the Speech-to-Unit Translation (S2UT)
model by a 3.69 BLEU score.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15968" title="Abstract">arXiv:2402.15968</a> [<a href="/pdf/2402.15968" title="Download PDF">pdf</a>, <a href="/format/2402.15968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDream: Exchanging dreams instead of models for federated aggregation  with heterogeneous models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Abhishek Singh</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gauri Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kapila%2C+R">Ritvik Kapila</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+A">Alex Dang</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+S">Sheshank Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+M">Mohammed Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Raskar%2C+R">Ramesh Raskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) enables collaborative optimization of machine
learning models across decentralized data by aggregating model parameters. Our
approach extends this concept by aggregating "knowledge" derived from models,
instead of model parameters. We present a novel framework called \codream,
where clients collaboratively optimize randomly initialized data using
federated optimization in the input data space, similar to how randomly
initialized model parameters are optimized in FL. Our key insight is that
jointly optimizing this data can effectively capture the properties of the
global data distribution. Sharing knowledge in data space offers numerous
benefits: (1) model-agnostic collaborative learning, i.e., different clients
can have different model architectures; (2) communication that is independent
of the model size, eliminating scalability concerns with model parameters; (3)
compatibility with secure aggregation, thus preserving the privacy benefits of
federated learning; (4) allowing of adaptive optimization of knowledge shared
for personalized learning. We empirically validate \codream on standard FL
tasks, demonstrating competitive performance despite not sharing model
parameters. Our code: https://mitmedialab.github.io/codream.github.io/
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15969" title="Abstract">arXiv:2402.15969</a> [<a href="/pdf/2402.15969" title="Download PDF">pdf</a>, <a href="/format/2402.15969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Online Learning for Networks of Two-Compartment Spiking  Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yujia Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The brain-inspired Spiking Neural Networks (SNNs) have garnered considerable
research interest due to their superior performance and energy efficiency in
processing temporal signals. Recently, a novel multi-compartment spiking neuron
model, namely the Two-Compartment LIF (TC-LIF) model, has been proposed and
exhibited a remarkable capacity for sequential modelling. However, training the
TC-LIF model presents challenges stemming from the large memory consumption and
the issue of gradient vanishing associated with the Backpropagation Through
Time (BPTT) algorithm. To address these challenges, online learning
methodologies emerge as a promising solution. Yet, to date, the application of
online learning methods in SNNs has been predominantly confined to simplified
Leaky Integrate-and-Fire (LIF) neuron models. In this paper, we present a novel
online learning method specifically tailored for networks of TC-LIF neurons.
Additionally, we propose a refined TC-LIF neuron model called Adaptive TC-LIF,
which is carefully designed to enhance temporal information integration in
online learning scenarios. Extensive experiments, conducted on various
sequential benchmarks, demonstrate that our approach successfully preserves the
superior sequential modeling capabilities of the TC-LIF neuron while
incorporating the training efficiency and hardware friendliness of online
learning. As a result, it offers a multitude of opportunities to leverage
neuromorphic solutions for processing temporal signals.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15972" title="Abstract">arXiv:2402.15972</a> [<a href="/pdf/2402.15972" title="Download PDF">pdf</a>, <a href="/format/2402.15972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Knowledge-Driven Meta-Learning for Task Offloading in  Vehicular Networks with Integrated Communications, Sensing and Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruijin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Nan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+W">Wei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+R">Rong Chai</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+Y">Yilong Hui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Task offloading is a potential solution to satisfy the strict requirements of
computation-intensive and latency-sensitive vehicular applications due to the
limited onboard computing resources. However, the overwhelming upload traffic
may lead to unacceptable uploading time. To tackle this issue, for tasks taking
environmental data as input, the data perceived by roadside units (RSU)
equipped with several sensors can be directly exploited for computation,
resulting in a novel task offloading paradigm with integrated communications,
sensing and computing (I-CSC). With this paradigm, vehicles can select to
upload their sensed data to RSUs or transmit computing instructions to RSUs
during the offloading. By optimizing the computation mode and network
resources, in this paper, we investigate an I-CSC-based task offloading problem
to reduce the cost caused by resource consumption while guaranteeing the
latency of each task. Although this non-convex problem can be handled by the
alternating minimization (AM) algorithm that alternatively minimizes the
divided four sub-problems, it leads to high computational complexity and local
optimal solution. To tackle this challenge, we propose a creative structural
knowledge-driven meta-learning (SKDML) method, involving both the model-based
AM algorithm and neural networks. Specifically, borrowing the iterative
structure of the AM algorithm, also referred to as structural knowledge, the
proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning
to learn an adaptive optimizer for updating variables in each sub-problem,
instead of the handcrafted counterpart in the AM algorithm.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15974" title="Abstract">arXiv:2402.15974</a> [<a href="/pdf/2402.15974" title="Download PDF">pdf</a>, <a href="/ps/2402.15974" title="Download PostScript">ps</a>, <a href="/format/2402.15974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Mixed Reality as the Everyday Computing Paradigm: Challenges &amp;  Design Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asadi%2C+A+R">Amir Reza Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Hemadi%2C+R">Reza Hemadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This research presents a proof-of-concept prototype of an all-in-one mixed
reality application platform, developed to investigate the needs and
expectations of users from mixed reality systems. The study involved an
extensive user study with 1,052 participants, including the collection of
diaries from 6 users and conducting interviews with 51 participants to gain
deeper insights into their experiences. The findings from the interviews
revealed that directly porting current user flows into 3D environments was not
well-received by the target users. Instead, users expressed a clear preference
for alternative 3D interactions along with the continued use of 2D interfaces.
This study provides insights for understanding user preferences and
interactions in mixed reality systems, and design recommendations to facilitate
the mass adoption of MR systems.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15977" title="Abstract">arXiv:2402.15977</a> [<a href="/pdf/2402.15977" title="Download PDF">pdf</a>, <a href="/format/2402.15977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Image Enhancement Method for Improving Small Intestinal Villi Clarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents, for the first time, an image enhancement methodology
designed to enhance the clarity of small intestinal villi in Wireless Capsule
Endoscopy (WCE) images. This method first separates the low-frequency and
high-frequency components of small intestinal villi images using guided
filtering. Subsequently, an adaptive light gain factor is generated based on
the low-frequency component, and an adaptive gradient gain factor is derived
from the convolution results of the Laplacian operator in different regions of
small intestinal villi images. The obtained light gain factor and gradient gain
factor are then combined to enhance the high-frequency components. Finally, the
enhanced high-frequency component is fused with the original image to achieve
adaptive sharpening of the edges of WCE small intestinal villi images. The
experiments affirm that, compared to established WCE image enhancement methods,
our approach not only accentuates the edge details of WCE small intestine villi
images but also skillfully suppresses noise amplification, thereby preventing
the occurrence of edge overshooting.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15978" title="Abstract">arXiv:2402.15978</a> [<a href="/pdf/2402.15978" title="Download PDF">pdf</a>, <a href="/format/2402.15978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaving Weights with Occam&#x27;s Razor: Bayesian Sparsification for Neural  Networks Using the Marginal Likelihood
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhahri%2C+R">Rayen Dhahri</a>, 
<a href="/search/cs?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/cs?searchtype=author&query=Charpentier%2C+B">Betrand Charpentier</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>, 
<a href="/search/cs?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Neural network sparsification is a promising avenue to save computational
time and memory costs, especially in an age where many successful AI models are
becoming too large to na\"ively deploy on consumer hardware. While much work
has focused on different weight pruning criteria, the overall sparsifiability
of the network, i.e., its capacity to be pruned without quality loss, has often
been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM),
a pruning framework that highlights the effectiveness of using the Bayesian
marginal likelihood in conjunction with sparsity-inducing priors for making
neural networks more sparsifiable. Our approach implements an automatic Occam's
razor that selects the most sparsifiable model that still explains the data
well, both for structured and unstructured sparsification. In addition, we
demonstrate that the pre-computed posterior Hessian approximation used in the
Laplace approximation can be re-used to define a cheap pruning criterion, which
outperforms many existing (more expensive) approaches. We demonstrate the
effectiveness of our framework, especially at high sparsity levels, across a
range of different neural network architectures and datasets.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15980" title="Abstract">arXiv:2402.15980</a> [<a href="/pdf/2402.15980" title="Download PDF">pdf</a>, <a href="/format/2402.15980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signed Graph Representation Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peiyao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofeng Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">With the prevalence of social media, the connectedness between people has
been greatly enhanced. Real-world relations between users on social media are
often not limited to expressing positive ties such as friendship, trust, and
agreement, but they also reflect negative ties such as enmity, mistrust, and
disagreement, which can be well modelled by signed graphs. Signed Graph
Representation Learning (SGRL) is an effective approach to analyze the complex
patterns in real-world signed graphs with the co-existence of positive and
negative links. In recent years, SGRL has witnesses fruitful results. SGRL
tries to allocate low-dimensional representations to nodes and edges which
could preserve the graph structure, attribute and some collective properties,
e.g., balance theory and status theory. To the best of knowledge, there is no
survey paper about SGRL up to now. In this paper, we present a broad review of
SGRL methods and discuss some future research directions.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15984" title="Abstract">arXiv:2402.15984</a> [<a href="/pdf/2402.15984" title="Download PDF">pdf</a>, <a href="/format/2402.15984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified Fourier slice method to derive ridgelet transform for a  variety of depth-2 neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonoda%2C+S">Sho Sonoda</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+I">Isao Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+M">Masahiro Ikeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Functional Analysis (math.FA); Machine Learning (stat.ML)

</div>
<p class="mathjax">To investigate neural network parameters, it is easier to study the
distribution of parameters than to study the parameters in each neuron. The
ridgelet transform is a pseudo-inverse operator that maps a given function $f$
to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$
reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected
networks on a Euclidean space, the ridgelet transform has been discovered up to
the closed-form expression, thus we could describe how the parameters are
distributed. However, for a variety of modern neural network architectures, the
closed-form expression has not been known. In this paper, we explain a
systematic method using Fourier expressions to derive ridgelet transforms for a
variety of modern networks such as networks on finite fields $\mathbb{F}_p$,
group convolutional networks on abstract Hilbert space $\mathcal{H}$,
fully-connected networks on noncompact symmetric spaces $G/K$, and pooling
layers, or the $d$-plane ridgelet transform.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15985" title="Abstract">arXiv:2402.15985</a> [<a href="/pdf/2402.15985" title="Download PDF">pdf</a>, <a href="/format/2402.15985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phonetic and Lexical Discovery of a Canine Language using HuBERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zeyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper delves into the pioneering exploration of potential communication
patterns within dog vocalizations and transcends traditional linguistic
analysis barriers, which heavily relies on human priori knowledge on limited
datasets to find sound units in dog vocalization. We present a self-supervised
approach with HuBERT, enabling the accurate classification of phoneme labels
and the identification of vocal patterns that suggest a rudimentary vocabulary
within dog vocalizations. Our findings indicate a significant acoustic
consistency in these identified canine vocabulary, covering the entirety of
observed dog vocalization sequences. We further develop a web-based dog
vocalization labeling system. This system can highlight phoneme n-grams,
present in the vocabulary, in the dog audio uploaded by users.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15987" title="Abstract">arXiv:2402.15987</a> [<a href="/pdf/2402.15987" title="Download PDF">pdf</a>, <a href="/format/2402.15987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likelihood-based Mitigation of Evaluation Bias in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohi%2C+M">Masanari Ohi</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Koike%2C+R">Ryuto Koike</a>, 
<a href="/search/cs?searchtype=author&query=Loem%2C+M">Mengsay Loem</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 main pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are widely used to evaluate natural language
generation tasks as automated metrics. However, the likelihood, a measure of
LLM's plausibility for a sentence, can vary due to superficial differences in
sentences, such as word order and sentence structure. It is therefore possible
that there might be a likelihood bias if LLMs are used for evaluation: they
might overrate sentences with higher likelihoods while underrating those with
lower likelihoods. In this paper, we investigate the presence and impact of
likelihood bias in LLM-based evaluators. We also propose a method to mitigate
the likelihood bias. Our method utilizes highly biased instances as few-shot
examples for in-context learning. Our experiments in evaluating the
data-to-text and grammatical error correction tasks reveal that several LLMs we
test display a likelihood bias. Furthermore, our proposed method successfully
mitigates this bias, also improving evaluation performance (in terms of
correlation of models with human scores) significantly.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15988" title="Abstract">arXiv:2402.15988</a> [<a href="/pdf/2402.15988" title="Download PDF">pdf</a>, <a href="/format/2402.15988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fair Graph Anomaly Detection: Problem, New Datasets, and  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neo%2C+N+K+N">Neng Kai Nigel Neo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yeon-Chang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sang-Wook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code and datasets are available at <a href="https://github.com/nigelnnk/FairGAD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect
anomalous nodes in an input graph while ensuring fairness and avoiding biased
predictions against individuals from sensitive subgroups such as gender or
political leanings. Fairness in graphs is particularly crucial in anomaly
detection areas such as misinformation detection in search/ranking systems,
where decision outcomes can significantly affect individuals. However, the
current literature does not comprehensively discuss this problem, nor does it
provide realistic datasets that encompass actual graph structures, anomaly
labels, and sensitive attributes for research in FairGAD. To bridge this gap,
we introduce a formal definition of the FairGAD problem and present two novel
graph datasets constructed from the globally prominent social media platforms
Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges
associated with 9,000 and 47,000 nodes, respectively, and leverage political
leanings as sensitive attributes and misinformation spreaders as anomaly
labels. We demonstrate that our FairGAD datasets significantly differ from the
synthetic datasets used currently by the research community. These new datasets
offer significant values for FairGAD by providing realistic data that captures
the intricacies of social networks. Using our datasets, we investigate the
performance-fairness trade-off in eleven existing GAD and non-graph AD methods
on five state-of-the-art fairness methods, which sheds light on their
effectiveness and limitations in addressing the FairGAD problem.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15989" title="Abstract">arXiv:2402.15989</a> [<a href="/pdf/2402.15989" title="Download PDF">pdf</a>, <a href="/format/2402.15989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIDformer: Transformer Meets Control Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tam Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Uribe%2C+C+A">C&#xe9;sar A. Uribe</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+M">Tan M. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this work, we address two main shortcomings of transformer architectures:
input corruption and rank collapse in their output representation. We unveil
self-attention as an autonomous state-space model that inherently promotes
smoothness in its solutions, leading to lower-rank outputs and diminished
representation capacity. Moreover, the steady-state solution of the model is
sensitive to input perturbations. We incorporate a
Proportional-Integral-Derivative (PID) closed-loop feedback control system with
a reference point into the model to improve robustness and representation
capacity. This integration aims to preserve high-frequency details while
bolstering model stability, rendering it more noise-resilient. The resulting
controlled state-space model is theoretically proven robust and adept at
addressing the rank collapse. Motivated by this control framework, we derive a
novel class of transformers, PID-controlled Transformer (PIDformer), aimed at
improving robustness and mitigating the rank-collapse issue inherent in softmax
transformers. We empirically evaluate the model for advantages and robustness
against baseline transformers across various practical tasks, including object
classification, image segmentation, and language modeling.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15990" title="Abstract">arXiv:2402.15990</a> [<a href="/pdf/2402.15990" title="Download PDF">pdf</a>, <a href="/format/2402.15990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Challenges in Machine Learning Asset Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhimin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bangash%2C+A+A">Abdul Ali Bangash</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+B">Bram Adams</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+A+E">Ahmed E. Hassan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In machine learning (ML), efficient asset management, including ML models,
datasets, algorithms, and tools, is vital for resource optimization, consistent
performance, and a streamlined development lifecycle. This enables quicker
iterations, adaptability, reduced development-to-deployment time, and reliable
outputs. Despite existing research, a significant knowledge gap remains in
operational challenges like model versioning, data traceability, and
collaboration, which are crucial for the success of ML projects. Our study aims
to address this gap by analyzing 15,065 posts from developer forums and
platforms, employing a mixed-method approach to classify inquiries, extract
challenges using BERTopic, and identify solutions through open card sorting and
BERTopic clustering. We uncover 133 topics related to asset management
challenges, grouped into 16 macro-topics, with software dependency, model
deployment, and model training being the most discussed. We also find 79
solution topics, categorized under 18 macro-topics, highlighting software
dependency, feature development, and file management as key solutions. This
research underscores the need for further exploration of identified pain points
and the importance of collaborative efforts across academia, industry, and the
research community.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15991" title="Abstract">arXiv:2402.15991</a> [<a href="/pdf/2402.15991" title="Download PDF">pdf</a>, <a href="/format/2402.15991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $C^3$: Confidence Calibration Model Cascade for Inference-Efficient  Cross-Lingual Natural Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Taixi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Huajie Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jing Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Cross-lingual natural language understanding (NLU) is a critical task in
natural language processing (NLP). Recent advancements have seen multilingual
pre-trained language models (mPLMs) significantly enhance the performance of
these tasks. However, mPLMs necessitate substantial resources and incur high
computational costs during inference, posing challenges for deployment in
real-world and real-time systems. Existing model cascade methods seek to
enhance inference efficiency by greedily selecting the lightest model capable
of processing the current input from a variety of models, based on model
confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and
confidence distributions vary across languages. This leads to the emission of
confident but incorrect predictions by smaller models, hindering their ability
to generalize effectively across test languages. In this study, we introduce a
confidence calibration model cascade ($C^3$) method. This approach, simple yet
effective, involves calibration prior to cascade inference, thereby enhancing
cascade accuracy through more reliable predictions. Extensive experiments
conducted on three cross-lingual benchmarks demonstrate that $C^3$
significantly outperforms all state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15992" title="Abstract">arXiv:2402.15992</a> [<a href="/pdf/2402.15992" title="Download PDF">pdf</a>, <a href="/format/2402.15992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Approach to Detect Customer Satisfaction From  Multiple Tweet Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+M">Md Mahmudul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Fattah%2C+D+S+A">Dr. Shaikh Anowarul Fattah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Since internet technologies have advanced, one of the primary factors in
company development is customer happiness. Online platforms have become
prominent places for sharing reviews. Twitter is one of these platforms where
customers frequently post their thoughts. Reviews of flights on these platforms
have become a concern for the airline business. A positive review can help the
company grow, while a negative one can quickly ruin its revenue and reputation.
So it's vital for airline businesses to examine the feedback and experiences of
their customers and enhance their services to remain competitive. But studying
thousands of tweets and analyzing them to find the satisfaction of the customer
is quite a difficult task. This tedious process can be made easier by using a
machine learning approach to analyze tweets to determine client satisfaction
levels. Some work has already been done on this strategy to automate the
procedure using machine learning and deep learning techniques. However, they
are all purely concerned with assessing the text's sentiment. In addition to
the text, the tweet also includes the time, location, username, airline name,
and so on. This additional information can be crucial for improving the model's
outcome. To provide a machine learning based solution, this work has broadened
its perspective to include these qualities. And it has come as no surprise that
the additional features beyond text sentiment analysis produce better outcomes
in machine learning based models.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15993" title="Abstract">arXiv:2402.15993</a> [<a href="/pdf/2402.15993" title="Download PDF">pdf</a>, <a href="/format/2402.15993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning method for S4 with Diagonal State Space Layers using Balanced  Truncation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezoe%2C+H">Haruka Ezoe</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+K">Kazuhiro Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce a novel learning method for Structured State Space Sequence (S4)
models incorporating Diagonal State Space (DSS) layers, tailored for processing
long-sequence data in edge intelligence applications, including sensor data
analysis and real-time analytics. This method utilizes the balanced truncation
technique, prevalent in control theory, applied specifically to DSS layers to
reduce computational costs during inference. By leveraging parameters from the
reduced model, we refine the initialization process of S4 models, outperforming
the widely used Skew-HiPPo initialization in terms of performance. Numerical
experiments demonstrate that our trained S4 models with DSS layers surpass
conventionally trained models in accuracy and efficiency metrics. Furthermore,
our observations reveal a positive correlation: higher accuracy in the original
model consistently leads to increased accuracy in models trained using our
method, suggesting that our approach effectively leverages the strengths of the
original model.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15995" title="Abstract">arXiv:2402.15995</a> [<a href="/pdf/2402.15995" title="Download PDF">pdf</a>, <a href="/ps/2402.15995" title="Download PostScript">ps</a>, <a href="/format/2402.15995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Hardness Results for Learning Intersections of Halfspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiegel%2C+S">Stefan Tiegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We show strong (and surprisingly simple) lower bounds for weakly learning
intersections of halfspaces in the improper setting. Strikingly little is known
about this problem. For instance, it is not even known if there is a
polynomial-time algorithm for learning the intersection of only two halfspaces.
On the other hand, lower bounds based on well-established assumptions (such as
approximating worst-case lattice problems or variants of Feige's 3SAT
hypothesis) are only known (or are implied by existing results) for the
intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With
intersections of fewer halfspaces being only ruled out under less standard
assumptions [DV21] (such as the existence of local pseudo-random generators
with large stretch). We significantly narrow this gap by showing that even
learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes
super-polynomial time under standard assumptions on worst-case lattice problems
(namely that SVP and SIVP are hard to approximate within polynomial factors).
Further, we give unconditional hardness results in the statistical query
framework. Specifically, we show that for any $k$ (even constant), learning $k$
halfspaces in dimension $N$ requires accuracy $N^{-\Omega(k)}$, or
exponentially many queries -- in particular ruling out SQ algorithms with
polynomial accuracy for $\omega(1)$ halfspaces. To the best of our knowledge
this is the first unconditional hardness result for learning a super-constant
number of halfspaces.
<br />Our lower bounds are obtained in a unified way via a novel connection we make
between intersections of halfspaces and the so-called parallel pancakes
distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower
bound constructions in (robust) high-dimensional statistics in the past few
years.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15997" title="Abstract">arXiv:2402.15997</a> [<a href="/pdf/2402.15997" title="Download PDF">pdf</a>, <a href="/format/2402.15997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cieran: Designing Sequential Colormaps via In-Situ Active Preference  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Matt-Heun Hong</a>, 
<a href="/search/cs?searchtype=author&query=Sunberg%2C+Z+N">Zachary N. Sunberg</a>, 
<a href="/search/cs?searchtype=author&query=Szafir%2C+D+A">Danielle Albers Szafir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI 2024. 12 pages/9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Quality colormaps can help communicate important data patterns. However,
finding an aesthetically pleasing colormap that looks "just right" for a given
scenario requires significant design and technical expertise. We introduce
Cieran, a tool that allows any data analyst to rapidly find quality colormaps
while designing charts within Jupyter Notebooks. Our system employs an active
preference learning paradigm to rank expert-designed colormaps and create new
ones from pairwise comparisons, allowing analysts who are novices in color
design to tailor colormaps to their data context. We accomplish this by
treating colormap design as a path planning problem through the CIELAB
colorspace with a context-specific reward model. In an evaluation with twelve
scientists, we found that Cieran effectively modeled user preferences to rank
colormaps and leveraged this model to create new quality designs. Our work
shows the potential of active preference learning for supporting efficient
visualization design optimization.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16000" title="Abstract">arXiv:2402.16000</a> [<a href="/pdf/2402.16000" title="Download PDF">pdf</a>, <a href="/format/2402.16000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian D-Optimal Experimental Designs via Column Subset Selection: The  Power of Reweighted Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eswar%2C+S">Srinivas Eswar</a>, 
<a href="/search/math?searchtype=author&query=Rao%2C+V">Vishwas Rao</a>, 
<a href="/search/math?searchtype=author&query=Saibaba%2C+A+K">Arvind K. Saibaba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper tackles optimal sensor placement for Bayesian linear inverse
problems, a popular version of the more general Optimal Experiment Design (OED)
problem, using the D-optimality criterion. This is done by establishing
connections between sensor placement and Column Subset Selection Problem
(CSSP), which is a well-studied problem in Numerical Linear Algebra (NLA). In
particular, we use the Golub-Klema-Stewart (GKS) approach which involves
computing the truncated Singular Value Decomposition (SVD) followed by a
pivoted QR factorization on the right singular vectors. The algorithms are
further accelerated by using randomization to compute the low-rank
approximation as well as for sampling the indices. The resulting algorithms are
robust, computationally efficient, require virtually no parameter tuning, and
come with strong theoretical guarantees. We also propose a new approach for
OED, called reweighted sensors, that selects $k$ sensors but judiciously
recombines sensor information to dramatically improve the D-optimality
criterion. Additionally, we develop a method for data completion without
solving the inverse problem. Numerical experiments on model inverse problems
involving the heat equation and seismic tomography in two spatial dimensions
demonstrate the performance of our approaches.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16001" title="Abstract">arXiv:2402.16001</a> [<a href="/pdf/2402.16001" title="Download PDF">pdf</a>, <a href="/ps/2402.16001" title="Download PostScript">ps</a>, <a href="/format/2402.16001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Resolution Land Cover Classification Using Outdated Products and  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+H">Huan Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yubin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Haiyan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Cheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y">Yongshi Jie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiyang Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large-scale high-resolution land cover classification is a prerequisite for
constructing Earth system models and addressing ecological and resource issues.
Advancements in satellite sensor technology have led to an improvement in
spatial resolution and wider coverage areas. Nevertheless, the lack of
high-resolution labeled data is still a challenge, hindering the largescale
application of land cover classification methods. In this paper, we propose a
Transformerbased weakly supervised method for cross-resolution land cover
classification using outdated data. First, to capture long-range dependencies
without missing the fine-grained details of objects, we propose a U-Net-like
Transformer based on a reverse difference mechanism (RDM) using dynamic sparse
attention. Second, we propose an anti-noise loss calculation (ANLC) module
based on optimal transport (OT). Anti-noise loss calculation identifies
confident areas (CA) and vague areas (VA) based on the OT matrix, which
relieves the impact of noises in outdated land cover products. By introducing a
weakly supervised loss with weights and employing unsupervised loss, the
RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m
resolution and the corresponding ground-truths of six states in the United
States were employed to validate the performance of the proposed method. The
experiments utilized outdated land cover products with 30 m resolution from
2013 as training labels, and produced land cover maps with 1 m resolution from
2017. The results show the superiority of the proposed method compared to
state-of-the-art methods. The code is available at
https://github.com/yu-ni1989/ANLC-Former.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16002" title="Abstract">arXiv:2402.16002</a> [<a href="/pdf/2402.16002" title="Download PDF">pdf</a>, <a href="/ps/2402.16002" title="Download PostScript">ps</a>, <a href="/format/2402.16002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-Quantum Cryptography Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+C+H">Abel C. H. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES) 7-8 July 2023. The manuscript was written in Chinese and submitted on 10 March 2023, but it was rejected on 22 April 2023. The appeal was accepted on 24 February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, quantum computers and Shor quantum algorithm have posed a
threat to current mainstream asymmetric cryptography methods (e.g. RSA and
Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a
Post-Quantum Cryptography (PQC) method to resist quantum computing attacks.
Therefore, this study proposes a PQC-based neural network that maps a
code-based PQC method to a neural network structure and enhances the security
of ciphertexts with non-linear activation functions, random perturbation of
ciphertexts, and uniform distribution of ciphertexts. In practical experiments,
this study uses cellular network signals as a case study to demonstrate that
encryption and decryption can be performed by the proposed PQC-based neural
network with the uniform distribution of ciphertexts. In the future, the
proposed PQC-based neural network could be applied to various applications.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16005" title="Abstract">arXiv:2402.16005</a> [<a href="/pdf/2402.16005" title="Download PDF">pdf</a>, <a href="/format/2402.16005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial-Robust Transfer Learning for Medical Imaging via Domain  Assimilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tie Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), May 2024, Taiwan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of Medical Imaging, extensive research has been dedicated to
leveraging its potential in uncovering critical diagnostic features in
patients. Artificial Intelligence (AI)-driven medical diagnosis relies on
sophisticated machine learning and deep learning models to analyze, detect, and
identify diseases from medical images. Despite the remarkable performance of
these models, characterized by high accuracy, they grapple with trustworthiness
issues. The introduction of a subtle perturbation to the original image
empowers adversaries to manipulate the prediction output, redirecting it to
other targeted or untargeted classes. Furthermore, the scarcity of publicly
available medical images, constituting a bottleneck for reliable training, has
led contemporary algorithms to depend on pretrained models grounded on a large
set of natural images -- a practice referred to as transfer learning. However,
a significant {\em domain discrepancy} exists between natural and medical
images, which causes AI models resulting from transfer learning to exhibit
heightened {\em vulnerability} to adversarial attacks. This paper proposes a
{\em domain assimilation} approach that introduces texture and color adaptation
into transfer learning, followed by a texture preservation component to
suppress undesired distortion. We systematically analyze the performance of
transfer learning in the face of various adversarial attacks under different
data modalities, with the overarching goal of fortifying the model's robustness
and security in medical imaging tasks. The results demonstrate high
effectiveness in reducing attack efficacy, contributing toward more trustworthy
transfer learning in biomedical applications.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16006" title="Abstract">arXiv:2402.16006</a> [<a href="/pdf/2402.16006" title="Download PDF">pdf</a>, <a href="/format/2402.16006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Noise to Clarity: Unraveling the Adversarial Suffix of Large  Language Model Attacks via Translation of Text Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Lei Sha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The safety defense methods of Large language models(LLMs) stays limited
because the dangerous prompts are manually curated to just few known attack
types, which fails to keep pace with emerging varieties. Recent studies found
that attaching suffixes to harmful instructions can hack the defense of LLMs
and lead to dangerous outputs. This method, while effective, leaves a gap in
understanding the underlying mechanics of such adversarial suffix due to the
non-readability and it can be relatively easily seen through by common defense
methods such as perplexity filters.To cope with this challenge, in this paper,
we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that
are able to translate the unreadable adversarial suffixes into coherent,
readable text, which makes it easier to understand and analyze the reasons
behind harmful content generation by large language models. We conducted
experiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's
harmful instructions. The results indicate that our method achieves a much
better attack success rate to existing techniques, while significantly
enhancing the textual fluency of the prompts. In addition, our approach can be
generalized into a broader method for generating transferable adversarial
suffixes that can successfully attack multiple LLMs, even black-box LLMs, such
as ChatGPT and Gemini. As a result, the prompts generated through our method
exhibit enriched semantic diversity, which potentially provides more
adversarial examples for LLM defense methods.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16008" title="Abstract">arXiv:2402.16008</a> [<a href="/pdf/2402.16008" title="Download PDF">pdf</a>, <a href="/format/2402.16008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach  to Model Interpretability and Precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+Y">Yasmine Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tie Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), May 2024, Taiwan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The evolution of deep learning and artificial intelligence has significantly
reshaped technological landscapes. However, their effective application in
crucial sectors such as medicine demands more than just superior performance,
but trustworthiness as well. While interpretability plays a pivotal role,
existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans}
behavior where a model makes (ungeneralizable) correct predictions using
spurious correlations or biases in data. Likewise, current post-hoc XAI methods
are susceptible to generating unjustified counterfactual examples. In this
paper, we approach XAI with an innovative {\em model debugging} methodology
realized through Jacobian Saliency Map (JSM). To cast the problem into a
concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case,
motivated by its significant impact on human lives and the formidable challenge
in its early detection, stemming from the intricate nature of its progression.
We introduce an interpretable, multimodal model for AD classification over its
multi-stage progression, incorporating JSM as a modality-agnostic tool that
provides insights into volumetric changes indicative of brain abnormalities.
Our extensive evaluation including ablation study manifests the efficacy of
using JSM for model debugging and interpretation, while significantly enhancing
model accuracy as well.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16009" title="Abstract">arXiv:2402.16009</a> [<a href="/pdf/2402.16009" title="Download PDF">pdf</a>, <a href="/format/2402.16009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PST-Bench: Tracing and Benchmarking the Source of Publications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fanjin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+K">Kun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+Y">Yukuo Cen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Tracing the source of research papers is a fundamental yet challenging task
for researchers. The billion-scale citation relations between papers hinder
researchers from understanding the evolution of science efficiently. To date,
there is still a lack of an accurate and scalable dataset constructed by
professional researchers to identify the direct source of their studied papers,
based on which automatic algorithms can be developed to expand the evolutionary
knowledge of science. In this paper, we study the problem of paper source
tracing (PST) and construct a high-quality and ever-increasing dataset
PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing
discoveries, such as the differing evolution patterns across various topics. An
exploration of various methods underscores the hardness of PST-Bench,
pinpointing potential directions on this topic. The dataset and codes have been
available at https://github.com/THUDM/paper-source-trace.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16010" title="Abstract">arXiv:2402.16010</a> [<a href="/pdf/2402.16010" title="Download PDF">pdf</a>, <a href="/format/2402.16010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-conserving intermittent-contact motion in complex models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pankov%2C+S">Sergey Pankov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications in Nonlinear Science and Numerical Simulation, 132
  (2024), 107895
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Biological Physics (physics.bio-ph); Classical Physics (physics.class-ph)

</div>
<p class="mathjax">Some mechanical systems, that are modeled to have inelastic collisions,
nonetheless possess energy-conserving intermittent-contact solutions, known as
collisionless solutions. Such a solution, representing a persistent hopping or
walking across a level ground, may be important for understanding animal
locomotion or for designing efficient walking machines. So far, collisionless
motion has been analytically studied in simple two degrees of freedom (DOF)
systems, or in a system that decouples into 2-DOF subsystems in the harmonic
approximation. In this paper we extend the consideration to a N-DOF system,
recovering the known solutions as a special N = 2 case of the general
formulation. We show that in the harmonic approximation the collisionless
solution is determined by the spectrum of the system. We formulate a solution
existence condition, which requires the presence of at least one oscillating
normal mode in the most constrained phase of the motion. An application of the
developed general framework is illustrated by finding a collisionless solution
for a rocking motion of a biped with an armed standing torso.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16012" title="Abstract">arXiv:2402.16012</a> [<a href="/pdf/2402.16012" title="Download PDF">pdf</a>, <a href="/format/2402.16012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Contrastive Graph Learning with Clustering-Oriented Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mulin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bocheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept at AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Convolutional Network (GCN) has exhibited remarkable potential in
improving graph-based clustering. To handle the general clustering scenario
without a prior graph, these models estimate an initial graph beforehand to
apply GCN. Throughout the literature, we have witnessed that 1) most models
focus on the initial graph while neglecting the original features. Therefore,
the discriminability of the learned representation may be corrupted by a
low-quality initial graph; 2) the training procedure lacks effective clustering
guidance, which may lead to the incorporation of clustering-irrelevant
information into the learned graph. To tackle these problems, the Deep
Contrastive Graph Learning (DCGL) model is proposed for general data
clustering. Specifically, we establish a pseudo-siamese network, which
incorporates auto-encoder with GCN to emphasize both the graph structure and
the original features. On this basis, feature-level contrastive learning is
introduced to enhance the discriminative capacity, and the relationship between
samples and centroids is employed as the clustering-oriented guidance.
Afterward, a two-branch graph learning mechanism is designed to extract the
local and global structural relationships, which are further embedded into a
unified graph under the cluster-level contrastive guidance. Experimental
results on several benchmark datasets demonstrate the superiority of DCGL
against state-of-the-art algorithms.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16013" title="Abstract">arXiv:2402.16013</a> [<a href="/pdf/2402.16013" title="Download PDF">pdf</a>, <a href="/format/2402.16013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Open-World Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mullappilly%2C+S+S">Sahal Shaji Mullappilly</a>, 
<a href="/search/cs?searchtype=author&query=Gehlot%2C+A+S">Abhishek Singh Gehlot</a>, 
<a href="/search/cs?searchtype=author&query=Anwer%2C+R+M">Rao Muhammad Anwer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024 (Main Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conventional open-world object detection (OWOD) problem setting first
distinguishes known and unknown classes and then later incrementally learns the
unknown objects when introduced with labels in the subsequent tasks. However,
the current OWOD formulation heavily relies on the external human oracle for
knowledge input during the incremental learning stages. Such reliance on
run-time makes this formulation less realistic in a real-world deployment. To
address this, we introduce a more realistic formulation, named semi-supervised
open-world detection (SS-OWOD), that reduces the annotation cost by casting the
incremental learning stages of OWOD in a semi-supervised manner. We demonstrate
that the performance of the state-of-the-art OWOD detector dramatically
deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel
SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme
to better align the object query representations between the original and
augmented images to leverage the large unlabeled and few labeled data. We
further introduce a pseudo-labeling scheme for unknown detection that exploits
the inherent capability of decoder object queries to capture object-specific
information. We demonstrate the effectiveness of our SS-OWOD problem setting
and approach for remote sensing object detection, proposing carefully curated
splits and baseline performance evaluations. Our experiments on 4 datasets
including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of
our approach. Our source code, models and splits are available here -
https://github.com/sahalshajim/SS-OWFormer
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16014" title="Abstract">arXiv:2402.16014</a> [<a href="/pdf/2402.16014" title="Download PDF">pdf</a>, <a href="/format/2402.16014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Flexible Machine Learning Models for Scientific Computing at  Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haoyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chonghan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Foundation models have revolutionized knowledge acquisition across domains,
and our study introduces OmniArch, a paradigm-shifting approach designed for
building foundation models in multi-physics scientific computing. OmniArch's
pre-training involves a versatile pipeline that processes multi-physics
spatio-temporal data, casting forward problem learning into scalable
auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning
(PIRL) technique during fine-tuning ensures alignment with physical laws.
Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new
performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional
adaptability to new physics via few-shot and zero-shot learning approaches. The
model's representations further extend to inverse problem-solving, highlighting
the transformative potential of AI-enabled Scientific Computing(AI4SC)
foundation models for engineering applications and physics discovery.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16016" title="Abstract">arXiv:2402.16016</a> [<a href="/pdf/2402.16016" title="Download PDF">pdf</a>, <a href="/ps/2402.16016" title="Download PostScript">ps</a>, <a href="/format/2402.16016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of Manipulation and Bribery in Premise-Based Judgment  Aggregation with Simple Formulas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bredereck%2C+R">Robert Bredereck</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junjie Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Judgment aggregation is a framework to aggregate individual opinions on
multiple, logically connected issues into a collective outcome. It is open to
manipulative attacks such as \textsc{Manipulation} where judges cast their
judgments strategically. Previous works have shown that most computational
problems corresponding to these manipulative attacks are \NP-hard. This desired
computational barrier, however, often relies on formulas that are either of
unbounded size or of complex structure.
<br />We revisit the computational complexity for various \textsc{Manipulation} and
\textsc{Bribery} problems in judgment aggregation, now focusing on simple and
realistic formulas. We restrict all formulas to be clauses that are (positive)
monotone, Horn-clauses, or have bounded length. For basic variants of
\textsc{Manipulation}, we show that these restrictions make several variants,
which were in general known to be \NP-hard, polynomial-time solvable. Moreover,
we provide a P vs.\ NP dichotomy for a large class of clause restrictions
(generalizing monotone and Horn clauses) by showing a close relationship
between variants of \textsc{Manipulation} and variants of
\textsc{Satisfiability}. For Hamming distance based \textsc{Manipulation}, we
show that \NP-hardness even holds for positive monotone clauses of length
three, but the problem becomes polynomial-time solvable for positive monotone
clauses of length two. For \textsc{Bribery}, we show that \NP-hardness even
holds for positive monotone clauses of length two, but it becomes
polynomial-time solvable for the same clause set if there is a constant budget.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16017" title="Abstract">arXiv:2402.16017</a> [<a href="/pdf/2402.16017" title="Download PDF">pdf</a>, <a href="/format/2402.16017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectrum Extraction and Clipping for Implicitly Linear Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boroojeny%2C+A+E">Ali Ebrahimpour Boroojeny</a>, 
<a href="/search/cs?searchtype=author&query=Telgarsky%2C+M">Matus Telgarsky</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+H">Hari Sundaram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16020" title="Abstract">arXiv:2402.16020</a> [<a href="/pdf/2402.16020" title="Download PDF">pdf</a>, <a href="/ps/2402.16020" title="Download PostScript">ps</a>, <a href="/format/2402.16020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Step-by-step Introduction to the Implementation of Automatic  Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yu-Hsueh Fang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">He-Zhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie-Jyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chih-Jen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Automatic differentiation is a key component in deep learning. This topic is
well studied and excellent surveys such as Baydin et al. (2018) have been
available to clearly describe the basic concepts. Further, sophisticated
implementations of automatic differentiation are now an important part of
popular deep learning frameworks. However, it is difficult, if not impossible,
to directly teach students the implementation of existing systems due to the
complexity. On the other hand, if the teaching stops at the basic concept,
students fail to sense the realization of an implementation. For example, we
often mention the computational graph in teaching automatic differentiation,
but students wonder how to implement and use it. In this document, we partially
fill the gap by giving a step by step introduction of implementing a simple
automatic differentiation system. We streamline the mathematical concepts and
the implementation. Further, we give the motivation behind each implementation
detail, so the whole setting becomes very natural.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16021" title="Abstract">arXiv:2402.16021</a> [<a href="/pdf/2402.16021" title="Download PDF">pdf</a>, <a href="/format/2402.16021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TMT: Tri-Modal Translation between Speech, Image, and Text by Processing  Different Modalities as Different Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Rha%2C+H">Hyeongseop Rha</a>, 
<a href="/search/cs?searchtype=author&query=Maiti%2C+S">Soumi Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Siddhant Arora</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The capability to jointly process multi-modal information is becoming an
essential task. However, the limited number of paired multi-modal data and the
large computational requirements in multi-modal learning hinder the
development. We propose a novel Tri-Modal Translation (TMT) model that
translates between arbitrary modalities spanning speech, image, and text. We
introduce a novel viewpoint, where we interpret different modalities as
different languages, and treat multi-modal translation as a well-established
machine translation problem. To this end, we tokenize speech and image data
into discrete tokens, which provide a unified interface across modalities and
significantly decrease the computational cost. In the proposed TMT, a
multi-modal encoder-decoder conducts the core translation, whereas
modality-specific processing is conducted only within the tokenization and
detokenization stages. We evaluate the proposed TMT on all six modality
translation tasks. TMT outperforms single model counterparts consistently,
demonstrating that unifying tasks is beneficial not only for practicality but
also for performance.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16024" title="Abstract">arXiv:2402.16024</a> [<a href="/pdf/2402.16024" title="Download PDF">pdf</a>, <a href="/format/2402.16024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiGPT: Heterogeneous Graph Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Long Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Heterogeneous graph learning aims to capture complex relationships and
diverse relational semantics among entities in a heterogeneous graph to obtain
meaningful representations for nodes and edges. Recent advancements in
heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art
performance by considering relation heterogeneity and using specialized message
functions and aggregation rules. However, existing frameworks for heterogeneous
graph learning have limitations in generalizing across diverse heterogeneous
graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune"
paradigm on the same dataset, which restricts their capacity to adapt to new
and unseen data. This raises the question: "Can we generalize heterogeneous
graph models to be well-adapted to diverse downstream learning tasks with
distribution shifts in both node token sets and relation type heterogeneity?''
To tackle those challenges, we propose HiGPT, a general large graph model with
Heterogeneous graph instruction-tuning paradigm. Our framework enables learning
from arbitrary heterogeneous graphs without the need for any fine-tuning
process from downstream datasets. To handle distribution shifts in
heterogeneity, we introduce an in-context heterogeneous graph tokenizer that
captures semantic relationships in different heterogeneous graphs, facilitating
model adaptation. We incorporate a large corpus of heterogeneity-aware graph
instructions into our HiGPT, enabling the model to effectively comprehend
complex relation heterogeneity and distinguish between various types of graph
tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction
augmentation paradigm to mitigate data scarcity by generating diverse and
informative instructions. Through comprehensive evaluations, our proposed
framework demonstrates exceptional performance in terms of generalization
performance.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16025" title="Abstract">arXiv:2402.16025</a> [<a href="/pdf/2402.16025" title="Download PDF">pdf</a>, <a href="/format/2402.16025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Semantics: Towards a Semantics-Aware Routing Anomaly  Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Q">Qilei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuotao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziqian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianping Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in USENIX Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">BGP is the de facto inter-domain routing protocol to ensure global
connectivity of the Internet. However, various reasons, such as deliberate
attacks or misconfigurations, could cause BGP routing anomalies. Traditional
methods for BGP routing anomaly detection require significant manual
investigation of routes by network operators. Although machine learning has
been applied to automate the process, prior arts typically impose significant
training overhead (such as large-scale data labeling and feature crafting), and
only produce uninterpretable results. To address these limitations, this paper
presents a routing anomaly detection system centering around a novel network
representation learning model named BEAM. The core design of BEAM is to
accurately learn the unique properties (defined as \emph{routing role}) of each
Autonomous System (AS) in the Internet by incorporating BGP semantics. As a
result, routing anomaly detection, given BEAM, is reduced to a matter of
discovering unexpected routing role churns upon observing new route
announcements. We implement a prototype of our routing anomaly detection system
and extensively evaluate its performance. The experimental results, based on 18
real-world RouteViews datasets containing over 11 billion route announcement
records, demonstrate that our system can detect all previously-confirmed
routing anomalies, while only introducing at most five false alarms every 180
million route announcements. We also deploy our system at a large ISP to
perform real-world detection for one month. During the course of deployment,
our system detects 497 true anomalies in the wild with an average of only 1.65
false alarms per day.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16026" title="Abstract">arXiv:2402.16026</a> [<a href="/pdf/2402.16026" title="Download PDF">pdf</a>, <a href="/ps/2402.16026" title="Download PostScript">ps</a>, <a href="/format/2402.16026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Selection Based on Orthogonal Constraints and Polygon Area
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jun Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yilei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The goal of feature selection is to choose the optimal subset of features for
a recognition task by evaluating the importance of each feature, thereby
achieving effective dimensionality reduction. Currently, proposed feature
selection methods often overlook the discriminative dependencies between
features and labels. To address this problem, this paper introduces a novel
orthogonal regression model incorporating the area of a polygon. The model can
intuitively capture the discriminative dependencies between features and
labels. Additionally, this paper employs a hybrid non-monotone linear search
method to efficiently tackle the non-convex optimization challenge posed by
orthogonal constraints. Experimental results demonstrate that our approach not
only effectively captures discriminative dependency information but also
surpasses traditional methods in reducing feature dimensions and enhancing
classification performance.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16027" title="Abstract">arXiv:2402.16027</a> [<a href="/pdf/2402.16027" title="Download PDF">pdf</a>, <a href="/format/2402.16027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance  Analysis and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hancheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yansha Deng</a>, 
<a href="/search/cs?searchtype=author&query=Nallanathan%2C+A">Arumugam Nallanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures, Submitted to IEEE for potential publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Massive interconnection has sparked people's envisioning for next-generation
ultra-reliable and low-latency communications (xURLLC), prompting the design of
customized next-generation advanced transceivers (NGAT). Rate-splitting
multiple access (RSMA) has emerged as a pivotal technology for NGAT design,
given its robustness to imperfect channel state information (CSI) and
resilience to quality of service (QoS). Additionally, xURLLC urgently appeals
to large-scale access techniques, thus massive multiple-input multiple-output
(mMIMO) is anticipated to integrate with RSMA to enhance xURLLC. In this paper,
we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC)
network architecture tailored to accommodate xURLLC's critical QoS constraints
in finite blocklength (FBL) regimes. Leveraging uplink pilot training under
imperfect CSI at the transmitter, we estimate channel gains and customize
linear precoders for efficient downlink short-packet data transmission.
Subsequently, we formulate a joint rate-splitting, beamforming, and transmit
antenna selection optimization problem to maximize the total effective
transmission rate (ETR). Addressing this multi-variable coupled non-convex
problem, we decompose it into three corresponding subproblems and propose a
low-complexity joint iterative algorithm for efficient optimization. Extensive
simulations substantiate that compared with non-orthogonal multiple access
(NOMA) and space division multiple access (SDMA), the developed architecture
improves the total ETR by 15.3% and 41.91%, respectively, as well as
accommodates larger-scale access.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16028" title="Abstract">arXiv:2402.16028</a> [<a href="/pdf/2402.16028" title="Download PDF">pdf</a>, <a href="/format/2402.16028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedFDP: Federated Learning with Fairness and Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">Xinpeng Ling</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhili Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuncan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huifa Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Tong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guanying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Federated learning (FL) is a new machine learning paradigm to overcome the
challenge of data silos and has garnered significant attention. However,
through our observations, a globally effective trained model may performance
disparities in different clients. This implies that the jointly trained models
by clients may lead to unfair outcomes. On the other hand, relevant studies
indicate that the transmission of gradients or models in federated learning can
also give rise to privacy leakage issues, such as membership inference attacks.
<br />To address the first issue mentioned above, we propose a federated algorithm
with fairness, termed FedFair. Building upon FedFair, we introduce privacy
protection to form the FedFDP algorithm to address the second issue mentioned
above. In FedFDP, we devise a fairness-aware clipping strategy to achieve
differential privacy while adjusting fairness. Additionally, for the extra
uploaded loss values, we present an adaptive clipping approach to maximize
utility. Furthermore, we theoretically prove that our algorithm converges and
ensures differential privacy. Lastly, Extensive experimental results
demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art
solutions in terms of model performance and fairness. The code is accessible at
https://anonymous.4open.science/r/FedFDP-E754.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16029" title="Abstract">arXiv:2402.16029</a> [<a href="/pdf/2402.16029" title="Download PDF">pdf</a>, <a href="/format/2402.16029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphWiz: An Instruction-Following Language Model for Graph Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jianheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27pages, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved impressive success across several
fields, but their proficiency in understanding and resolving complex graph
problems is less explored. To bridge this gap, we introduce GraphInstruct, a
novel and comprehensive instruction-tuning dataset designed to equip language
models with the ability to tackle a broad spectrum of graph problems using
explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an
open-source language model capable of resolving various graph problem types
while generating clear reasoning processes. To enhance the model's capability
and reliability, we incorporate the Direct Preference Optimization (DPO)
framework into the graph problem-solving context. The enhanced model,
GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with
different complexity levels, surpassing GPT-4 which has an average accuracy of
43.8%. Moreover, our research delves into the delicate balance between training
data volume and model performance, highlighting the potential for overfitting
with increased data. We also explore the transferability of the model's
reasoning ability across different graph tasks, indicating the model's
adaptability and practical application potential. Our investigation offers a
new blueprint and valuable insights for developing LLMs specialized in graph
reasoning and problem-solving.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16030" title="Abstract">arXiv:2402.16030</a> [<a href="/pdf/2402.16030" title="Download PDF">pdf</a>, <a href="/format/2402.16030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Forget Your Reward Values: Language Model Alignment via  Value-based Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xin Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Feng-Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huimin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Reinforcement Learning from Human Feedback (RLHF) significantly
enhances the generation quality of Large Language Models (LLMs), recent studies
have raised concerns regarding the complexity and instability associated with
the Proximal Policy Optimization (PPO) algorithm, proposing a series of
order-based calibration methods as viable alternatives. This paper delves
further into current order-based methods, examining their inefficiencies in
utilizing reward values and addressing misalignment issues. Building upon these
findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration
(VCB) method to better align LLMs with human preferences. Experimental results
demonstrate that VCB surpasses existing alignment methods on AI assistant and
summarization datasets, providing impressive generalizability, robustness, and
stability in diverse settings.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16033" title="Abstract">arXiv:2402.16033</a> [<a href="/pdf/2402.16033" title="Download PDF">pdf</a>, <a href="/format/2402.16033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diving Deep into Regions: Exploiting Regional Information Transformer  for Single Image Deraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaogang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanyan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jicong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Transformer-based Single Image Deraining (SID) methods have achieved
remarkable success, primarily attributed to their robust capability in
capturing long-range interactions. However, we've noticed that current methods
handle rain-affected and unaffected regions concurrently, overlooking the
disparities between these areas, resulting in confusion between rain streaks
and background parts, and inabilities to obtain effective interactions,
ultimately resulting in suboptimal deraining outcomes. To address the above
issue, we introduce the Region Transformer (Regformer), a novel SID method that
underlines the importance of independently processing rain-affected and
unaffected regions while considering their combined impact for high-quality
image reconstruction. The crux of our method is the innovative Region
Transformer Block (RTB), which integrates a Region Masked Attention (RMA)
mechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention
selection of rain-affected and unaffected regions and local modeling of mixed
scales. The RMA generates attention maps tailored to these two regions and
their interactions, enabling our model to capture comprehensive features
essential for rain removal. To better recover high-frequency textures and
capture more local details, we develop the MGFB as a compensation module to
complete local mixed scale modeling. Extensive experiments demonstrate that our
model reaches state-of-the-art performance, significantly improving the image
deraining quality. Our code and trained models are publicly available.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16034" title="Abstract">arXiv:2402.16034</a> [<a href="/pdf/2402.16034" title="Download PDF">pdf</a>, <a href="/format/2402.16034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotion Classification in Short English Texts using Deep Learning  Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S">Siddhanth Bhat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Detecting emotions in limited text datasets from under-resourced languages
presents a formidable obstacle, demanding specialized frameworks and
computational strategies. This study conducts a thorough examination of deep
learning techniques for discerning emotions in short English texts. Deep
learning approaches employ transfer learning and word embedding, notably BERT,
to attain superior accuracy. To evaluate these methods, we introduce the
"SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts
annotated with five primary emotion categories. Our experiments reveal that
transfer learning and BERT-based text embedding outperform alternative methods
in accurately categorizing the text in the dataset.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16035" title="Abstract">arXiv:2402.16035</a> [<a href="/pdf/2402.16035" title="Download PDF">pdf</a>, <a href="/ps/2402.16035" title="Download PostScript">ps</a>, <a href="/format/2402.16035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Understanding and Generation Using Transformer Models for  Intelligent E-commerce Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rapid development of artificial intelligence technology, Transformer
structural pre-training model has become an important tool for large language
model (LLM) tasks. In the field of e-commerce, these models are especially
widely used, from text understanding to generating recommendation systems,
which provide powerful technical support for improving user experience and
optimizing service processes. This paper reviews the core application scenarios
of Transformer pre-training model in e-commerce text understanding and
recommendation generation, including but not limited to automatic generation of
product descriptions, sentiment analysis of user comments, construction of
personalized recommendation system and automated processing of customer service
conversations. Through a detailed analysis of the model's working principle,
implementation process, and application effects in specific cases, this paper
emphasizes the unique advantages of pre-trained models in understanding complex
user intentions and improving the quality of recommendations. In addition, the
challenges and improvement directions for the future are also discussed, such
as how to further improve the generalization ability of the model, the ability
to handle large-scale data sets, and technical strategies to protect user
privacy. Ultimately, the paper points out that the application of Transformer
structural pre-training models in e-commerce has not only driven technological
innovation, but also brought substantial benefits to merchants and consumers,
and looking forward, these models will continue to play a key role in
e-commerce and beyond.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16036" title="Abstract">arXiv:2402.16036</a> [<a href="/pdf/2402.16036" title="Download PDF">pdf</a>, <a href="/ps/2402.16036" title="Download PostScript">ps</a>, <a href="/format/2402.16036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-Based Vehicle Intention Trajectory Recognition and  Prediction for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, the expansion of internet technology and advancements in
automation have brought significant attention to autonomous driving technology.
Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have
progressively introduced products ranging from assisted-driving vehicles to
semi-autonomous vehicles. However, this period has also witnessed several
traffic safety incidents involving self-driving vehicles. For instance, in
March 2016, a Google self-driving car was involved in a minor collision with a
bus. At the time of the accident, the autonomous vehicle was attempting to
merge into the right lane but failed to dynamically respond to the real-time
environmental information during the lane change. It incorrectly assumed that
the approaching bus would slow down to avoid it, leading to a low-speed
collision with the bus. This incident highlights the current technological
shortcomings and safety concerns associated with autonomous lane-changing
behavior, despite the rapid advancements in autonomous driving technology.
Lane-changing is among the most common and hazardous behaviors in highway
driving, significantly impacting traffic safety and flow. Therefore,
lane-changing is crucial for traffic safety, and accurately predicting drivers'
lane change intentions can markedly enhance driving safety. This paper
introduces a deep learning-based prediction method for autonomous driving lane
change behavior, aiming to facilitate safe lane changes and thereby improve
road safety.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16038" title="Abstract">arXiv:2402.16038</a> [<a href="/pdf/2402.16038" title="Download PDF">pdf</a>, <a href="/ps/2402.16038" title="Download PostScript">ps</a>, <a href="/format/2402.16038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Approaches for Improving Question Answering Systems in  Hepatocellular Carcinoma Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, advancements in natural language processing (NLP) have been
fueled by deep learning techniques, particularly through the utilization of
powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,
trained on vast amounts of data, have revolutionized language understanding and
generation. These pre-trained models serve as robust bases for various tasks
including semantic understanding, intelligent writing, and reasoning, paving
the way for a more generalized form of artificial intelligence. NLP, as a vital
application of AI, aims to bridge the gap between humans and computers through
natural language interaction. This paper delves into the current landscape and
future prospects of large-scale model-based NLP, focusing on the
question-answering systems within this domain. Practical cases and developments
in artificial intelligence-driven question-answering systems are analyzed to
foster further exploration and research in the realm of large-scale NLP.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16039" title="Abstract">arXiv:2402.16039</a> [<a href="/pdf/2402.16039" title="Download PDF">pdf</a>, <a href="/ps/2402.16039" title="Download PostScript">ps</a>, <a href="/format/2402.16039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Public Perceptions of AI Conversational Agents: A  Cross-Cultural Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anfan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yi-Chieh Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, 7 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Conversational Agents (CAs) have increasingly been integrated into everyday
life, sparking significant discussions on social media. While previous research
has examined public perceptions of AI in general, there is a notable lack in
research focused on CAs, with fewer investigations into cultural variations in
CA perceptions. To address this gap, this study used computational methods to
analyze about one million social media discussions surrounding CAs and compared
people's discourses and perceptions of CAs in the US and China. We find Chinese
participants tended to view CAs hedonically, perceived voice-based and
physically embodied CAs as warmer and more competent, and generally expressed
positive emotions. In contrast, US participants saw CAs more functionally, with
an ambivalent attitude. Warm perception was a key driver of positive emotions
toward CAs in both countries. We discussed practical implications for designing
contextually sensitive and user-centric CAs to resonate with various users'
preferences and needs.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16040" title="Abstract">arXiv:2402.16040</a> [<a href="/pdf/2402.16040" title="Download PDF">pdf</a>, <a href="/format/2402.16040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EHRNoteQA: A Patient-Specific Question Answering Benchmark for  Evaluating Large Language Models in Clinical Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+S">Sunjun Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiyoun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Heeyoung Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+D">Dongchul Cha</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hangyul Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kwanghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seunghyun Won</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study introduces EHRNoteQA, a novel patient-specific question answering
benchmark tailored for evaluating Large Language Models (LLMs) in clinical
environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three
medical professionals has curated the dataset comprising 962 unique questions,
each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA
distinct from existing EHR-based benchmarks is as follows: Firstly, it is the
first dataset to adopt a multi-choice question answering format, a design
choice that effectively evaluates LLMs with reliable scores in the context of
automatic evaluation, compared to other formats. Secondly, it requires an
analysis of multiple clinical notes to answer a single question, reflecting the
complex nature of real-world clinical decision-making where clinicians review
extensive records of patient histories. Our comprehensive evaluation on various
large language models showed that their scores on EHRNoteQA correlate more
closely with their performance in addressing real-world medical questions
evaluated by clinicians than their scores from other LLM benchmarks. This
underscores the significance of EHRNoteQA in evaluating LLMs for medical
applications and highlights its crucial role in facilitating the integration of
LLMs into healthcare systems. The dataset will be made available to the public
under PhysioNet credential access, promoting further research in this vital
field.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16041" title="Abstract">arXiv:2402.16041</a> [<a href="/pdf/2402.16041" title="Download PDF">pdf</a>, <a href="/format/2402.16041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Machine-Generated Texts by Multi-Population Aware Optimization  for Maximum Mean Discrepancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuhai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiahao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mingkui Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) such as ChatGPT have exhibited remarkable
performance in generating human-like texts. However, machine-generated texts
(MGTs) may carry critical risks, such as plagiarism issues, misleading
information, or hallucination issues. Therefore, it is very urgent and
important to detect MGTs in many situations. Unfortunately, it is challenging
to distinguish MGTs and human-written texts because the distributional
discrepancy between them is often very subtle due to the remarkable performance
of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy}
(MMD) to address this issue in the sense that MMD can well identify
distributional discrepancies. However, directly training a detector with MMD
using diverse MGTs will incur a significantly increased variance of MMD since
MGTs may contain \textit{multiple text populations} due to various LLMs. This
will severely impair MMD's ability to measure the difference between two
samples. To tackle this, we propose a novel \textit{multi-population} aware
optimization method for MMD called MMD-MP, which can \textit{avoid variance
increases} and thus improve the stability to measure the distributional
discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and
sentence-based detection, respectively. Extensive experiments on various LLMs,
\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The
source code is available at \url{https://github.com/ZSHsh98/MMD-MP}.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16043" title="Abstract">arXiv:2402.16043</a> [<a href="/pdf/2402.16043" title="Download PDF">pdf</a>, <a href="/format/2402.16043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LuaTaint: A Static Taint Analysis System for Web Interface Framework  Vulnerability of IoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jiahui Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Tong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peiyu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">IoT devices are currently facing continuous malicious attacks due to their
widespread use. Among these IoT devices, web vulnerabilities are also widely
exploited because of their inherent characteristics, such as improper
permission controls and insecure interfaces. Recently, the embedded system web
interface framework has become highly diverse, and specific vulnerabilities can
arise if developers forget to detect user input parameters or if the detection
process is not strict enough. Therefore, discovering vulnerabilities in the web
interfaces of IoT devices accurately and comprehensively through an automated
method is a major challenge. This paper aims to work out the challenge. We have
developed an automated vulnerability detection system called LuaTaint for the
typical web interface framework, LuCI. The system employs static taint analysis
to address web security issues on mobile terminal platforms to ensure detection
coverage. It integrates rules pertaining to page handler control logic within
the taint detection process to improve its extensibility. We also implemented a
post-processing step with the assistance of large language models to enhance
accuracy and reduce the need for manual analysis. We have created a prototype
of LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors.
LuaTaint has discovered 68 unknown vulnerabilities.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16045" title="Abstract">arXiv:2402.16045</a> [<a href="/pdf/2402.16045" title="Download PDF">pdf</a>, <a href="/format/2402.16045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Synergy between Pushing, Grasping, and Throwing to  Enhance Object Manipulation in Cluttered Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+H">Hamidreza Kasaei</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+M">Mohammadreza Kasaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we delve into the intricate synergy among non-prehensile
actions like pushing, and prehensile actions such as grasping and throwing,
within the domain of robotic manipulation. We introduce an innovative approach
to learning these synergies by leveraging model-free deep reinforcement
learning. The robot's workflow involves detecting the pose of the target object
and the basket at each time step, predicting the optimal push configuration to
isolate the target object, determining the appropriate grasp configuration, and
inferring the necessary parameters for an accurate throw into the basket. This
empowers robots to skillfully reconfigure cluttered scenarios through pushing,
creating space for collision-free grasping actions. Simultaneously, we
integrate throwing behavior, showcasing how this action significantly extends
the robot's operational reach. Ensuring safety, we developed a simulation
environment in Gazebo for robot training, applying the learned policy directly
to our real robot. Notably, this work represents a pioneering effort to learn
the synergy between pushing, grasping, and throwing actions. Extensive
experimentation in both simulated and real-robot scenarios substantiates the
effectiveness of our approach across diverse settings. Our approach achieves a
success rate exceeding 80\% in both simulated and real-world scenarios. A video
showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16046" title="Abstract">arXiv:2402.16046</a> [<a href="/pdf/2402.16046" title="Download PDF">pdf</a>, <a href="/ps/2402.16046" title="Download PostScript">ps</a>, <a href="/format/2402.16046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Driver Takeover Behaviour in Conditional Automation with  Immersive Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ansar%2C+M+S">Muhammad Sajjad Ansar</a>, 
<a href="/search/cs?searchtype=author&query=Farooq%2C+B">Bilal Farooq</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The safe transition from conditional automation to manual driving control is
significantly intertwined with the vehicle's lateral and longitudinal dynamics.
The transition may occur as a result of a system-initiated mandatory takeover
(MTOR) or as a driver-initiated discretionary takeover (DTOR). In either
condition, the takeover process entails differing cognitive demands and may
affect the driving behaviour differently. This study analyzes driving stability
and perceived mental workload in 304 takeover attempts recorded from 104
participants within virtual and immersive reality environments. Adopting an
exploratory approach, this dynamic simulator study employs a mixed factorial
design. Utilizing a deep neural network-based survival analysis with SHAP
interpretability, the study investigated the influence of covariates on
perception-reaction time (PRT), distinguishing between safe and unsafe control
transition and offering insights into the temporal dynamics of these shifts.
The distributions of key parameters in experimental groups were analyzed and
factors influencing the perceived mental workload were estimated using
multivariate linear regression. The findings indicate a notable decrease in the
risk of unsafe takeovers (described by a longer PRT) when drivers have prior
control-transition experience and familiarity with Automated Vehicles (AVs).
However, driver's prior familiarity and experience with AVs only decreased the
perceived mental workload associated with DTOR, with an insignificant impact on
the cognitive demand of MTOR. Furthermore, multitasking during automated
driving significantly elevated the cognitive demand linked to DTOR and led to
longer PRT in MTOR situations.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16048" title="Abstract">arXiv:2402.16048</a> [<a href="/pdf/2402.16048" title="Download PDF">pdf</a>, <a href="/format/2402.16048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs with Chain-of-Thought Are Non-Causal Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangsheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cunxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper explores the role of the Chain of Thought (CoT) in Large Language
Models (LLMs) reasoning. Despite its potential to improve task performance, our
analysis reveals a surprising frequency of correct answers following incorrect
CoTs and vice versa. We employ causal analysis to assess the cause-effect
relationship between CoTs/instructions and answers in LLMs, uncovering the
Structural Causal Model (SCM) that LLMs approximate. By comparing the implied
SCM with that of human reasoning, we highlight discrepancies between LLM and
human reasoning processes. We further examine the factors influencing the
causal structure of the implied SCM, revealing that in-context learning,
supervised fine-tuning, and reinforcement learning on human feedback
significantly impact the causal relations. We release the code and results at
https://github.com/StevenZHB/CoT_Causal_Analysis.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16050" title="Abstract">arXiv:2402.16050</a> [<a href="/pdf/2402.16050" title="Download PDF">pdf</a>, <a href="/format/2402.16050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form  Video-Text Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jianxin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zilong Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite progress in video-language modeling, the computational challenge of
interpreting long-form videos in response to task-specific linguistic queries
persists, largely due to the complexity of high-dimensional video data and the
misalignment between language and visual cues over space and time. To tackle
this issue, we introduce a novel approach called Language-guided
Spatial-Temporal Prompt Learning (LSTP). This approach features two key
components: a Temporal Prompt Sampler (TPS) with optical flow prior that
leverages temporal information to efficiently extract relevant video content,
and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial
relationships between visual and textual elements. By harmonizing TPS and SPS
with a cohesive training strategy, our framework significantly enhances
computational efficiency, temporal understanding, and spatial-temporal
alignment. Empirical evaluations across two challenging tasks--video question
answering and temporal question grounding in videos--using a variety of
video-language pretrainings (VLPs) and large language models (LLMs) demonstrate
the superior performance, speed, and versatility of our proposed LSTP paradigm.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16052" title="Abstract">arXiv:2402.16052</a> [<a href="/pdf/2402.16052" title="Download PDF">pdf</a>, <a href="/format/2402.16052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing UAV Fog Deployment Efficiency for Critical Rescue Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naouri%2C+A">Abdenacer Naouri</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+H">Huansheng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+N+A">Nabil Abdelkader Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Khelloufi%2C+A">Amar Khelloufi</a>, 
<a href="/search/cs?searchtype=author&query=Sada%2C+A+B">Abdelkarim Ben Sada</a>, 
<a href="/search/cs?searchtype=author&query=Naouri%2C+S">Salim Naouri</a>, 
<a href="/search/cs?searchtype=author&query=Qammar%2C+A">Attia Qammar</a>, 
<a href="/search/cs?searchtype=author&query=Dhelim%2C+S">Sahraoui Dhelim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">In disaster scenarios and high-stakes rescue operations, integrating Unmanned
Aerial Vehicles (UAVs) as fog nodes has become crucial. This integration
ensures a smooth connection between affected populations and essential health
monitoring devices, supported by the Internet of Things (IoT). Integrating UAVs
in such environments is inherently challenging, where the primary objectives
involve maximizing network connectivity and coverage while extending the
network's lifetime through energy-efficient strategies to serve the maximum
number of affected individuals. In this paper, We propose a novel model centred
around dynamic UAV-based fog deployment that optimizes the system's
adaptability and operational efficacy within the afflicted areas. First, we
decomposed the problem into two subproblems. Connectivity and coverage
subproblem, and network lifespan optimization subproblem. We shape our UAV fog
deployment problem as a uni-objective optimization and introduce a specialized
UAV fog deployment algorithm tailored specifically for UAV fog nodes deployed
in rescue missions. While the network lifespan optimization subproblem is
efficiently solved via a one-dimensional swapping method. Following that, We
introduce a novel optimization strategy for UAV fog node placement in dynamic
networks during evacuation scenarios, with a primary focus on ensuring robust
connectivity and maximal coverage for mobile users, while extending the
network's lifespan. Finally, we introduce Adaptive Whale Optimization Algorithm
(WOA) for fog node deployment in a dynamic network. Its agility, rapid
convergence, and low computational demands make it an ideal fit for
high-pressure environments.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16057" title="Abstract">arXiv:2402.16057</a> [<a href="/pdf/2402.16057" title="Download PDF">pdf</a>, <a href="/ps/2402.16057" title="Download PostScript">ps</a>, <a href="/format/2402.16057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fractal Gripper: Adaptive manipulator with mode switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhigong Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Although the multi-jointed underactuated manipulator is highly dexterous, its
grasping capacity does not match that of the parallel jaw gripper. This work
introduces a fractal gripper to enhance the grasping capacity of multi-joint
underactuated manipulators, preserving their passive clamping features. We
describe in detail the working principle and manufacturing process of the
fractal gripper. This work, inspired by the 'Fractal Vise' structure, resulted
in the invention of a fractal gripper with mode switching capabilities. The
fractal gripper inherits the inherent adaptive properties of the fractal
structure and realizes the self-resetting function by integrating spring into
the original design, thereby enhancing the efficiency of object grasping tasks.
The fractal gripper prevents object damage by distributing pressure evenly and
applying it at multiple points through its fractal structure during closure.
Objects of various shapes are effectively grasped by the fractal gripper, which
ensures a safe and secure grasp. The superior performance was provided by the
force distribution characteristics of the fractal gripper. By applying the
flexible polymer PDMS, which possesses superior elasticity, to the fractal
structure's wrapping surface, potential scratching during grasping is
effectively prevented, thus protecting the object's geometric surface. Grab
experiments with objects of diverse shapes and sizes confirm fractal gripper
multi-scale adaptability and superior grasping stability.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16058" title="Abstract">arXiv:2402.16058</a> [<a href="/pdf/2402.16058" title="Download PDF">pdf</a>, <a href="/format/2402.16058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Say More with Less: Understanding Prompt Learning Behaviors through Gist  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinze Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) require lengthy prompts as the input context to
produce output aligned with user intentions, a process that incurs extra costs
during inference. In this paper, we propose the Gist COnditioned deCOding
(Gist-COCO) model, introducing a novel method for compressing prompts which
also can assist the prompt interpretation and engineering. Gist-COCO employs an
encoder-decoder based language model and then incorporates an additional
encoder as a plugin module to compress prompts with inputs using gist tokens.
It finetunes the compression plugin module and uses the representations of gist
tokens to emulate the raw prompts in the vanilla language model. By verbalizing
the representations of gist tokens into gist prompts, the compression ability
of Gist-COCO can be generalized to different LLMs with high compression rates.
Our experiments demonstrate that Gist-COCO outperforms previous prompt
compression models in both passage and instruction compression tasks. Further
analysis on gist verbalization results suggests that our gist prompts serve
different functions in aiding language models. They may directly provide
potential answers, generate the chain-of-thought, or simply repeat the inputs.
All data and codes are available at https://github.com/OpenMatch/Gist-COCO .
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16061" title="Abstract">arXiv:2402.16061</a> [<a href="/pdf/2402.16061" title="Download PDF">pdf</a>, <a href="/format/2402.16061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Large Language Models Encode Context Knowledge? A Layer-Wise Probing  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+T">Tianjie Ju</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xinwei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024 (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Previous work has showcased the intriguing capability of large language
models (LLMs) in retrieving facts and processing context knowledge. However,
only limited research exists on the layer-wise capability of LLMs to encode
knowledge, which challenges our understanding of their internal mechanisms. In
this paper, we devote the first attempt to investigate the layer-wise
capability of LLMs through probing tasks. We leverage the powerful generative
capability of ChatGPT to construct probing datasets, providing diverse and
coherent evidence corresponding to various facts. We employ $\mathcal V$-usable
information as the validation metric to better reflect the capability in
encoding context knowledge across different layers. Our experiments on
conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode
more context knowledge in the upper layers; (2) primarily encode context
knowledge within knowledge-related entity tokens at lower layers while
progressively expanding more knowledge within other tokens at upper layers; and
(3) gradually forget the earlier context knowledge retained within the
intermediate layers when provided with irrelevant evidence. Code is publicly
available at https://github.com/Jometeorie/probing_llama.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16063" title="Abstract">arXiv:2402.16063</a> [<a href="/pdf/2402.16063" title="Download PDF">pdf</a>, <a href="/format/2402.16063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citation-Enhanced Generation for LLM-based Chatbot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junkai Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration
(\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous
studies that focus on preventing hallucinations during generation, our method
addresses this issue in a post-hoc way. It incorporates a retrieval module to
search for supporting documents relevant to the generated content, and employs
a natural language inference-based citation generation module. Once the
statements in the generated content lack of reference, our model can regenerate
responses until all statements are supported by citations. Note that our method
is a training-free plug-and-play plugin that is capable of various LLMs.
Experiments on various hallucination-related datasets show our framework
outperforms state-of-the-art methods in both hallucination detection and
response regeneration on three benchmarks. Our codes and dataset will be
publicly available.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16065" title="Abstract">arXiv:2402.16065</a> [<a href="/pdf/2402.16065" title="Download PDF">pdf</a>, <a href="/format/2402.16065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training a Bilingual Language Model by Mapping Tokens onto a Shared  Character Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rom%2C+A">Aviad Rom</a>, 
<a href="/search/cs?searchtype=author&query=Bar%2C+K">Kfir Bar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We train a bilingual Arabic-Hebrew language model using a transliterated
version of Arabic texts in Hebrew, to ensure both languages are represented in
the same script. Given the morphological, structural similarities, and the
extensive number of cognates shared among Arabic and Hebrew, we assess the
performance of a language model that employs a unified script for both
languages, on machine translation which requires cross-lingual knowledge. The
results are promising: our model outperforms a contrasting model which keeps
the Arabic texts in the Arabic script, demonstrating the efficacy of the
transliteration step. Despite being trained on a dataset approximately 60%
smaller than that of other existing language models, our model appears to
deliver comparable performance in machine translation across both translation
directions.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16068" title="Abstract">arXiv:2402.16068</a> [<a href="/pdf/2402.16068" title="Download PDF">pdf</a>, <a href="/format/2402.16068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot  Interaction Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castri%2C+L">Luca Castri</a>, 
<a href="/search/cs?searchtype=author&query=Beraldo%2C+G">Gloria Beraldo</a>, 
<a href="/search/cs?searchtype=author&query=Mghames%2C+S">Sariah Mghames</a>, 
<a href="/search/cs?searchtype=author&query=Hanheide%2C+M">Marc Hanheide</a>, 
<a href="/search/cs?searchtype=author&query=Bellotto%2C+N">Nicola Bellotto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the "Causal-HRI: Causal Learning for Human-Robot Interaction" workshop at the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deploying robots in human-shared spaces requires understanding interactions
among nearby agents and objects. Modelling cause-and-effect relations through
causal inference aids in predicting human behaviours and anticipating robot
interventions. However, a critical challenge arises as existing causal
discovery methods currently lack an implementation inside the ROS ecosystem,
the standard de facto in robotics, hindering effective utilisation in robotics.
To address this gap, this paper introduces ROS-Causal, a ROS-based framework
for onboard data collection and causal discovery in human-robot spatial
interactions. An ad-hoc simulator, integrated with ROS, illustrates the
approach's effectiveness, showcasing the robot onboard generation of causal
models during data collection. ROS-Causal is available on GitHub:
https://github.com/lcastri/roscausal.git.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16072" title="Abstract">arXiv:2402.16072</a> [<a href="/pdf/2402.16072" title="Download PDF">pdf</a>, <a href="/ps/2402.16072" title="Download PostScript">ps</a>, <a href="/format/2402.16072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demonstration of 3 V Programmable Josephson Junction Arrays Using  Non-Integer-Multiple Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wenhui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Erkun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinjin Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+H">Huan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qing Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Da Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueshen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">This article demonstrates a new kind of programmable logic for the
representation of an integer that can be used for the programmable Josephson
voltage standard. It can enable the numbers of junctions in most bits to be
variable integer values, which is different from normal binary logic or ternary
logic. Consequently, missing junctions due to superconducting short circuits
can be tolerated under this logic. This logic can also have nearly the same
segmentation efficiency as ternary logic. The completeness of the sequences
using this logic is proven by the recursive method in mathematics in this
paper. After that, a new algorithm for the representation of integers is
presented according to the proven process, and an analysis of the number of
fault-tolerant junctions for each bit is provided. Although the first and
second bits are not tolerant to missing junctions, bits beyond these can
tolerate one to hundreds of missing junctions. Due to the non-fixed multiples
between the bits of the sequence, this logic is called non-integer-multiple
logic. Finally, the design and fabrication of a 3 V programmable Josephson
junction array using this logic are described, and the measurements and
analysis of the characteristic parameters are presented.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16073" title="Abstract">arXiv:2402.16073</a> [<a href="/pdf/2402.16073" title="Download PDF">pdf</a>, <a href="/format/2402.16073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pfeed: Generating near real-time personalized feeds using precomputed  embedding similarities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gebre%2C+B">Binyam Gebre</a>, 
<a href="/search/cs?searchtype=author&query=Ranta%2C+K">Karoliina Ranta</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Elzen%2C+S">Stef van den Elzen</a>, 
<a href="/search/cs?searchtype=author&query=Kuiper%2C+E">Ernst Kuiper</a>, 
<a href="/search/cs?searchtype=author&query=Baars%2C+T">Thijs Baars</a>, 
<a href="/search/cs?searchtype=author&query=Heskes%2C+T">Tom Heskes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In personalized recommender systems, embeddings are often used to encode
customer actions and items, and retrieval is then performed in the embedding
space using approximate nearest neighbor search. However, this approach can
lead to two challenges: 1) user embeddings can restrict the diversity of
interests captured and 2) the need to keep them up-to-date requires an
expensive, real-time infrastructure. In this paper, we propose a method that
overcomes these challenges in a practical, industrial setting. The method
dynamically updates customer profiles and composes a feed every two minutes,
employing precomputed embeddings and their respective similarities. We tested
and deployed this method to personalise promotional items at Bol, one of the
largest e-commerce platforms of the Netherlands and Belgium. The method
enhanced customer engagement and experience, leading to a significant 4.9%
uplift in conversions.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16075" title="Abstract">arXiv:2402.16075</a> [<a href="/pdf/2402.16075" title="Download PDF">pdf</a>, <a href="/format/2402.16075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behavioral Refinement via Interpolant-based Policy Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kaiqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+E">Eugene Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kelvin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Soh%2C+H">Harold Soh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Imitation learning empowers artificial agents to mimic behavior by learning
from demonstrations. Recently, diffusion models, which have the ability to
model high-dimensional and multimodal distributions, have shown impressive
performance on imitation learning tasks. These models learn to shape a policy
by diffusing actions (or states) from standard Gaussian noise. However, the
target policy to be learned is often significantly different from Gaussian and
this mismatch can result in poor performance when using a small number of
diffusion steps (to improve inference speed) and under limited data. The key
idea in this work is that initiating from a more informative source than
Gaussian enables diffusion methods to overcome the above limitations. We
contribute both theoretical results, a new method, and empirical findings that
show the benefits of using an informative source policy. Our method, which we
call BRIDGER, leverages the stochastic interpolants framework to bridge
arbitrary policies, thus enabling a flexible approach towards imitation
learning. It generalizes prior work in that standard Gaussians can still be
applied, but other source policies can be used if available. In experiments on
challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies
and we provide further analysis on design considerations when applying BRIDGER.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16077" title="Abstract">arXiv:2402.16077</a> [<a href="/pdf/2402.16077" title="Download PDF">pdf</a>, <a href="/format/2402.16077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Frames and the Impossibility of Continuous Canonicalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+H">Hannah Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Siegel%2C+J+W">Jonathan W. Siegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Canonicalization provides an architecture-agnostic method for enforcing
equivariance, with generalizations such as frame-averaging recently gaining
prominence as a lightweight and flexible alternative to equivariant
architectures. Recent works have found an empirical benefit to using
probabilistic frames instead, which learn weighted distributions over group
elements. In this work, we provide strong theoretical justification for this
phenomenon: for commonly-used groups, there is no efficiently computable choice
of frame that preserves continuity of the function being averaged. In other
words, unweighted frame-averaging can turn a smooth, non-symmetric function
into a discontinuous, symmetric function. To address this fundamental
robustness problem, we formally define and construct \emph{weighted} frames,
which provably preserve continuity, and demonstrate their utility by
constructing efficient and continuous weighted frames for the actions of
$SO(2)$, $SO(3)$, and $S_n$ on point clouds.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16078" title="Abstract">arXiv:2402.16078</a> [<a href="/pdf/2402.16078" title="Download PDF">pdf</a>, <a href="/format/2402.16078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Spatio-Temporal Representations: Evolving Fourier Transform for  Temporal Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bastos%2C+A">Anson Bastos</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kuldeep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Nadgeri%2C+A">Abhishek Nadgeri</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Manish Singh</a>, 
<a href="/search/cs?searchtype=author&query=Suzumura%2C+T">Toyotaro Suzumura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full conference paper in the International Conference on Learning Representations 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present the Evolving Graph Fourier Transform (EFT), the first invertible
spectral transform that captures evolving representations on temporal graphs.
We motivate our work by the inadequacy of existing methods for capturing the
evolving graph spectra, which are also computationally expensive due to the
temporal aspect along with the graph vertex domain. We view the problem as an
optimization over the Laplacian of the continuous time dynamic graph.
Additionally, we propose pseudo-spectrum relaxations that decompose the
transformation process, making it highly computationally efficient. The EFT
method adeptly captures the evolving graph's structural and positional
properties, making it effective for downstream tasks on evolving graphs. Hence,
as a reference implementation, we develop a simple neural model induced with
EFT for capturing evolving graph spectra. We empirically validate our
theoretical findings on a number of large-scale and standard temporal graph
benchmarks and demonstrate that our model achieves state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16080" title="Abstract">arXiv:2402.16080</a> [<a href="/pdf/2402.16080" title="Download PDF">pdf</a>, <a href="/ps/2402.16080" title="Download PostScript">ps</a>, <a href="/format/2402.16080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalised Soft Finite Element Method for Elliptic Eigenvalue Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jipei Chen</a>, 
<a href="/search/math?searchtype=author&query=Calo%2C+V+M">Victor M. Calo</a>, 
<a href="/search/math?searchtype=author&query=Deng%2C+Q">Quanling Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The recently proposed soft finite element method (SoftFEM) reduces the
stiffness (condition numbers), consequently improving the overall approximation
accuracy. The method subtracts a least-square term that penalizes the gradient
jumps across mesh interfaces from the FEM stiffness bilinear form while
maintaining the system's coercivity. Herein, we present two generalizations for
SoftFEM that aim to improve the approximation accuracy and further reduce the
discrete systems' stiffness. Firstly and most naturally, we generalize SoftFEM
by adding a least-square term to the mass bilinear form. Superconvergent
results of rates $h^6$ and $h^8$ for eigenvalues are established for linear
uniform elements; $h^8$ is the highest order of convergence known in the
literature. Secondly, we generalize SoftFEM by applying the blended
Gaussian-type quadratures. We demonstrate further reductions in stiffness
compared to traditional FEM and SoftFEM. The coercivity and analysis of the
optimal error convergences follow the work of SoftFEM. Thus, this paper focuses
on the numerical study of these generalizations. For linear and uniform
elements, analytical eigenpairs, exact eigenvalue errors, and superconvergent
error analysis are established. Various numerical examples demonstrate the
potential of generalized SoftFEMs for spectral approximation, particularly in
high-frequency regimes.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16081" title="Abstract">arXiv:2402.16081</a> [<a href="/pdf/2402.16081" title="Download PDF">pdf</a>, <a href="/ps/2402.16081" title="Download PostScript">ps</a>, <a href="/format/2402.16081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming  Under Nonconvex QoS Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ya-Feng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper studies the quality-of-service (QoS) constrained multi-group
multicast beamforming design problem, where each multicast group is composed of
a number of users requiring the same content. Due to the nonconvex QoS
constraints, this problem is nonconvex and NP-hard. While existing
optimization-based iterative algorithms can obtain a suboptimal solution, their
iterative nature results in large computational complexity and delay. To
facilitate real-time implementations, this paper proposes a deep learning-based
approach, which consists of a beamforming structure assisted problem
transformation and a customized neural network architecture named hierarchical
permutation equivariance (HPE) transformer. The proposed HPE transformer is
proved to be permutation equivariant with respect to the users within each
multicast group, and also permutation equivariant with respect to different
multicast groups. Simulation results demonstrate that the proposed HPE
transformer outperforms state-of-the-art optimization-based and deep
learning-based approaches for multi-group multicast beamforming design in terms
of the total transmit power, the constraint violation, and the computational
time. In addition, the proposed HPE transformer achieves pretty good
generalization performance on different numbers of users, different numbers of
multicast groups, and different signal-to-interference-plus-noise ratio
targets.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16082" title="Abstract">arXiv:2402.16082</a> [<a href="/pdf/2402.16082" title="Download PDF">pdf</a>, <a href="/format/2402.16082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Point Uncertainty in Radar SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiucan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Huan Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While visual and laser-based simultaneous localization and mapping (SLAM)
techniques have gained significant attention, radar SLAM remains a robust
option for challenging conditions. This paper aims to improve the performance
of radar SLAM by modeling point uncertainty. The basic SLAM system is a
radar-inertial odometry (RIO) system that leverages velocity-aided radar points
and high-frequency inertial measurements. We first propose to model the
uncertainty of radar points in polar coordinates by considering the nature of
radar sensing. Then in the SLAM system, the uncertainty model is designed into
the data association module and is incorporated to weight the motion
estimation. Real-world experiments on public and self-collected datasets
validate the effectiveness of the proposed models and approaches. The findings
highlight the potential of incorporating radar point uncertainty modeling to
improve the radar SLAM system in adverse environments.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16084" title="Abstract">arXiv:2402.16084</a> [<a href="/pdf/2402.16084" title="Download PDF">pdf</a>, <a href="/format/2402.16084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Concept to Implementation: Streamlining Sensor and Actuator  Selection for Collaborative Design and Engineering of Interactive Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Y%C4%B1ld%C4%B1r%C4%B1m%2C+%C4%B0+O">&#x130;hsan Ozan Y&#x131;ld&#x131;r&#x131;m</a>, 
<a href="/search/cs?searchtype=author&query=Keskin%2C+E">Ege Keskin</a>, 
<a href="/search/cs?searchtype=author&query=Kocaman%2C+Y">Ya&#x11f;mur Kocaman</a>, 
<a href="/search/cs?searchtype=author&query=Ku%C5%9Fcu%2C+M">Murat Ku&#x15f;cu</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zcan%2C+O">O&#x11f;uzhan &#xd6;zcan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 14 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Selecting appropriate sensors and actuators is a pivotal aspect of design and
engineering, particularly in projects involving interactive systems. This
article introduces the Design Thinking Based Iterative Sensor and Actuator
Selection Flow, a structured decision-making approach aimed at streamlining
this essential, yet often complex task. Created to accommodate individuals with
diverse levels of technical expertise, our approach is uniquely suited for
interdisciplinary teams of designers and engineers. Through the application of
the flow to four real-world case studies, we highlight its broad applicability
and demonstrate its efficacy in expediting project timelines and enhancing
resource utilization. Our work lays a foundation for a more streamlined and
user-centered process in selecting sensors and actuators, significantly
benefiting the practice of interactive system design. This contribution serves
as a seminal foundation for future research, offering significant contributions
to both academic inquiry and practical applications across various industries.
While the focus of the flow is on streamlining the selection process rather
than on in-depth technical considerations, which are beyond the scope of this
study, it provides a comprehensive guide for efficient and informed
decision-making in the realm of interactive system design.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16085" title="Abstract">arXiv:2402.16085</a> [<a href="/pdf/2402.16085" title="Download PDF">pdf</a>, <a href="/format/2402.16085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Drone Scheduling for Last-mile Delivery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jana%2C+S">Saswata Jana</a>, 
<a href="/search/cs?searchtype=author&query=Italiano%2C+G+F">Giuseppe F. Italiano</a>, 
<a href="/search/cs?searchtype=author&query=Kashyop%2C+M+J">Manas Jyoti Kashyop</a>, 
<a href="/search/cs?searchtype=author&query=Konstantinidis%2C+A+L">Athanasios L. Konstantinidis</a>, 
<a href="/search/cs?searchtype=author&query=Kosinas%2C+E">Evangelos Kosinas</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+P+S">Partha Sarathi Mandal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Delivering a parcel from the distribution hub to the customer's doorstep is
called the \textit{last-mile delivery} step in delivery logistics. In this
paper, we study a hybrid {\it truck-drones} model for the last-mile delivery
step, in which a truck moves on a predefined path carrying parcels and drones
deliver the parcels. We define the \textsc{online drone scheduling} problem,
where the truck moves in a predefined path, and the customer's requests appear
online during the truck's movement. The objective is to schedule a drone
associated with every request to minimize the number of drones used subject to
the battery budget of the drones and compatibility of the schedules. We propose
a 3-competitive deterministic algorithm using the next-fit strategy and
2.7-competitive algorithms using the first-fit strategy for the problem with
$O(\log n)$ worst-case time complexity per request, where $n$ is the maximum
number of active requests at any time. We also introduce \textsc{online
variable-size drone scheduling} problem (OVDS). Here, we know all the
customer's requests in advance; however, the drones with different battery
capacities appear online. The objective is to schedule customers' requests for
drones to minimize the number of drones used. We propose a $(2\alpha +
1)$-competitive algorithm for the OVDS problem with total running time $O(n
\log n)$ for $n$ customer requests, where $\alpha$ is the ratio of the maximum
battery capacity to the minimum battery capacity of the drones. Finally, we
address how to generate intervals corresponding to each customer request when
there are discrete stopping points on the truck's route, from where the drone
can fly and meet with the truck.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16086" title="Abstract">arXiv:2402.16086</a> [<a href="/pdf/2402.16086" title="Download PDF">pdf</a>, <a href="/format/2402.16086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Homography Estimation for Visual Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Feng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiangyuan Lan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dongmei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Visual place recognition (VPR) is a fundamental task for many applications
such as robot localization and augmented reality. Recently, the hierarchical
VPR methods have received considerable attention due to the trade-off between
accuracy and efficiency. They usually first use global features to retrieve the
candidate images, then verify the spatial consistency of matched local features
for re-ranking. However, the latter typically relies on the RANSAC algorithm
for fitting homography, which is time-consuming and non-differentiable. This
makes existing methods compromise to train the network only in global feature
extraction. Here, we propose a transformer-based deep homography estimation
(DHE) network that takes the dense feature map extracted by a backbone network
as input and fits homography for fast and learnable geometric verification.
Moreover, we design a re-projection error of inliers loss to train the DHE
network without additional homography labels, which can also be jointly trained
with the backbone network to help it extract the features that are more
suitable for local matching. Extensive experiments on benchmark datasets show
that our method can outperform several state-of-the-art methods. And it is more
than one order of magnitude faster than the mainstream hierarchical VPR methods
using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16087" title="Abstract">arXiv:2402.16087</a> [<a href="/pdf/2402.16087" title="Download PDF">pdf</a>, <a href="/format/2402.16087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Privately Tune Hyperparameters in Federated Learning? Insights  from a Benchmark Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitic%2C+N">Natalija Mitic</a>, 
<a href="/search/cs?searchtype=author&query=Pyrgelis%2C+A">Apostolos Pyrgelis</a>, 
<a href="/search/cs?searchtype=author&query=Sav%2C+S">Sinem Sav</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this paper, we address the problem of privacy-preserving hyperparameter
(HP) tuning for cross-silo federated learning (FL). We first perform a
comprehensive measurement study that benchmarks various HP strategies suitable
for FL. Our benchmarks show that the optimal parameters of the FL server, e.g.,
the learning rate, can be accurately and efficiently tuned based on the HPs
found by each client on its local data. We demonstrate that HP averaging is
suitable for iid settings, while density-based clustering can uncover the
optimal set of parameters in non-iid ones. Then, to prevent information leakage
from the exchange of the clients' local HPs, we design and implement PrivTuna,
a novel framework for privacy-preserving HP tuning using multiparty homomorphic
encryption. We use PrivTuna to implement privacy-preserving federated averaging
and density-based clustering, and we experimentally evaluate its performance
demonstrating its computation/communication efficiency and its precision in
tuning hyperparameters.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16090" title="Abstract">arXiv:2402.16090</a> [<a href="/pdf/2402.16090" title="Download PDF">pdf</a>, <a href="/format/2402.16090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key Design Choices in Source-Free Unsupervised Domain Adaptation: An  In-depth Empirical Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maracani%2C+A">Andrea Maracani</a>, 
<a href="/search/cs?searchtype=author&query=Camoriano%2C+R">Raffaello Camoriano</a>, 
<a href="/search/cs?searchtype=author&query=Maiettini%2C+E">Elisa Maiettini</a>, 
<a href="/search/cs?searchtype=author&query=Talon%2C+D">Davide Talon</a>, 
<a href="/search/cs?searchtype=author&query=Rosasco%2C+L">Lorenzo Rosasco</a>, 
<a href="/search/cs?searchtype=author&query=Natale%2C+L">Lorenzo Natale</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study provides a comprehensive benchmark framework for Source-Free
Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to
achieve a rigorous empirical understanding of the complex relationships between
multiple key design factors in SF-UDA methods. The study empirically examines a
diverse set of SF-UDA techniques, assessing their consistency across datasets,
sensitivity to specific hyperparameters, and applicability across different
families of backbone architectures. Moreover, it exhaustively evaluates
pre-training datasets and strategies, particularly focusing on both supervised
and self-supervised methods, as well as the impact of fine-tuning on the source
domain. Our analysis also highlights gaps in existing benchmark practices,
guiding SF-UDA research towards more effective and general approaches. It
emphasizes the importance of backbone architecture and pre-training dataset
selection on SF-UDA performance, serving as an essential reference and
providing key insights. Lastly, we release the source code of our experimental
framework. This facilitates the construction, training, and testing of SF-UDA
methods, enabling systematic large-scale experimental analysis and supporting
further research efforts in this field.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16091" title="Abstract">arXiv:2402.16091</a> [<a href="/pdf/2402.16091" title="Download PDF">pdf</a>, <a href="/format/2402.16091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Network For Personalized Federated Learning Parameter  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Mengen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kuruoglu%2C+E+E">Ercan Engin Kuruoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated learning's poor performance in the presence of heterogeneous data
remains one of the most pressing issues in the field. Personalized federated
learning departs from the conventional paradigm in which all clients employ the
same model, instead striving to discover an individualized model for each
client to address the heterogeneity in the data. One of such approach involves
personalizing specific layers of neural networks. However, prior endeavors have
not provided a dependable rationale, and some have selected personalized layers
that are entirely distinct and conflicting. In this work, we take a step
further by proposing personalization at the elemental level, rather than the
traditional layer-level personalization. To select personalized parameters, we
introduce Bayesian neural networks and rely on the uncertainty they offer to
guide our selection of personalized parameters. Finally, we validate our
algorithm's efficacy on several real-world datasets, demonstrating that our
proposed approach outperforms existing baselines.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16092" title="Abstract">arXiv:2402.16092</a> [<a href="/pdf/2402.16092" title="Download PDF">pdf</a>, <a href="/format/2402.16092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StochCA: A Novel Approach for Exploiting Pretrained Models with  Cross-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Seungwon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Sangheum Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Utilizing large-scale pretrained models is a well-known strategy to enhance
performance on various target tasks. It is typically achieved through
fine-tuning pretrained models on target tasks. However, na\"{\i}ve fine-tuning
may not fully leverage knowledge embedded in pretrained models. In this study,
we introduce a novel fine-tuning method, called stochastic cross-attention
(StochCA), specific to Transformer architectures. This method modifies the
Transformer's self-attention mechanism to selectively utilize knowledge from
pretrained models during fine-tuning. Specifically, in each block, instead of
self-attention, cross-attention is performed stochastically according to the
predefined probability, where keys and values are extracted from the
corresponding block of a pretrained model. By doing so, queries and
channel-mixing multi-layer perceptron layers of a target model are fine-tuned
to target tasks to learn how to effectively exploit rich representations of
pretrained models. To verify the effectiveness of StochCA, extensive
experiments are conducted on benchmarks in the areas of transfer learning and
domain generalization, where the exploitation of pretrained models is critical.
Our experimental results show the superiority of StochCA over state-of-the-art
approaches in both areas. Furthermore, we demonstrate that StochCA is
complementary to existing approaches, i.e., it can be combined with them to
further improve performance. Our code is available at
https://github.com/daintlab/stochastic_cross_attention
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16094" title="Abstract">arXiv:2402.16094</a> [<a href="/pdf/2402.16094" title="Download PDF">pdf</a>, <a href="/ps/2402.16094" title="Download PostScript">ps</a>, <a href="/format/2402.16094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bistochastically private release of data streams with zero delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+N">Nicolas Ruiz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Although the bulk of the research in privacy and statistical disclosure
control is designed for static data, more and more data are often collected as
continuous streams, and extensions of popular privacy tools and models have
been proposed for this scenario. However, most of these proposals require
buffers, where incoming individuals are momentarily stored, anonymized, and
then released following a delay, thus considering a data stream as a succession
of batches while it is by nature continuous. Having a delay unavoidably alters
data freshness but also, more critically, inordinately exerts constraints on
what can be achieved in terms of protection and information preservation. By
considering randomized response, and specifically its recent bistochastic
extension, in the context of dynamic data, this paper proposes a protocol for
the anonymization of data streams that achieves zero delay while exhibiting
formal privacy guarantees. Using a new tool in the privacy literature that
introduces the concept of elementary plausible deniability, we show that it is
feasible to achieve an atomic processing of individuals entering a stream,
in-stead of proceeding by batches. We illustrate the application of the
proposed approach by an empirical example.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16095" title="Abstract">arXiv:2402.16095</a> [<a href="/pdf/2402.16095" title="Download PDF">pdf</a>, <a href="/format/2402.16095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> chainBoost: A Secure Performance Booster for Blockchain-based Resource  Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motaqy%2C+Z">Zahra Motaqy</a>, 
<a href="/search/cs?searchtype=author&query=Najd%2C+M+E">Mohamed E. Najd</a>, 
<a href="/search/cs?searchtype=author&query=Almashaqbeh%2C+G">Ghada Almashaqbeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Cryptocurrencies and blockchain technology provide an innovative model for
reshaping digital services. Driven by the movement toward Web 3.0, recent
systems started to provide distributed services, such as computation
outsourcing or file storage, on top of the currency exchange medium. By
allowing anyone to join and collect cryptocurrency payments for serving others,
these systems create decentralized markets for trading digital resources. Yet,
there is still a big gap between the promise of these markets and their
practical viability. Existing initiatives are still early-stage and have
already encountered security and efficiency obstacles. At the same time,
existing work around promising ideas, specifically sidechains, fall short in
exploiting their full potential in addressing these problems.
<br />To bridge this gap, we propose chainBoost, a secure performance booster for
decentralized resource markets. It expedites service related operations,
reduces the blockchain size, and supports flexible service-payment exchange
modalities at low overhead. At its core, chainBoost employs a sidechain, that
has a (security and semantic) mutual-dependence with the mainchain, to which
the system offloads heavy/frequent operations. To enable it, we develop a novel
sidechain architecture composed of temporary and permanent blocks, a block
suppression mechanism to prune the sidechain, a syncing protocol to permit
arbitrary data exchange between the two chains, and an autorecovery protocol to
support robustness and resilience. We analyze the security of chainBoost, and
implement a proof-of-concept prototype for a distributed file storage market as
a use case. For a market handling around 2000 transactions per round, our
experiments show up to 11x improvement in throughput and 94\% reduction in
confirmation time. They also show that chainBoost can reduce the main
blockchain size by around 90%.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16097" title="Abstract">arXiv:2402.16097</a> [<a href="/pdf/2402.16097" title="Download PDF">pdf</a>, <a href="/format/2402.16097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecular Code-Division Multiple-Access: Signaling, Detection, and  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Weidong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lie-Liang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">To accomplish relatively complex tasks, in Internet of Bio-Nano Things
(IoBNT), information collected by different nano-machines (NMs) is usually sent
via multiple-access channels to fusion centers (FCs) for further processing.
Relying on two types of molecules, in this paper, a molecular code-division
multiple-access (MoCDMA) scheme is designed for multiple NMs to simultaneously
send information to an access-point (AP) in a diffusive molecular
communications (DMC) environment. We assume that different NMs may have
different distances from AP, which generates `near-far' effect.
Correspondingly, the uniform and channel-inverse based molecular emission
schemes are proposed for NMs to emit information molecules. To facilitate the
design of different signal detection schemes, the received signals by AP are
represented in different forms. Specifically, by considering the limited
computational power of nano-machines, three low-complexity detectors are
designed in the principles of matched-filtering (MF), zero-forcing (ZF), and
minimum mean-square error (MMSE). The noise characteristics in MoCDMA systems
and the complexity of various detection schemes are analyzed. The error
performance of the MoCDMA systems with various molecular emission and detection
schemes is demonstrated and compared. Our studies and performance results
demonstrate that MoCDMA constitutes a promising scheme for supporting
multiple-access transmission in DMC, while the channel-inverse based
transmission can ensure the fairness of communication qualities (FoCQ) among
different NMs. Furthermore, different detection schemes may be implemented to
attain a good trade-off between implementation complexity and communication
reliability.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16101" title="Abstract">arXiv:2402.16101</a> [<a href="/pdf/2402.16101" title="Download PDF">pdf</a>, <a href="/format/2402.16101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven  Approach by Analyzing Working Pattern
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jeonghyeon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junhyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyojae Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hakyoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangwon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+M">Minho Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In robot-assisted minimally invasive surgery (RAMIS), optimal placement of
the surgical robot's base is crucial for successful surgery. Improper placement
can hinder performance due to manipulator limitations and inaccessible
workspaces. Traditionally, trained medical staff rely on experience for base
placement, but this approach lacks objectivity. This paper proposes a novel
method to determine the optimal base pose based on the individual surgeon's
working pattern. The proposed method analyzes recorded end-effector poses using
machine-learning based clustering technique to identify key positions and
orientations preferred by the surgeon. To address joint limits and
singularities problems, we introduce two scoring metrics: joint margin score
and manipulability score. We then train a multi-layer perceptron (MLP)
regressor to predict the optimal base pose based on these scores. Evaluation in
a simulated environment using the da Vinci Research Kit (dVRK) showed unique
base pose-score maps for four volunteers, highlighting the individuality of
working patterns. After conducting tests on the base poses identified using the
proposed method, we confirmed that they have a score approximately 28.2\%
higher than when the robots were placed randomly, with respect to the score we
defined. This emphasizes the need for operator-specific optimization in RAMIS
base placement.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16102" title="Abstract">arXiv:2402.16102</a> [<a href="/pdf/2402.16102" title="Download PDF">pdf</a>, <a href="/format/2402.16102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Predictive Probabilities: Model Confidence or Human Label  Variation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baan%2C+J">Joris Baan</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+W">Wilker Aziz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the rise of increasingly powerful and user-facing NLP systems, there is
growing interest in assessing whether they have a good representation of
uncertainty by evaluating the quality of their predictive distribution over
outcomes. We identify two main perspectives that drive starkly different
evaluation protocols. The first treats predictive probability as an indication
of model confidence; the second as an indication of human label variation. We
discuss their merits and limitations, and take the position that both are
crucial for trustworthy and fair NLP systems, but that exploiting a single
predictive distribution is limiting. We recommend tools and highlight exciting
directions towards models with disentangled representations of uncertainty
about predictions and uncertainty about human labels.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16105" title="Abstract">arXiv:2402.16105</a> [<a href="/pdf/2402.16105" title="Download PDF">pdf</a>, <a href="/format/2402.16105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informed Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobalczyk%2C+K">Katarzyna Kobalczyk</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In noisy and low-data regimes prevalent in real-world applications, an
outstanding challenge of machine learning lies in effectively incorporating
inductive biases that promote data efficiency and robustness. Meta-learning and
informed ML stand out as two approaches for incorporating prior knowledge into
the ML pipeline. While the former relies on a purely data-driven source of
priors, the latter is guided by a formal representation of expert knowledge.
This paper introduces a novel hybrid paradigm, informed meta-learning, seeking
complementarity in cross-task knowledge sharing of humans and machines. We
establish the foundational components of informed meta-learning and present a
concrete instantiation of this framework--the Informed Neural Process. Through
a series of illustrative and larger-scale experiments, we demonstrate the
potential benefits of informed meta-learning in improving data efficiency and
robustness to observational noise, task distribution shifts, and heterogeneity.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16107" title="Abstract">arXiv:2402.16107</a> [<a href="/pdf/2402.16107" title="Download PDF">pdf</a>, <a href="/format/2402.16107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseChat: Knowledge Fusion of Chat Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Longguang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While training large language models (LLMs) from scratch can indeed lead to
models with distinct capabilities and strengths, this approach incurs
substantial costs and may lead to potential redundancy in competencies. An
alternative strategy is to combine existing LLMs into a more robust LLM,
thereby diminishing the necessity for expensive pre-training. However, due to
the diverse architectures of LLMs, direct parameter blending proves to be
unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge
fusion to transfer the collective knowledge of multiple structurally varied
LLMs into a target LLM through lightweight continual training. In this report,
we extend the scalability and flexibility of the \textsc{FuseLLM} framework to
realize the fusion of chat LLMs, resulting in \textsc{FuseChat}.
\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge
fusion for structurally and scale-varied source LLMs to derive multiple target
LLMs of identical structure and size via lightweight fine-tuning. Then, these
target LLMs are merged within the parameter space, wherein we propose a novel
method for determining the merging weights based on the variation ratio of
parameter matrices before and after fine-tuning. We validate our approach using
three prominent chat LLMs with diverse architectures and scales, namely
\texttt{NH2-Mixtral-8x7B}, \texttt{NH2-Solar-10.7B}, and
\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains
demonstrate the superiority of \texttt{\textsc{FuseChat}-7B} across a broad
spectrum of chat LLMs at 7B and 34B scales, even surpassing \texttt{GPT-3.5
(March)} and approaching \texttt{Mixtral-8x7B-Instruct}. Our code, model
weights, and data are openly accessible at
\url{https://github.com/fanqiwan/FuseLLM}.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16110" title="Abstract">arXiv:2402.16110</a> [<a href="/pdf/2402.16110" title="Download PDF">pdf</a>, <a href="/format/2402.16110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Graph Variational Auto-Encoder for Multimodal  Recommendation with Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+C">Chunyan Miao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Multimodal recommender systems amalgamate multimodal information (e.g.,
textual descriptions, images) into a collaborative filtering framework to
provide more accurate recommendations. While the incorporation of multimodal
information could enhance the interpretability of these systems, current
multimodal models represent users and items utilizing entangled numerical
vectors, rendering them arduous to interpret. To address this, we propose a
Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both
model and recommendation interpretability. DGVAE initially projects multimodal
information into textual contents, such as converting images to text, by
harnessing state-of-the-art multimodal pre-training technologies. It then
constructs a frozen item-item graph and encodes the contents and interactions
into two sets of disentangled representations utilizing a simplified residual
graph convolutional network. DGVAE further regularizes these disentangled
representations through mutual information maximization, aligning the
representations derived from the interactions between users and items with
those learned from textual content. This alignment facilitates the
interpretation of user binary interactions via text. Our empirical analysis
conducted on three real-world datasets demonstrates that DGVAE significantly
surpasses the performance of state-of-the-art baselines by a margin of 10.02%.
We also furnish a case study from a real-world dataset to illustrate the
interpretability of DGVAE. Code is available at:
\url{https://github.com/enoche/DGVAE}.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16114" title="Abstract">arXiv:2402.16114</a> [<a href="/pdf/2402.16114" title="Download PDF">pdf</a>, <a href="/ps/2402.16114" title="Download PostScript">ps</a>, <a href="/format/2402.16114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based Manipulation of Deformable Objects with Non-negligible  Dynamics as Shape Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiburzio%2C+S">Sebastien Tiburzio</a> (1), 
<a href="/search/cs?searchtype=author&query=Coleman%2C+T">Tom&#xe1;s Coleman</a> (1), 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a> (1 and 2) ((1) Department of Cognitive Robotics, Delft University of Technology, Delft, The Netherlands, (2) Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Oberpfaffenhofen, Germany)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Model-based manipulation of deformable objects has traditionally dealt with
objects in the quasi-static regimes, either because they are extremely
lightweight/small or constrained to move very slowly. On the contrary, soft
robotic research has made considerable strides toward general modeling and
control - despite soft robots and deformable linear objects being very similar
from a mechanical standpoint. In this work, we leverage these recent results to
develop a fully dynamic framework of slender deformable objects grasped at one
of their ends by a robotic manipulator. We introduce a dynamic model of this
system using functional strain parameterizations and describe the manipulation
challenge as a regulation control problem. This enables us to define a fully
model-based control architecture, for which we can prove analytically
closed-loop stability and provide sufficient conditions for steady state
convergence to the desired manipulation state. The nature of this work is
intended to be markedly experimental. We propose an extensive experimental
validation of the proposed ideas. For that, we use a 7-DoF robot tasked with
the goal of positioning the distal end of six different electric cables, moving
on a plane, in a given position and orientation in space.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16116" title="Abstract">arXiv:2402.16116</a> [<a href="/pdf/2402.16116" title="Download PDF">pdf</a>, <a href="/format/2402.16116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Performance of RIS-Aided Fluid Antenna Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghadi%2C+F+R">Farshad Rostami Ghadi</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=New%2C+W+K">Wee Kiat New</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Murch%2C+R">Ross Murch</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yangyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This letter studies the performance of reconfigurable intelligent surface
(RIS)-aided communications for a fluid antenna system (FAS) enabled receiver.
Specifically, a fixed singleantenna base station (BS) transmits information
through a RIS to a mobile user (MU) which is equipped with a planar fluid
antenna in the absence of a direct link.We first analyze the spatial
correlation structures among the positions (or ports) in the planar FAS, and
then derive the joint distribution of the equivalent channel gain at the user
by exploiting the central limit theorem. Furthermore, we obtain compact
analytical expressions for the outage probability (OP) and delay outage rate
(DOR). Numerical results illustrate that using FAS with only one activated port
into the RIS-aided communication network can greatly enhance the performance,
when compared to traditional antenna systems (TAS).
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16117" title="Abstract">arXiv:2402.16117</a> [<a href="/pdf/2402.16117" title="Download PDF">pdf</a>, <a href="/format/2402.16117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shoufa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiaojun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Chongjian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Runjian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhixuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengkang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chaofan Tao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peize Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haibao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jifeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Robotic behavior synthesis, the problem of understanding multimodal inputs
and generating precise physical control for robots, is an important part of
Embodied AI. Despite successes in applying multimodal large language models for
high-level understanding, it remains challenging to translate these conceptual
understandings into detailed robotic actions while achieving generalization
across various scenarios. In this paper, we propose a tree-structured
multimodal code generation framework for generalized robotic behavior
synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions
into multiple object-centric manipulation units consisting of physical
preferences such as affordance and safety constraints, and applies code
generation to introduce generalization ability across various robotics
platforms. To further enhance the capability to map conceptual and perceptual
understanding into control commands, a specialized multimodal reasoning dataset
is collected for pre-training and an iterative self-updating methodology is
introduced for supervised fine-tuning. Extensive experiments demonstrate that
RoboCodeX achieves state-of-the-art performance in both simulators and real
robots on four different kinds of manipulation tasks and one navigation task.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16119" title="Abstract">arXiv:2402.16119</a> [<a href="/pdf/2402.16119" title="Download PDF">pdf</a>, <a href="/format/2402.16119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepForge: Leveraging AI for Microstructural Control in Metal Forming  via Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrik%2C+J">Jan Petrik</a>, 
<a href="/search/cs?searchtype=author&query=Bambach%2C+M">Markus Bambach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This study presents a novel method for microstructure control in closed die
hot forging that combines Model Predictive Control (MPC) with a developed
machine learning model called DeepForge. DeepForge uses an architecture that
combines 1D convolutional neural networks and gated recurrent units. It uses
surface temperature measurements of a workpiece as input to predict
microstructure changes during forging. The paper also details DeepForge's
architecture and the finite element simulation model used to generate the data
set, using a three-stroke forging process. The results demonstrate DeepForge's
ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%.
In addition, the study explores the use of MPC to adjust inter-stroke wait
times, effectively counteracting temperature disturbances to achieve a target
grain size of less than 35 microns within a specific 2D region of the
workpiece. These results are then verified experimentally, demonstrating a
significant step towards improved control and quality in forging processes
where temperature can be used as an additional degree of freedom in the
process.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16121" title="Abstract">arXiv:2402.16121</a> [<a href="/pdf/2402.16121" title="Download PDF">pdf</a>, <a href="/format/2402.16121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Accurate Post-training Quantization for Reparameterized Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Luoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+W">Wen Fei</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zhenyu Lou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+Y">YangWei Ying</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model reparameterization is a widely accepted technique for improving
inference speed without compromising performance. However, current
Post-training Quantization (PTQ) methods often lead to significant accuracy
degradation when applied to reparameterized models. This is primarily caused by
channel-specific and sample-specific outliers, which appear only at specific
samples and channels and impact on the selection of quantization parameters. To
address this issue, we propose RepAPQ, a novel framework that preserves the
accuracy of quantized reparameterization models. Different from previous
frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean
Absolute Error (MAE) to mitigate the influence of outliers on quantization
parameters. Our framework comprises two main components: Quantization
Protecting Reparameterization and Across-block Calibration. For effective
calibration, Quantization Protecting Reparameterization combines multiple
branches into a single convolution with an affine layer. During training, the
affine layer accelerates convergence and amplifies the output of the
convolution to better accommodate samples with outliers. Additionally,
Across-block Calibration leverages the measurement of stage output as
supervision to address the gradient problem introduced by MAE and enhance the
interlayer correlation with quantization parameters. Comprehensive experiments
demonstrate the effectiveness of RepAPQ across various models and tasks. Our
framework outperforms previous methods by approximately 1\% for 8-bit PTQ and
2\% for 6-bit PTQ, showcasing its superior performance. The code is available
at \url{https://github.com/ilur98/DLMC-QUANT}.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16123" title="Abstract">arXiv:2402.16123</a> [<a href="/pdf/2402.16123" title="Download PDF">pdf</a>, <a href="/format/2402.16123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructEdit: Instruction-based Knowledge Editing for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Bozhong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaozhuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+K">Kouying Xue</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Y">Yanjie Gou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; the project website is at <a href="https://www.zjukg.org/project/InstructEdit/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge editing for large language models can offer an efficient solution
to alter a model's behavior without negatively impacting the overall
performance. However, the current approach encounters issues with limited
generalizability across tasks, necessitating one distinct editor for each task,
which significantly hinders the broader applications. To address this, we take
the first step to analyze the multi-task generalization issue in knowledge
editing. Specifically, we develop an instruction-based editing technique,
termed InstructEdit, which facilitates the editor's adaptation to various task
performances simultaneously using simple instructions. With only one unified
editor for each LLM, we empirically demonstrate that InstructEdit can improve
the editor's control, leading to an average 14.86% increase in Reliability in
multi-task editing setting. Furthermore, experiments involving holdout unseen
task illustrate that InstructEdit consistently surpass previous strong
baselines. To further investigate the underlying mechanisms of
instruction-based knowledge editing, we analyze the principal components of the
editing gradient directions, which unveils that instructions can help control
optimization direction with stronger OOD generalization. Code and datasets will
be available in https://github.com/zjunlp/EasyEdit.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16124" title="Abstract">arXiv:2402.16124</a> [<a href="/pdf/2402.16124" title="Download PDF">pdf</a>, <a href="/format/2402.16124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D  Talking Face Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yasheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+W">Wenqing Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaisiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koike%2C+H">Hideki Koike</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While considerable progress has been made in achieving accurate lip
synchronization for 3D speech-driven talking face generation, the task of
incorporating expressive facial detail synthesis aligned with the speaker's
speaking status remains challenging. Our goal is to directly leverage the
inherent style information conveyed by human speech for generating an
expressive talking face that aligns with the speaking status. In this paper, we
propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking
face generation. This system harnesses the robust contextual reasoning and
hallucination capability offered by Large Language Models (LLMs) to instruct
the realistic synthesis of 3D talking faces. Instead of directly learning
facial movements from human speech, our two-stage strategy involves the LLMs
first comprehending audio information and generating instructions implying
expressive facial details seamlessly corresponding to the speech. Subsequently,
a diffusion-based generative network executes these instructions. This
two-stage process, coupled with the incorporation of LLMs, enhances model
interpretability and provides users with flexibility to comprehend instructions
and specify desired operations or modifications. Extensive experiments showcase
the effectiveness of our approach in producing vivid talking faces with
expressive facial movements and consistent emotional status.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16126" title="Abstract">arXiv:2402.16126</a> [<a href="/pdf/2402.16126" title="Download PDF">pdf</a>, <a href="/format/2402.16126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A statistical method for crack detection in 3D concrete images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makogin%2C+V">Vitalii Makogin</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Spodarev%2C+E">Evgeny Spodarev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Applications (stat.AP)

</div>
<p class="mathjax">In practical applications, effectively segmenting cracks in large-scale
computed tomography (CT) images holds significant importance for understanding
the structural integrity of materials. However, classical methods and Machine
Learning algorithms often incur high computational costs when dealing with the
substantial size of input images. Hence, a robust algorithm is needed to
pre-detect crack regions, enabling focused analysis and reducing computational
overhead. The proposed approach addresses this challenge by offering a
streamlined method for identifying crack regions in CT images with high
probability. By efficiently identifying areas of interest, our algorithm allows
for a more focused examination of potential anomalies within the material
structure. Through comprehensive testing on both semi-synthetic and real 3D CT
images, we validate the efficiency of our approach in enhancing crack
segmentation while reducing computational resource requirements.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16131" title="Abstract">arXiv:2402.16131</a> [<a href="/pdf/2402.16131" title="Download PDF">pdf</a>, <a href="/format/2402.16131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A VAE-based Framework for Learning Multi-Level Neural Granger-Causal  Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiahe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Huitian Lei</a>, 
<a href="/search/cs?searchtype=author&query=Michailidis%2C+G">George Michailidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Transactions on Machine Learning Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Granger causality has been widely used in various application domains to
capture lead-lag relationships amongst the components of complex dynamical
systems, and the focus in extant literature has been on a single dynamical
system. In certain applications in macroeconomics and neuroscience, one has
access to data from a collection of related such systems, wherein the modeling
task of interest is to extract the shared common structure that is embedded
across them, as well as to identify the idiosyncrasies within individual ones.
This paper introduces a Variational Autoencoder (VAE) based framework that
jointly learns Granger-causal relationships amongst components in a collection
of related-yet-heterogeneous dynamical systems, and handles the aforementioned
task in a principled way. The performance of the proposed framework is
evaluated on several synthetic data settings and benchmarked against existing
approaches designed for individual system learning. The method is further
illustrated on a real dataset involving time series data from a
neurophysiological experiment and produces interpretable results.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16132" title="Abstract">arXiv:2402.16132</a> [<a href="/pdf/2402.16132" title="Download PDF">pdf</a>, <a href="/format/2402.16132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by  Long-Short-Term Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+H">Harshavardhan Kamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 3 tables, 2 page references, 2 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time-series forecasting (TSF) finds broad applications in real-world
scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates
strong zero-shot TSF capabilities while preserving computational efficiency.
However, existing prompting methods oversimplify TSF as language next-token
predictions, overlooking its dynamic nature and lack of integration with
state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose
LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks.
LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks,
tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess
forecasting mechanisms to enhance adaptability. Extensive evaluations
demonstrate consistently better performance of LSTPrompt than existing
prompting methods, and competitive results compared to foundation TSF models.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16139" title="Abstract">arXiv:2402.16139</a> [<a href="/pdf/2402.16139" title="Download PDF">pdf</a>, <a href="/format/2402.16139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Generative Artificial Intelligence Means for Terminological  Definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn%2C+A+S">Antonio San Mart&#xed;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper examines the impact of Generative Artificial Intelligence (GenAI)
on the creation and consumption of terminological definitions. GenAI tools like
ChatGPT present a mix of benefits and drawbacks compared to traditional
terminological resources. ChatGPT excels in providing context-specific meanings
in an interactive and customized fashion but faces challenges with accuracy.
Terminological definitions in recognized resources will likely survive because
of their reliability. From the point of view of the terminologist, tools like
ChatGPT enable AI-assisted terminography, including post-editing terminography,
as an approach blending AI efficiency with human expertise for faster
definition creation.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16140" title="Abstract">arXiv:2402.16140</a> [<a href="/pdf/2402.16140" title="Download PDF">pdf</a>, <a href="/ps/2402.16140" title="Download PostScript">ps</a>, <a href="/format/2402.16140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-access Distributed Computing Models from Map-Reduce Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sasi%2C+S">Shanuja Sasi</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnl%C3%BC%2C+O">Onur G&#xfc;nl&#xfc;</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+B+S">B. Sundar Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A novel distributed computing model called "Multi-access Distributed
Computing (MADC)" was recently introduced in <a href="http://www.<a href="/abs/2206.12851">arXiv:2206.12851</a>.">this http URL</a> In
this paper, we represent MADC models via 2-layered bipartite graphs called
Map-Reduce Graphs (MRGs) and a set of arrays called Map-Reduce Arrays (MRAs)
inspired from the Placement Delivery Arrays (PDAs) used in the coded caching
literature. The connection between MRAs and MRGs is established, thereby
exploring new topologies and providing coded shuffling schemes for the MADC
models with MRGs using the structure of MRAs. A novel \textit{Nearest Neighbor
Connect-MRG (NNC-MRG)} is explored and a coding scheme is provided for MADC
models with NNC-MRG, exploiting the connections between MRAs and PDAs.
Moreover, CT is generalized to Generalized Combinatorial-MRG (GC-MRG). A set of
$g-$regular MRAs is provided which corresponds to the existing scheme for MADC
models with CT and extended those to generate another set of MRAs to represent
MADC models with GC-MRG. A lower bound on the computation-communication curve
for MADC model with GC-MRG under homogeneous setting is derived and certain
cases are explored where the existing scheme is optimal under CT. One of the
major limitations of the existing scheme for CT is that it requires an
exponentially large number of reducer nodes and input files for large
$\Lambda$. This can be overcome by representing CT by MRAs, where coding
schemes can be derived even if some of the reducer nodes are not present.
Another way of tackling this is by using a different MRG, specifically NNC-MRG,
where the number of reducer nodes and files required are significantly smaller
compared to CT. Hence, the advantages are two-fold, which is achievable at the
expense of a slight increase in the communication load.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16141" title="Abstract">arXiv:2402.16141</a> [<a href="/pdf/2402.16141" title="Download PDF">pdf</a>, <a href="/format/2402.16141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiangdi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Damai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weiyao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shaoxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Supervised fine-tuning is the most common method to adapt large language
models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive
computational resources. Recently, parameter-efficient fine-tuning (PEFT)
methods have been widely studied due to its cost-effectiveness. LoRA is one of
the most widely used methods, which assumes that the optimization process is
essentially low-dimensional. Although LoRA fine-tuning is effective, there is
still a performance gap compared to full fine-tuning, since its weight update
is limited to low-rank matrices. In order to break the low-rank bottleneck in
LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank
update matrices multiple times to achieve a higher update rank. PLoRA has
multiple training stages. During each stage, we still update only the LoRA
weights. However, at the end of each stage, we unload the LoRA weights into the
backbone parameters and then reinitialize the LoRA states. Experimental results
show that PLoRA has stronger learning ability, approximately 1.8 times that of
LoRA's learning ability at most, but it does not increase memory usage.
Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate
the training instability.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16142" title="Abstract">arXiv:2402.16142</a> [<a href="/pdf/2402.16142" title="Download PDF">pdf</a>, <a href="/ps/2402.16142" title="Download PostScript">ps</a>, <a href="/format/2402.16142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Transformation: A Comprehensive Review of Large Language  Models&#x27; Versatility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaur%2C+P">Pravneet Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+G+S">Gautam Siddharth Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ankit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Nafis%2C+M+T">Md Tabrez Nafis</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sandeep Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shokeen%2C+V">Vikrant Shokeen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This groundbreaking study explores the expanse of Large Language Models
(LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional
Encoder Representations from Transformers (BERT) across varied domains ranging
from technology, finance, healthcare to education. Despite their established
prowess in Natural Language Processing (NLP), these LLMs have not been
systematically examined for their impact on domains such as fitness, and
holistic well-being, urban planning, climate modelling as well as disaster
management. This review paper, in addition to furnishing a comprehensive
analysis of the vast expanse and extent of LLMs' utility in diverse domains,
recognizes the research gaps and realms where the potential of LLMs is yet to
be harnessed. This study uncovers innovative ways in which LLMs can leave a
mark in the fields like fitness and wellbeing, urban planning, climate
modelling and disaster response which could inspire future researches and
applications in the said avenues.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16143" title="Abstract">arXiv:2402.16143</a> [<a href="/pdf/2402.16143" title="Download PDF">pdf</a>, <a href="/format/2402.16143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cinematographic Camera Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hongda Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Christie%2C+M">Marc Christie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Designing effective camera trajectories in virtual 3D environments is a
challenging task even for experienced animators. Despite an elaborate film
grammar, forged through years of experience, that enables the specification of
camera motions through cinematographic properties (framing, shots sizes,
angles, motions), there are endless possibilities in deciding how to place and
move cameras with characters. Dealing with these possibilities is part of the
complexity of the problem. While numerous techniques have been proposed in the
literature (optimization-based solving, encoding of empirical rules, learning
from real examples,...), the results either lack variety or ease of control.
<br />In this paper, we propose a cinematographic camera diffusion model using a
transformer-based architecture to handle temporality and exploit the
stochasticity of diffusion models to generate diverse and qualitative
trajectories conditioned by high-level textual descriptions. We extend the work
by integrating keyframing constraints and the ability to blend naturally
between motions using latent interpolation, in a way to augment the degree of
control of the designers. We demonstrate the strengths of this text-to-camera
motion approach through qualitative and quantitative experiments and gather
feedback from professional artists. The code and data are available at
\URL{https://github.com/jianghd1996/Camera-control}.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16144" title="Abstract">arXiv:2402.16144</a> [<a href="/pdf/2402.16144" title="Download PDF">pdf</a>, <a href="/ps/2402.16144" title="Download PostScript">ps</a>, <a href="/format/2402.16144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 100 Gbps Indoor Access and 4.8 Gbps Outdoor Point-to-Point LiFi  Transmission Systems using Laser-based Light Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cheng%2C+C">Cheng Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Das%2C+S">Sovan Das</a>, 
<a href="/search/eess?searchtype=author&query=Videv%2C+S">Stefan Videv</a>, 
<a href="/search/eess?searchtype=author&query=Spark%2C+A">Adrian Spark</a>, 
<a href="/search/eess?searchtype=author&query=Babadi%2C+S">Sina Babadi</a>, 
<a href="/search/eess?searchtype=author&query=Krishnamoorthy%2C+A">Aravindh Krishnamoorthy</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+C">Changmin Lee</a>, 
<a href="/search/eess?searchtype=author&query=Grieder%2C+D">Daniel Grieder</a>, 
<a href="/search/eess?searchtype=author&query=Hartnett%2C+K">Kathleen Hartnett</a>, 
<a href="/search/eess?searchtype=author&query=Rudy%2C+P">Paul Rudy</a>, 
<a href="/search/eess?searchtype=author&query=Raring%2C+J">James Raring</a>, 
<a href="/search/eess?searchtype=author&query=Najafi%2C+M">Marzieh Najafi</a>, 
<a href="/search/eess?searchtype=author&query=Papanikolaou%2C+V+K">Vasilis K. Papanikolaou</a>, 
<a href="/search/eess?searchtype=author&query=Schober%2C+R">Robert Schober</a>, 
<a href="/search/eess?searchtype=author&query=Haas%2C+H">Harald Haas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optics (physics.optics)

</div>
<p class="mathjax">In this paper, we demonstrate the communication capabilities of
light-fidelity (LiFi) systems based on highbrightness and high-bandwidth
integrated laser-based sources in a surface mount device (SMD) packaging
platform. The laserbased source is able to deliver 450 lumens of white light
illumination and the resultant light brightness is over 1000 cd mm2. It is
demonstrated that a wavelength division multiplexing (WDM) LiFi system with ten
parallel channels is able to deliver over 100 Gbps data rate with the
assistance of Volterra filter-based nonlinear equalisers. In addition, an
aggregated transmission data rate of 4.8 Gbps has been achieved over a link
distance of 500 m with the same type of SMD light source. This work
demonstrates the scalability of LiFi systems that employ laserbased light
sources, particularly in their capacity to enable highspeed short range, as
well as long-range data transmission.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16145" title="Abstract">arXiv:2402.16145</a> [<a href="/pdf/2402.16145" title="Download PDF">pdf</a>, <a href="/ps/2402.16145" title="Download PostScript">ps</a>, <a href="/format/2402.16145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Egalitarian Price of Fairness for Indivisible Goods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Celine%2C+K+F">Karen Frilya Celine</a> (1), 
<a href="/search/cs?searchtype=author&query=Dzulfikar%2C+M+A">Muhammad Ayaz Dzulfikar</a> (1), 
<a href="/search/cs?searchtype=author&query=Koswara%2C+I+A">Ivan Adrian Koswara</a> (1) ((1) National University of Singapore)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A shorter version appears in the 20th Pacific Rim International Conference on Artificial Intelligence (PRICAI), 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 20th PRICAI 1 (2023) 23-28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In the context of fair division, the concept of price of fairness has been
introduced to quantify the loss of welfare when we have to satisfy some
fairness condition. In other words, it is the price we have to pay to guarantee
fairness. Various settings of fair division have been considered previously; we
extend to the setting of indivisible goods by using egalitarian welfare as the
welfare measure, instead of the commonly used utilitarian welfare. We provide
lower and upper bounds for various fairness and efficiency conditions such as
envy-freeness up to one good (EF1) and maximum Nash welfare (MNW).
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16150" title="Abstract">arXiv:2402.16150</a> [<a href="/pdf/2402.16150" title="Download PDF">pdf</a>, <a href="/format/2402.16150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective MSO-Definability for Tree-width Bounded Models of an Inductive  Separation Logic of Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bueri%2C+L">Lucas Bueri</a>, 
<a href="/search/cs?searchtype=author&query=Iosif%2C+R">Radu Iosif</a>, 
<a href="/search/cs?searchtype=author&query=Zuleger%2C+F">Florian Zuleger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">A class of graph languages is definable in Monadic Second-Order logic (MSO)
if and only if it consists of sets of models of MSO formul{\ae}. If, moreover,
there is a computable bound on the tree-widths of the graphs in each such set,
the satisfiability and entailment problems are decidable, by Courcelle's
Theorem. This motivates the comparison of other graph logics to MSO. In this
paper, we consider the MSO definability of a Separation Logic of Relations
(SLR) that describes simple hyper-graphs, in which each sequence of vertices is
attached to at most one edge with a given label. Our logic SLR uses inductive
predicates whose recursive definitions consist of existentially quantified
separated conjunctions of relation and predicate atoms. The main contribution
of this paper is an expressive fragment of SLR that describes bounded
tree-width sets of graphs which can, moreover, be effectively translated into
MSO.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16153" title="Abstract">arXiv:2402.16153</a> [<a href="/pdf/2402.16153" title="Download PDF">pdf</a>, <a href="/format/2402.16153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatMusician: Understanding and Generating Music Intrinsically with LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruibin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hanfeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zeyue Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziya Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yizhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yinghao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yiming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+X">Xiaowei Chi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingcheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qifeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Benetos%2C+E">Emmanouil Benetos</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Dannenberg%2C+R">Roger Dannenberg</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Shiyin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub: <a href="https://shanghaicannon.github.io/ChatMusician/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">While Large Language Models (LLMs) demonstrate impressive capabilities in
text generation, we find that their ability has yet to be generalized to music,
humanity's creative language. We introduce ChatMusician, an open-source LLM
that integrates intrinsic musical abilities. It is based on continual
pre-training and finetuning LLaMA2 on a text-compatible music representation,
ABC notation, and the music is treated as a second language. ChatMusician can
understand and generate music with a pure text tokenizer without any external
multi-modal neural structures or tokenizers. Interestingly, endowing musical
abilities does not harm language abilities, even achieving a slightly higher
MMLU score. Our model is capable of composing well-structured, full-length
music, conditioned on texts, chords, melodies, motifs, musical forms, etc,
surpassing GPT-4 baseline. On our meticulously curated college-level music
understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and
GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs
can be an excellent compressor for music, but there remains significant
territory to be conquered. We release our 4B token music-language corpora
MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16154" title="Abstract">arXiv:2402.16154</a> [<a href="/pdf/2402.16154" title="Download PDF">pdf</a>, <a href="/format/2402.16154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yeping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sifferman%2C+C">Carter Sifferman</a>, 
<a href="/search/cs?searchtype=author&query=Gleicher%2C+M">Michael Gleicher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a contributed paper at the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Many applications require a robot to accurately track reference end-effector
trajectories. Certain trajectories may not be tracked as single, continuous
paths due to the robot's kinematic constraints or obstacles elsewhere in the
environment. In this situation, it becomes necessary to divide the trajectory
into shorter segments. Each such division introduces a reconfiguration, in
which the robot deviates from the reference trajectory, repositions itself in
configuration space, and then resumes task execution. The occurrence of
reconfigurations should be minimized because they increase the time and energy
usage. In this paper, we present IKLink, a method for finding joint motions to
track reference end-effector trajectories while executing minimal
reconfigurations. Our graph-based method generates a diverse set of Inverse
Kinematics (IK) solutions for every waypoint on the reference trajectory and
utilizes a dynamic programming algorithm to find the globally optimal motion by
linking the IK solutions. We demonstrate the effectiveness of IKLink through a
simulation experiment and an illustrative demonstration using a physical robot.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16157" title="Abstract">arXiv:2402.16157</a> [<a href="/pdf/2402.16157" title="Download PDF">pdf</a>, <a href="/format/2402.16157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus learning: A novel decentralised ensemble learning paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magureanu%2C+H">Horia Magureanu</a>, 
<a href="/search/cs?searchtype=author&query=Usher%2C+N">Na&#xef;ri Usher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages plus appendices, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The widespread adoption of large-scale machine learning models in recent
years highlights the need for distributed computing for efficiency and
scalability. This work introduces a novel distributed machine learning paradigm
-- \emph{consensus learning} -- which combines classical ensemble methods with
consensus protocols deployed in peer-to-peer systems. These algorithms consist
of two phases: first, participants develop their models and submit predictions
for any new data inputs; second, the individual predictions are used as inputs
for a communication phase, which is governed by a consensus protocol. Consensus
learning ensures user data privacy, while also inheriting the safety measures
against Byzantine attacks from the underlying consensus mechanism. We provide a
detailed theoretical analysis for a particular consensus protocol and compare
the performance of the consensus learning ensemble with centralised ensemble
learning algorithms. The discussion is supplemented by various numerical
simulations, which describe the robustness of the algorithms against Byzantine
participants.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16159" title="Abstract">arXiv:2402.16159</a> [<a href="/pdf/2402.16159" title="Download PDF">pdf</a>, <a href="/format/2402.16159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistALANER: Distantly Supervised Active Learning Augmented Named Entity  Recognition in the Open Source Software Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somnath Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Avik Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aaditya Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Hazra%2C+R">Rima Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper proposes a novel named entity recognition (NER) technique
specifically tailored for the open-source software systems. Our approach aims
to address the scarcity of annotated software data by employing a comprehensive
two-step distantly supervised annotation process. This process strategically
leverages language heuristics, unique lookup tables, external knowledge
sources, and an active learning approach. By harnessing these powerful
techniques, we not only enhance model performance but also effectively mitigate
the limitations associated with cost and the scarcity of expert annotators. It
is noteworthy that our framework significantly outperforms the state-of-the-art
LLMs by a substantial margin. We also show the effectiveness of NER in the
downstream task of relation extraction.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16162" title="Abstract">arXiv:2402.16162</a> [<a href="/pdf/2402.16162" title="Download PDF">pdf</a>, <a href="/format/2402.16162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Catch Me If You Can: Combatting Fraud in Artificial Currency Based  Government Benefits Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jalota%2C+D">Devansh Jalota</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+M">Matthew Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Artificial currencies have grown in popularity in many real-world resource
allocation settings, gaining traction in government benefits programs like food
assistance and transit benefits programs. However, such programs are
susceptible to misreporting fraud, wherein users can misreport their private
attributes to gain access to more artificial currency (credits) than they are
entitled to. To address the problem of misreporting fraud in artificial
currency based benefits programs, we introduce an audit mechanism that induces
a two-stage game between an administrator and users. In our proposed mechanism,
the administrator running the benefits program can audit users at some cost and
levy fines against them for misreporting their information. For this audit
game, we study the natural solution concept of a signaling game equilibrium and
investigate conditions on the administrator budget to establish the existence
of equilibria. The computation of equilibria can be done via linear programming
in our problem setting through an appropriate design of the audit rules. Our
analysis also provides upper bounds that hold in any signaling game equilibrium
on the expected excess payments made by the administrator and the probability
that users misreport their information. We further show that the decrease in
misreporting fraud corresponding to our audit mechanism far outweighs the
administrator spending to run it by establishing that its total costs are lower
than that of the status quo with no audits. Finally, to highlight the practical
viability of our audit mechanism in mitigating misreporting fraud, we present a
case study based on the Washington D.C. federal transit benefits program. In
this case study, the proposed audit mechanism achieves several orders of
magnitude improvement in total cost compared to a no-audit strategy for some
parameter ranges.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16164" title="Abstract">arXiv:2402.16164</a> [<a href="/pdf/2402.16164" title="Download PDF">pdf</a>, <a href="/format/2402.16164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Specific Pretraining with Noisy Labels for Remote sensing Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+C">Conrad Albrecht</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, self-supervision has drawn a lot of attention in remote
sensing society due to its ability to reduce the demand of exact labels in
supervised deep learning model training. Self-supervision methods generally
utilize image-level information to pretrain models in an unsupervised fashion.
Though these pretrained encoders show effectiveness in many downstream tasks,
their performance on segmentation tasks is often not as good as that on
classification tasks. On the other hand, many easily available label sources
(e.g., automatic labeling tools and land cover land use products) exist, which
can provide a large amount of noisy labels for segmentation model training. In
this work, we propose to explore the under-exploited potential of noisy labels
for segmentation task specific pretraining, and exam its robustness when
confronted with mismatched categories and different decoders during
fine-tuning. Specifically, we inspect the impacts of noisy labels on different
layers in supervised model training to serve as the basis of our work.
Experiments on two datasets indicate the effectiveness of task specific
supervised pretraining with noisy labels. The findings are expected to shed
light on new avenues for improving the accuracy and versatility of pretraining
strategies for remote sensing image segmentation.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16165" title="Abstract">arXiv:2402.16165</a> [<a href="/pdf/2402.16165" title="Download PDF">pdf</a>, <a href="/ps/2402.16165" title="Download PostScript">ps</a>, <a href="/format/2402.16165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Feasibility of Deep Learning Classification from Raw Signal Data  in Radiology, Ultrasonography and Electrophysiology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Enyedi%2C+S">Szilard Enyedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 1 table, 55 references, submitted to AQTR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Neural and Evolutionary Computing (cs.NE); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Medical imaging is a very useful tool in healthcare, various technologies
being employed to non-invasively peek inside the human body. Deep learning with
neural networks in radiology was welcome - albeit cautiously - by the
radiologist community. Most of the currently deployed or researched deep
learning solutions are applied on already generated images of medical scans,
use the neural networks to aid in the generation of such images, or use them
for identifying specific substance markers in spectrographs. This paper's
author posits that if the neural networks were trained directly on the raw
signals from the scanning machines, they would gain access to more nuanced
information than from the already processed images, hence the training - and
later, the inferences - would become more accurate. The paper presents the main
current applications of deep learning in radiography, ultrasonography, and
electrophysiology, and discusses whether the proposed neural network training
directly on raw signals is feasible.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16168" title="Abstract">arXiv:2402.16168</a> [<a href="/pdf/2402.16168" title="Download PDF">pdf</a>, <a href="/format/2402.16168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hitting &quot;Probe&quot;rty with Non-Linearity, and More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Avik Pal</a>, 
<a href="/search/cs?searchtype=author&query=Pawar%2C+M">Madhura Pawar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Structural probes learn a linear transformation to find how dependency trees
are embedded in the hidden states of language models. This simple design may
not allow for full exploitation of the structure of the encoded information.
Hence, to investigate the structure of the encoded information to its full
extent, we incorporate non-linear structural probes. We reformulate the design
of non-linear structural probes introduced by White et al. making its design
simpler yet effective. We also design a visualization framework that lets us
qualitatively assess how strongly two words in a sentence are connected in the
predicted dependency tree. We use this technique to understand which non-linear
probe variant is good at encoding syntactical information. Additionally, we
also use it to qualitatively investigate the structure of dependency trees that
BERT encodes in each of its layers. We find that the radial basis function
(RBF) is an effective non-linear probe for the BERT model than the linear
probe.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16170" title="Abstract">arXiv:2402.16170</a> [<a href="/pdf/2402.16170" title="Download PDF">pdf</a>, <a href="/ps/2402.16170" title="Download PostScript">ps</a>, <a href="/format/2402.16170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Steady-state Learning for Robust Output Regulation of  Nonlinear Output Feedback Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shimin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Guay%2C+M">Martin Guay</a>, 
<a href="/search/eess?searchtype=author&query=Braatz%2C+R+D">Richard D. Braatz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This article addresses the nonadaptive and robust output regulation problem
of the general nonlinear output feedback system with error output. The global
robust output regulation problem for a class of general output feedback
nonlinear systems with an uncertain exosystem and high relative degree can be
tackled by constructing a linear generic internal model provided that a
continuous nonlinear mapping exists. Leveraging the presented nonadaptive
framework facilitates the conversion of the nonlinear robust output regulation
problem into a robust nonadaptive stabilization endeavour for the augmented
system endowed with Input-to-State Stable dynamics, removing the need for
constructing a specific Lyapunov function with positive semidefinite
derivatives. To ensure the feasibility of the nonlinear mapping, the approach
is extended by incorporating the nonparametric learning framework. Moreover,
the introduced nonparametric learning framework provides the ability to learn
the dynamics of the steady-state/input behaviour from the signal generated from
the internal model only using the output error feedback. As a result, the
nonadaptive/nonparametric approach can be advantageous by guaranteeing
convergence of the estimation and tracking error even when the underlying
controlled system dynamics are complex or poorly understood. The effectiveness
of the theoretical results is illustrated for a controlled duffing system and a
continuously stirred tank reactor
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16171" title="Abstract">arXiv:2402.16171</a> [<a href="/pdf/2402.16171" title="Download PDF">pdf</a>, <a href="/ps/2402.16171" title="Download PostScript">ps</a>, <a href="/format/2402.16171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to avoid the commuting conversions of IPC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santo%2C+J+E">Jos&#xe9; Esp&#xed;rito Santo</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+G">Gilda Ferreira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">Since the observation in 2006 that it is possible to embed IPC into the
atomic polymorphic lambda-calculus (a predicative fragment of system F with
universal instantiations restricted to atomic formulas) different such
embeddings appeared in the literature. All of them comprise the Russell-Prawitz
translation of formulas, but have different strategies for the translation of
proofs. Although these embeddings preserve proof identity, all fail in
delivering preservation of reduction steps. In fact, they translate the
commuting conversions of IPC to beta-equality, or to other kinds of reduction
or equality generated by new principles added to system F. The cause for this
is the generation of redexes by the translation itself. In this paper, we
present an embedding of IPC into atomic system F, still based on the same
translation of formulas, but which maps commuting conversions to syntactic
identity, while simulating the other kinds of reduction steps present in IPC
beta\eta-reduction. In this sense the translation achieves a truly commuting
conversion-free image of IPC in atomic system F.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16173" title="Abstract">arXiv:2402.16173</a> [<a href="/pdf/2402.16173" title="Download PDF">pdf</a>, <a href="/ps/2402.16173" title="Download PostScript">ps</a>, <a href="/format/2402.16173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication Traffic Characteristics Reveal an IoT Devices Identity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+R+R">Rajarshi Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Debashish Roy</a>, 
<a href="/search/cs?searchtype=author&query=Abas%2C+P+E">Pg Emeroylariffion Abas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Internet of Things (IoT) is one of the technological advancements of the
twenty-first century which can improve living standards. However, it also
imposes new types of security challenges, including device authentication,
traffic types classification, and malicious traffic identification, in the
network domain. Traditionally, internet protocol (IP) and media access control
(MAC) addresses are utilized for identifying network-connected devices in a
network, whilst these addressing schemes are prone to be compromised, including
spoofing attacks and MAC randomization. Therefore, device identification using
only explicit identifiers is a challenging task. Accurate device identification
plays a key role in securing a network. In this paper, a supervised machine
learning-based device fingerprinting (DFP) model has been proposed for
identifying network-connected IoT devices using only communication traffic
characteristics (or implicit identifiers). A single transmission control
protocol/internet protocol (TCP/IP) packet header features have been utilized
for generating unique fingerprints, with the fingerprints represented as a
vector of 22 features. Experimental results have shown that the proposed DFP
method achieves over 98% in classifying individual IoT devices using the UNSW
dataset with 22 smart-home IoT devices. This signifies that the proposed
approach is invaluable to network operators in making their networks more
secure.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16174" title="Abstract">arXiv:2402.16174</a> [<a href="/pdf/2402.16174" title="Download PDF">pdf</a>, <a href="/format/2402.16174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Quanyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianfan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiangmiao Pang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">While recent advances in neural radiance field enable realistic digitization
for large-scale scenes, the image-capturing process is still time-consuming and
labor-intensive. Previous works attempt to automate this process using the
Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing
NBV policies heavily rely on hand-crafted criteria, limited action space, or
per-scene optimized representations. These constraints limit their
cross-dataset generalizability. To overcome them, we propose GenNBV, an
end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning
(RL)-based framework and extends typical limited action space to 5D free space.
It empowers our agent drone to scan from any viewpoint, and even interact with
unseen geometries during training. To boost the cross-dataset generalizability,
we also propose a novel multi-source state embedding, including geometric,
semantic, and action representations. We establish a benchmark using the Isaac
Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV
policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12%
coverage ratio on unseen building-scale objects from these datasets,
respectively, outperforming prior solutions.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16175" title="Abstract">arXiv:2402.16175</a> [<a href="/pdf/2402.16175" title="Download PDF">pdf</a>, <a href="/ps/2402.16175" title="Download PostScript">ps</a>, <a href="/format/2402.16175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI-based gait analysis of patients walking with Knee-Ankle-Foot  orthosis using video cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Arnav Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Shetkar%2C+A">Aditi Shetkar</a>, 
<a href="/search/cs?searchtype=author&query=Bapat%2C+G+M">Ganesh M. Bapat</a>, 
<a href="/search/cs?searchtype=author&query=Ojha%2C+R">Rajdeep Ojha</a>, 
<a href="/search/cs?searchtype=author&query=Verlekar%2C+T+T">Tanmay Tulsidas Verlekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages 6 figures 3 tables Dataset Link : <a href="http://tinyurl.com/5ds5f33c">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent technological advancements in artificial intelligence and computer
vision have enabled gait analysis on portable devices such as cell phones.
However, most state-of-the-art vision-based systems still impose numerous
constraints for capturing a patient's video, such as using a static camera and
maintaining a specific distance from it. While these constraints are manageable
under professional observation, they pose challenges in home settings. Another
issue with most vision-based systems is their output, typically a
classification label and confidence value, whose reliability is often
questioned by medical professionals. This paper addresses these challenges by
presenting a novel system for gait analysis robust to camera movements and
providing explanations for its output. The study utilizes a dataset comprising
videos of subjects wearing two types of Knee Ankle Foot Orthosis (KAFO), namely
"Locked Knee" and "Semi-flexion," for mobility, along with metadata and ground
truth for explanations. The ground truth highlights the statistical
significance of seven features captured using motion capture systems to
differentiate between the two gaits. To address camera movement challenges, the
proposed system employs super-resolution and pose estimation during
pre-processing. It then identifies the seven features - Stride Length, Step
Length and Duration of single support of orthotic and non-orthotic leg,
Cadence, and Speed - using the skeletal output of pose estimation. These
features train a multi-layer perceptron, with its output explained by
highlighting the features' contribution to classification. While most
state-of-the-art systems struggle with processing the video or training on the
proposed dataset, our system achieves an average accuracy of 94%. The model's
explainability is validated using ground truth and can be considered reliable.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16181" title="Abstract">arXiv:2402.16181</a> [<a href="/pdf/2402.16181" title="Download PDF">pdf</a>, <a href="/format/2402.16181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Can LLM Guide RL? A Value-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sirui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+S">Shuqi Ke</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wanxin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yingxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) has become the de facto standard practice for
sequential decision-making problems by improving future acting policies with
feedback. However, RL algorithms may require extensive trial-and-error
interactions to collect useful feedback for improvement. On the other hand,
recent developments in large language models (LLMs) have showcased impressive
capabilities in language understanding and generation, yet they fall short in
exploration and self-improvement capabilities for planning tasks, lacking the
ability to autonomously refine their responses based on feedback. Therefore, in
this paper, we study how the policy prior provided by the LLM can enhance the
sample efficiency of RL algorithms. Specifically, we develop an algorithm named
LINVIT that incorporates LLM guidance as a regularization factor in value-based
RL, leading to significant reductions in the amount of data needed for
learning, particularly when the difference between the ideal policy and the
LLM-informed policy is small, which suggests that the initial policy is close
to optimal, reducing the need for further exploration. Additionally, we present
a practical algorithm SLINVIT that simplifies the construction of the value
function and employs subgoals to reduce the search complexity. Our experiments
across three interactive environments ALFWorld, InterCode, and BlocksWorld
demonstrate that our method achieves state-of-the-art success rates and also
surpasses previous RL and LLM approaches in terms of sample efficiency. Our
code is available at https://github.com/agentification/Language-Integrated-VI.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16182" title="Abstract">arXiv:2402.16182</a> [<a href="/pdf/2402.16182" title="Download PDF">pdf</a>, <a href="/format/2402.16182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoodCapture: Depression Detection Using In-the-Wild Smartphone Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nepal%2C+S">Subigya Nepal</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+A">Arvind Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+T">Tess Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+A+C">Amanda C. Collins</a>, 
<a href="/search/cs?searchtype=author&query=Heinz%2C+M">Michael Heinz</a>, 
<a href="/search/cs?searchtype=author&query=Lekkas%2C+D">Damien Lekkas</a>, 
<a href="/search/cs?searchtype=author&query=Mirjafari%2C+S">Shayan Mirjafari</a>, 
<a href="/search/cs?searchtype=author&query=Nemesure%2C+M">Matthew Nemesure</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+G">George Price</a>, 
<a href="/search/cs?searchtype=author&query=Jacobson%2C+N+C">Nicholas C. Jacobson</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+A+T">Andrew T. Campbell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">MoodCapture presents a novel approach that assesses depression based on
images automatically captured from the front-facing camera of smartphones as
people go about their daily lives. We collect over 125,000 photos in the wild
from N=177 participants diagnosed with major depressive disorder for 90 days.
Images are captured naturalistically while participants respond to the PHQ-8
depression survey question: \textit{``I have felt down, depressed, or
hopeless''}. Our analysis explores important image attributes, such as angle,
dominant colors, location, objects, and lighting. We show that a random forest
trained with face landmarks can classify samples as depressed or non-depressed
and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides
several insights through an ablation study, feature importance analysis, and
bias assessment. Importantly, we evaluate user concerns about using MoodCapture
to detect depression based on sharing photos, providing critical insights into
privacy concerns that inform the future design of in-the-wild image-based
mental health assessment tools.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16184" title="Abstract">arXiv:2402.16184</a> [<a href="/pdf/2402.16184" title="Download PDF">pdf</a>, <a href="/format/2402.16184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Neural Network Initialization with Sparsity Inducing Activations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Price%2C+I">Ilan Price</a>, 
<a href="/search/cs?searchtype=author&query=Ball%2C+N+D">Nicholas Daultry Ball</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+S+C+H">Samuel C.H. Lam</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+A+C">Adam C. Jones</a>, 
<a href="/search/cs?searchtype=author&query=Tanner%2C+J">Jared Tanner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Inducing and leveraging sparse activations during training and inference is a
promising avenue for improving the computational efficiency of deep networks,
which is increasingly important as network sizes continue to grow and their
application becomes more widespread. Here we use the large width Gaussian
process limit to analyze the behaviour, at random initialization, of nonlinear
activations that induce sparsity in the hidden outputs. A previously unreported
form of training instability is proven for arguably two of the most natural
candidates for hidden layer sparsification; those being a shifted ReLU
($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$
for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|&gt;\tau$). We show that this
instability is overcome by clipping the nonlinear activation magnitude, at a
level prescribed by the shape of the associated Gaussian process variance map.
Numerical experiments verify the theory and show that the proposed magnitude
clipped sparsifying activations can be trained with training and test
fractional sparsity as high as 85\% while retaining close to full accuracy.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16186" title="Abstract">arXiv:2402.16186</a> [<a href="/pdf/2402.16186" title="Download PDF">pdf</a>, <a href="/format/2402.16186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Execution-time-certified Riccati-based IPM Algorithm for RTI-based  Input-constrained NMPC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Ganko%2C+K">Krystian Ganko</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shimin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Braatz%2C+R+D">Richard D. Braatz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Establishing an execution time certificate in deploying model predictive
control (MPC) is a pressing and challenging requirement. As nonlinear MPC
(NMPC) results in nonlinear programs, differing from quadratic programs
encountered in linear MPC, deriving an execution time certificate for NMPC
seems an impossible task. Our prior work \cite{wu2023direct} introduced an
input-constrained MPC algorithm with the exact and only
\textit{dimension-dependent} (\textit{data-independent}) number of
floating-point operations ([flops]). This paper extends it to input-constrained
NMPC problems via the real-time iteration (RTI) scheme, which results in
\textit{data-varying} (but \textit{dimension-invariant}) input-constrained MPC
problems. Therefore, applying our previous algorithm can certify the execution
time based on the assumption that processors perform fixed [flops] in constant
time. As the RTI-based scheme generally results in MPC with a long prediction
horizon, this paper employs the efficient factorized Riccati recursion, whose
computational cost scales linearly with the prediction horizon, to solve the
Newton system at each iteration. The execution-time certified capability of the
algorithm is theoretically and numerically validated through a case study
involving nonlinear control of the chaotic Lorenz system.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16187" title="Abstract">arXiv:2402.16187</a> [<a href="/pdf/2402.16187" title="Download PDF">pdf</a>, <a href="/format/2402.16187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacking LLM Watermarks by Exploiting Their Strengths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Q">Qi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenting Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+V">Virginia Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Advances in generative models have made it possible for AI-generated text,
code, and images to mirror human-generated content in many applications.
Watermarking, a technique that aims to embed information in the output of a
model to verify its source, is useful for mitigating misuse of such
AI-generated content. However, existing watermarking schemes remain
surprisingly susceptible to attack. In particular, we show that desirable
properties shared by existing LLM watermarking systems such as quality
preservation, robustness, and public detection APIs can in turn make these
systems vulnerable to various attacks. We rigorously study potential attacks in
terms of common watermark design choices, and propose best practices and
defenses for mitigation -- establishing a set of practical guidelines for
embedding and detection of LLM watermarks.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16188" title="Abstract">arXiv:2402.16188</a> [<a href="/pdf/2402.16188" title="Download PDF">pdf</a>, <a href="/format/2402.16188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARIN: Adaptive Resampling and Instance Normalization for Robust Blind  Inpainting of Dunhuang Cave Paintings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+A">Alexander Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Madhu%2C+P">Prathmesh Madhu</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+A">Andreas Maier</a>, 
<a href="/search/cs?searchtype=author&query=Christlein%2C+V">Vincent Christlein</a>, 
<a href="/search/cs?searchtype=author&query=Kosti%2C+R">Ronak Kosti</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 Eleventh International Conference on Image Processing Theory,
  Tools and Applications (IPTA), Salzburg, Austria, 2022, pp. 1-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image enhancement algorithms are very useful for real world computer vision
tasks where image resolution is often physically limited by the sensor size.
While state-of-the-art deep neural networks show impressive results for image
enhancement, they often struggle to enhance real-world images. In this work, we
tackle a real-world setting: inpainting of images from Dunhuang caves. The
Dunhuang dataset consists of murals, half of which suffer from corrosion and
aging. These murals feature a range of rich content, such as Buddha statues,
bodhisattvas, sponsors, architecture, dance, music, and decorative patterns
designed by different artists spanning ten centuries, which makes manual
restoration challenging. We modify two different existing methods (CAR, HINet)
that are based upon state-of-the-art (SOTA) super resolution and deblurring
networks. We show that those can successfully inpaint and enhance these
deteriorated cave paintings. We further show that a novel combination of CAR
and HINet, resulting in our proposed inpainting network (ARIN), is very robust
to external noise, especially Gaussian noise. To this end, we present a
quantitative and qualitative comparison of our proposed approach with existing
SOTA networks and winners of the Dunhuang challenge. One of the proposed
methods HINet) represents the new state of the art and outperforms the 1st
place of the Dunhuang Challenge, while our combination ARIN, which is robust to
noise, is comparable to the 1st place. We also present and discuss qualitative
results showing the impact of our method for inpainting on Dunhuang cave
images.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16189" title="Abstract">arXiv:2402.16189</a> [<a href="/pdf/2402.16189" title="Download PDF">pdf</a>, <a href="/format/2402.16189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-stage Prompt-based Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Priyadarshini Panda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt-based Continual Learning (PCL) has gained considerable attention as a
promising continual learning solution as it achieves state-of-the-art
performance while preventing privacy violation and memory overhead issues.
Nonetheless, existing PCL approaches face significant computational burdens
because of two Vision Transformer (ViT) feed-forward stages; one is for the
query ViT that generates a prompt query to select prompts inside a prompt pool;
the other one is a backbone ViT that mixes information between selected prompts
and image tokens. To address this, we introduce a one-stage PCL framework by
directly using the intermediate layer's token embedding as a prompt query. This
design removes the need for an additional feed-forward stage for query ViT,
resulting in ~50% computational cost reduction for both training and inference
with marginal accuracy drop &lt; 1%. We further introduce a Query-Pool
Regularization (QR) loss that regulates the relationship between the prompt
query and the prompt pool to improve representation power. The QR loss is only
applied during training time, so there is no computational overhead at
inference from the QR loss. With the QR loss, our approach maintains ~ 50%
computational cost reduction during inference as well as outperforms the prior
two-stage PCL methods by ~1.4% on public class-incremental continual learning
benchmarks including CIFAR-100, ImageNet-R, and DomainNet.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16190" title="Abstract">arXiv:2402.16190</a> [<a href="/pdf/2402.16190" title="Download PDF">pdf</a>, <a href="/ps/2402.16190" title="Download PostScript">ps</a>, <a href="/format/2402.16190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate predictions of keyhole depths using machine learning-aided  simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Runbo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kangming Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+X">Xiao Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hattrick-Simpers%2C+J">Jason Hattrick-Simpers</a>, 
<a href="/search/cs?searchtype=author&query=Simonds%2C+B+J">Brian J. Simonds</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Q">Qianglong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Rollett%2C+A+D">Anthony D. Rollett</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yu Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">The keyhole phenomenon is widely observed in laser materials processing,
including laser welding, remelting, cladding, drilling, and additive
manufacturing. Keyhole-induced defects, primarily pores, dramatically affect
the performance of final products, impeding the broad use of these laser-based
technologies. The formation of these pores is typically associated with the
dynamic behavior of the keyhole. So far, the accurate characterization and
prediction of keyhole features, particularly keyhole depth, as a function of
time has been a challenging task. In situ characterization of keyhole dynamic
behavior using a synchrotron X-ray is complicated and expensive. Current
simulations are hindered by their poor accuracies in predicting keyhole depths
due to the lack of real-time laser absorptance data. Here, we develop a machine
learning-aided simulation method that allows us to accurately predict keyhole
depth over a wide range of processing parameters. Based on titanium and
aluminum alloys, two commonly used engineering materials as examples, we
achieve an accuracy with an error margin of 10 %, surpassing those simulated
using other existing models (with an error margin in a range of 50-200 %). Our
machine learning-aided simulation method is affordable and readily deployable
for a large variety of materials, opening new doors to eliminate or reduce
defects for a wide range of laser materials processing techniques.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16192" title="Abstract">arXiv:2402.16192</a> [<a href="/pdf/2402.16192" title="Download PDF">pdf</a>, <a href="/format/2402.16192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Large Language Models against Jailbreak Attacks via Semantic  Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiabao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Bairu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Robey%2C+A">Alexander Robey</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shiyu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,
which bypass the safeguards of targeted LLMs and fool them into generating
objectionable content. While initial defenses show promise against token-based
threat models, there do not exist defenses that provide robustness against
semantic attacks and avoid unfavorable trade-offs between robustness and
nominal performance. To meet this need, we propose SEMANTICSMOOTH, a
smoothing-based defense that aggregates the predictions of multiple
semantically transformed copies of a given input prompt. Experimental results
demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against
GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on
instruction following benchmarks such as InstructionFollowing and AlpacaEval.
The codes will be publicly available at
https://github.com/UCSB-NLP-Chang/SemanticSmooth.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16194" title="Abstract">arXiv:2402.16194</a> [<a href="/pdf/2402.16194" title="Download PDF">pdf</a>, <a href="/format/2402.16194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and  Emotion Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamad%2C+O">Omama Hamad</a>, 
<a href="/search/cs?searchtype=author&query=Hamdi%2C+A">Ali Hamdi</a>, 
<a href="/search/cs?searchtype=author&query=Shaban%2C+K">Khaled Shaban</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Effective feature representations play a critical role in enhancing the
performance of text generation models that rely on deep neural networks.
However, current approaches suffer from several drawbacks, such as the
inability to capture the deep semantics of language and sensitivity to minor
input variations, resulting in significant changes in the generated text. In
this paper, we present a novel solution to these challenges by employing a
mixture of experts, multiple encoders, to offer distinct perspectives on the
emotional state of the user's utterance while simultaneously enhancing
performance. We propose an end-to-end model architecture called ASEM that
performs emotion analysis on top of sentiment analysis for open-domain
chatbots, enabling the generation of empathetic responses that are fluent and
relevant. In contrast to traditional attention mechanisms, the proposed model
employs a specialized attention strategy that uniquely zeroes in on sentiment
and emotion nuances within the user's utterance. This ensures the generation of
context-rich representations tailored to the underlying emotional tone and
sentiment intricacies of the text. Our approach outperforms existing methods
for generating empathetic embeddings, providing empathetic and diverse
responses. The performance of our proposed model significantly exceeds that of
existing models, enhancing emotion detection accuracy by 6.2% and lexical
diversity by 1.4%.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16196" title="Abstract">arXiv:2402.16196</a> [<a href="/pdf/2402.16196" title="Download PDF">pdf</a>, <a href="/format/2402.16196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Machine Learning with Computational Fluid Dynamics using  OpenFOAM and SmartSim
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maric%2C+T">Tomislav Maric</a>, 
<a href="/search/cs?searchtype=author&query=Fadeli%2C+M+E">Mohammed Elwardi Fadeli</a>, 
<a href="/search/cs?searchtype=author&query=Rigazzi%2C+A">Alessandro Rigazzi</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+A">Andrew Shao</a>, 
<a href="/search/cs?searchtype=author&query=Weiner%2C+A">Andre Weiner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Combining machine learning (ML) with computational fluid dynamics (CFD) opens
many possibilities for improving simulations of technical and natural systems.
However, CFD+ML algorithms require exchange of data, synchronization, and
calculation on heterogeneous hardware, making their implementation for
large-scale problems exceptionally challenging.
<br />We provide an effective and scalable solution to developing CFD+ML algorithms
using open source software OpenFOAM and SmartSim. SmartSim provides an
Orchestrator that significantly simplifies the programming of CFD+ML algorithms
and a Redis database that ensures highly scalable data exchange between ML and
CFD clients. We show how to leverage SmartSim to effectively couple different
segments of OpenFOAM with ML, including pre/post-processing applications,
solvers, function objects, and mesh motion solvers. We additionally provide an
OpenFOAM sub-module with examples that can be used as starting points for
real-world applications in CFD+ML.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16197" title="Abstract">arXiv:2402.16197</a> [<a href="/pdf/2402.16197" title="Download PDF">pdf</a>, <a href="/ps/2402.16197" title="Download PostScript">ps</a>, <a href="/format/2402.16197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models for Code Completion: A Practical Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Izadi%2C+M">Maliheh Izadi</a>, 
<a href="/search/cs?searchtype=author&query=Katzy%2C+J">Jonathan Katzy</a>, 
<a href="/search/cs?searchtype=author&query=van+Dam%2C+T">Tim van Dam</a>, 
<a href="/search/cs?searchtype=author&query=Otten%2C+M">Marc Otten</a>, 
<a href="/search/cs?searchtype=author&query=Popescu%2C+R+M">Razvan Mihai Popescu</a>, 
<a href="/search/cs?searchtype=author&query=van+Deursen%2C+A">Arie van Deursen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Transformer-based language models for automatic code completion have shown
great promise so far, yet the evaluation of these models rarely uses real data.
This study provides both quantitative and qualitative assessments of three
public code language models when completing real-world code. We first developed
an open-source IDE extension, Code4Me, for the online evaluation of the models.
We collected real auto-completion usage data for over a year from more than
1200 users, resulting in over 600K valid completions. These models were then
evaluated using six standard metrics across twelve programming languages. Next,
we conducted a qualitative study of 1690 real-world completion requests to
identify the reasons behind the poor model performance. A comparative analysis
of the models' performance in online and offline settings was also performed,
using benchmark synthetic datasets and two masking strategies. Our findings
suggest that while developers utilize code completion across various languages,
the best results are achieved for mainstream languages such as Python and Java.
InCoder outperformed the other models across all programming languages,
highlighting the significance of training data and objectives. Our study also
revealed that offline evaluations do not accurately reflect real-world
scenarios. Upon qualitative analysis of the model's predictions, we found that
66.3% of failures were due to the models' limitations, 24.4% occurred due to
inappropriate model usage in a development context, and 9.3% were valid
requests that developers overwrote. Given these findings, we propose several
strategies to overcome the current limitations. These include refining training
objectives, improving resilience to typographical errors, adopting hybrid
approaches, and enhancing implementations and usability.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16200" title="Abstract">arXiv:2402.16200</a> [<a href="/pdf/2402.16200" title="Download PDF">pdf</a>, <a href="/format/2402.16200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IR2: Information Regularization for Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Weili Cao</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Ramamohan Paturi</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+L">Leon Bergen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Effective information retrieval (IR) in settings with limited training data,
particularly for complex queries, remains a challenging task. This paper
introduces IR2, Information Regularization for Information Retrieval, a
technique for reducing overfitting during synthetic data generation. This
approach, representing a novel application of regularization techniques in
synthetic data creation for IR, is tested on three recent IR tasks
characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.
Experimental results indicate that our regularization techniques not only
outperform previous synthetic query generation methods on the tasks considered
but also reduce cost by up to 50%. Furthermore, this paper categorizes and
explores three regularization methods at different stages of the query
synthesis pipeline-input, prompt, and output-each offering varying degrees of
performance improvement compared to models where no regularization is applied.
This provides a systematic approach for optimizing synthetic data generation in
data-limited, complex-query IR scenarios. All code, prompts and synthetic data
are available at
https://github.com/Info-Regularization/Information-Regularization.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16201" title="Abstract">arXiv:2402.16201</a> [<a href="/pdf/2402.16201" title="Download PDF">pdf</a>, <a href="/format/2402.16201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for  Blockchain Data Sharding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Venkatakrishnan%2C+S+B">Shaileshh Bojja Venkatakrishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Data sharding - in which block data is sharded without sharding compute - is
at the present the favored approach for scaling Ethereum. A key challenge
toward implementing data sharding is verifying whether the entirety of a
block's data is available in the network (across its shards). A central
technique proposed to conduct this verification uses erasure coded blocks and
is called data availability sampling (DAS). While the high-level protocol
details of DAS has been well discussed in the community, discussions around how
such a protocol will be implemented at the peer-to-peer layer are lacking. We
identify random sampling of nodes as a fundamental primitive necessary to carry
out DAS and present Honeybee, a decentralized algorithm for sampling node that
uses verifiable random walks. Honeybee is secure against attacks even in the
presence of a large number of Byzantine nodes (e.g., 50% of the network). We
evaluate Honeybee through experiments and show that the quality of sampling
achieved by Honeybee is significantly better compared to the state-of-the-art.
Our proposed algorithm has implications for DAS functions in both full nodes
and light nodes.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16205" title="Abstract">arXiv:2402.16205</a> [<a href="/pdf/2402.16205" title="Download PDF">pdf</a>, <a href="/ps/2402.16205" title="Download PostScript">ps</a>, <a href="/format/2402.16205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Graph Pattern Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotumaccio%2C+N">Nicola Cotumaccio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Pattern matching queries on strings can be solved in linear time by
Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree
of a string [FOCS 1973] and showed that the seemingly more difficult problem of
computing matching statistics can also be solved in liner time. Pattern
matching queries on graphs are inherently more difficult: under the Orthogonal
Vector hypothesis, the graph pattern matching problem cannot be solved in
subquadratic time [TALG 2023]. The complexity of graph pattern matching can be
parameterized by the topological complexity of the considered graph, which is
captured by a parameter $ p $ [JACM 2023].
<br />In this paper, we show that, as in the string setting, computing matching
statistics on graph is as difficult as solving standard pattern matching
queries. To this end, we introduce a notion of longest common prefix (LCP)
array for arbitrary graphs.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16211" title="Abstract">arXiv:2402.16211</a> [<a href="/pdf/2402.16211" title="Download PDF">pdf</a>, <a href="/format/2402.16211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination  Tendency of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uluoglakci%2C+C">Cem Uluoglakci</a>, 
<a href="/search/cs?searchtype=author&query=Temizel%2C+T+T">Tugba Taskaya Temizel</a> (Middle East Technical University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL SRW 2024 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hallucinations pose a significant challenge to the reliability and alignment
of Large Language Models (LLMs), limiting their widespread acceptance beyond
chatbot applications. Despite ongoing efforts, hallucinations remain a
prevalent challenge in LLMs. The detection of hallucinations itself is also a
formidable task, frequently requiring manual labeling or constrained
evaluations. This paper introduces an automated scalable framework that
combines benchmarking LLMs' hallucination tendencies with efficient
hallucination detection. We leverage LLMs to generate challenging tasks related
to hypothetical phenomena, subsequently employing them as agents for efficient
hallucination detection. The framework is domain-agnostic, allowing the use of
any language model for benchmark creation or evaluation in any domain. We
introduce the publicly available HypoTermQA Benchmarking Dataset, on which
state-of-the-art models' performance ranged between 3% and 11%, and evaluator
agents demonstrated a 6% error rate in hallucination prediction. The proposed
framework provides opportunities to test and improve LLMs. Additionally, it has
the potential to generate benchmarking datasets tailored to specific domains,
such as law, health, and finance.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16227" title="Abstract">arXiv:2402.16227</a> [<a href="/pdf/2402.16227" title="Download PDF">pdf</a>, <a href="/format/2402.16227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Robust Optimization for Multi-Agent Robotic Systems: A  Distributed Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdul%2C+A+T">Arshiya Taj Abdul</a>, 
<a href="/search/cs?searchtype=author&query=Saravanos%2C+A+D">Augustinos D. Saravanos</a>, 
<a href="/search/cs?searchtype=author&query=Theodorou%2C+E+A">Evangelos A. Theodorou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper presents a novel distributed robust optimization scheme for
steering distributions of multi-agent systems under stochastic and
deterministic uncertainty. Robust optimization is a subfield of optimization
which aims in discovering an optimal solution that remains robustly feasible
for all possible realizations of the problem parameters within a given
uncertainty set. Such approaches would naturally constitute an ideal candidate
for multi-robot control, where in addition to stochastic noise, there might be
exogenous deterministic disturbances. Nevertheless, as these methods are
usually associated with significantly high computational demands, their
application to multi-agent robotics has remained limited. The scope of this
work is to propose a scalable robust optimization framework that effectively
addresses both types of uncertainties, while retaining computational efficiency
and scalability. In this direction, we provide tractable approximations for
robust constraints that are relevant in multi-robot settings. Subsequently, we
demonstrate how computations can be distributed through an Alternating
Direction Method of Multipliers (ADMM) approach towards achieving scalability
and communication efficiency. Simulation results highlight the performance of
the proposed algorithm in effectively handling both stochastic and
deterministic uncertainty in multi-robot systems. The scalability of the method
is also emphasized by showcasing tasks with up to 100 agents. The results of
this work indicate the promise of blending robust optimization, distribution
steering and distributed optimization towards achieving scalable, safe and
robust multi-robot control.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16230" title="Abstract">arXiv:2402.16230</a> [<a href="/pdf/2402.16230" title="Download PDF">pdf</a>, <a href="/format/2402.16230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GARNN: An Interpretable Graph Attentive Recurrent Neural Network for  Predicting Blood Glucose Levels via Multivariate Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piao%2C+C">Chengzhe Piao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Taiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Baldeweg%2C+S+E">Stephanie E Baldeweg</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+P">Paul Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Georgiou%2C+P">Pantelis Georgiou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiahao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kezhi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Accurate prediction of future blood glucose (BG) levels can effectively
improve BG management for people living with diabetes, thereby reducing
complications and improving quality of life. The state of the art of BG
prediction has been achieved by leveraging advanced deep learning methods to
model multi-modal data, i.e., sensor data and self-reported event data,
organised as multi-variate time series (MTS). However, these methods are mostly
regarded as ``black boxes'' and not entirely trusted by clinicians and
patients. In this paper, we propose interpretable graph attentive recurrent
neural networks (GARNNs) to model MTS, explaining variable contributions via
summarizing variable importance and generating feature maps by graph attention
mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets,
representing diverse clinical scenarios. Upon comparison with twelve
well-established baseline methods, GARNNs not only achieve the best prediction
accuracy but also provide high-quality temporal interpretability, in particular
for postprandial glucose levels as a result of corresponding meal intake and
insulin injection. These findings underline the potential of GARNN as a robust
tool for improving diabetes care, bridging the gap between deep learning
technology and real-world healthcare solutions.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16235" title="Abstract">arXiv:2402.16235</a> [<a href="/pdf/2402.16235" title="Download PDF">pdf</a>, <a href="/format/2402.16235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-AI Co-Creation of Worked Examples for Programming Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassany%2C+M">Mohammad Hassany</a>, 
<a href="/search/cs?searchtype=author&query=Brusilovsky%2C+P">Peter Brusilovsky</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+J">Jiaze Ke</a>, 
<a href="/search/cs?searchtype=author&query=Akhuseyinoglu%2C+K">Kamil Akhuseyinoglu</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A+B+L">Arun Balajiee Lekshmi Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2312.02105">arXiv:2312.02105</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Worked examples (solutions to typical programming problems presented as a
source code in a certain language and are used to explain the topics from a
programming class) are among the most popular types of learning content in
programming classes. Most approaches and tools for presenting these examples to
students are based on line-by-line explanations of the example code. However,
instructors rarely have time to provide line-by-line explanations for a large
number of examples typically used in a programming class. In this paper, we
explore and assess a human-AI collaboration approach to authoring worked
examples for Java programming. We introduce an authoring system for creating
Java worked examples that generates a starting version of code explanations and
presents it to the instructor to edit if necessary.We also present a study that
assesses the quality of explanations created with this approach
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16237" title="Abstract">arXiv:2402.16237</a> [<a href="/pdf/2402.16237" title="Download PDF">pdf</a>, <a href="/format/2402.16237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Level Set Estimation for Continuous Search Space with Theoretical  Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngo%2C+G">Giang Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Phan-Trong%2C+D">Dat Phan-Trong</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sunil Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A common problem encountered in many real-world applications is level set
estimation where the goal is to determine the region in the function domain
where the function is above or below a given threshold. When the function is
black-box and expensive to evaluate, the level sets need to be found in a
minimum set of function evaluations. Existing methods often assume a discrete
search space with a finite set of data points for function evaluations and
estimating the level sets. When applied to a continuous search space, these
methods often need to first discretize the space which leads to poor results
while needing high computational time. While some methods cater for the
continuous setting, they still lack a proper guarantee for theoretical
convergence. To address this problem, we propose a novel algorithm that does
not need any discretization and can directly work in continuous search spaces.
Our method suggests points by constructing an acquisition function that is
defined as a measure of confidence of the function being higher or lower than
the given threshold. A theoretical analysis for the convergence of the
algorithm to an accurate solution is provided. On multiple synthetic and
real-world datasets, our algorithm successfully outperforms state-of-the-art
methods.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16240" title="Abstract">arXiv:2402.16240</a> [<a href="/pdf/2402.16240" title="Download PDF">pdf</a>, <a href="/format/2402.16240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Frequency-aware Hierarchical Contrastive Selective Coding for  Representation Learning on Text-attributed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peiyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaozhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+L">Liying Kang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Senzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sunghun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024. arXiv admin note: text overlap with <a href="/abs/2009.10273">arXiv:2009.10273</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We investigate node representation learning on text-attributed graphs (TAGs),
where nodes are associated with text information. Although recent studies on
graph neural networks (GNNs) and pretrained language models (PLMs) have
exhibited their power in encoding network and text signals, respectively, less
attention has been paid to delicately coupling these two types of models on
TAGs. Specifically, existing GNNs rarely model text in each node in a
contextualized way; existing PLMs can hardly be applied to characterize graph
structures due to their sequence architecture. To address these challenges, we
propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive
Selective Coding method that integrates GNNs and PLMs into a unified model.
Different from previous "cascaded architectures" that directly add GNN layers
upon a PLM, our HASH-CODE relies on five self-supervised optimization
objectives to facilitate thorough mutual enhancement between network and text
signals in diverse granularities. Moreover, we show that existing contrastive
objective learns the low-frequency component of the augmentation graph and
propose a high-frequency component (HFC)-aware contrastive learning objective
that makes the learned embeddings more distinctive. Extensive experiments on
six real-world benchmarks substantiate the efficacy of our proposed approach.
In addition, theoretical analysis and item embedding visualization provide
insights into our model interoperability.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16242" title="Abstract">arXiv:2402.16242</a> [<a href="/pdf/2402.16242" title="Download PDF">pdf</a>, <a href="/format/2402.16242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HSONet:A Siamese foreground association-driven hard case sample  optimization network for high-resolution remote sensing image change  detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chao Tao</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+D">Dongsheng Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chengli Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haifeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 figures, 8 tables, 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the later training stages, further improvement of the models ability to
determine changes relies on how well the change detection (CD) model learns
hard cases; however, there are two additional challenges to learning hard case
samples: (1) change labels are limited and tend to pointer only to foreground
targets, yet hard case samples are prevalent in the background, which leads to
optimizing the loss function focusing on the foreground targets and ignoring
the background hard cases, which we call imbalance. (2) Complex situations,
such as light shadows, target occlusion, and seasonal changes, induce hard case
samples, and in the absence of both supervisory and scene information, it is
difficult for the model to learn hard case samples directly to accurately
obtain the feature representations of the change information, which we call
missingness. We propose a Siamese foreground association-driven hard case
sample optimization network (HSONet). To deal with this imbalance, we propose
an equilibrium optimization loss function to regulate the optimization focus of
the foreground and background, determine the hard case samples through the
distribution of the loss values, and introduce dynamic weights in the loss term
to gradually shift the optimization focus of the loss from the foreground to
the background hard cases as the training progresses. To address this
missingness, we understand hard case samples with the help of the scene
context, propose the scene-foreground association module, use potential remote
sensing spatial scene information to model the association between the target
of interest in the foreground and the related context to obtain scene
embedding, and apply this information to the feature reinforcement of hard
cases. Experiments on four public datasets show that HSONet outperforms current
state-of-the-art CD methods, particularly in detecting hard case samples.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16245" title="Abstract">arXiv:2402.16245</a> [<a href="/pdf/2402.16245" title="Download PDF">pdf</a>, <a href="/ps/2402.16245" title="Download PostScript">ps</a>, <a href="/format/2402.16245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Staircase Generator Matrix Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianfan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jifan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper, we propose a class of codes, referred to as random staircase
generator matrix codes (SGMCs), which have staircase-like generator matrices.
In the infinite-length region, we prove that the random SGMC is
capacity-achieving over binary-input output-symmetric (BIOS) channels. In the
finite-length region, we present the representative ordered statistics decoding
with local constraints (LC-ROSD) algorithm for the SGMCs. The most
distinguished feature of the SGMCs with LC-ROSD is that the staircase-like
matrices enable parallel implementation of the Gaussian elimination (GE),
avoiding the serial GE of conventional OSD and supporting a potential low
decoding latency, as implied from simulations. To analyze the performance of
random SGMCs in the finite-length region, we derive the ensemble weight
spectrum and invoke the conventional union bound. We also derive a partially
random coding union (RCU) bound, which is tighter than the conventional one and
is used as a criterion to design the SGMCs. Staircase-like generator matrices
allow us to derive a series of (tighter and tighter) lower bounds based on the
second-order Bonferroni inequality with the incremental number of codewords.
The numerical results show that the decoding performance can match well with
the proposed partially RCU bound for different code rates and different
profiles. The numerical results also show that the tailored SGMCs with the
LC-ROSD algorithm can approach the finite-length performance bound,
outperforming the 5G low-density parity-check (LDPC) codes, 5G polar codes, and
Reed-Muller (RM) codes.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16246" title="Abstract">arXiv:2402.16246</a> [<a href="/pdf/2402.16246" title="Download PDF">pdf</a>, <a href="/ps/2402.16246" title="Download PostScript">ps</a>, <a href="/format/2402.16246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on  UAV Traffic Videos on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Y">Yadong An</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yiming Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages,26 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper focuses on a real-time vehicle detection and urban traffic
behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video.
By using UAV to collect traffic data and combining the YOLOv8 model and SORT
tracking algorithm, the object detection and tracking functions are implemented
on the iOS mobile platform. For the problem of traffic data acquisition and
analysis, the dynamic computing method is used to process the performance in
real time and calculate the micro and macro traffic parameters of the vehicles,
and real-time traffic behavior analysis is conducted and visualized. The
experiment results reveals that the vehicle object detection can reach 98.27%
precision rate and 87.93% recall rate, and the real-time processing capacity is
stable at 30 frames per seconds. This work integrates drone technology, iOS
development, and deep learning techniques to integrate traffic video
acquisition, object detection, object tracking, and traffic behavior analysis
functions on mobile devices. It provides new possibilities for lightweight
traffic information collection and data analysis, and offers innovative
solutions to improve the efficiency of analyzing road traffic conditions and
addressing transportation issues for transportation authorities.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16247" title="Abstract">arXiv:2402.16247</a> [<a href="/pdf/2402.16247" title="Download PDF">pdf</a>, <a href="/format/2402.16247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Translations: Emergent Communication Pretraining for  Cooperative Language Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cope%2C+D">Dylan Cope</a>, 
<a href="/search/cs?searchtype=author&query=McBurney%2C+P">Peter McBurney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In Emergent Communication (EC) agents learn to communicate with one another,
but the protocols that they develop are specialised to their training
community. This observation led to research into Zero-Shot Coordination (ZSC)
for learning communication strategies that are robust to agents not encountered
during training. However, ZSC typically assumes that no prior data is available
about the agents that will be encountered in the zero-shot setting. In many
cases, this presents an unnecessarily hard problem and rules out communication
via preestablished conventions. We propose a novel AI challenge called a
Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions
are relaxed by allowing a 'joiner' agent to learn from a dataset of
interactions between agents in a target community. We propose and compare two
methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication
pretraining and Translation Learning (ECTL), in which an agent is trained in
self-play with EC and then learns from the data to translate between the
emergent protocol and the target community's protocol.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16248" title="Abstract">arXiv:2402.16248</a> [<a href="/pdf/2402.16248" title="Download PDF">pdf</a>, <a href="/format/2402.16248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic-to-essay generation with knowledge-based content selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jieyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chunyao Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The topic-to-essay generation task is a challenging natural language
generation task that aims to generate paragraph-level text with high semantic
coherence based on a given set of topic words. Previous work has focused on the
introduction of external knowledge, ignoring the insufficient generated text
diversity. In order to improve the generation diversity, we propose a novel
copy mechanism model with a content selection module that integrates rich
semantic knowledge from the language model into the decoder. Furthermore, we
introduce the improved prefix tuning method to train the model, enabling it to
adapt to varying input complexities. In addition, we have contributed a new
Chinese dataset for TEG tasks. Experimental results demonstrate that the
proposed model can improve the generated text diversity by 35\% to 59\%
compared to the state-of-the-art method, while maintaining a high level of
topic consistency.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16249" title="Abstract">arXiv:2402.16249</a> [<a href="/pdf/2402.16249" title="Download PDF">pdf</a>, <a href="/format/2402.16249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yubo Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D single object tracking (SOT) is an important and challenging task for the
autonomous driving and mobile robotics. Most existing methods perform tracking
between two consecutive frames while ignoring the motion patterns of the target
over a series of frames, which would cause performance degradation in the
scenes with sparse points. To break through this limitation, we introduce
Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to
capture target motion across continuous frames. Unlike previous methods that
primarily adopted three strategies: matching two consecutive point clouds,
predicting relative motion, or utilizing sequential point clouds to address
feature degradation, our SeqTrack3D combines both historical point clouds and
bounding box sequences. This novel method ensures robust tracking by leveraging
location priors from historical boxes, even in scenes with sparse points.
Extensive experiments conducted on large-scale datasets show that SeqTrack3D
achieves new state-of-the-art performances, improving by 6.00% on NuScenes and
14.13% on Waymo dataset. The code will be made public at
https://github.com/aron-lin/seqtrack3d.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16253" title="Abstract">arXiv:2402.16253</a> [<a href="/pdf/2402.16253" title="Download PDF">pdf</a>, <a href="/ps/2402.16253" title="Download PostScript">ps</a>, <a href="/format/2402.16253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To be, or not to be, that is the Question: Exploring the pseudorandom  generation of texts to write Hamlet from the perspective of the Infinite  Monkey Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Moraes+Silva%2C+E+C">Ergon Cugler de Moraes Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
<p class="mathjax">This article explores the theoretical and computational aspects of the
Infinite Monkey Theorem, investigating the number of attempts and the time
required for a set of pseudorandom characters to assemble and recite Hamlets
iconic phrase, To be, or not to be, that is the Question. Drawing inspiration
from Emile Borels original concept (1913), the study delves into the practical
implications of pseudorandomness using Python. Employing Python simulations to
generate excerpts from Hamlet, the research navigates historical perspectives
and bridges early theoretical foundations with contemporary computational
approaches. A set of tests reveals the attempts and time required to generate
incremental parts of the target phrase. Utilizing these results, growth factors
are calculated, projecting estimated attempts and time for each text part. The
findings indicate an astronomical challenge to generate the entire phrase,
requiring approximately 2.68x10e69 attempts and 2.95x10e66 seconds - equivalent
to 8.18x10e62 hours or 9.32x10e55 years. This temporal scale, exceeding the age
of the universe by 6.75x10e45 times, underscores the immense complexity and
improbability of random literary creation. The article concludes with
reflections on the mathematical intricacies and statistical feasibility within
the context of the Infinite Monkey Theorem, emphasizing the theoretical musings
surrounding infinite time and the profound limitations inherent in such
endeavors. And that only infinity could write Hamlet randomly.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16255" title="Abstract">arXiv:2402.16255</a> [<a href="/pdf/2402.16255" title="Download PDF">pdf</a>, <a href="/format/2402.16255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watch Your Head: Assembling Projection Heads to Save the Reliability of  Federated Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinqian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinghai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiqiang Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated learning encounters substantial challenges with heterogeneous data,
leading to performance degradation and convergence issues. While considerable
progress has been achieved in mitigating such an impact, the reliability aspect
of federated models has been largely disregarded. In this study, we conduct
extensive experiments to investigate the reliability of both generic and
personalized federated models. Our exploration uncovers a significant finding:
\textbf{federated models exhibit unreliability when faced with heterogeneous
data}, demonstrating poor calibration on in-distribution test data and low
uncertainty levels on out-of-distribution data. This unreliability is primarily
attributed to the presence of biased projection heads, which introduce
miscalibration into the federated models. Inspired by this observation, we
propose the "Assembled Projection Heads" (APH) method for enhancing the
reliability of federated models. By treating the existing projection head
parameters as priors, APH randomly samples multiple initialized parameters of
projection heads from the prior and further performs targeted fine-tuning on
locally available data under varying learning rates. Such a head ensemble
introduces parameter diversity into the deterministic model, eliminating the
bias and producing reliable predictions via head averaging. We evaluate the
effectiveness of the proposed APH method across three prominent federated
benchmarks. Experimental results validate the efficacy of APH in model
calibration and uncertainty estimation. Notably, APH can be seamlessly
integrated into various federated approaches but only requires less than 30\%
additional computation cost for 100$\times$ inferences within large models.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16259" title="Abstract">arXiv:2402.16259</a> [<a href="/pdf/2402.16259" title="Download PDF">pdf</a>, <a href="/format/2402.16259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Problems on Group-labeled Matroid Bases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B6rsch%2C+F">Florian H&#xf6;rsch</a>, 
<a href="/search/cs?searchtype=author&query=Imolay%2C+A">Andr&#xe1;s Imolay</a>, 
<a href="/search/cs?searchtype=author&query=Mizutani%2C+R">Ryuhei Mizutani</a>, 
<a href="/search/cs?searchtype=author&query=Oki%2C+T">Taihei Oki</a>, 
<a href="/search/cs?searchtype=author&query=Schwarcz%2C+T">Tam&#xe1;s Schwarcz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">Consider a matroid equipped with a labeling of its ground set to an abelian
group. We define the label of a subset of the ground set as the sum of the
labels of its elements. We study a collection of problems on finding bases and
common bases of matroids with restrictions on their labels. For zero bases and
zero common bases, the results are mostly negative. While finding a non-zero
basis of a matroid is not difficult, it turns out that the complexity of
finding a non-zero common basis depends on the group. Namely, we show that the
problem is hard for a fixed group if it contains an element of order two,
otherwise it is polynomially solvable.
<br />As a generalization of both zero and non-zero constraints, we further study
$F$-avoiding constraints where we seek a basis or common basis whose label is
not in a given set $F$ of forbidden labels. Using algebraic techniques, we give
a randomized algorithm for finding an $F$-avoiding common basis of two matroids
represented over the same field for finite groups given as operation tables.
The study of $F$-avoiding bases with groups given as oracles leads to a
conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding
basis can be obtained from an arbitrary basis by exchanging at most $|F|$
elements. We prove the conjecture for the special cases when $|F|\le 2$ or the
group is ordered. By relying on structural observations on matroids
representable over fixed, finite fields, we verify a relaxed version of the
conjecture for these matroids. As a consequence, we obtain a polynomial-time
algorithm in these special cases for finding an $F$-avoiding basis when $|F|$
is fixed.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16260" title="Abstract">arXiv:2402.16260</a> [<a href="/pdf/2402.16260" title="Download PDF">pdf</a>, <a href="/ps/2402.16260" title="Download PostScript">ps</a>, <a href="/format/2402.16260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Finite-time Differentiator for Multi-agent Systems Under  Directed Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weile Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Haibo Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shihua Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper proposes a new distributed finite-time differentiator (DFD) for
multi-agent systems (MAS) under directed graph, which extends the
differentiator algorithm from the centralized case to the distributed case by
only using relative/absolute position information. By skillfully constructing a
Lyapunov function, the finite-time stability of the closed-loop system under
DFD is proved. Inspired by the duality principle of control theory, a
distributed continuous finite-time output consensus algorithm extended from DFD
for a class of leader-follower MAS is provided, which not only completely
suppresses disturbance, but also avoids chattering. Finally, several simulation
examples are given to verify the effectiveness of the DFD.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16261" title="Abstract">arXiv:2402.16261</a> [<a href="/pdf/2402.16261" title="Download PDF">pdf</a>, <a href="/format/2402.16261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniRetriever: Multi-task Candidates Selection for Various  Context-Adaptive Conversational Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Baohang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Conversational retrieval refers to an information retrieval system that
operates in an iterative and interactive manner, requiring the retrieval of
various external resources, such as persona, knowledge, and even response, to
effectively engage with the user and successfully complete the dialogue.
However, most previous work trained independent retrievers for each specific
resource, resulting in sub-optimal performance and low efficiency. Thus, we
propose a multi-task framework function as a universal retriever for three
dominant retrieval tasks during the conversation: persona selection, knowledge
selection, and response selection. To this end, we design a dual-encoder
architecture consisting of a context-adaptive dialogue encoder and a candidate
encoder, aiming to attention to the relevant context from the long dialogue and
retrieve suitable candidates by simply a dot product. Furthermore, we introduce
two loss constraints to capture the subtle relationship between dialogue
context and different candidates by regarding historically selected candidates
as hard negatives. Extensive experiments and analysis establish
state-of-the-art retrieval quality both within and outside its training domain,
revealing the promising potential and generalization capability of our model to
serve as a universal retriever for different candidate selection tasks
simultaneously.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16262" title="Abstract">arXiv:2402.16262</a> [<a href="/pdf/2402.16262" title="Download PDF">pdf</a>, <a href="/format/2402.16262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoGenT: A Content-oriented Generative-hit Framework for Content Delivery  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Ming-Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Ke Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Ke Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhihai Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The service provided by content delivery networks (CDNs) may overlook content
locality, leaving room for potential performance improvement. In this study, we
explore the feasibility of leveraging generated data as a replacement for
fetching data in missing scenarios based on content locality. Due to sufficient
local computing resources and reliable generation efficiency, we propose a
content-oriented generative-hit framework (CoGenT) for CDNs. CoGenT utilizes
idle computing resources on edge nodes to generate requested data based on
similar or related cached data to achieve hits. Our implementation in a
real-world system demonstrates that CoGenT reduces the average access latency
by half. Additionally, experiments conducted on a simulator also confirm that
CoGenT can enhance existing caching algorithms, resulting in reduced latency
and bandwidth usage.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16267" title="Abstract">arXiv:2402.16267</a> [<a href="/pdf/2402.16267" title="Download PDF">pdf</a>, <a href="/format/2402.16267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrared and visible Image Fusion with Language-driven Loss in CLIP  Embedding Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+L">Lingjuan Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhiqiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yajun Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared-visible image fusion (IVIF) has attracted much attention owing to
the highly-complementary properties of the two image modalities. Due to the
lack of ground-truth fused images, the fusion output of current deep-learning
based methods heavily depends on the loss functions defined mathematically. As
it is hard to well mathematically define the fused image without ground truth,
the performance of existing fusion methods is limited. In this paper, we first
propose to use natural language to express the objective of IVIF, which can
avoid the explicit mathematical modeling of fusion output in current losses,
and make full use of the advantage of language expression to improve the fusion
performance. For this purpose, we present a comprehensive language-expressed
fusion objective, and encode relevant texts into the multi-modal embedding
space using CLIP. A language-driven fusion model is then constructed in the
embedding space, by establishing the relationship among the embedded vectors to
represent the fusion objective and input image modalities. Finally, a
language-driven loss is derived to make the actual IVIF aligned with the
embedded language-driven fusion model via supervised training. Experiments show
that our method can obtain much better fusion results than existing techniques.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16268" title="Abstract">arXiv:2402.16268</a> [<a href="/pdf/2402.16268" title="Download PDF">pdf</a>, <a href="/format/2402.16268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model Transparency Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bommasani%2C+R">Rishi Bommasani</a>, 
<a href="/search/cs?searchtype=author&query=Klyman%2C+K">Kevin Klyman</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Betty Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+S">Sayash Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Maslej%2C+N">Nestor Maslej</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A">Arvind Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Foundation models are critical digital technologies with sweeping societal
impact that necessitates transparency. To codify how foundation model
developers should provide transparency about the development and deployment of
their models, we propose Foundation Model Transparency Reports, drawing upon
the transparency reporting practices in social media. While external
documentation of societal harms prompted social media transparency reports, our
objective is to institutionalize transparency reporting for foundation models
while the industry is still nascent. To design our reports, we identify 6
design principles given the successes and shortcomings of social media
transparency reporting. To further schematize our reports, we draw upon the 100
transparency indicators from the Foundation Model Transparency Index. Given
these indicators, we measure the extent to which they overlap with the
transparency requirements included in six prominent government policies (e.g.,
the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI).
Well-designed transparency reports could reduce compliance costs, in part due
to overlapping regulatory requirements across different jurisdictions. We
encourage foundation model developers to regularly publish transparency
reports, building upon recommendations from the G7 and the White House.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16269" title="Abstract">arXiv:2402.16269</a> [<a href="/pdf/2402.16269" title="Download PDF">pdf</a>, <a href="/format/2402.16269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Large Language Models and Optimization to Decision Optimization  CoPilot: A Research Manifesto
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wasserkrug%2C+S">Segev Wasserkrug</a>, 
<a href="/search/cs?searchtype=author&query=Boussioux%2C+L">Leonard Boussioux</a>, 
<a href="/search/cs?searchtype=author&query=Hertog%2C+D+d">Dick den Hertog</a>, 
<a href="/search/cs?searchtype=author&query=Mirzazadeh%2C+F">Farzaneh Mirzazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Birbil%2C+I">Ilker Birbil</a>, 
<a href="/search/cs?searchtype=author&query=Kurtz%2C+J">Jannis Kurtz</a>, 
<a href="/search/cs?searchtype=author&query=Maragno%2C+D">Donato Maragno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Significantly simplifying the creation of optimization models for real-world
business problems has long been a major goal in applying mathematical
optimization more widely to important business and societal decisions. The
recent capabilities of Large Language Models (LLMs) present a timely
opportunity to achieve this goal. Therefore, we propose research at the
intersection of LLMs and optimization to create a Decision Optimization CoPilot
(DOCP) - an AI tool designed to assist any decision maker, interacting in
natural language to grasp the business problem, subsequently formulating and
solving the corresponding optimization model. This paper outlines our DOCP
vision and identifies several fundamental requirements for its implementation.
We describe the state of the art through a literature survey and experiments
using ChatGPT. We show that a) LLMs already provide substantial novel
capabilities relevant to a DOCP, and b) major research challenges remain to be
addressed. We also propose possible research directions to overcome these gaps.
We also see this work as a call to action to bring together the LLM and
optimization communities to pursue our vision, thereby enabling much more
widespread improved decision-making.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16278" title="Abstract">arXiv:2402.16278</a> [<a href="/pdf/2402.16278" title="Download PDF">pdf</a>, <a href="/format/2402.16278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Self-matching Training Method with Annotation Embedding Models for  Ontology Subsumption Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiraishi%2C+Y">Yukihiro Shiraishi</a>, 
<a href="/search/cs?searchtype=author&query=Kaneiwa%2C+K">Ken Kaneiwa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, ontology embeddings representing entities in a low-dimensional
space have been proposed for ontology completion. However, the ontology
embeddings for concept subsumption prediction do not address the difficulties
of similar and isolated entities and fail to extract the global information of
annotation axioms from an ontology. In this paper, we propose a self-matching
training method for the two ontology embedding models: Inverted-index Matrix
Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings
capture the global and local information in annotation axioms by means of the
occurring locations of each word in a set of axioms and the co-occurrences of
words in each axiom. The self-matching training method increases the robustness
of the concept subsumption prediction when predicted superclasses are similar
to subclasses and are isolated to other entities in an ontology. Our evaluation
experiments show that the self-matching training method with InME outperforms
the existing ontology embeddings for the GO and FoodOn ontologies and that the
method with the concatenation of CoME and OWL2Vec* outperforms them for the
HeLiS ontology.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16279" title="Abstract">arXiv:2402.16279</a> [<a href="/pdf/2402.16279" title="Download PDF">pdf</a>, <a href="/format/2402.16279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadratic Message Passing for Generalized Quadratic Equations Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huimin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">For approximate inference in the generalized quadratic equations model, many
state-of-the-art algorithms lack any prior knowledge of the target signal
structure, exhibits slow convergence, and can not handle any analytic prior
knowledge of the target signal structure. So, this paper proposes a new
algorithm, Quadratic Message passing (QMP). QMP has a complexity as low as
$O(N^{3})$. The SE derived for QMP can capture precisely the per-iteration
behavior of the simulated algorithm. Simulation results confirm QMP outperforms
many state-of-the-art algorithms.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16280" title="Abstract">arXiv:2402.16280</a> [<a href="/pdf/2402.16280" title="Download PDF">pdf</a>, <a href="/format/2402.16280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ming%2C+Y">Yu Ming</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Danyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Changxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gui-Song Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Li Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jin-Gang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nucleus instance segmentation from histopathology images suffers from the
extremely laborious and expert-dependent annotation of nucleus instances. As a
promising solution to this task, annotation-efficient deep learning paradigms
have recently attracted much research interest, such as weakly-/semi-supervised
learning, generative adversarial learning, etc. In this paper, we propose to
formulate annotation-efficient nucleus instance segmentation from the
perspective of few-shot learning (FSL). Our work was motivated by that, with
the prosperity of computational pathology, an increasing number of
fully-annotated datasets are publicly accessible, and we hope to leverage these
external datasets to assist nucleus instance segmentation on the target dataset
which only has very limited annotation. To achieve this goal, we adopt the
meta-learning based FSL paradigm, which however has to be tailored in two
substantial aspects before adapting to our task. First, since the novel classes
may be inconsistent with those of the external dataset, we extend the basic
definition of few-shot instance segmentation (FSIS) to generalized few-shot
instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of
nucleus segmentation, including touching between adjacent cells, cellular
heterogeneity, etc., we further introduce a structural guidance mechanism into
the GFSIS network, finally leading to a unified Structurally-Guided Generalized
Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a
couple of publicly accessible datasets demonstrate that, SGFSIS can outperform
other annotation-efficient learning baselines, including semi-supervised
learning, simple transfer learning, etc., with comparable performance to fully
supervised learning with less than 5% annotations.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16281" title="Abstract">arXiv:2402.16281</a> [<a href="/pdf/2402.16281" title="Download PDF">pdf</a>, <a href="/format/2402.16281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Agile Robots: Intuitive Robot Position Speculation with Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yanlong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhigang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yisheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Ming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The robot position speculation, which determines where the chassis should
move, is one key step to control the mobile manipulators. The target position
must ensure the feasibility of chassis movement and manipulability, which is
guaranteed by randomized sampling and kinematic checking in traditional
methods. Addressing the demands of agile robotics, this paper proposes a robot
position speculation network(RPSN), a learning-based approach to enhance the
agility of mobile manipulators. The RPSN incorporates a differentiable inverse
kinematic algorithm and a neural network. Through end-to-end training, the RPSN
can speculate positions with a high success rate. We apply the RPSN to mobile
manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs).
Extensive experiments on various simulated environments and physical mobile
manipulators demonstrate that the probability of the initial position provided
by RPSN being the ideal position is 96.67%. From the kinematic constraint
perspective, it achieves 100% generation of the ideal position on average
within 1.28 attempts. Much lower than that of random sampling, 31.04. Moreover,
the proposed method demonstrates superior data efficiency over pure neural
network approaches. The proposed RPSN enables the robot to quickly infer
feasible target positions by intuition. This work moves towards building agile
robots that can act swiftly like humans.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16284" title="Abstract">arXiv:2402.16284</a> [<a href="/pdf/2402.16284" title="Download PDF">pdf</a>, <a href="/format/2402.16284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Assembly of Patterns in the abstract Tile Assembly Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drake%2C+P">Phillip Drake</a>, 
<a href="/search/cs?searchtype=author&query=Patitz%2C+M+J">Matthew J. Patitz</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+S+M">Scott M. Summers</a>, 
<a href="/search/cs?searchtype=author&query=Tracy%2C+T">Tyler Tracy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Computational Complexity (cs.CC); Computational Geometry (cs.CG)

</div>
<p class="mathjax">In the abstract Tile Assembly Model, self-assembling systems consisting of
tiles of different colors can form structures on which colored patterns are
``painted.'' We explore the complexity, in terms of the unique tile types
required, of assembling various patterns, proving several upper and lower
bounds.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16288" title="Abstract">arXiv:2402.16288</a> [<a href="/pdf/2402.16288" title="Download PDF">pdf</a>, <a href="/format/2402.16288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,  Retrieval, and Synthesis in Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yiming Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baojun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zezhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Long-term memory plays a critical role in personal interaction, considering
long-term memory can better leverage world knowledge, historical information,
and preferences in dialogues. Our research introduces PerLTQA, an innovative QA
dataset that combines semantic and episodic memories, including world
knowledge, profiles, social relationships, events, and dialogues. This dataset
is collected to investigate the use of personalized memories, focusing on
social interactions and events in the QA task. PerLTQA features two types of
memory and a comprehensive benchmark of 8,593 questions for 30 characters,
facilitating the exploration and application of personalized memories in Large
Language Models (LLMs). Based on PerLTQA, we propose a novel framework for
memory integration and generation, consisting of three main components: Memory
Classification, Memory Retrieval, and Memory Synthesis. We evaluate this
framework using five LLMs and three retrievers. Experimental results
demonstrate that BERT-based classification models significantly outperform LLMs
such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,
our study highlights the importance of effective memory integration in the QA
task.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16290" title="Abstract">arXiv:2402.16290</a> [<a href="/pdf/2402.16290" title="Download PDF">pdf</a>, <a href="/ps/2402.16290" title="Download PostScript">ps</a>, <a href="/format/2402.16290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Card-Based Overwriting Protocol for Equality Function and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruangwises%2C+S">Suthee Ruangwises</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+T">Tomoki Ono</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+Y">Yoshiki Abe</a>, 
<a href="/search/cs?searchtype=author&query=Hatsugai%2C+K">Kyosuke Hatsugai</a>, 
<a href="/search/cs?searchtype=author&query=Iwamoto%2C+M">Mitsugu Iwamoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Research in the area of secure multi-party computation with an unconventional
method of using a physical deck of playing cards began in 1989 when den Boar
proposed a protocol to compute the logical AND function using five cards. Since
then, the area has gained interest from many researchers and several card-based
protocols to compute various functions have been developed. In this paper, we
propose a card-based protocol called the overwriting protocol that can securely
compute the $k$-candidate $n$-variable equality function $f: \{0,1,\ldots
,k-1\}^n \rightarrow \{0,1\}$. We also apply the technique used in this
protocol to compute other similar functions.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16291" title="Abstract">arXiv:2402.16291</a> [<a href="/pdf/2402.16291" title="Download PDF">pdf</a>, <a href="/ps/2402.16291" title="Download PostScript">ps</a>, <a href="/format/2402.16291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation  in RLD detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haruna%2C+Y">Yunusa Haruna</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Shiyin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chukkol%2C+A+H+A">Abdulrahman Hamman Adama Chukkol</a>, 
<a href="/search/cs?searchtype=author&query=Bello%2C+I">Isah Bello</a>, 
<a href="/search/cs?searchtype=author&query=Lawan%2C+A">Adamu Lawan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Detecting objects across various scales remains a significant challenge in
computer vision, particularly in tasks such as Rice Leaf Disease (RLD)
detection, where objects exhibit considerable scale variations. Traditional
object detection methods often struggle to address these variations, resulting
in missed detections or reduced accuracy. In this study, we propose the
multi-scale Attention Pyramid module (mAPm), a novel approach that integrates
dilated convolutions into the Feature Pyramid Network (FPN) to enhance
multi-scale information ex-traction. Additionally, we incorporate a global
Multi-Head Self-Attention (MHSA) mechanism and a deconvolutional layer to
refine the up-sampling process. We evaluate mAPm on YOLOv7 using the MRLD and
COCO datasets. Compared to vanilla FPN, BiFPN, NAS-FPN, PANET, and ACFPN, mAPm
achieved a significant improvement in Average Precision (AP), with a +2.61%
increase on the MRLD dataset compared to the baseline FPN method in YOLOv7.
This demonstrates its effectiveness in handling scale variations. Furthermore,
the versatility of mAPm allows its integration into various FPN-based object
detection models, showcasing its potential to advance object detection
techniques.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16294" title="Abstract">arXiv:2402.16294</a> [<a href="/pdf/2402.16294" title="Download PDF">pdf</a>, <a href="/format/2402.16294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Federated Unlearning on Blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guangsheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lixiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Haipeng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renping Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Blockchained Federated Learning (FL) has been gaining traction for ensuring
the integrity and traceability of FL processes. Blockchained FL involves
participants training models locally with their data and subsequently
publishing the models on the blockchain, forming a Directed Acyclic Graph
(DAG)-like inheritance structure that represents the model relationship.
However, this particular DAG-based structure presents challenges in updating
models with sensitive data, due to the complexity and overhead involved. To
address this, we propose Blockchained Federated Unlearning (BlockFUL), a
generic framework that redesigns the blockchain structure using Chameleon Hash
(CH) technology to mitigate the complexity of model updating, thereby reducing
the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL
supports various federated unlearning methods, ensuring the integrity and
traceability of model updates, whether conducted in parallel or serial. We
conduct a comprehensive study of two typical unlearning methods, gradient
ascent and re-training, demonstrating the efficient unlearning workflow in
these two categories with minimal CH and block update operations. Additionally,
we compare the computation and communication costs of these methods.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16297" title="Abstract">arXiv:2402.16297</a> [<a href="/pdf/2402.16297" title="Download PDF">pdf</a>, <a href="/format/2402.16297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sikun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuzhen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Pengfei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoming Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Bayesian methodologies for handling count-valued time series have gained
prominence due to their ability to infer interpretable latent structures and to
estimate uncertainties, and thus are especially suitable for dealing with noisy
and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical
Systems (PGDSs) are proven to be effective in capturing the evolving dynamics
underlying observed count sequences. However, the state-of-the-art PGDS still
falls short in capturing the time-varying transition dynamics that are commonly
observed in real-world count time series. To mitigate this limitation, a
non-stationary PGDS is proposed to allow the underlying transition matrices to
evolve over time, and the evolving transition matrices are modeled by
sophisticatedly-designed Dirichlet Markov chains. Leveraging
Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and
efficient Gibbs sampler is developed to perform posterior simulation.
Experiments show that, in comparison with related models, the proposed
non-stationary PGDS achieves improved predictive performance due to its
capacity to learn non-stationary dependency structure captured by the
time-evolving transition matrices.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16298" title="Abstract">arXiv:2402.16298</a> [<a href="/pdf/2402.16298" title="Download PDF">pdf</a>, <a href="/format/2402.16298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarker%2C+S">Sushmita Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Sarker%2C+P">Prithul Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Bebis%2C+G">George Bebis</a>, 
<a href="/search/cs?searchtype=author&query=Tavakkoli%2C+A">Alireza Tavakkoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional deep learning approaches for breast cancer classification has
predominantly concentrated on single-view analysis. In clinical practice,
however, radiologists concurrently examine all views within a mammography exam,
leveraging the inherent correlations in these views to effectively detect
tumors. Acknowledging the significance of multi-view analysis, some studies
have introduced methods that independently process mammogram views, either
through distinct convolutional branches or simple fusion strategies,
inadvertently leading to a loss of crucial inter-view correlations. In this
paper, we propose an innovative multi-view network exclusively based on
transformers to address challenges in mammographic image classification. Our
approach introduces a novel shifted window-based dynamic attention block,
facilitating the effective integration of multi-view information and promoting
the coherent transfer of this information between views at the spatial feature
map level. Furthermore, we conduct a comprehensive comparative analysis of the
performance and effectiveness of transformer-based models under diverse
settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is
publicly available at https://github.com/prithuls/MV-Swin-T
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16299" title="Abstract">arXiv:2402.16299</a> [<a href="/pdf/2402.16299" title="Download PDF">pdf</a>, <a href="/format/2402.16299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Against Filter Bubbles: Diversified Music Recommendation via Weighted  Hypergraph Embedding Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chaoguang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Liuying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhineng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recommender systems serve a dual purpose for users: sifting out inappropriate
or mismatched information while accurately identifying items that align with
their preferences. Numerous recommendation algorithms are designed to provide
users with a personalized array of information tailored to their preferences.
Nevertheless, excessive personalization can confine users within a "filter
bubble". Consequently, achieving the right balance between accuracy and
diversity in recommendations is a pressing concern. To address this challenge,
exemplified by music recommendation, we introduce the Diversified Weighted
Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,
the initial connections between users and listened tracks are represented by a
weighted hypergraph. Simultaneously, associations between artists, albums and
tags with tracks are also appended to the hypergraph. To explore users' latent
preferences, a hypergraph-based random walk embedding method is applied to the
constructed hypergraph. In our investigation, accuracy is gauged by the
alignment between the user and the track, whereas the array of recommended
track types measures diversity. We rigorously compared DWHRec against seven
state-of-the-art recommendation algorithms using two real-world music datasets.
The experimental results validate DWHRec as a solution that adeptly harmonizes
accuracy and diversity, delivering a more enriched musical experience. Beyond
music recommendation, DWHRec can be extended to cater to other scenarios with
similar data structures.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16300" title="Abstract">arXiv:2402.16300</a> [<a href="/pdf/2402.16300" title="Download PDF">pdf</a>, <a href="/format/2402.16300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformalized Selective Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sokol%2C+A">Anna Sokol</a>, 
<a href="/search/cs?searchtype=author&query=Moniz%2C+N">Nuno Moniz</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N">Nitesh Chawla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Should prediction models always deliver a prediction? In the pursuit of
maximum predictive performance, critical considerations of reliability and
fairness are often overshadowed, particularly when it comes to the role of
uncertainty. Selective regression, also known as the "reject option," allows
models to abstain from predictions in cases of considerable uncertainty.
Initially proposed seven decades ago, approaches to selective regression have
mostly focused on distribution-based proxies for measuring uncertainty,
particularly conditional variance. However, this focus neglects the significant
influence of model-specific biases on a model's performance. In this paper, we
propose a novel approach to selective regression by leveraging conformal
prediction, which provides grounded confidence measures for individual
predictions based on model-specific biases. In addition, we propose a
standardized evaluation framework to allow proper comparison of selective
regression approaches. Via an extensive experimental approach, we demonstrate
how our proposed approach, conformalized selective regression, demonstrates an
advantage over multiple state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16302" title="Abstract">arXiv:2402.16302</a> [<a href="/pdf/2402.16302" title="Download PDF">pdf</a>, <a href="/format/2402.16302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Diffusion Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yijing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Recent research has made significant progress in optimizing diffusion models
for specific downstream objectives, which is an important pursuit in fields
such as graph generation for drug design. However, directly applying these
models to graph diffusion presents challenges, resulting in suboptimal
performance. This paper introduces graph diffusion policy optimization (GDPO),
a novel approach to optimize graph diffusion models for arbitrary (e.g.,
non-differentiable) objectives using reinforcement learning. GDPO is based on
an eager policy gradient tailored for graph diffusion models, developed through
meticulous analysis and promising improved performance. Experimental results
show that GDPO achieves state-of-the-art performance in various graph
generation tasks with complex and diverse objectives. Code is available at
https://github.com/sail-sg/GDPO.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16304" title="Abstract">arXiv:2402.16304</a> [<a href="/pdf/2402.16304" title="Download PDF">pdf</a>, <a href="/format/2402.16304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-Personalized-K Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+W">Wonbin Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">SeongKu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+S">Sanghwan Jang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hwanjo Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The conventional top-K recommendation, which presents the top-K items with
the highest ranking scores, is a common practice for generating personalized
ranking lists. However, is this fixed-size top-K recommendation the optimal
approach for every user's satisfaction? Not necessarily. We point out that
providing fixed-size recommendations without taking into account user utility
can be suboptimal, as it may unavoidably include irrelevant items or limit the
exposure to relevant ones. To address this issue, we introduce
Top-Personalized-K Recommendation, a new recommendation task aimed at
generating a personalized-sized ranking list to maximize individual user
satisfaction. As a solution to the proposed task, we develop a model-agnostic
framework named PerK. PerK estimates the expected user utility by leveraging
calibrated interaction probabilities, subsequently selecting the recommendation
size that maximizes this expected utility. Through extensive experiments on
real-world datasets, we demonstrate the superiority of PerK in
Top-Personalized-K recommendation task. We expect that Top-Personalized-K
recommendation has the potential to offer enhanced solutions for various
real-world recommendation scenarios, based on its great compatibility with
existing models.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16305" title="Abstract">arXiv:2402.16305</a> [<a href="/pdf/2402.16305" title="Download PDF">pdf</a>, <a href="/format/2402.16305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Referee Can Play: An Alternative Approach to Conditional Generation via  Model Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuantong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As a dominant force in text-to-image generation tasks, Diffusion
Probabilistic Models (DPMs) face a critical challenge in controllability,
struggling to adhere strictly to complex, multi-faceted instructions. In this
work, we aim to address this alignment challenge for conditional generation
tasks. First, we provide an alternative view of state-of-the-art DPMs as a way
of inverting advanced Vision-Language Models (VLMs). With this formulation, we
naturally propose a training-free approach that bypasses the conventional
sampling process associated with DPMs. By directly optimizing images with the
supervision of discriminative VLMs, the proposed method can potentially achieve
a better text-image alignment. As proof of concept, we demonstrate the pipeline
with the pre-trained BLIP-2 model and identify several key designs for improved
image generation. To further enhance the image fidelity, a Score Distillation
Sampling module of Stable Diffusion is incorporated. By carefully balancing the
two components during optimization, our method can produce high-quality images
with near state-of-the-art performance on T2I-Compbench.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16308" title="Abstract">arXiv:2402.16308</a> [<a href="/pdf/2402.16308" title="Download PDF">pdf</a>, <a href="/format/2402.16308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene  Understanding and Real-to-Sim Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yizhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+J">Jack Collins</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+O+P">Oiwi Parker Jones</a>, 
<a href="/search/cs?searchtype=author&query=Posner%2C+I">Ingmar Posner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">3D scene understanding for robotic applications exhibits a unique set of
requirements including real-time inference, object-centric latent
representation learning, accurate 6D pose estimation and 3D reconstruction of
objects. Current methods for scene understanding typically rely on a
combination of trained models paired with either an explicit or learnt
volumetric representation, all of which have their own drawbacks and
limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model
(OCGM) designed explicitly to perform inference on a 3D scene informed only by
a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end,
and is capable of segmenting objects, providing 3D object reconstructions,
generating object-centric latent representations and accurate per-object 6D
pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained
CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene
reconstruction, object matching and object pose estimation. Our experiments
show that our model outperforms all baselines by a significant margin in
real-world scenarios displaying its applicability for 3D scene understanding
tasks while meeting the strict demands exhibited in robotics applications.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16310" title="Abstract">arXiv:2402.16310</a> [<a href="/pdf/2402.16310" title="Download PDF">pdf</a>, <a href="/format/2402.16310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility  for Location Prediction over Sparse Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+B">Bangchao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+B">Bingqing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dingqi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Location prediction forecasts a user's location based on historical user
mobility traces. To tackle the intrinsic sparsity issue of real-world user
mobility traces, spatiotemporal contexts have been shown as significantly
useful. Existing solutions mostly incorporate spatiotemporal distances between
locations in mobility traces, either by feeding them as additional inputs to
Recurrent Neural Networks (RNNs) or by using them to search for informative
past hidden states for prediction. However, such distance-based methods fail to
capture the time-varying temporal regularities of human mobility, where human
mobility is often more regular in the morning than in other periods, for
example; this suggests the usefulness of the actual timestamps besides the
temporal distances. Against this background, we propose REPLAY, a general RNN
architecture learning to capture the time-varying temporal regularities for
location prediction. Specifically, REPLAY not only resorts to the
spatiotemporal distances in sparse trajectories to search for the informative
past hidden states, but also accommodates the time-varying temporal
regularities by incorporating smoothed timestamp embeddings using Gaussian
weighted averaging with timestamp-specific learnable bandwidths, which can
flexibly adapt to the temporal regularities of different strengths across
different timestamps. Our extensive evaluation compares REPLAY against a
sizable collection of state-of-the-art techniques on two real-world datasets.
Results show that REPLAY consistently and significantly outperforms
state-of-the-art methods by 7.7\%-10.9\% in the location prediction task, and
the bandwidths reveal interesting patterns of the time-varying temporal
regularities.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16311" title="Abstract">arXiv:2402.16311</a> [<a href="/pdf/2402.16311" title="Download PDF">pdf</a>, <a href="/format/2402.16311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-domain Chinese Sentence Pattern Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yingsi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Cunliang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liner Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meishan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haozhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Erhong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sentence Pattern Structure (SPS) parsing is a syntactic analysis method
primarily employed in language teaching.Existing SPS parsers rely heavily on
textbook corpora for training, lacking cross-domain capability.To overcome this
constraint, this paper proposes an innovative approach leveraging large
language models (LLMs) within a self-training framework. Partial syntactic
rules from a source domain are combined with target domain sentences to
dynamically generate training data, enhancing the adaptability of the parser to
diverse domains.Experiments conducted on textbook and news domains demonstrate
the effectiveness of the proposed method, outperforming rule-based baselines by
1.68 points on F1 metrics.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16312" title="Abstract">arXiv:2402.16312</a> [<a href="/pdf/2402.16312" title="Download PDF">pdf</a>, <a href="/format/2402.16312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Contextual Cascading Bandits with Asynchronous Communication  and Heterogeneous Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hantao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xutong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C. S. Lui</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the problem of federated contextual combinatorial cascading bandits,
where $|\mathcal{U}|$ agents collaborate under the coordination of a central
server to provide tailored recommendations to the $|\mathcal{U}|$ corresponding
users. Existing works consider either a synchronous framework, necessitating
full agent participation and global synchronization, or assume user homogeneity
with identical behaviors. We overcome these limitations by considering (1)
federated agents operating in an asynchronous communication paradigm, where no
mandatory synchronization is required and all agents communicate independently
with the server, (2) heterogeneous user behaviors, where users can be
stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting
distinct preferences. For this setting, we propose a UCB-type algorithm with
delicate communication protocols. Through theoretical analysis, we give
sub-linear regret bounds on par with those achieved in the synchronous
framework, while incurring only logarithmic communication costs. Empirical
evaluation on synthetic and real-world datasets validates our algorithm's
superior performance in terms of regrets and communication costs.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16313" title="Abstract">arXiv:2402.16313</a> [<a href="/pdf/2402.16313" title="Download PDF">pdf</a>, <a href="/format/2402.16313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingxu Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Open-ended question answering requires models to find appropriate evidence to
form well-reasoned, comprehensive and helpful answers. In practical
applications, models also need to engage in extended discussions on potential
scenarios closely relevant to the question. With augmentation of retrieval
module, open-source Large Language Models (LLMs) can produce coherent answers
often with different focuses, but are still sub-optimal in terms of reliable
evidence selection and in-depth question analysis. In this paper, we propose a
novel Chain-of-Discussion framework to leverage the synergy among multiple
open-source LLMs aiming to provide \textbf{more correct} and \textbf{more
comprehensive} answers for open-ended QA, although they are not strong enough
individually. Our experiments show that discussions among multiple LLMs play a
vital role in enhancing the quality of answers. We release our data and code at
\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16314" title="Abstract">arXiv:2402.16314</a> [<a href="/pdf/2402.16314" title="Download PDF">pdf</a>, <a href="/format/2402.16314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equational Bit-Vector Solving via Strong Gr&#xf6;bner Bases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongfei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Charles Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Bit-vectors, which are integers in a finite number of bits, are ubiquitous in
software and hardware systems. In this work, we consider the satisfiability
modulo theories (SMT) of bit-vectors. Unlike normal integers, the arithmetics
of bit-vectors are modular upon integer overflow. Therefore, the SMT solving of
bit-vectors needs to resolve the underlying modular arithmetics. In the
literature, two prominent approaches for SMT solving are bit-blasting (that
transforms the SMT problem into boolean satisfiability) and integer solving
(that transforms the SMT problem into integer properties). Both approaches
ignore the algebraic properties of the modular arithmetics and hence could not
utilize these properties to improve the efficiency of SMT solving.
<br />In this work, we consider the equational theory of bit-vectors and capture
the algebraic properties behind them via strong Gr\"obner bases. First, we
apply strong Gr\"obner bases to the quantifier-free equational theory of
bit-vectors and propose a novel algorithmic improvement in the key computation
of multiplicative inverse modulo a power of two. Second, we resolve the
important case of invariant generation in quantified equational bit-vector
properties via strong Gr\"obner bases and linear congruence solving.
Experimental results over an extensive range of benchmarks show that our
approach outperforms existing methods in both time efficiency and memory
consumption.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16315" title="Abstract">arXiv:2402.16315</a> [<a href="/pdf/2402.16315" title="Download PDF">pdf</a>, <a href="/format/2402.16315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finer: Investigating and Enhancing Fine-Grained Visual Concept  Recognition in Large Vision Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)
have imbued the models with the ability to generate high-level, image-grounded
explanations with ease. While such capability is largely attributed to the rich
world knowledge contained within the Large Language Models (LLMs), our work
reveals their shortcomings in fine-grained visual categorization (FGVC) across
six different benchmark settings. Most recent state-of-the-art LVLMs like
LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of
classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs
for LLaVA-1.5, but also struggle to generate an accurate explanation with
detailed attributes based on the concept that appears within an input image
despite their capability to generate holistic image-level descriptions.
In-depth analyses show that instruction-tuned LVLMs exhibit modality gap,
showing discrepancy when given textual and visual inputs that correspond to the
same concept, preventing the image modality from leveraging the rich parametric
knowledge within the LLMs. In an effort to further the community's endeavor in
this direction, we propose a multiple granularity attribute-centric evaluation
benchmark, Finer, which aims to establish a ground to evaluate LVLMs'
fine-grained visual comprehension ability and provide significantly improved
explainability.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16316" title="Abstract">arXiv:2402.16316</a> [<a href="/pdf/2402.16316" title="Download PDF">pdf</a>, <a href="/ps/2402.16316" title="Download PostScript">ps</a>, <a href="/format/2402.16316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial-Time Computation of Exact $&#x3a6;$-Equilibria in Polyhedral  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Pipis%2C+C">Charilaos Pipis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">It is a well-known fact that correlated equilibria can be computed in
polynomial time in a large class of concisely represented games using the
celebrated Ellipsoid Against Hope algorithm (Papadimitriou and Roughgarden,
2008; Jiang and Leyton-Brown, 2015). However, the landscape of efficiently
computable equilibria in sequential (extensive-form) games remains unknown. The
Ellipsoid Against Hope does not apply directly to these games, because they do
not have the required "polynomial type" property. Despite this barrier, Huang
and von Stengel (2008) altered the algorithm to compute exact extensive-form
correlated equilibria.
<br />In this paper, we generalize the Ellipsoid Against Hope and develop a simple
algorithmic framework for efficiently computing saddle-points in bilinear
zero-sum games, even when one of the dimensions is exponentially large.
Moreover, the framework only requires a "good-enough-response" oracle, which is
a weakened notion of a best-response oracle.
<br />Using this machinery, we develop a general algorithmic framework for
computing exact linear $\Phi$-equilibria in any polyhedral game (under mild
assumptions), including correlated equilibria in normal-form games, and
extensive-form correlated equilibria in extensive-form games. This enables us
to give the first polynomial-time algorithm for computing exact
linear-deviation correlated equilibria in extensive-form games, thus resolving
an open question by Farina and Pipis (2023). Furthermore, even for the cases
for which a polynomial time algorithm for exact equilibria was already known,
our framework provides a conceptually simpler solution.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16318" title="Abstract">arXiv:2402.16318</a> [<a href="/pdf/2402.16318" title="Download PDF">pdf</a>, <a href="/format/2402.16318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-Guided Modality Decoupling for Missing-Modality Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shengda Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Guosheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Multimodal learning with incomplete input data (missing modality) is
practical and challenging. In this work, we conduct an in-depth analysis of
this challenge and find that modality dominance has a significant negative
impact on the model training, greatly degrading the missing modality
performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients,
to monitor and reduce modality dominance which widely exists in the
missing-modality scenario. In aid of this indicator, we present a novel
Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on
dominating modalities. Specifically, GMD removes the conflicted gradient
components from different modalities to achieve this decoupling, significantly
improving the performance. In addition, to flexibly handle modal-incomplete
data, we design a parameter-efficient Dynamic Sharing (DS) framework which can
adaptively switch on/off the network parameters based on whether one modality
is available. We conduct extensive experiments on three popular multimodal
benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and
CMU-MOSEI for sentiment analysis. The results show that our method can
significantly outperform the competitors, showing the effectiveness of the
proposed solutions. Our code is released here:
https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16319" title="Abstract">arXiv:2402.16319</a> [<a href="/pdf/2402.16319" title="Download PDF">pdf</a>, <a href="/format/2402.16319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-freeWeight Compress and Denoise for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Runyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are reshaping the research landscape in
artificial intelligence, particularly as model parameters scale up
significantly, unlocking remarkable capabilities across various domains.
Nevertheless, the scalability of model parameters faces constraints due to
limitations in GPU memory and computational speed. To address these
constraints, various weight compression methods have emerged, such as Pruning
and Quantization. Given the low-rank nature of weight matrices in language
models, the reduction of weights through matrix decomposition undoubtedly holds
significant potential and promise. In this paper, drawing upon the intrinsic
structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k
Approximation for compressing the parameter matrices. Significantly, our method
is characterized by without necessitating additional involvement of any corpus,
while simultaneously preserving orthogonality in conjunction with pruning and
quantization methods. We achieve a model pruning of 80% parameters while
retaining 93.43% of the original performance without any calibration data.
Additionally, we explore the fundamental properties of the weight matrix of
LLMs undergone Rank-k Approximation and conduct comprehensive experiments to
elucidate our hypothesis.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16321" title="Abstract">arXiv:2402.16321</a> [<a href="/pdf/2402.16321" title="Download PDF">pdf</a>, <a href="/format/2402.16321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Speech Quality Estimation and Enhancement Using Only  Clean Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Szu-Wei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+K">Kuo-Hsuan Hung</a>, 
<a href="/search/cs?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+F">Yu-Chiang Frank Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech quality estimation has recently undergone a paradigm shift from
human-hearing expert designs to machine-learning models. However, current
models rely mainly on supervised learning, which is time-consuming and
expensive for label collection. To solve this problem, we propose VQScore, a
self-supervised metric for evaluating speech based on the quantization error of
a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE
relies on clean speech; hence, large quantization errors can be expected when
the speech is distorted. To further improve correlation with real quality
scores, domain knowledge of speech processing is incorporated into the model
design. We found that the vector quantization mechanism could also be used for
self-supervised speech enhancement (SE) model training. To improve the
robustness of the encoder for SE, a novel self-distillation mechanism combined
with adversarial training is introduced. In summary, the proposed speech
quality estimation method and enhancement models require only clean speech for
training without any label requirements. Experimental results show that the
proposed VQScore and enhancement model are competitive with supervised
baselines. The code will be released after publication.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16323" title="Abstract">arXiv:2402.16323</a> [<a href="/pdf/2402.16323" title="Download PDF">pdf</a>, <a href="/format/2402.16323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for Halfplane Coverage and Related Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haitao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jie Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SoCG 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Given in the plane a set of points and a set of halfplanes, we consider the
problem of computing a smallest subset of halfplanes whose union covers all
points. In this paper, we present an $O(n^{4/3}\log^{5/3}n\log^{O(1)}\log
n)$-time algorithm for the problem, where $n$ is the total number of all points
and halfplanes. This improves the previously best algorithm of
$n^{10/3}2^{O(\log^*n)}$ time by roughly a quadratic factor. For the special
case where all halfplanes are lower ones, our algorithm runs in $O(n\log n)$
time, which improves the previously best algorithm of $n^{4/3}2^{O(\log^*n)}$
time and matches an $\Omega(n\log n)$ lower bound. Further, our techniques can
be extended to solve a star-shaped polygon coverage problem in $O(n\log n)$
time, which in turn leads to an $O(n\log n)$-time algorithm for computing an
instance-optimal $\epsilon$-kernel of a set of $n$ points in the plane. Agarwal
and Har-Peled presented an $O(nk\log n)$-time algorithm for this problem in
SoCG 2023, where $k$ is the size of the $\epsilon$-kernel; they also raised an
open question whether the problem can be solved in $O(n\log n)$ time. Our
result thus answers the open question affirmatively.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16324" title="Abstract">arXiv:2402.16324</a> [<a href="/pdf/2402.16324" title="Download PDF">pdf</a>, <a href="/ps/2402.16324" title="Download PostScript">ps</a>, <a href="/format/2402.16324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving $\tilde{O}(1/&#x3b5;)$ Sample Complexity for Constrained  Markov Decision Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiashuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the reinforcement learning problem for the constrained Markov
decision process (CMDP), which plays a central role in satisfying safety or
resource constraints in sequential learning and decision-making. In this
problem, we are given finite resources and a MDP with unknown transition
probabilities. At each stage, we take an action, collecting a reward and
consuming some resources, all assumed to be unknown and need to be learned over
time. In this work, we take the first step towards deriving optimal
problem-dependent guarantees for the CMDP problems. We derive a logarithmic
regret bound, which translates into a
$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound,
with $\kappa$ being a problem-dependent parameter, yet independent of
$\epsilon$. Our sample complexity bound improves upon the state-of-art
$O(1/\epsilon^2)$ sample complexity for CMDP problems established in the
previous literature, in terms of the dependency on $\epsilon$. To achieve this
advance, we develop a new framework for analyzing CMDP problems. To be
specific, our algorithm operates in the primal space and we resolve the primal
LP for the CMDP problem at each period in an online manner, with
\textit{adaptive} remaining resource capacities. The key elements of our
algorithm are: i). an eliminating procedure that characterizes one optimal
basis of the primal LP, and; ii) a resolving procedure that is adaptive to the
remaining resources and sticks to the characterized optimal basis.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16325" title="Abstract">arXiv:2402.16325</a> [<a href="/pdf/2402.16325" title="Download PDF">pdf</a>, <a href="/format/2402.16325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence Calibration for Recommender Systems and Its Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+W">Wonbin Kweon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Doctoral Dissertation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Despite the importance of having a measure of confidence in recommendation
results, it has been surprisingly overlooked in the literature compared to the
accuracy of the recommendation. In this dissertation, I propose a model
calibration framework for recommender systems for estimating accurate
confidence in recommendation results based on the learned ranking scores.
Moreover, I subsequently introduce two real-world applications of confidence on
recommendations: (1) Training a small student model by treating the confidence
of a big teacher model as additional learning guidance, (2) Adjusting the
number of presented items based on the expected user utility estimated with
calibrated probability.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16327" title="Abstract">arXiv:2402.16327</a> [<a href="/pdf/2402.16327" title="Download PDF">pdf</a>, <a href="/format/2402.16327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Rating Elicitation for New Users in Collaborative Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+W">Wonbin Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">SeongKu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Junyoung Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hwanjo Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recent recommender systems started to use rating elicitation, which asks new
users to rate a small seed itemset for inferring their preferences, to improve
the quality of initial recommendations. The key challenge of the rating
elicitation is to choose the seed items which can best infer the new users'
preference. This paper proposes a novel end-to-end Deep learning framework for
Rating Elicitation (DRE), that chooses all the seed items at a time with
consideration of the non-linear interactions. To this end, it first defines
categorical distributions to sample seed items from the entire itemset, then it
trains both the categorical distributions and a neural reconstruction network
to infer users' preferences on the remaining items from CF information of the
sampled seed items. Through the end-to-end training, the categorical
distributions are learned to select the most representative seed items while
reflecting the complex non-linear interactions. Experimental results show that
DRE outperforms the state-of-the-art approaches in the recommendation quality
by accurately inferring the new users' preferences and its seed itemset better
represents the latent space than the seed itemset obtained by the other
methods.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16328" title="Abstract">arXiv:2402.16328</a> [<a href="/pdf/2402.16328" title="Download PDF">pdf</a>, <a href="/format/2402.16328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Joint Communication and Computation Design for Probabilistic Semantic  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhouxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingzhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, the problem of joint transmission and computation resource
allocation for a multi-user probabilistic semantic communication (PSC) network
is investigated. In the considered model, users employ semantic information
extraction techniques to compress their large-sized data before transmitting
them to a multi-antenna base station (BS). Our model represents large-sized
data through substantial knowledge graphs, utilizing shared probability graphs
between the users and the BS for efficient semantic compression. The resource
allocation problem is formulated as an optimization problem with the objective
of maximizing the sum of equivalent rate of all users, considering total power
budget and semantic resource limit constraints. The computation load considered
in the PSC network is formulated as a non-smooth piecewise function with
respect to the semantic compression ratio. To tackle this non-convex non-smooth
optimization challenge, a three-stage algorithm is proposed where the solutions
for the receive beamforming matrix of the BS, transmit power of each user, and
semantic compression ratio of each user are obtained stage by stage. Numerical
results validate the effectiveness of our proposed scheme.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16333" title="Abstract">arXiv:2402.16333</a> [<a href="/pdf/2402.16333" title="Download PDF">pdf</a>, <a href="/format/2402.16333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Truth and Facilitating Change: Towards Agent-based  Large-scale Social Movement Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mou%2C+X">Xinyi Mou</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Social media has emerged as a cornerstone of social movements, wielding
significant influence in driving societal change. Simulating the response of
the public and forecasting the potential impact has become increasingly
important. However, existing methods for simulating such phenomena encounter
challenges concerning their efficacy and efficiency in capturing the behaviors
of social movement participants. In this paper, we introduce a hybrid framework
for social media user simulation, wherein users are categorized into two types.
Core users are driven by Large Language Models, while numerous ordinary users
are modeled by deductive agent-based models. We further construct a
Twitter-like environment to replicate their response dynamics following trigger
events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for
evaluation and conduct comprehensive experiments across real-world datasets.
Experimental results demonstrate the effectiveness and flexibility of our
method.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16338" title="Abstract">arXiv:2402.16338</a> [<a href="/pdf/2402.16338" title="Download PDF">pdf</a>, <a href="/format/2402.16338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning  of SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Youwei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengtao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The Segment Anything Model (SAM), a foundation model pretrained on millions
of images and segmentation masks, has significantly advanced semantic
segmentation, a fundamental task in computer vision. Despite its strengths, SAM
encounters two major challenges. Firstly, it struggles with segmenting specific
objects autonomously, as it relies on users to manually input prompts like
points or bounding boxes to identify targeted objects. Secondly, SAM faces
challenges in excelling at specific downstream tasks, like medical imaging, due
to a disparity between the distribution of its pretraining data, which
predominantly consists of general-domain images, and the data used in
downstream tasks. Current solutions to these problems, which involve finetuning
SAM, often lead to overfitting, a notable issue in scenarios with very limited
data, like in medical imaging. To overcome these limitations, we introduce
BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach
allows for automatic image segmentation without the need for manual prompts, by
optimizing a learnable prompt embedding. Furthermore, it significantly reduces
the risk of overfitting by training the model's weight parameters and the
prompt embedding on two separate subsets of the training dataset, each at a
different level of optimization. We apply BLO-SAM to diverse semantic
segmentation tasks in general and medical domains. The results demonstrate
BLO-SAM's superior performance over various state-of-the-art image semantic
segmentation methods.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16342" title="Abstract">arXiv:2402.16342</a> [<a href="/pdf/2402.16342" title="Download PDF">pdf</a>, <a href="/format/2402.16342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contingency Planning Using Bi-level Markov Decision Processes for Space  Missions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somrita Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Balaban%2C+E">Edward Balaban</a>, 
<a href="/search/cs?searchtype=author&query=Shirley%2C+M">Mark Shirley</a>, 
<a href="/search/cs?searchtype=author&query=Bradner%2C+K">Kevin Bradner</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This work focuses on autonomous contingency planning for scientific missions
by enabling rapid policy computation from any off-nominal point in the state
space in the event of a delay or deviation from the nominal mission plan.
Successful contingency planning involves managing risks and rewards, often
probabilistically associated with actions, in stochastic scenarios. Markov
Decision Processes (MDPs) are used to mathematically model decision-making in
such scenarios. However, in the specific case of planetary rover traverse
planning, the vast action space and long planning time horizon pose
computational challenges. A bi-level MDP framework is proposed to improve
computational tractability, while also aligning with existing mission planning
practices and enhancing explainability and trustworthiness of AI-driven
solutions. We discuss the conversion of a mission planning MDP into a bi-level
MDP, and test the framework on RoverGridWorld, a modified GridWorld environment
for rover mission planning. We demonstrate the computational tractability and
near-optimal policies achievable with the bi-level MDP approach, highlighting
the trade-offs between compute time and policy optimality as the problem's
complexity grows. This work facilitates more efficient and flexible contingency
planning in the context of scientific missions.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16343" title="Abstract">arXiv:2402.16343</a> [<a href="/pdf/2402.16343" title="Download PDF">pdf</a>, <a href="/format/2402.16343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Boyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingyu Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Hybrid main memory systems combine both performance and capacity advantages
from heterogeneous memory technologies. With larger capacities, higher
associativities, and finer granularities, hybrid memory systems currently
exhibit significant metadata storage and lookup overheads for flexibly
remapping data blocks between the two memory tiers. To alleviate the
inefficiencies of existing designs, we propose Trimma, the combination of a
multi-level metadata structure and an efficient metadata cache design. Trimma
uses a multi-level metadata table to only track truly necessary address remap
entries. The saved memory space is effectively utilized as extra DRAM cache
capacity to improve performance. Trimma also uses separate formats to store the
entries with non-identity and identity mappings. This improves the overall
remap cache hit rate, further boosting the performance. Trimma is transparent
to software and compatible with various types of hybrid memory systems. When
evaluated on a representative DDR4 + NVM hybrid memory system, Trimma achieves
up to 2.4$\times$ and on average 58.1\% speedup benefits, compared with a
state-of-the-art design that only leverages the unallocated fast memory space
for caching. Trimma addresses metadata management overheads and targets future
scalable large-scale hybrid memory architectures.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16346" title="Abstract">arXiv:2402.16346</a> [<a href="/pdf/2402.16346" title="Download PDF">pdf</a>, <a href="/format/2402.16346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Graph Pooling with Persistent Homology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+C">Chaolong Ying</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinjian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">Recently, there has been an emerging trend to integrate persistent homology
(PH) into graph neural networks (GNNs) to enrich expressive power. However,
naively plugging PH features into GNN layers always results in marginal
improvement with low interpretability. In this paper, we investigate a novel
mechanism for injecting global topological invariance into pooling layers using
PH, motivated by the observation that filtration operation in PH naturally
aligns graph pooling in a cut-off manner. In this fashion, message passing in
the coarsened graph acts along persistent pooled topology, leading to improved
performance. Experimentally, we apply our mechanism to a collection of graph
pooling methods and observe consistent and substantial performance gain over
several popular datasets, demonstrating its wide applicability and flexibility.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16347" title="Abstract">arXiv:2402.16347</a> [<a href="/pdf/2402.16347" title="Download PDF">pdf</a>, <a href="/format/2402.16347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeS: Towards Building Open-source Language Models for Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanbing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Ju Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Renjie Wei</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hongyan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cuiping Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGMOD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Language models have shown promising performance on the task of translating
natural language questions into SQL queries (Text-to-SQL). However, most of the
state-of-the-art (SOTA) approaches rely on powerful yet closed-source large
language models (LLMs), such as ChatGPT and GPT-4, which may have the
limitations of unclear model architectures, data privacy risks, and expensive
inference overheads. To address the limitations, we introduce CodeS, a series
of pre-trained language models with parameters ranging from 1B to 15B,
specifically designed for the text-to-SQL task. CodeS is a fully open-source
language model, which achieves superior accuracy with much smaller parameter
sizes. This paper studies the research challenges in building CodeS. To enhance
the SQL generation abilities of CodeS, we adopt an incremental pre-training
approach using a specifically curated SQL-centric corpus. Based on this, we
address the challenges of schema linking and rapid domain adaptation through
strategic prompt construction and a bi-directional data augmentation technique.
We conduct comprehensive evaluations on multiple datasets, including the widely
used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic
benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as
well as two real-world datasets created for financial and academic
applications. The experimental results show that our CodeS achieves new SOTA
accuracy and robustness on nearly all challenging text-to-SQL benchmarks.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16348" title="Abstract">arXiv:2402.16348</a> [<a href="/pdf/2402.16348" title="Download PDF">pdf</a>, <a href="/format/2402.16348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Star-Searcher: A Complete and Efficient Aerial System for Autonomous  Target Search in Complex Unknown Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yiming Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zixuan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+N">Neng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Boyu Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE RA-L. Code: <a href="https://github.com/SYSU-STAR/STAR-Searcher.">this https URL</a> Video: <a href="https://www.youtube.com/watch?v=08ll_oo_DtU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper tackles the challenge of autonomous target search using unmanned
aerial vehicles (UAVs) in complex unknown environments. To fill the gap in
systematic approaches for this task, we introduce Star-Searcher, an aerial
system featuring specialized sensor suites, mapping, and planning modules to
optimize searching. Path planning challenges due to increased inspection
requirements are addressed through a hierarchical planner with a
visibility-based viewpoint clustering method. This simplifies planning by
breaking it into global and local sub-problems, ensuring efficient global and
local path coverage in real-time. Furthermore, our global path planning employs
a history-aware mechanism to reduce motion inconsistency from frequent map
changes, significantly enhancing search efficiency. We conduct comparisons with
state-of-the-art methods in both simulation and the real world, demonstrating
shorter flight paths, reduced time, and higher target search completeness. Our
approach will be open-sourced for community benefit at
https://github.com/SYSU-STAR/STAR-Searcher.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16349" title="Abstract">arXiv:2402.16349</a> [<a href="/pdf/2402.16349" title="Download PDF">pdf</a>, <a href="/format/2402.16349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C-GAIL: Stabilizing Generative Adversarial Imitation Learning with  Control Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tianjiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+T">Tim Pearce</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Generative Adversarial Imitation Learning (GAIL) trains a generative policy
to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to
optimize a reward signal derived from a GAN-like discriminator. A major
drawback of GAIL is its training instability - it inherits the complex training
dynamics of GANs, and the distribution shift introduced by RL. This can cause
oscillations during training, harming its sample efficiency and final policy
performance. Recent work has shown that control theory can help with the
convergence of a GAN's training. This paper extends this line of work,
conducting a control-theoretic analysis of GAIL and deriving a novel controller
that not only pushes GAIL to the desired equilibrium but also achieves
asymptotic stability in a 'one-step' setting. Based on this, we propose a
practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled
variant is able to speed up the rate of convergence, reduce the range of
oscillation and match the expert's distribution more closely both for vanilla
GAIL and GAIL-DAC.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16350" title="Abstract">arXiv:2402.16350</a> [<a href="/pdf/2402.16350" title="Download PDF">pdf</a>, <a href="/format/2402.16350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubota%2C+Y">Yugo Kubota</a>, 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fonts convey different impressions to readers. These impressions often come
from the font shapes. However, the correlation between fonts and their
impression is weak and unstable because impressions are subjective. To capture
such weak and unstable cross-modal correlation between font shapes and their
impressions, we propose Impression-CLIP, which is a novel machine-learning
model based on CLIP (Contrastive Language-Image Pre-training). By using the
CLIP-based model, font image features and their impression features are pulled
closer, and font image features and unrelated impression features are pushed
apart. This procedure realizes co-embedding between font image and their
impressions. In our experiment, we perform cross-modal retrieval between fonts
and impressions through co-embedding. The results indicate that Impression-CLIP
achieves better retrieval accuracy than the state-of-the-art method.
Additionally, our model shows the robustness to noise and missing tags.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16352" title="Abstract">arXiv:2402.16352</a> [<a href="/pdf/2402.16352" title="Download PDF">pdf</a>, <a href="/format/2402.16352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathGenie: Generating Synthetic Data with Question Back-translation for  Enhancing Mathematical Reasoning of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zimu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aojun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Houxing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weikang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have exhibited great potential in mathematical
reasoning. However, there remains a performance gap in this area between
existing open-source models and closed-source models such as GPT-4. In this
paper, we introduce MathGenie, a novel method for generating diverse and
reliable math problems from a small-scale problem-solution dataset (denoted as
seed data). We augment the ground-truth solutions of our seed data and train a
back-translation model to translate the augmented solutions back into new
questions. Subsequently, we generate code-integrated solutions for the new
questions. To ensure the correctness of the code-integrated solutions, we
employ rationale-based strategy for solution verification. Various pretrained
models, ranging from 7B to 70B, are trained on the newly curated data to test
the effectiveness of the proposed augmentation technique, resulting in a family
of models known as MathGenieLM. These models consistently outperform previous
open-source models across five representative mathematical reasoning datasets,
achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2
achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best
overall score among open-source language models.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16354" title="Abstract">arXiv:2402.16354</a> [<a href="/pdf/2402.16354" title="Download PDF">pdf</a>, <a href="/format/2402.16354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-guided Skill Learning with Temporal Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haotian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pratyusha Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Konidaris%2C+G">George Konidaris</a>, 
<a href="/search/cs?searchtype=author&query=Roux%2C+N+L">Nicolas Le Roux</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre C&#xf4;t&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We present an algorithm for skill discovery from expert demonstrations. The
algorithm first utilizes Large Language Models (LLMs) to propose an initial
segmentation of the trajectories. Following that, a hierarchical variational
inference framework incorporates the LLM-generated segmentation information to
discover reusable skills by merging trajectory segments. To further control the
trade-off between compression and reusability, we introduce a novel auxiliary
objective based on the Minimum Description Length principle that helps guide
this skill discovery process. Our results demonstrate that agents equipped with
our method are able to discover skills that help accelerate learning and
outperform baseline skill learning approaches on new long-horizon tasks in
BabyAI, a grid world navigation environment, as well as ALFRED, a household
simulation environment.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16356" title="Abstract">arXiv:2402.16356</a> [<a href="/pdf/2402.16356" title="Download PDF">pdf</a>, <a href="/format/2402.16356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Text Design Characterizes Book Genres?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Iwana%2C+B+K">Brian Kenji Iwana</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This study analyzes the relationship between non-verbal information (e.g.,
genres) and text design (e.g., font style, character color, etc.) through the
classification of book genres using text design on book covers. Text images
have both semantic information about the word itself and other information
(non-semantic information or visual design), such as font style, character
color, etc. When we read a word printed on some materials, we receive
impressions or other information from both the word itself and the visual
design. Basically, we can understand verbal information only from semantic
information, i.e., the words themselves; however, we can consider that text
design is helpful for understanding other additional information (i.e.,
non-verbal information), such as impressions, genre, etc. To investigate the
effect of text design, we analyze text design using words printed on book
covers and their genres in two scenarios. First, we attempted to understand the
importance of visual design for determining the genre (i.e., non-verbal
information) of books by analyzing the differences in the relationship between
semantic information/visual design and genres. In the experiment, we found that
semantic information is sufficient to determine the genre; however, text design
is helpful in adding more discriminative features for book genres. Second, we
investigated the effect of each text design on book genres. As a result, we
found that each text design characterizes some book genres. For example, font
style is useful to add more discriminative features for genres of ``Mystery,
Thriller \&amp; Suspense'' and ``Christian books \&amp; Bibles.''
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16358" title="Abstract">arXiv:2402.16358</a> [<a href="/pdf/2402.16358" title="Download PDF">pdf</a>, <a href="/format/2402.16358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrated Data Processing Framework for Pretraining Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiding Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiaxin Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The ability of the foundation models heavily relies on large-scale, diverse,
and high-quality pretraining data. In order to improve data quality,
researchers and practitioners often have to manually curate datasets from
difference sources and develop dedicated data cleansing pipeline for each data
repository. Lacking a unified data processing framework, this process is
repetitive and cumbersome. To mitigate this issue, we propose a data processing
framework that integrates a Processing Module which consists of a series of
operators at different granularity levels, and an Analyzing Module which
supports probing and evaluation of the refined data. The proposed framework is
easy to use and highly flexible. In this demo paper, we first introduce how to
use this framework with some example use cases and then demonstrate its
effectiveness in improving the data quality with an automated evaluation with
ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code
and demonstration videos are accessible on GitHub.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16359" title="Abstract">arXiv:2402.16359</a> [<a href="/pdf/2402.16359" title="Download PDF">pdf</a>, <a href="/format/2402.16359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback Efficient Online Fine-Tuning of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yulai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Hajiramezanali%2C+E">Ehsan Hajiramezanali</a>, 
<a href="/search/cs?searchtype=author&query=Scalia%2C+G">Gabriele Scalia</a>, 
<a href="/search/cs?searchtype=author&query=Diamant%2C+N+L">Nathaniel Lee Diamant</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+A+M">Alex M Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>, 
<a href="/search/cs?searchtype=author&query=Biancalani%2C+T">Tommaso Biancalani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review (codes will be released soon)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models excel at modeling complex data distributions, including
those of images, proteins, and small molecules. However, in many cases, our
goal is to model parts of the distribution that maximize certain properties:
for example, we may want to generate images with high aesthetic quality, or
molecules with high bioactivity. It is natural to frame this as a reinforcement
learning (RL) problem, in which the objective is to fine-tune a diffusion model
to maximize a reward function that corresponds to some property. Even with
access to online queries of the ground-truth reward function, efficiently
discovering high-reward samples can be challenging: they might have a low
probability in the initial distribution, and there might be many infeasible
samples that do not even have a well-defined reward (e.g., unnatural images or
physically impossible molecules). In this work, we propose a novel
reinforcement learning procedure that efficiently explores on the manifold of
feasible samples. We present a theoretical analysis providing a regret
guarantee, as well as empirical validation across three domains: images,
biological sequences, and molecules.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16361" title="Abstract">arXiv:2402.16361</a> [<a href="/pdf/2402.16361" title="Download PDF">pdf</a>, <a href="/format/2402.16361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Layer-wise Regularized Dropout for Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiwen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Among the various pre-trained neural language models that are popular today,
dropout is already an indispensable regularization technique. To solve the
inconsistency between training and inference caused by the randomness of
dropout, some studies use consistency training to regularize dropout at the
output layer. In this paper, we propose a novel Layer-wise Regularized Dropout
(LR-Drop), which is specially designed for Transformer-based Language models.
Specifically, LR-Drop layer-wise regularizes each Transformer layer using the
consistency training strategy. Each training sample passes through the two
siamese sub-models sampled by dropout, and then LR-Drop forces the hidden
states, multi-head attention matrices, and output distribution of the two
siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a
"self-distillation" framework, in which each sub-model generated by dropout is
the other's "teacher" model and "student" model. Through extensive experiments
on 8 natural language understanding datasets, 6 neural machine translation
datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we
show that LR-Drop achieves superior performances, including state-of-the-art
results.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16363" title="Abstract">arXiv:2402.16363</a> [<a href="/pdf/2402.16363" title="Download PDF">pdf</a>, <a href="/format/2402.16363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Inference Unveiled: Survey and Roofline Model Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Yuzhang Shang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chenhao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qingyi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+J">Yong Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The field of efficient Large Language Model (LLM) inference is rapidly
evolving, presenting a unique blend of opportunities and challenges. Although
the field has expanded and is vibrant, there hasn't been a concise framework
that analyzes the various methods of LLM Inference to provide a clear
understanding of this domain. Our survey stands out from traditional literature
reviews by not only summarizing the current state of research but also by
introducing a framework based on roofline model for systematic analysis of LLM
inference techniques. This framework enables identifying the bottlenecks in LLM
deployments and provides a deeper understanding of the practical aspects on
real devices, thereby informing more effective strategies for deploying LLM.
Furthermore, we systematically collate the latest advancements in efficient LLM
inference, covering crucial areas such as weight optimization (e.g., Knowledge
Distillation and Quantization), decoding algorithm improvements (e.g., Early
Exit and Mixture-of-Expert), and both hardware and system-level enhancements.
Distinguished by the integration of roofline model analysis, our survey
provides a comprehensive and nuanced exploration of efficient LLM inference
challenges and solutions. This distinctive approach not only showcases the
current research landscape but also delivers valuable insights for practical
implementation, positioning our work as an indispensable resource for
researchers new to the field as well as for those seeking to deepen their
understanding of efficient LLM deployment. The tool LLM-Viewer is open-sourced.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16364" title="Abstract">arXiv:2402.16364</a> [<a href="/pdf/2402.16364" title="Download PDF">pdf</a>, <a href="/format/2402.16364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where Do We Go from Here? Multi-scale Allocentric Relational Inference  from Natural Spatial Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paz-Argaman%2C+T">Tzuf Paz-Argaman</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sayali Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Palowitch%2C+J">John Palowitch</a>, 
<a href="/search/cs?searchtype=author&query=Baldridge%2C+J">Jason Baldridge</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">When communicating routes in natural language, the concept of {\em acquired
spatial knowledge} is crucial for geographic information retrieval (GIR) and in
spatial cognitive research. However, NLP navigation studies often overlook the
impact of such acquired knowledge on textual descriptions. Current navigation
studies concentrate on egocentric local descriptions (e.g., `it will be on your
right') that require reasoning over the agent's local perception. These
instructions are typically given as a sequence of steps, with each action-step
explicitly mentioning and being followed by a landmark that the agent can use
to verify they are on the right path (e.g., `turn right and then you will
see...'). In contrast, descriptions based on knowledge acquired through a map
provide a complete view of the environment and capture its overall structure.
These instructions (e.g., `it is south of Central Park and a block north of a
police station') are typically non-sequential, contain allocentric relations,
with multiple spatial relations and implicit actions, without any explicit
verification. This paper introduces the Rendezvous (RVS) task and dataset,
which includes 10,404 examples of English geospatial instructions for reaching
a target location using map-knowledge. Our analysis reveals that RVS exhibits a
richer use of spatial allocentric relations, and requires resolving more
spatial relations simultaneously compared to previous text-based navigation
benchmarks.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16366" title="Abstract">arXiv:2402.16366</a> [<a href="/pdf/2402.16366" title="Download PDF">pdf</a>, <a href="/format/2402.16366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zetian Song</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+W">Wenhong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Siwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wen Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Representing the Neural Radiance Field (NeRF) with the explicit voxel grid
(EVG) is a promising direction for improving NeRFs. However, the EVG
representation is not efficient for storage and transmission because of the
terrific memory cost. Current methods for compressing EVG mainly inherit the
methods designed for neural network compression, such as pruning and
quantization, which do not take full advantage of the spatial correlation of
voxels. Inspired by prosperous digital image compression techniques, this paper
proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG
compression. The proposed framework can remove spatial redundancy efficiently
for better compression performance.Moreover, we model the bitrate and design a
novel form of the loss function, where we can jointly optimize compression
ratio and distortion to achieve higher coding efficiency. Extensive experiments
demonstrate that our method can achieve 32% bit saving compared to the
state-of-the-art method VQRF on multiple representative test datasets, with
comparable training time.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16367" title="Abstract">arXiv:2402.16367</a> [<a href="/pdf/2402.16367" title="Download PDF">pdf</a>, <a href="/format/2402.16367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling Babel: Exploring Multilingual Activation Patterns within  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weize Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongxia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jintai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, large language models (LLMs) have achieved tremendous breakthroughs
in the field of language processing, yet their mechanisms in processing
multiple languages remain agnostic. Therefore, in this work we study the
multilingual activation patterns of LLMs. By transforming the original Large
Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze
the expert activation patterns when processing various languages and
demonstrate the connections of these activation patterns at the level of
language families. We discover the existence of non-language-specific neurons
as well as language-specific activation neurons. Further exploration even
showcases that merely leveraging high-frequency activation neurons can
accelerate inference while maintaining comparable performance. These findings
shed light on the LLMs' multilingual processing mechanism, and are of
significant importance in guiding the multilingual training and model pruning
of LLMs.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16369" title="Abstract">arXiv:2402.16369</a> [<a href="/pdf/2402.16369" title="Download PDF">pdf</a>, <a href="/format/2402.16369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI in Vision: A Survey on Models, Metrics and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raut%2C+G">Gaurav Raut</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Apoorv Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative AI models have revolutionized various fields by enabling the
creation of realistic and diverse data samples. Among these models, diffusion
models have emerged as a powerful approach for generating high-quality images,
text, and audio. This survey paper provides a comprehensive overview of
generative AI diffusion and legacy models, focusing on their underlying
techniques, applications across different domains, and their challenges. We
delve into the theoretical foundations of diffusion models, including concepts
such as denoising diffusion probabilistic models (DDPM) and score-based
generative modeling. Furthermore, we explore the diverse applications of these
models in text-to-image, image inpainting, and image super-resolution, along
with others, showcasing their potential in creative tasks and data
augmentation. By synthesizing existing research and highlighting critical
advancements in this field, this survey aims to provide researchers and
practitioners with a comprehensive understanding of generative AI diffusion and
legacy models and inspire future innovations in this exciting area of
artificial intelligence.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16370" title="Abstract">arXiv:2402.16370</a> [<a href="/pdf/2402.16370" title="Download PDF">pdf</a>, <a href="/format/2402.16370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEYO: DETR with YOLO for End-to-End Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+H">Haodong Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2309.11851">arXiv:2309.11851</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The training paradigm of DETRs is heavily contingent upon pre-training their
backbone on the ImageNet dataset. However, the limited supervisory signals
provided by the image classification task and one-to-one matching strategy
result in an inadequately pre-trained neck for DETRs. Additionally, the
instability of matching in the early stages of training engenders
inconsistencies in the optimization objectives of DETRs. To address these
issues, we have devised an innovative training methodology termed step-by-step
training. Specifically, in the first stage of training, we employ a classic
detector, pre-trained with a one-to-many matching strategy, to initialize the
backbone and neck of the end-to-end detector. In the second stage of training,
we froze the backbone and neck of the end-to-end detector, necessitating the
training of the decoder from scratch. Through the application of step-by-step
training, we have introduced the first real-time end-to-end object detection
model that utilizes a purely convolutional structure encoder, DETR with YOLO
(DEYO). Without reliance on any supplementary training data, DEYO surpasses all
existing real-time object detectors in both speed and accuracy. Moreover, the
comprehensive DEYO series can complete its second-phase training on the COCO
dataset using a single 8GB RTX 4060 GPU, significantly reducing the training
expenditure. Source code and pre-trained models are available at
https://github.com/ouyanghaodong/DEYO.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16372" title="Abstract">arXiv:2402.16372</a> [<a href="/pdf/2402.16372" title="Download PDF">pdf</a>, <a href="/format/2402.16372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laue%2C+F">Friedemann Laue</a>, 
<a href="/search/cs?searchtype=author&query=Jamali%2C+V">Vahid Jamali</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Efficient beam training is the key challenge in the codebook-based
configuration of reconfigurable intelligent surfaces (RISs) because the beam
training overhead can have a strong impact on the achievable system
performance. In this paper, we study the performance tradeoff between overhead
and achievable signal-to-noise ratio (SNR) in RIS beam training while taking
into account the size of the targeted coverage area, the RIS response time, and
the delay for feedback transmissions. Thereby, we consider three common beam
training strategies: full search (FS), hierarchical search (HS), and
tracking-based search (TS). Our analysis shows that the codebook-based
illumination of a given coverage area can be realized with wide- or narrow-beam
designs, which result in two different scaling laws for the achievable SNR.
Similarly, there are two regimes for the overhead, where the number of pilot
symbols required for reliable beam training is dependent on and independent of
the SNR, respectively. Based on these insights, we investigate the impact of
the beam training overhead on the effective rate and provide an upper bound on
the user velocity for which the overhead is negligible. Moreover, when the
overhead is not negligible, we show that TS beam training achieves higher
effective rates than HS and FS beam training, while HS beam training may or may
not outperform FS beam training, depending on the RIS response time, feedback
delay, and codebook size. Finally, we present numerical simulation results that
verify our theoretical analysis. In particular, our results confirm the
existence of the proposed regimes, reveal that fast RISs can lead to negligible
overhead for FS beam training, and show that large feedback delays can
significantly reduce the performance for HS beam training.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16374" title="Abstract">arXiv:2402.16374</a> [<a href="/pdf/2402.16374" title="Download PDF">pdf</a>, <a href="/format/2402.16374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Learning under Distribution Shifts: A Comprehensive Survey on  Domain Adaptation, Out-of-distribution, and Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Man Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingquan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph learning plays a pivotal role and has gained significant attention in
various application scenarios, from social network analysis to recommendation
systems, for its effectiveness in modeling complex data relations represented
by graph structural data. In reality, the real-world graph data typically show
dynamics over time, with changing node attributes and edge structure, leading
to the severe graph data distribution shift issue. This issue is compounded by
the diverse and complex nature of distribution shifts, which can significantly
impact the performance of graph learning methods in degraded generalization and
adaptation capabilities, posing a substantial challenge to their effectiveness.
In this survey, we provide a comprehensive review and summary of the latest
approaches, strategies, and insights that address distribution shifts within
the context of graph learning. Concretely, according to the observability of
distributions in the inference stage and the availability of sufficient
supervision information in the training stage, we categorize existing graph
learning methods into several essential scenarios, including graph domain
adaptation learning, graph out-of-distribution learning, and graph continual
learning. For each scenario, a detailed taxonomy is proposed, with specific
descriptions and discussions of existing progress made in distribution-shifted
graph learning. Additionally, we discuss the potential applications and future
directions for graph learning under distribution shifts with a systematic
analysis of the current state in this field. The survey is positioned to
provide general guidance for the development of effective graph learning
algorithms in handling graph distribution shifts, and to stimulate future
research and advancements in this area.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16379" title="Abstract">arXiv:2402.16379</a> [<a href="/pdf/2402.16379" title="Download PDF">pdf</a>, <a href="/format/2402.16379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving LLM-based Machine Translation with Systematic Self-Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhaopeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenqiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+J">Jun Lang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved impressive results in Machine
Translation (MT). However, careful evaluations by human reveal that the
translations produced by LLMs still contain multiple errors. Importantly,
feeding back such error information into the LLMs can lead to self-correction
and result in improved translation performance. Motivated by these insights, we
introduce a systematic LLM-based self-correcting translation framework, named
TER, which stands for Translate, Estimate, and Refine, marking a significant
step forward in this direction. Our findings demonstrate that 1) our
self-correction framework successfully assists LLMs in improving their
translation quality across a wide range of languages, whether it's from
high-resource languages to low-resource ones or whether it's English-centric or
centered around other languages; 2) TER exhibits superior systematicity and
interpretability compared to previous methods; 3) different estimation
strategies yield varied impacts on AI feedback, directly affecting the
effectiveness of the final corrections. We further compare different LLMs and
conduct various experiments involving self-correction and cross-model
correction to investigate the potential relationship between the translation
and evaluation capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16382" title="Abstract">arXiv:2402.16382</a> [<a href="/pdf/2402.16382" title="Download PDF">pdf</a>, <a href="/format/2402.16382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immunization against harmful fine-tuning attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosati%2C+D">Domenic Rosati</a>, 
<a href="/search/cs?searchtype=author&query=Wehner%2C+J">Jan Wehner</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+K">Kai Williams</a>, 
<a href="/search/cs?searchtype=author&query=Bartoszcze%2C+%C5%81">&#x141;ukasz Bartoszcze</a>, 
<a href="/search/cs?searchtype=author&query=Batzner%2C+J">Jan Batzner</a>, 
<a href="/search/cs?searchtype=author&query=Sajjad%2C+H">Hassan Sajjad</a>, 
<a href="/search/cs?searchtype=author&query=Rudzicz%2C+F">Frank Rudzicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Approaches to aligning large language models (LLMs) with human values has
focused on correcting misalignment that emerges from pretraining. However, this
focus overlooks another source of misalignment: bad actors might purposely
fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging
threat model that has arisen from alignment circumvention and fine-tuning
attacks. However, lacking in previous works is a clear presentation of the
conditions for effective defence. We propose a set of conditions for effective
defence against harmful fine-tuning in LLMs called "Immunization conditions,"
which help us understand how we would construct and measure future defences.
Using this formal framework for defence, we offer a synthesis of different
research directions that might be persued to prevent harmful fine-tuning
attacks and provide a demonstration of how to use these conditions
experimentally showing early results of using an adversarial loss to immunize
LLama2-7b-chat.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16383" title="Abstract">arXiv:2402.16383</a> [<a href="/pdf/2402.16383" title="Download PDF">pdf</a>, <a href="/format/2402.16383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self Supervised Correlation-based Permutations for Multi-View Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eisenberg%2C+R">Ran Eisenberg</a>, 
<a href="/search/cs?searchtype=author&query=Svirsky%2C+J">Jonathan Svirsky</a>, 
<a href="/search/cs?searchtype=author&query=Lindenbaum%2C+O">Ofir Lindenbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Fusing information from different modalities can enhance data analysis tasks,
including clustering. However, existing multi-view clustering (MVC) solutions
are limited to specific domains or rely on a suboptimal and computationally
demanding two-stage procedure of representation and clustering. We propose an
end-to-end deep learning-based MVC framework for general data (image, tabular,
etc.). Our approach involves learning meaningful fused data representations
with a novel permutation-based canonical correlation objective. Concurrently,
we learn cluster assignments by identifying consistent pseudo-labels across
multiple views. We demonstrate the effectiveness of our model using ten MVC
benchmark datasets. Theoretically, we show that our model approximates the
supervised linear discrimination analysis (LDA) representation. Additionally,
we provide an error bound induced by false-pseudo label annotations.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16387" title="Abstract">arXiv:2402.16387</a> [<a href="/pdf/2402.16387" title="Download PDF">pdf</a>, <a href="/format/2402.16387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Generalization Capability of Temporal Graph Learning Algorithms:  Theoretical Insights and a Simpler Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+W">Weilin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+M">Mehrdad Mahdavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal Graph Learning (TGL) has become a prevalent technique across diverse
real-world applications, especially in domains where data can be represented as
a graph and evolves over time. Although TGL has recently seen notable progress
in algorithmic solutions, its theoretical foundations remain largely
unexplored. This paper aims at bridging this gap by investigating the
generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based,
and memory-based methods) under the finite-wide over-parameterized regime. We
establish the connection between the generalization error of TGL algorithms and
"the number of layers/steps" in the GNN-/RNN-based TGL methods and "the
feature-label alignment (FLA) score", where FLA can be used as a proxy for the
expressive power and explains the performance of memory-based methods. Guided
by our theoretical analysis, we propose Simplified-Temporal-Graph-Network,
which enjoys a small generalization error, improved overall performance, and
lower model complexity. Extensive experiments on real-world datasets
demonstrate the effectiveness of our method. Our theoretical findings and
proposed algorithm offer essential insights into TGL from a theoretical
standpoint, laying the groundwork for the designing practical TGL algorithms in
future studies.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16389" title="Abstract">arXiv:2402.16389</a> [<a href="/pdf/2402.16389" title="Download PDF">pdf</a>, <a href="/format/2402.16389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in  Intellectual Property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiwen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Minghuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuelin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+F">Fuqiang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ye Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianping Fan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive performance in
various natural language processing (NLP) tasks. However, there is limited
understanding of how well LLMs perform in specific domains (e.g, the
intellectual property (IP) domain). In this paper, we contribute a new
benchmark, the first Multilingual-oriented quiZ on Intellectual Property
(MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark
includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question
answering (IPQA), and patent matching (PatentMatch). In addition, we also
develop a new IP-oriented multilingual large language model (called MoZi),
which is a BLOOMZ-based model that has been supervised fine-tuned with
multilingual IP-related text data. We evaluate our proposed MoZi model and four
well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP
benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE
and ChatGLM by a noticeable margin, while it had lower scores compared with
ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has
much room for improvement, and even the most powerful ChatGPT does not reach
the passing level. Our source code, data, and models are available at
\url{https://github.com/AI-for-Science/MoZi}.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16391" title="Abstract">arXiv:2402.16391</a> [<a href="/pdf/2402.16391" title="Download PDF">pdf</a>, <a href="/format/2402.16391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Assurance for Artificial Intelligence: A Study of Industrial  Concerns, Challenges and Best Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z+S">Ze Shi Li</a>, 
<a href="/search/cs?searchtype=author&query=Damian%2C+D">Daniela Damian</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Quality Assurance (QA) aims to prevent mistakes and defects in manufactured
products and avoid problems when delivering products or services to customers.
QA for AI systems, however, poses particular challenges, given their
data-driven and non-deterministic nature as well as more complex architectures
and algorithms. While there is growing empirical evidence about practices of
machine learning in industrial contexts, little is known about the challenges
and best practices of quality assurance for AI systems (QA4AI). In this paper,
we report on a mixed-method study of QA4AI in industry practice from various
countries and companies. Through interviews with fifteen industry practitioners
and a validation survey with 50 practitioner responses, we studied the concerns
as well as challenges and best practices in ensuring the QA4AI properties
reported in the literature, such as correctness, fairness, interpretability and
others. Our findings suggest correctness as the most important property,
followed by model relevance, efficiency and deployability. In contrast,
transferability (applying knowledge learned in one task to another task),
security and fairness are not paid much attention by practitioners compared to
other properties. Challenges and solutions are identified for each QA4AI
property. For example, interviewees highlighted the trade-off challenge among
latency, cost and accuracy for efficiency (latency and cost are parts of
efficiency concern). Solutions like model compression are proposed. We
identified 21 QA4AI practices across each stage of AI development, with 10
practices being well recognized and another 8 practices being marginally agreed
by the survey practitioners.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16392" title="Abstract">arXiv:2402.16392</a> [<a href="/pdf/2402.16392" title="Download PDF">pdf</a>, <a href="/format/2402.16392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Placing Objects in Context via Inpainting for Out-of-distribution  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Jorge%2C+P">Pau de Jorge</a>, 
<a href="/search/cs?searchtype=author&query=Volpi%2C+R">Riccardo Volpi</a>, 
<a href="/search/cs?searchtype=author&query=Dokania%2C+P+K">Puneet K. Dokania</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H. S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Rogez%2C+G">Gregory Rogez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">When deploying a semantic segmentation model into the real world, it will
inevitably be confronted with semantic classes unseen during training. Thus, to
safely deploy such systems, it is crucial to accurately evaluate and improve
their anomaly segmentation capabilities. However, acquiring and labelling
semantic segmentation data is expensive and unanticipated conditions are
long-tail and potentially hazardous. Indeed, existing anomaly segmentation
datasets capture a limited number of anomalies, lack realism or have strong
domain shifts. In this paper, we propose the Placing Objects in Context (POC)
pipeline to realistically add any object into any image via diffusion models.
POC can be used to easily extend any dataset with an arbitrary number of
objects. In our experiments, we present different anomaly segmentation datasets
based on POC-generated data and show that POC can improve the performance of
recent state-of-the-art anomaly fine-tuning methods in several standardized
benchmarks. POC is also effective to learn new classes. For example, we use it
to edit Cityscapes samples by adding a subset of Pascal classes and show that
models trained on such data achieve comparable performance to the
Pascal-trained baseline. This corroborates the low sim-to-real gap of models
trained on POC-generated images.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16393" title="Abstract">arXiv:2402.16393</a> [<a href="/pdf/2402.16393" title="Download PDF">pdf</a>, <a href="/format/2402.16393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication Optimal Unbalanced Private Set Union
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumas%2C+J">Jean-Guillaume Dumas</a> (UGA, LJK, CASC), 
<a href="/search/cs?searchtype=author&query=Galan%2C+A">Alexis Galan</a> (CASC), 
<a href="/search/cs?searchtype=author&query=Grenet%2C+B">Bruno Grenet</a> (CASC), 
<a href="/search/cs?searchtype=author&query=Maignan%2C+A">Aude Maignan</a> (CASC), 
<a href="/search/cs?searchtype=author&query=Roche%2C+D+S">Daniel S. Roche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We consider the private set union (PSU) problem, where two parties each hold
a private set of elements, and they want one of the parties (the receiver) to
learn the union of the two sets and nothing else. Our protocols are targeted
for the unbalanced case where the receiver's set size is larger than the
sender's set size, with the goal of minimizing the costs for the sender both in
terms of communication volume and local computation time. This setting is
motivated by applications where the receiver has significantly more data (input
set size) and computational resources than the sender which might be realized
on a small, low-power device. Asymptotically, we achieve communication cost
linear in the sender's (smaller) set size, and computation costs for sender and
receiver which are nearly-linear in their respective set sizes. To our
knowledge, ours is the first algorithm to achieve nearly-linear communication
and computation for PSU in this unbalanced setting. Our protocols utilize fully
homomorphic encryption (FHE) and, optionally, linearly homomorphic encryption
(LHE) to perform the necessary computations while preserving privacy. The
underlying computations are based on univariate polynomial arithmetic realized
within homomorphic encryption, namely fast multiplication, modular reduction,
and multi-point evaluation. These asymptotically fast HE polynomial arithmetic
algorithms may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16397" title="Abstract">arXiv:2402.16397</a> [<a href="/pdf/2402.16397" title="Download PDF">pdf</a>, <a href="/format/2402.16397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Deep Watermark Security: An Adversarial Transferability  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+B">Biqing Qi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yiang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianxing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Ligang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bowen Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rise of generative neural networks has triggered an increased demand for
intellectual property (IP) protection in generated content. Deep watermarking
techniques, recognized for their flexibility in IP protection, have garnered
significant attention. However, the surge in adversarial transferable attacks
poses unprecedented challenges to the security of deep watermarking
techniques-an area currently lacking systematic investigation. This study fills
this gap by introducing two effective transferable attackers to assess the
vulnerability of deep watermarks against erasure and tampering risks.
Specifically, we initially define the concept of local sample density,
utilizing it to deduce theorems on the consistency of model outputs. Upon
discovering that perturbing samples towards high sample density regions (HSDR)
of the target class enhances targeted adversarial transferability, we propose
the Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack
(ESMA) method. Additionally, we propose the Bottleneck Enhanced Mixup (BEM)
that integrates information bottleneck theory to reduce the generator's
dependence on irrelevant noise. Experiments show a significant enhancement in
the success rate of targeted transfer attacks for both ESMA and BEM-ESMA
methods. We further conduct a comprehensive evaluation using ESMA and BEM-ESMA
as measurements, considering model architecture and watermark encoding length,
and achieve some impressive findings.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16398" title="Abstract">arXiv:2402.16398</a> [<a href="/pdf/2402.16398" title="Download PDF">pdf</a>, <a href="/format/2402.16398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Continuous-Time Ego-Motion Estimation for Asynchronous  Event-based Data Associations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xudong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianle Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Panfeng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Event cameras are bio-inspired vision sensors that asynchronously measure
per-pixel brightness changes. The high temporal resolution and asynchronicity
of event cameras offer great potential for estimating the robot motion state.
Recent works have adopted the continuous-time ego-motion estimation methods to
exploit the inherent nature of event cameras. However, most of the adopted
methods have poor real-time performance. To alleviate it, a lightweight
Gaussian Process (GP)-based estimation framework is proposed to efficiently
estimate motion trajectory from asynchronous event-driven data associations.
Concretely, an asynchronous front-end pipeline is designed to adapt
event-driven feature trackers and generate feature trajectories from event
streams; a parallel dynamic sliding-window back-end is presented within the
framework of sparse GP regression on SE(3). Notably, a specially designed state
marginalization strategy is employed to ensure the consistency and sparsity of
this GP regression. Experiments conducted on synthetic and real-world datasets
demonstrate that the proposed method achieves competitive precision and
superior robustness compared to the state-of-the-art. Furthermore, the
evaluations on three 60 s trajectories show that the proposal outperforms the
ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and
11.70 times, respectively.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16399" title="Abstract">arXiv:2402.16399</a> [<a href="/pdf/2402.16399" title="Download PDF">pdf</a>, <a href="/format/2402.16399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Embeddings Learned by End-to-End Machine Learning Eye  Movement-driven Biometrics Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raju%2C+M+H">Mehedi Hasan Raju</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+L">Lee Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Lohr%2C+D+J">Dillon J Lohr</a>, 
<a href="/search/cs?searchtype=author&query=Komogortsev%2C+O+V">Oleg V Komogortsev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper expands on the foundational concept of temporal persistence in
biometric systems, specifically focusing on the domain of eye movement
biometrics facilitated by machine learning. Unlike previous studies that
primarily focused on developing biometric authentication systems, our research
delves into the embeddings learned by these systems, particularly examining
their temporal persistence, reliability, and biometric efficacy in response to
varying input data. Utilizing two publicly available eye-movement datasets, we
employed the state-of-the-art Eye Know You Too machine learning pipeline for
our analysis. We aim to validate whether the machine learning-derived
embeddings in eye movement biometrics mirror the temporal persistence observed
in traditional biometrics. Our methodology involved conducting extensive
experiments to assess how different lengths and qualities of input data
influence the performance of eye movement biometrics more specifically how it
impacts the learned embeddings. We also explored the reliability and
consistency of the embeddings under varying data conditions. Three key metrics
(kendall's coefficient of concordance, intercorrelations, and equal error rate)
were employed to quantitatively evaluate our findings. The results reveal while
data length significantly impacts the stability of the learned embeddings,
however, the intercorrelations among embeddings show minimal effect.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16402" title="Abstract">arXiv:2402.16402</a> [<a href="/pdf/2402.16402" title="Download PDF">pdf</a>, <a href="/format/2402.16402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Learning with Distributional Edge Layouts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinjian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+C">Chaolong Ying</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) learn from graph-structured data by passing
local messages between neighboring nodes along edges on certain topological
layouts. Typically, these topological layouts in modern GNNs are
deterministically computed (e.g., attention-based GNNs) or locally sampled
(e.g., GraphSage) under heuristic assumptions. In this paper, we for the first
time pose that these layouts can be globally sampled via Langevin dynamics
following Boltzmann distribution equipped with explicit physical energy,
leading to higher feasibility in the physical world. We argue that such a
collection of sampled/optimized layouts can capture the wide energy
distribution and bring extra expressivity on top of WL-test, therefore easing
downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to
serve as a complement to a variety of GNNs. DEL is a pre-processing strategy
independent of subsequent GNN variants, thus being highly flexible.
Experimental results demonstrate that DELs consistently and substantially
improve a series of GNN baselines, achieving state-of-the-art performance on
multiple datasets.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16405" title="Abstract">arXiv:2402.16405</a> [<a href="/pdf/2402.16405" title="Download PDF">pdf</a>, <a href="/format/2402.16405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser  Massive MIMO Communications in the Wave Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papazafeiropoulos%2C+A">Anastasios Papazafeiropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Kourtessis%2C+P">Pandelis Kourtessis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Although reconfigurable intelligent surface (RIS) is a promising technology
for shaping the propagation environment, it consists of a single-layer
structure within inherent limitations regarding the number of beam steering
patterns. Based on the recently revolutionary technology, denoted as stacked
intelligent metasurface (SIM), we propose its implementation not only on the
base station (BS) side in a massive multiple-input multiple-output (mMIMO)
setup but also in the intermediate space between the base station and the users
to adjust the environment further as needed. For the sake of convenience, we
call the former BS SIM (BSIM), and the latter channel SIM (CSIM). Hence, we
achieve wave-based combining at the BS and wave-based configuration at the
intermediate space. Specifically, we propose a channel estimation method with
reduced overhead, being crucial for SIMassisted communications. Next, we derive
the uplink sum spectral efficiency (SE) in closed form in terms of statistical
channel state information (CSI). Notably, we optimize the phase shifts of both
BSIM and CSIM simultaneously by using the projected gradient ascent method
(PGAM). Compared to previous works on SIMs, we study the uplink transmission, a
mMIMO setup, channel estimation in a single phase, a second SIM at the
intermediate space, and simultaneous optimization of the two SIMs. Simulation
results show the impact of various parameters on the sum SE, and demonstrate
the superiority of our optimization approach compared to the alternating
optimization (AO) method.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16406" title="Abstract">arXiv:2402.16406</a> [<a href="/pdf/2402.16406" title="Download PDF">pdf</a>, <a href="/ps/2402.16406" title="Download PostScript">ps</a>, <a href="/format/2402.16406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From RAGs to riches: Using large language models to write documents for  clinical trials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Markey%2C+N">Nigel Markey</a>, 
<a href="/search/cs?searchtype=author&query=El-Mansouri%2C+I">Ilyass El-Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Rensonnet%2C+G">Gaetan Rensonnet</a>, 
<a href="/search/cs?searchtype=author&query=van+Langen%2C+C">Casper van Langen</a>, 
<a href="/search/cs?searchtype=author&query=Meier%2C+C">Christoph Meier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Clinical trials require numerous documents to be written -- protocols,
consent forms, clinical study reports and others. Large language models (LLMs)
offer the potential to rapidly generate first versions of these documents,
however there are concerns about the quality of their output Here we report an
evaluation of LLMs in generating parts of one such document, clinical trial
protocols. We find that an offthe-shelf LLM delivers reasonable results,
especially when assessing content relevance and the correct use of terminology.
However, deficiencies remain: specifically clinical thinking and logic, and
appropriate use of references. To improve performance, we used
retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date
information. As a result of using RAG, the writing quality of the LLM improves
substantially, which has implications for the practical useability of LLMs in
clinical trial-related writing.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16407" title="Abstract">arXiv:2402.16407</a> [<a href="/pdf/2402.16407" title="Download PDF">pdf</a>, <a href="/format/2402.16407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16412" title="Abstract">arXiv:2402.16412</a> [<a href="/pdf/2402.16412" title="Download PDF">pdf</a>, <a href="/format/2402.16412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Talukder%2C+S">Sabera Talukder</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yisong Yue</a>, 
<a href="/search/cs?searchtype=author&query=Gkioxari%2C+G">Georgia Gkioxari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The field of general time series analysis has recently begun to explore
unified modeling, where a common architectural backbone can be retrained on a
specific task for a specific dataset. In this work, we approach unification
from a complementary vantage point: unification across tasks and domains. To
this end, we explore the impact of discrete, learnt, time series data
representations that enable generalist, cross-domain training. Our method,
TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer
architecture that embeds time series data from varying domains using a discrete
vectorized representation learned in a self-supervised manner. TOTEM works
across multiple tasks and domains with minimal to no tuning. We study the
efficacy of TOTEM with an extensive evaluation on 17 real world time series
datasets across 3 tasks. We evaluate both the specialist (i.e., training a
model on each domain) and generalist (i.e., training a single model on many
domains) settings, and show that TOTEM matches or outperforms previous best
methods on several popular benchmarks. The code can be found at:
https://github.com/SaberaTalukder/TOTEM.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16415" title="Abstract">arXiv:2402.16415</a> [<a href="/pdf/2402.16415" title="Download PDF">pdf</a>, <a href="/format/2402.16415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achievable Rate Optimization for Stacked Intelligent  Metasurface-Assisted Holographic MIMO Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papazafeiropoulos%2C+A">Anastasios Papazafeiropoulos</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jiancheng An</a>, 
<a href="/search/cs?searchtype=author&query=Kourtessis%2C+P">Pandelis Kourtessis</a>, 
<a href="/search/cs?searchtype=author&query=Ratnarajah%2C+T">Tharmalingam Ratnarajah</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Stacked intelligent metasurfaces (SIM) is a revolutionary technology, which
can outperform its single-layer counterparts by performing advanced signal
processing relying on wave propagation. In this work, we exploit SIM to enable
transmit precoding and receiver combining in holographic multiple-input
multiple-output (HMIMO) communications, and we study the achievable rate by
formulating a joint optimization problem of the SIM phase shifts at both sides
of the transceiver and the covariance matrix of the transmitted signal.
Notably, we propose its solution by means of an iterative optimization
algorithm that relies on the projected gradient method, and accounts for all
optimization parameters simultaneously. We also obtain the step size
guaranteeing the convergence of the proposed algorithm. Simulation results
provide fundamental insights such the performance improvements compared to the
single-RIS counterpart and conventional MIMO system. Remarkably, the proposed
algorithm results in the same achievable rate as the alternating optimization
(AO) benchmark but with a less number of iterations.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16416" title="Abstract">arXiv:2402.16416</a> [<a href="/pdf/2402.16416" title="Download PDF">pdf</a>, <a href="/ps/2402.16416" title="Download PostScript">ps</a>, <a href="/format/2402.16416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Information Spreading Evolution on The Control Role of  Announcements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jinhu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Nian%2C+F">Fuzhong Nian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaochen Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Modern social media networks have become an important platform for
information competition among countries, regions, companies and other parties.
This paper utilizes the research method of spread dynamics to investigate the
influence of the control role of announcements in social networks on the
spreading process. This paper distinguishes two spreading phases using the
authentication intervention as a boundary: the unconfirmed spreading phase and
the confirmed spreading phase. Based on the actual rules of spreading in online
social networks, two kinds of verification results are defined: true
information and false information. The Two-stage information spreading dynamics
model is developed to analyze the changes in spreading effects due to different
validation results. The impact of the intervention time on the overall spread
process is analyzed by combining important control factors such as response
cost and time-sensitivity. The validity of the model is verified by comparing
the model simulation results with real cases and the adaptive capacity
experiments. This work is analyzed and visualized from multiple perspectives,
providing more quantitative results. The research content will provide a
scientific basis for the intervention behavior of information management
control by relevant departments or authorities.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16420" title="Abstract">arXiv:2402.16420</a> [<a href="/pdf/2402.16420" title="Download PDF">pdf</a>, <a href="/format/2402.16420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Sustainable Development Goals Using Course Descriptions --  from LLMs to Conventional Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kharlashkin%2C+L">Lev Kharlashkin</a>, 
<a href="/search/cs?searchtype=author&query=Macias%2C+M">Melany Macias</a>, 
<a href="/search/cs?searchtype=author&query=Huovinen%2C+L">Leo Huovinen</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4m%C3%A4l%C3%A4inen%2C+M">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present our work on predicting United Nations sustainable development
goals (SDG) for university courses. We use an LLM named PaLM 2 to generate
training data given a noisy human-authored course description input as input.
We use this data to train several different smaller language models to predict
SDGs for university courses. This work contributes to better university level
adaptation of SDGs. The best performing model in our experiments was BART with
an F1-score of 0.786.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16421" title="Abstract">arXiv:2402.16421</a> [<a href="/pdf/2402.16421" title="Download PDF">pdf</a>, <a href="/format/2402.16421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outline-Guided Object Inpainting with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pobitzer%2C+M">Markus Pobitzer</a>, 
<a href="/search/cs?searchtype=author&query=Janicki%2C+F">Filip Janicki</a>, 
<a href="/search/cs?searchtype=author&query=Rigotti%2C+M">Mattia Rigotti</a>, 
<a href="/search/cs?searchtype=author&query=Malossi%2C+C">Cristiano Malossi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Instance segmentation datasets play a crucial role in training accurate and
robust computer vision models. However, obtaining accurate mask annotations to
produce high-quality segmentation datasets is a costly and labor-intensive
process. In this work, we show how this issue can be mitigated by starting with
small annotated instance segmentation datasets and augmenting them to
effectively obtain a sizeable annotated dataset. We achieve that by creating
variations of the available annotated object instances in a way that preserves
the provided mask annotations, thereby resulting in new image-mask pairs to be
added to the set of annotated images. Specifically, we generate new images
using a diffusion-based inpainting model to fill out the masked area with a
desired object class by guiding the diffusion through the object outline. We
show that the object outline provides a simple, but also reliable and
convenient training-free guidance signal for the underlying inpainting model
that is often sufficient to fill out the mask with an object of the correct
class without further text guidance and preserve the correspondence between
generated images and the mask annotations with high precision. Our experimental
results reveal that our method successfully generates realistic variations of
object instances, preserving their shape characteristics while introducing
diversity within the augmented area. We also show that the proposed method can
naturally be combined with text guidance and other image augmentation
techniques.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16424" title="Abstract">arXiv:2402.16424</a> [<a href="/pdf/2402.16424" title="Download PDF">pdf</a>, <a href="/format/2402.16424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Q">Qingqing Long</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zeyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zhen Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency
and generalization in large-scale retrieval scenarios. While considerable
success has been achieved, there still exist urgent limitations. Existing works
ignore the locality relationships of representations and attributes, which have
effective transferability between seeable classes and unseeable classes. Also,
the continuous-value attributes are not fully harnessed. In response, we
conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which
depicts the relationships from seen classes to unseen ones through three
meticulously designed explorations, i.e., point-wise, pair-wise and class-wise
consistency constraints. By regressing attributes from the proposed attribute
prototype network, COMAE learns the local features that are relevant to the
visual attributes. Then COMAE utilizes contrastive learning to comprehensively
depict the context of attributes, rather than instance-independent
optimization. Finally, the class-wise constraint is designed to cohesively
learn the hash code, image representation, and visual attributes more
effectively. Experimental results on the popular ZSH datasets demonstrate that
COMAE outperforms state-of-the-art hashing techniques, especially in scenarios
with a larger number of unseen label classes.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16429" title="Abstract">arXiv:2402.16429</a> [<a href="/pdf/2402.16429" title="Download PDF">pdf</a>, <a href="/ps/2402.16429" title="Download PostScript">ps</a>, <a href="/format/2402.16429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effect of utterance duration and phonetic content on speaker  identification using second-order statistical methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magrin-Chagnolleau%2C+I">Ivan Magrin-Chagnolleau</a>, 
<a href="/search/cs?searchtype=author&query=Bonastre%2C+J+F">Jean Fran&#xe7;ois Bonastre</a> (LIA), 
<a href="/search/cs?searchtype=author&query=Bimbot%2C+F">Fr&#xe9;d&#xe9;ric Bimbot</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Eurospeech 1995, Sep 1995, Madrid, Spain. pp.337-340
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Second-order statistical methods show very good results for automatic speaker
identification in controlled recording conditions. These approaches are
generally used on the entire speech material available. In this paper, we study
the influence of the content of the test speech material on the performances of
such methods, i.e. under a more analytical approach. The goal is to investigate
on the kind of information which is used by these methods, and where it is
located in the speech signal. Liquids and glides together, vowels, and more
particularly nasal vowels and nasal consonants, are found to be particularly
speaker specific: test utterances of 1 second, composed in majority of acoustic
material from one of these classes provide better speaker identification
results than phonetically balanced test utterances, even though the training is
done, in both cases, with 15 seconds of phonetically balanced speech.
Nevertheless, results with other phoneme classes are never dramatically poor.
These results tend to show that the speaker-dependent information captured by
long-term second-order statistics is consistently common to all phonetic
classes, and that the homogeneity of the test material may improve the quality
of the estimates.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16430" title="Abstract">arXiv:2402.16430</a> [<a href="/pdf/2402.16430" title="Download PDF">pdf</a>, <a href="/format/2402.16430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving behavior based authentication against adversarial attack using  XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+D">Dong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Amariucai%2C+G">George Amariucai</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Daji Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yong Guan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In recent years, machine learning models, especially deep neural networks,
have been widely used for classification tasks in the security domain. However,
these models have been shown to be vulnerable to adversarial manipulation:
small changes learned by an adversarial attack model, when applied to the
input, can cause significant changes in the output. Most research on
adversarial attacks and corresponding defense methods focuses only on scenarios
where adversarial samples are directly generated by the attack model. In this
study, we explore a more practical scenario in behavior-based authentication,
where adversarial samples are collected from the attacker. The generated
adversarial samples from the model are replicated by attackers with a certain
level of discrepancy. We propose an eXplainable AI (XAI) based defense strategy
against adversarial attacks in such scenarios. A feature selector, trained with
our method, can be used as a filter in front of the original authenticator. It
filters out features that are more vulnerable to adversarial attacks or
irrelevant to authentication, while retaining features that are more robust.
Through comprehensive experiments, we demonstrate that our XAI based defense
strategy is effective against adversarial attacks and outperforms other defense
strategies, such as adversarial training and defensive distillation.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16431" title="Abstract">arXiv:2402.16431</a> [<a href="/pdf/2402.16431" title="Download PDF">pdf</a>, <a href="/format/2402.16431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoCoIns: Enhancing Robustness of Large Language Models through  Code-Style Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Han Xia</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have showcased remarkable capabilities in
following human instructions. However, recent studies have raised concerns
about the robustness of LLMs when prompted with instructions combining textual
adversarial samples. In this paper, drawing inspiration from recent works that
LLMs are sensitive to the design of the instructions, we utilize instructions
in code style, which are more structural and less ambiguous, to replace
typically natural language instructions. Through this conversion, we provide
LLMs with more precise instructions and strengthen the robustness of LLMs.
Moreover, under few-shot scenarios, we propose a novel method to compose
in-context demonstrations using both clean and adversarial samples
(\textit{adversarial context method}) to further boost the robustness of the
LLMs. Experiments on eight robustness datasets show that our method
consistently outperforms prompting LLMs with natural language instructions. For
example, with gpt-3.5-turbo, our method achieves an improvement of 5.68\% in
test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16434" title="Abstract">arXiv:2402.16434</a> [<a href="/pdf/2402.16434" title="Download PDF">pdf</a>, <a href="/format/2402.16434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of the Downlink Spectral- and Energy-Efficiency of  RIS-aided Multi-user URLLC MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mohammad Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+I">Ignacio Santamaria</a>, 
<a href="/search/cs?searchtype=author&query=Jorswieck%2C+E">Eduard Jorswieck</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Modern wireless communication systems are expected to provide improved
latency and reliability. To meet these expectations, a short packet length is
needed, which makes the first-order Shannon rate an inaccurate performance
metric for such communication systems. A more accurate approximation of the
achievable rates of finite-block-length (FBL) coding regimes is known as the
normal approximation (NA). It is therefore of substantial interest to study the
optimization of the FBL rate in multi-user multiple-input multiple-output
(MIMO) systems, in which each user may transmit and/or receive multiple data
streams. Hence, we formulate a general optimization problem for improving the
spectral and energy efficiency of multi-user MIMO-aided ultra-reliable
low-latency communication (URLLC) systems, which are assisted by reconfigurable
intelligent surfaces (RISs). We show that a RIS is capable of substantially
improving the performance of multi-user MIMO-aided URLLC systems. Moreover, the
benefits of RIS increase as the packet length and/or the tolerable bit error
rate are reduced. This reveals that RISs can be even more beneficial in URLLC
systems for improving the FBL rates than in conventional systems approaching
Shannon rates.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16435" title="Abstract">arXiv:2402.16435</a> [<a href="/pdf/2402.16435" title="Download PDF">pdf</a>, <a href="/format/2402.16435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Implicit Generative Models via an Invariant Statistical Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Frutos%2C+J+M">Jos&#xe9; Manuel de Frutos</a>, 
<a href="/search/cs?searchtype=author&query=Olmos%2C+P+M">Pablo M. Olmos</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1zquez%2C+M+A">Manuel A. V&#xe1;zquez</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%ADguez%2C+J">Joaqu&#xed;n M&#xed;guez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Implicit generative models have the capability to learn arbitrary complex
data distributions. On the downside, training requires telling apart real data
from artificially-generated ones using adversarial discriminators, leading to
unstable training and mode-dropping issues. As reported by Zahee et al. (2017),
even in the one-dimensional (1D) case, training a generative adversarial
network (GAN) is challenging and often suboptimal. In this work, we develop a
discriminator-free method for training one-dimensional (1D) generative implicit
models and subsequently expand this method to accommodate multivariate cases.
Our loss function is a discrepancy measure between a suitably chosen
transformation of the model samples and a uniform distribution; hence, it is
invariant with respect to the true distribution of the data. We first formulate
our method for 1D random variables, providing an effective solution for
approximate reparameterization of arbitrary complex distributions. Then, we
consider the temporal setting (both univariate and multivariate), in which we
model the conditional distribution of each sample given the history of the
process. We demonstrate through numerical simulations that this new method
yields promising results, successfully learning true distributions in a variety
of scenarios and mitigating some of the well-known problems that
state-of-the-art implicit methods present.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16438" title="Abstract">arXiv:2402.16438</a> [<a href="/pdf/2402.16438" title="Download PDF">pdf</a>, <a href="/format/2402.16438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Specific Neurons: The Key to Multilingual Capabilities in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tianyi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wenyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) demonstrate remarkable multilingual capabilities
without being pre-trained on specially curated multilingual parallel corpora.
It remains a challenging problem to explain the underlying mechanisms by which
LLMs process multilingual texts. In this paper, we delve into the composition
of Transformer architectures in LLMs to pinpoint language-specific regions.
Specially, we propose a novel detection method, language activation probability
entropy (LAPE), to identify language-specific neurons within LLMs. Based on
LAPE, we conduct comprehensive experiments on two representative LLMs, namely
LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a
particular language is predominantly due to a small subset of neurons,
primarily situated in the models' top and bottom layers. Furthermore, we
showcase the feasibility to "steer" the output language of LLMs by selectively
activating or deactivating language-specific neurons. Our research provides
important evidence to the understanding and exploration of the multilingual
capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16440" title="Abstract">arXiv:2402.16440</a> [<a href="/pdf/2402.16440" title="Download PDF">pdf</a>, <a href="/ps/2402.16440" title="Download PostScript">ps</a>, <a href="/format/2402.16440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrouver l&#x27;inventeur-auteur : la lev{&#xe9;}e d&#x27;homonymies d&#x27;autorat entre  les brevets et les publications scientifiques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reymond%2C+D">David Reymond</a> (IMSIC), 
<a href="/search/cs?searchtype=author&query=Khouilla%2C+H">Heman Khouilla</a> (LEAD), 
<a href="/search/cs?searchtype=author&query=Wolff%2C+S">Sandrine Wolff</a> (BETA), 
<a href="/search/cs?searchtype=author&query=Durand-Barthez%2C+M">Manuel Durand-Barthez</a> (CJM, URFIST Paris)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JADT2022: 16th International Conference on Statistical Analysis of
  Textual Data, Univ. of Naples Federico II, Jul 2022, Naples, Italie.
  pp.712-720
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Patents and scientific papers provide an essential source for measuring
science and technology output, to be used as a basis for the most varied
scientometric analyzes. Authors' and inventors' names are the key identifiers
to carry out these analyses, which however, run up against the issue of
disambiguation. By extension identifying inventors who are also academic
authors is a non-trivial challenge. We propose a method using the International
Patent Classification (IPC) and the IPCCAT API to assess the degree of
similarity of patents and papers abstracts of a given inventor, in order to
match both types of documents. The method is developed and manually qualified
based on three corpora of patents extracted from the international EPO database
Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501
authors. The proposed algorithm solves the general problem of disambiguation
with an error rate lower than 5%.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16442" title="Abstract">arXiv:2402.16442</a> [<a href="/pdf/2402.16442" title="Download PDF">pdf</a>, <a href="/format/2402.16442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Distributed Larger-Than-Memory Subset Selection With Pairwise  Submodular Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%B6ther%2C+M">Maximilian B&#xf6;ther</a>, 
<a href="/search/cs?searchtype=author&query=Sebastian%2C+A">Abraham Sebastian</a>, 
<a href="/search/cs?searchtype=author&query=Awasthi%2C+P">Pranjal Awasthi</a>, 
<a href="/search/cs?searchtype=author&query=Klimovic%2C+A">Ana Klimovic</a>, 
<a href="/search/cs?searchtype=author&query=Ramalingam%2C+S">Srikumar Ramalingam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
<p class="mathjax">Many learning problems hinge on the fundamental problem of subset selection,
i.e., identifying a subset of important and representative points. For example,
selecting the most significant samples in ML training cannot only reduce
training costs but also enhance model quality. Submodularity, a discrete
analogue of convexity, is commonly used for solving subset selection problems.
However, existing algorithms for optimizing submodular functions are
sequential, and the prior distributed methods require at least one central
machine to fit the target subset. In this paper, we relax the requirement of
having a central machine for the target subset by proposing a novel distributed
bounding algorithm with provable approximation guarantees. The algorithm
iteratively bounds the minimum and maximum utility values to select high
quality points and discard the unimportant ones. When bounding does not find
the complete subset, we use a multi-round, partition-based distributed greedy
algorithm to identify the remaining subset. We show that these algorithms find
high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in
quality compared to centralized methods, and scale to a dataset with 13 billion
points.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16444" title="Abstract">arXiv:2402.16444</a> [<a href="/pdf/2402.16444" title="Download PDF">pdf</a>, <a href="/format/2402.16444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable  Safety Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhexin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yida Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jingyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Lei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The safety of Large Language Models (LLMs) has gained increasing attention in
recent years, but there still lacks a comprehensive approach for detecting
safety issues within LLMs' responses in an aligned, customizable and
explainable manner. In this paper, we propose ShieldLM, an LLM-based safety
detector, which aligns with general human safety standards, supports
customizable detection rules, and provides explanations for its decisions. To
train ShieldLM, we compile a large bilingual dataset comprising 14,387
query-response pairs, annotating the safety of responses based on various
safety standards. Through extensive experiments, we demonstrate that ShieldLM
surpasses strong baselines across four test sets, showcasing remarkable
customizability and explainability. Besides performing well on standard
detection datasets, ShieldLM has also been shown to be effective in real-world
situations as a safety evaluator for advanced LLMs. We release ShieldLM at
\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable
safety detection under various safety standards, contributing to the ongoing
efforts to enhance the safety of LLMs.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16445" title="Abstract">arXiv:2402.16445</a> [<a href="/pdf/2402.16445" title="Download PDF">pdf</a>, <a href="/format/2402.16445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+L">Liuzhenghao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zongying Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+Y">Calvin Yu-Chian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved
remarkable performance in multiple Natural Language Processing (NLP) tasks.
Under the premise that protein sequences constitute the protein language,
Protein Large Language Models (ProLLMs) trained on protein corpora excel at de
novo protein sequence generation. However, as of now, unlike LLMs in NLP, no
ProLLM is capable of multiple tasks in the Protein Language Processing (PLP)
field. This prompts us to delineate the inherent limitations in current
ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient
instruction understanding, and (iii) high training resource demands. To address
these challenges, we introduce a training framework to transform any general
LLM into a ProLLM capable of handling multiple PLP tasks. Specifically, our
framework utilizes low-rank adaptation and employs a two-stage training
approach, and it is distinguished by its universality, low overhead, and
scalability. Through training under this framework, we propose the ProLLaMA
model, the first known ProLLM to handle multiple PLP tasks simultaneously.
Experiments show that ProLLaMA achieves state-of-the-art results in the
unconditional protein sequence generation task. In the controllable protein
sequence generation task, ProLLaMA can design novel proteins with desired
functionalities. In the protein property prediction task, ProLLaMA achieves
nearly 100\% accuracy across many categories. The latter two tasks are beyond
the reach of other ProLLMs. Code is available at
\url{https://github.com/Lyu6PosHao/ProLLaMA}.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16448" title="Abstract">arXiv:2402.16448</a> [<a href="/pdf/2402.16448" title="Download PDF">pdf</a>, <a href="/format/2402.16448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WetLinks: a Large-Scale Longitudinal Starlink Dataset with Contiguous  Weather Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laniewski%2C+D">Dominic Laniewski</a>, 
<a href="/search/cs?searchtype=author&query=Lanfer%2C+E">Eric Lanfer</a>, 
<a href="/search/cs?searchtype=author&query=Meijerink%2C+B">Bernd Meijerink</a>, 
<a href="/search/cs?searchtype=author&query=van+Rijswijk-Deij%2C+R">Roland van Rijswijk-Deij</a>, 
<a href="/search/cs?searchtype=author&query=Aschenbruck%2C+N">Nils Aschenbruck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the 2024 Network Traffic Measurement and Analysis Conference (TMA) for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Low Orbit Satellite (LEO) networks such as Starlink promise Internet access
everywhere around the world. In this paper, we present WetLinks - a large and
publicly available trace-based dataset of Starlink measurements. The
measurements were concurrently collected from two European vantage points over
a span of six months. Consisting of approximately 140,000 measurements, the
dataset comprises all relevant network parameters such as the upload and
download throughputs, the RTT, packet loss, and traceroutes. We further augment
the dataset with concurrent data from professional weather stations placed next
to both Starlink terminals. Based on our dataset, we analyse Starlink
performance, including its susceptibility to weather conditions. We use this to
validate our dataset by replicating the results of earlier smaller-scale
studies. We release our datasets and all accompanying tooling as open data. To
the best of our knowledge, ours is the largest Starlink dataset to date.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16449" title="Abstract">arXiv:2402.16449</a> [<a href="/pdf/2402.16449" title="Download PDF">pdf</a>, <a href="/format/2402.16449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Efficient Safety-Critical Control for Mobile Robots in Unknown  Dynamic Multi-Obstacle Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+G">Guangyao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Long Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiangtong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liding Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes a LiDAR-based goal-seeking and exploration framework,
addressing the efficiency of online obstacle avoidance in unstructured
environments populated with static and moving obstacles. This framework
addresses two significant challenges associated with traditional dynamic
control barrier functions (D-CBFs): their online construction and the
diminished real-time performance caused by utilizing multiple D-CBFs. To tackle
the first challenge, the framework's perception component begins with
clustering point clouds via the DBSCAN algorithm, followed by encapsulating
these clusters with the minimum bounding ellipses (MBEs) algorithm to create
elliptical representations. By comparing the current state of MBEs with those
stored from previous moments, the differentiation between static and dynamic
obstacles is realized, and the Kalman filter is utilized to predict the
movements of the latter. Such analysis facilitates the D-CBF's online
construction for each MBE. To tackle the second challenge, we introduce buffer
zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing
these buffer zones as activation areas substantially reduces the number of
D-CBFs that need to be activated. Upon entering these buffer zones, the system
prioritizes safety, autonomously navigating safe paths, and hence referred to
as the exploration mode. Exiting these buffer zones triggers the system's
transition to goal-seeking mode. We demonstrate that the system's states under
this framework achieve safety and asymptotic stabilization. Experimental
results in simulated and real-world environments have validated our framework's
capability, allowing a LiDAR-equipped mobile robot to efficiently and safely
reach the desired location within dynamic environments containing multiple
obstacles.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16454" title="Abstract">arXiv:2402.16454</a> [<a href="/pdf/2402.16454" title="Download PDF">pdf</a>, <a href="/format/2402.16454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Integration of TSN-unaware Applications with QoS Requirements  in TSN Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fluechter%2C+M">Moritz Fluechter</a>, 
<a href="/search/cs?searchtype=author&query=Lindner%2C+S">Steffen Lindner</a>, 
<a href="/search/cs?searchtype=author&query=Osswald%2C+L">Lukas Osswald</a>, 
<a href="/search/cs?searchtype=author&query=Arnaud%2C+J">J&#xe9;r&#xf4;me Arnaud</a>, 
<a href="/search/cs?searchtype=author&query=Menth%2C+M">Michael Menth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Computer Communications Journal, Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Modern industrial networks transport both best-effort and real-time traffic.
Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an
enhancement to Ethernet to provide high quality of service (QoS) for real-time
traffic. In a TSN network, applications signal their QoS requirements to the
network before transmitting data. The network then allocates resources to meet
these requirements. However, TSN-unaware applications can neither perform this
registration process nor profit from TSN's QoS benefits. The contributions of
this paper are twofold. First, we introduce a novel network architecture in
which an additional device autonomously signals the QoS requirements of
TSN-unaware applications to the network. Second, we propose a processing method
to detect real-time streams in a network and extract the necessary information
for the TSN stream signaling. It leverages a Deep Recurrent Neural Network
(DRNN) to detect periodic traffic, extracts an accurate traffic description,
and uses traffic classification to determine the source application. As a
result, our proposal allows TSN-unaware applications to benefit from TSNs QoS
guarantees. Our evaluations underline the effectiveness of the proposed
architecture and processing method.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16455" title="Abstract">arXiv:2402.16455</a> [<a href="/pdf/2402.16455" title="Download PDF">pdf</a>, <a href="/format/2402.16455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on  Computational Fluid Dynamics Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudela%2C+J">Jakub Kudela</a>, 
<a href="/search/cs?searchtype=author&query=Dobrovsky%2C+L">Ladislav Dobrovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the
most widely studied methods for their capability to solve expensive real-world
optimization problems. However, the development of new methods and benchmarking
with other techniques still relies almost exclusively on artificially created
problems. In this paper, we use two real-world computational fluid dynamics
problems to compare the performance of eleven state-of-the-art single-objective
SAEAs. We analyze the performance by investigating the quality and robustness
of the obtained solutions and the convergence properties of the selected
methods. Our findings suggest that the more recently published methods, as well
as the techniques that utilize differential evolution as one of their
optimization mechanisms, perform significantly better than the other considered
methods.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16457" title="Abstract">arXiv:2402.16457</a> [<a href="/pdf/2402.16457" title="Download PDF">pdf</a>, <a href="/format/2402.16457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for  Short-form Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine
the necessity of retrieval for queries instead of retrieving indiscriminately
to enhance the efficiency and relevance of the sourced information. However,
previous works largely overlook the evaluation of ARAG approaches, leading to
their effectiveness being understudied. This work presents a benchmark,
RetrievalQA, comprising 1,271 short-form questions covering new world and
long-tail knowledge. The knowledge necessary to answer the questions is absent
from LLMs; therefore, external information must be retrieved to answer
correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG
methods. We observe that calibration-based methods heavily rely on threshold
tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable
retrieval decisions. Based on our findings, we propose Time-Aware Adaptive
Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the
necessity of retrieval without calibration or additional training. The dataset
and code will be available at \url{https://github.com/hyintell/RetrievalQA}
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16458" title="Abstract">arXiv:2402.16458</a> [<a href="/pdf/2402.16458" title="Download PDF">pdf</a>, <a href="/format/2402.16458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D-XCB: Data-independent Debiasing for Fair and Accurate  Transformer-based Cyberbullying Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+P">Peiling Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zubiaga%2C+A">Arkaitz Zubiaga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Swear words are a common proxy to collect datasets with cyberbullying
incidents. Our focus is on measuring and mitigating biases derived from
spurious associations between swear words and incidents occurring as a result
of such data collection strategies. After demonstrating and quantifying these
biases, we introduce ID-XCB, the first data-independent debiasing technique
that combines adversarial training, bias constraints and debias fine-tuning
approach aimed at alleviating model attention to bias-inducing words without
impacting overall model performance. We explore ID-XCB on two popular
session-based cyberbullying datasets along with comprehensive ablation and
generalisation studies. We show that ID-XCB learns robust cyberbullying
detection capabilities while mitigating biases, outperforming state-of-the-art
debiasing methods in both performance and bias mitigation. Our quantitative and
qualitative analyses demonstrate its generalisability to unseen data.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16459" title="Abstract">arXiv:2402.16459</a> [<a href="/pdf/2402.16459" title="Download PDF">pdf</a>, <a href="/format/2402.16459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending LLMs against Jailbreaking Attacks via Backtranslation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhouxing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+A">Andrew Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although many large language models (LLMs) have been trained to refuse
harmful requests, they are still vulnerable to jailbreaking attacks, which
rewrite the original prompt to conceal its harmful intent. In this paper, we
propose a new method for defending LLMs against jailbreaking attacks by
``backtranslation''. Specifically, given an initial response generated by the
target LLM from an input prompt, our backtranslation prompts a language model
to infer an input prompt that can lead to the response. The inferred prompt is
called the backtranslated prompt which tends to reveal the actual intent of the
original prompt, since it is generated based on the LLM's response and is not
directly manipulated by the attacker. We then run the target LLM again on the
backtranslated prompt, and we refuse the original prompt if the model refuses
the backtranslated prompt. We explain that the proposed defense provides
several benefits on its effectiveness and efficiency. We empirically
demonstrate that our defense significantly outperforms the baselines, in the
cases that are hard for the baselines, and our defense also has little impact
on the generation quality for benign input prompts.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16462" title="Abstract">arXiv:2402.16462</a> [<a href="/pdf/2402.16462" title="Download PDF">pdf</a>, <a href="/format/2402.16462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Communication and Control Co-Design in 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayan%2C+O">Onur Ayan</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Estevez%2C+M+A+G">Miguel Angel Gutierrez Estevez</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+X">Xueli An</a>, 
<a href="/search/cs?searchtype=author&query=Kellerer%2C+W">Wolfgang Kellerer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Networked control systems (NCSs), which are feedback control loops closed
over a communication network, have been a popular research topic over the past
decades. Numerous works in the literature propose novel algorithms and
protocols with joint consideration of communication and control. However, the
vast majority of the recent research results, which have shown remarkable
performance improvements if a cross-layer methodology is followed, have not
been widely adopted by the industry. In this work, we review the shortcomings
of today's mobile networks that render cross-layer solutions, such as semantic
and goal-oriented communications, very challenging in practice. To tackle this,
we propose a new framework for 6G user plane design that simplifies the
adoption of recent research results in networked control, thereby facilitating
the joint communication and control design in next-generation mobile networks.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16463" title="Abstract">arXiv:2402.16463</a> [<a href="/pdf/2402.16463" title="Download PDF">pdf</a>, <a href="/format/2402.16463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Schedule Online Tasks with Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shangshang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hengquan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Ziyu Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Online task scheduling serves an integral role for task-intensive
applications in cloud computing and crowdsourcing. Optimal scheduling can
enhance system performance, typically measured by the reward-to-cost ratio,
under some task arrival distribution. On one hand, both reward and cost are
dependent on task context (e.g., evaluation metric) and remain black-box in
practice. These render reward and cost hard to model thus unknown before
decision making. On the other hand, task arrival behaviors remain sensitive to
factors like unpredictable system fluctuation whereby a prior estimation or the
conventional assumption of arrival distribution (e.g., Poisson) may fail. This
implies another practical yet often neglected challenge, i.e., uncertain task
arrival distribution. Towards effective scheduling under a stationary
environment with various uncertainties, we propose a double-optimistic learning
based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a
learning module that incorporates optimistic estimation for reward-to-cost
ratio and a decision module that utilizes the Robbins-Monro method to
implicitly learn task arrival distribution while making scheduling decisions.
Theoretically, DOL-RM achieves convergence gap and no regret learning with a
sub-linear regret of $O(T^{3/4})$, which is the first result for online task
scheduling under uncertain task arrival distribution and unknown reward and
cost. Our numerical results in a synthetic experiment and a real-world
application demonstrate the effectiveness of DOL-RM in achieving the best
cumulative reward-to-cost ratio compared with other state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16466" title="Abstract">arXiv:2402.16466</a> [<a href="/pdf/2402.16466" title="Download PDF">pdf</a>, <a href="/format/2402.16466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized and approximation algorithms for coverings points with  segments in the plane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowalska%2C+K">Katarzyna Kowalska</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 figures. The extended abstract will appear in the proceedings of STACS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study parameterized and approximation algorithms for a variant of Set
Cover, where the universe of elements to be covered consists of points in the
plane and the sets with which the points should be covered are segments. We
call this problem Segment Set Cover. We also consider a relaxation of the
problem called $\delta$-extension, where we need to cover the points by
segments that are extended by a tiny fraction, but we compare the solution's
quality to the optimum without extension.
<br />For the unparameterized variant, we prove that Segment Set Cover does not
admit a PTAS unless $\mathsf{P}=\mathsf{NP}$, even if we restrict segments to
be axis-parallel and allow $\frac{1}{2}$-extension. On the other hand, we show
that parameterization helps for the tractability of Segment Set Cover: we give
an FPT algorithm for unweighted Segment Set Cover parameterized by the solution
size $k$, a parameterized approximation scheme for Weighted Segment Set Cover
with $k$ being the parameter, and an FPT algorithm for Weighted Segment Set
Cover with $\delta$-extension parameterized by $k$ and $\delta$. In the last
two results, relaxing the problem is probably necessary: we prove that Weighted
Segment Set Cover without any relaxation is $\mathsf{W}[1]$-hard and, assuming
ETH, there does not exist an algorithm running in time $f(k)\cdot n^{o(k / \log
k)}$. This holds even if one restricts attention to axis-parallel segments.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16467" title="Abstract">arXiv:2402.16467</a> [<a href="/pdf/2402.16467" title="Download PDF">pdf</a>, <a href="/format/2402.16467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploratory Landscape Analysis for Mixed-Variable Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prager%2C+R+P">Raphael Patrick Prager</a>, 
<a href="/search/cs?searchtype=author&query=Trautmann%2C+H">Heike Trautmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Exploratory landscape analysis and fitness landscape analysis in general have
been pivotal in facilitating problem understanding, algorithm design and
endeavors such as automated algorithm selection and configuration. These
techniques have largely been limited to search spaces of a single domain. In
this work, we provide the means to compute exploratory landscape features for
mixed-variable problems where the decision space is a mixture of continuous,
binary, integer, and categorical variables. This is achieved by utilizing
existing encoding techniques originating from machine learning. We provide a
comprehensive juxtaposition of the results based on these different techniques.
To further highlight their merit for practical applications, we design and
conduct an automated algorithm selection study based on a hyperparameter
optimization benchmark suite. We derive a meaningful compartmentalization of
these benchmark problems by clustering based on the used landscape features.
The identified clusters mimic the behavior the used algorithms exhibit.
Meaning, the different clusters have different best performing algorithms.
Finally, our trained algorithm selector is able to close the gap between the
single best and the virtual best solver by 57.5% over all benchmark problems.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16468" title="Abstract">arXiv:2402.16468</a> [<a href="/pdf/2402.16468" title="Download PDF">pdf</a>, <a href="/ps/2402.16468" title="Download PostScript">ps</a>, <a href="/format/2402.16468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Sensing and Communications with Affine Frequency Division  Multiplexing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bemani%2C+A">Ali Bemani</a>, 
<a href="/search/cs?searchtype=author&query=Ksairi%2C+N">Nassar Ksairi</a>, 
<a href="/search/cs?searchtype=author&query=Kountouris%2C+M">Marios Kountouris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE Wireless Communications Letters, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Integrated sensing and communications (ISAC) is regarded as a key technology
in next-generation (6G) mobile communication systems. Affine frequency division
multiplexing (AFDM) is a recently proposed waveform that achieves optimal
diversity gain in high mobility scenarios and has appealing properties in
high-frequency communication. In this letter, we present an AFDM-based ISAC
system. We first show that in order to identify all delay and Doppler
components associated with the propagation medium, either the full AFDM signal
or only its pilot part consisting of one discrete affine Fourier transform
(DAFT) domain symbol and its guard interval can be used. Our results show that
using one pilot symbol achieves almost the same sensing performance as using
the entire AFDM frame. Furthermore, due to the chirp nature of AFDM, sensing
with one pilot provides a unique feature allowing for simple self-interference
cancellation, thus avoiding the need for expensive full duplex methods.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16469" title="Abstract">arXiv:2402.16469</a> [<a href="/pdf/2402.16469" title="Download PDF">pdf</a>, <a href="/ps/2402.16469" title="Download PostScript">ps</a>, <a href="/format/2402.16469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast implementation of the good-suffix array for the Boyer-Moore  string matching algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lecroq%2C+T">Thierry Lecroq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to FUN 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">String matching is the problem of finding all the occurrences of a pattern in
a text. It has been intensively studied and the Boyer-Moore string matching
algorithm is probably one of the most famous solution to this problem. This
algorithm uses two precomputed shift tables called the good-suffix table and
the bad-character table. The good-suffix table is tricky to compute in linear
time. Text book solutions perform redundant operations. Here we present a fast
implementation for this good-suffix table based on a tight analysis of the
pattern. Experimental results show two versions of this new implementation are
the fastest in almost all tested situations.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16470" title="Abstract">arXiv:2402.16470</a> [<a href="/pdf/2402.16470" title="Download PDF">pdf</a>, <a href="/format/2402.16470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Vulnerability of Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liong%2C+K+J">Khai Jiet Liong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/liongkj/HackAttend">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (PLMs) are shown to be vulnerable to minor word
changes, which poses a big threat to real-world systems. While previous studies
directly focus on manipulating word inputs, they are limited by their means of
generating adversarial samples, lacking generalization to versatile real-world
attack. This paper studies the basic structure of transformer-based PLMs, the
self-attention (SA) mechanism. (1) We propose a powerful perturbation technique
\textit{HackAttend}, which perturbs the attention scores within the SA matrices
via meticulously crafted attention masks. We show that state-of-the-art PLMs
fall into heavy vulnerability that minor attention perturbations $(1\%)$ can
produce a very high attack success rate $(98\%)$. Our paper expands the
conventional text attack of word perturbations to more general structural
perturbations. (2) We introduce \textit{S-Attend}, a novel smoothing technique
that effectively makes SA robust via structural perturbations. We empirically
demonstrate that this simple yet effective technique achieves robust
performance on par with adversarial training when facing various text
attackers. Code is publicly available at \url{github.com/liongkj/HackAttend}.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16472" title="Abstract">arXiv:2402.16472</a> [<a href="/pdf/2402.16472" title="Download PDF">pdf</a>, <a href="/format/2402.16472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mEdIT: Multilingual Text Editing via Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>, 
<a href="/search/cs?searchtype=author&query=Alikaniotis%2C+D">Dimitris Alikaniotis</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+V">Vivek Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Alhafni%2C+B">Bashar Alhafni</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL ARR December 2023. 22 pages, 8 tables, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent
state-of-the-art text editing models for writing assistance. mEdIT models are
trained by fine-tuning multi-lingual large, pre-trained language models (LLMs)
via instruction tuning. They are designed to take instructions from the user
specifying the attributes of the desired text in the form of natural language
instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on
(Spanish). We build mEdIT by curating data from multiple publicly available
human-annotated text editing datasets for three text editing tasks (Grammatical
Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse
languages belonging to six different language families. We detail the design
and training of mEdIT models and demonstrate their strong performance on many
multi-lingual text editing benchmarks against other multilingual LLMs. We also
find that mEdIT generalizes effectively to new languages over multilingual
baselines. We publicly release our data, code, and trained models at
https://github.com/vipulraheja/medit.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16473" title="Abstract">arXiv:2402.16473</a> [<a href="/pdf/2402.16473" title="Download PDF">pdf</a>, <a href="/format/2402.16473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCVSMNet: Double Cost Volume Stereo Matching Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tahmasebi%2C+M">Mahmoud Tahmasebi</a>, 
<a href="/search/cs?searchtype=author&query=Huq%2C+S">Saif Huq</a>, 
<a href="/search/cs?searchtype=author&query=Meehan%2C+K">Kevin Meehan</a>, 
<a href="/search/cs?searchtype=author&query=McAfee%2C+M">Marion McAfee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a
novel architecture characterised by by two small upper (group-wise) and lower
(norm correlation) cost volumes. Each cost volume is processed separately, and
a coupling module is proposed to fuse the geometry information extracted from
the upper and lower cost volumes. DCVSMNet is a fast stereo matching network
with a 67 ms inference time and strong generalization ability which can produce
competitive results compared to state-of-the-art methods. The results on
several bench mark datasets show that DCVSMNet achieves better accuracy than
methods such as CGI-Stereo and BGNet at the cost of greater inference time.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16475" title="Abstract">arXiv:2402.16475</a> [<a href="/pdf/2402.16475" title="Download PDF">pdf</a>, <a href="/ps/2402.16475" title="Download PostScript">ps</a>, <a href="/format/2402.16475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covert Communication Over Additive-Noise Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouette%2C+C">C&#xe9;cile Bouette</a>, 
<a href="/search/cs?searchtype=author&query=Luzzi%2C+L">Laura Luzzi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Ligong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, submitted to IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We study the fundamental limits of covert communications over general
memoryless additive-noise channels. Under mild integrability assumptions, we
find a general upper bound on the square-root scaling constant, which only
involves the variance of the logarithm of the probability density function of
the noise. Furthermore, we show that under some additional assumptions, this
upper bound is tight. We also provide upper bounds on the length of the secret
key required to achieve the optimal scaling.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16479" title="Abstract">arXiv:2402.16479</a> [<a href="/pdf/2402.16479" title="Download PDF">pdf</a>, <a href="/format/2402.16479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Detectors Can Make Deep Convolutional Neural Networks More Robust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie-Chao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yong-Zhi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Ping Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jia-Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Ji-En Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">You-Tong Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 18 figures, 7 tables. submitted to Neural Networks, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep convolutional neural networks (DCNN for short) are vulnerable to
examples with small perturbations. Improving DCNN's robustness is of great
significance to the safety-critical applications, such as autonomous driving
and industry automation. Inspired by the principal way that human eyes
recognize objects, i.e., largely relying on the shape features, this paper
first employs the edge detectors as layer kernels and designs a binary edge
feature branch (BEFB for short) to learn the binary edge features, which can be
easily integrated into any popular backbone. The four edge detectors can learn
the horizontal, vertical, positive diagonal, and negative diagonal edge
features, respectively, and the branch is stacked by multiple Sobel layers
(using edge detectors as kernels) and one threshold layer. The binary edge
features learned by the branch, concatenated with the texture features learned
by the backbone, are fed into the fully connected layers for classification. We
integrate the proposed branch into VGG16 and ResNet34, respectively, and
conduct experiments on multiple datasets. Experimental results demonstrate the
BEFB is lightweight and has no side effects on training. And the accuracy of
the BEFB integrated models is better than the original ones on all datasets
when facing FGSM, PGD, and C\&amp;W attacks. Besides, BEFB integrated models
equipped with the robustness enhancing techniques can achieve better
classification accuracy compared to the original models. The work in this paper
for the first time shows it is feasible to enhance the robustness of DCNNs
through combining both shape-like features and texture features.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16480" title="Abstract">arXiv:2402.16480</a> [<a href="/pdf/2402.16480" title="Download PDF">pdf</a>, <a href="/format/2402.16480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling ChatGPT&#x27;s Usage in Open Source Projects: A Mining-based Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tufano%2C+R">Rosalia Tufano</a>, 
<a href="/search/cs?searchtype=author&query=Mastropaolo%2C+A">Antonio Mastropaolo</a>, 
<a href="/search/cs?searchtype=author&query=Pepe%2C+F">Federica Pepe</a>, 
<a href="/search/cs?searchtype=author&query=Dabi%C4%87%2C+O">Ozren Dabi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Di+Penta%2C+M">Massimiliano Di Penta</a>, 
<a href="/search/cs?searchtype=author&query=Bavota%2C+G">Gabriele Bavota</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for publication at 21st International Conference on Mining Software Repositories (MASR'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have gained significant attention in the
software engineering community. Nowadays developers have the possibility to
exploit these models through industrial-grade tools providing a handy interface
toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting
developers across several tasks has been documented in the literature, there is
a lack of empirical evidence mapping the actual usage of LLMs in software
projects. In this work, we aim at filling such a gap. First, we mine 1,501
commits, pull requests (PRs), and issues from open-source projects by matching
regular expressions likely to indicate the usage of ChatGPT to accomplish the
task. Then, we manually analyze these instances, discarding false positives
(i.e., instances in which ChatGPT was mentioned but not actually used) and
categorizing the task automated in the 467 true positive instances (165
commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which
developers automate via ChatGPT. The taxonomy, accompanied with representative
examples, provides (i) developers with valuable insights on how to exploit LLMs
in their workflow and (ii) researchers with a clear overview of tasks that,
according to developers, could benefit from automated solutions.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16482" title="Abstract">arXiv:2402.16482</a> [<a href="/pdf/2402.16482" title="Download PDF">pdf</a>, <a href="/ps/2402.16482" title="Download PostScript">ps</a>, <a href="/format/2402.16482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Languaging a Simulation Engine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liantang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
<p class="mathjax">Language model intelligence is revolutionizing the way we program materials
simulations. However, the diversity of simulation scenarios renders it
challenging to precisely transform human language into a tailored simulator.
Here, using three functionalized types of language model, we propose a
language-to-simulation (Lang2Sim) framework that enables interactive navigation
on languaging a simulation engine, by taking a scenario instance of water
sorption in porous matrices. Unlike line-by-line coding of a target simulator,
the language models interpret each simulator as an assembly of invariant tool
function and its variant input-output pair. Lang2Sim enables the precise
transform of textual description by functionalizing and sequentializing the
language models of, respectively, rationalizing the tool categorization,
customizing its input-output combinations, and distilling the simulator input
into executable format. Importantly, depending on its functionalized type, each
language model features a distinct processing of chat history to best balance
its memory limit and information completeness, thus leveraging the model
intelligence to unstructured nature of human request. Overall, this work
establishes language model as an intelligent platform to unlock the era of
languaging a simulation engine.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16486" title="Abstract">arXiv:2402.16486</a> [<a href="/pdf/2402.16486" title="Download PDF">pdf</a>, <a href="/format/2402.16486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Known and Novel Aircraft Recognition -- A Shift from  Classification to Similarity Learning for Combat Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saeed%2C+A">Ahmad Saeed</a>, 
<a href="/search/cs?searchtype=author&query=Atif%2C+H+B">Haasha Bin Atif</a>, 
<a href="/search/cs?searchtype=author&query=Habib%2C+U">Usman Habib</a>, 
<a href="/search/cs?searchtype=author&query=Bilal%2C+M">Mohsin Bilal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Precise aircraft recognition in low-resolution remote sensing imagery is a
challenging yet crucial task in aviation, especially combat identification.
This research addresses this problem with a novel, scalable, and AI-driven
solution. The primary hurdle in combat identification in remote sensing imagery
is the accurate recognition of Novel/Unknown types of aircraft in addition to
Known types. Traditional methods, human expert-driven combat identification and
image classification, fall short in identifying Novel classes. Our methodology
employs similarity learning to discern features of a broad spectrum of military
and civilian aircraft. It discerns both Known and Novel aircraft types,
leveraging metric learning for the identification and supervised few-shot
learning for aircraft type classification. To counter the challenge of limited
low-resolution remote sensing data, we propose an end-to-end framework that
adapts to the diverse and versatile process of military aircraft recognition by
training a generalized embedder in fully supervised manner. Comparative
analysis with earlier aircraft image classification methods shows that our
approach is effective for aircraft image classification (F1-score Aircraft Type
of 0.861) and pioneering for quantifying the identification of Novel types
(F1-score Bipartitioning of 0.936). The proposed methodology effectively
addresses inherent challenges in remote sensing data, thereby setting new
standards in dataset quality. The research opens new avenues for domain experts
and demonstrates unique capabilities in distinguishing various aircraft types,
contributing to a more robust, domain-adapted potential for real-time aircraft
recognition.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16497" title="Abstract">arXiv:2402.16497</a> [<a href="/pdf/2402.16497" title="Download PDF">pdf</a>, <a href="/format/2402.16497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAND: Decoupling Sanitization from Fuzzing for Low Overhead
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Ziqiao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohua Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heqing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhendong Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Sanitizers provide robust test oracles for various software vulnerabilities.
Fuzzing on sanitizer-enabled programs has been the best practice to find
software bugs. Since sanitizers need to heavily instrument a target program to
insert run-time checks, sanitizer-enabled programs have much higher overhead
compared to normally built programs. In this paper, we present SAND, a new
fuzzing framework that decouples sanitization from the fuzzing loop. SAND
performs fuzzing on a normally built program and only invokes sanitizer-enabled
programs when input is shown to be interesting. Since most of the generated
inputs are not interesting, i.e., not bug-triggering, SAND allows most of the
fuzzing time to be spent on the normally built program. To identify interesting
inputs, we introduce execution pattern for a practical execution analysis on
the normally built program. We realize SAND on top of AFL++ and evaluate it on
12 real-world programs. Our extensive evaluation highlights its effectiveness:
on a period of 24 hours, compared to fuzzing on ASan/UBSan-enabled and
MSan-enabled programs, SAND respectively achieves 2.6x and 15x throughput and
detects 51% and 242% more bugs.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16499" title="Abstract">arXiv:2402.16499</a> [<a href="/pdf/2402.16499" title="Download PDF">pdf</a>, <a href="/format/2402.16499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMArena: Assessing Capabilities of Large Language Models in Dynamic  Multi-Agent Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junzhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuodi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+W">Wei-Wei Tu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16501" title="Abstract">arXiv:2402.16501</a> [<a href="/pdf/2402.16501" title="Download PDF">pdf</a>, <a href="/format/2402.16501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory Prediction for Autonomous Driving Using a Transformer Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenning Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Joint Conferences on Artificial Intelligence 2021
  AI4AD
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Predicting the trajectories of surrounding agents is still considered one of
the most challenging tasks for autonomous driving. In this paper, we introduce
a multi-modal trajectory prediction framework based on the transformer network.
The semantic maps of each agent are used as inputs to convolutional networks to
automatically derive relevant contextual information. A novel auxiliary loss
that penalizes unfeasible off-road predictions is also proposed in this study.
Experiments on the Lyft l5kit dataset show that the proposed model achieves
state-of-the-art performance, substantially improving the accuracy and
feasibility of the prediction outcomes.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16505" title="Abstract">arXiv:2402.16505</a> [<a href="/pdf/2402.16505" title="Download PDF">pdf</a>, <a href="/format/2402.16505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory GAPS: Would LLM pass the Tulving Test?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauvet%2C+J">Jean-Marie Chauvet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The Tulving Test was designed to investigate memory performance in
recognition and recall tasks. Its results help assess the relevance of the
"Synergistic Ecphory Model" of memory and similar RK paradigms in human
performance. This paper starts investigating whether the more than
forty-year-old framework sheds some light on LLMs' acts of remembering.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16506" title="Abstract">arXiv:2402.16506</a> [<a href="/pdf/2402.16506" title="Download PDF">pdf</a>, <a href="/format/2402.16506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Conditional Diffusion Models for Semantic Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Juyeon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+I">Inho Kong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic image synthesis (SIS) is a task to generate realistic images
corresponding to semantic maps (labels). It can be applied to diverse
real-world practices such as photo editing or content creation. However, in
real-world applications, SIS often encounters noisy user inputs. To address
this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a
robust conditional diffusion model that features novel forward and generation
processes tailored for SIS with noisy labels. It enhances robustness by
stochastically perturbing the semantic label maps through Label Diffusion,
which diffuses the labels with discrete diffusion. Through the diffusion of
labels, the noisy and clean semantic maps become similar as the timestep
increases, eventually becoming identical at $t=T$. This facilitates the
generation of an image close to a clean image, enabling robust generation.
Furthermore, we propose a class-wise noise schedule to differentially diffuse
the labels depending on the class. We demonstrate that the proposed method
generates high-quality samples through extensive experiments and analyses on
benchmark datasets, including a novel experimental setup simulating human
errors during real-world applications.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16508" title="Abstract">arXiv:2402.16508</a> [<a href="/pdf/2402.16508" title="Download PDF">pdf</a>, <a href="/format/2402.16508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Cross-lingual Open Domain Question Answering with  Large-scale Synthetic Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Drummond%2C+T">Tom Drummond</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+T">Trevor Cohn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Cross-lingual question answering (CLQA) is a complex problem, comprising
cross-lingual retrieval from a multilingual knowledge base, followed by answer
generation either in English or the query language. Both steps are usually
tackled by separate models, requiring substantial annotated datasets, and
typically auxiliary resources, like machine translation systems to bridge
between languages. In this paper, we show that CLQA can be addressed using a
single encoder-decoder model. To effectively train this model, we propose a
self-supervised method based on exploiting the cross-lingual link structure
within Wikipedia. We demonstrate how linked Wikipedia pages can be used to
synthesise supervisory signals for cross-lingual retrieval, through a form of
cloze query, and generate more natural queries to supervise answer generation.
Together, we show our approach, \texttt{CLASS}, outperforms comparable methods
on both supervised and zero-shot language adaptation settings, including those
using machine translation.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16514" title="Abstract">arXiv:2402.16514</a> [<a href="/pdf/2402.16514" title="Download PDF">pdf</a>, <a href="/format/2402.16514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancement of 3D Camera Synthetic Training Data with Noise Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osvaldov%C3%A1%2C+K">Katar&#xed;na Osvaldov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Gajdo%C5%A1ech%2C+L">Luk&#xe1;&#x161; Gajdo&#x161;ech</a>, 
<a href="/search/cs?searchtype=author&query=Kocur%2C+V">Viktor Kocur</a>, 
<a href="/search/cs?searchtype=author&query=Madaras%2C+M">Martin Madaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in 2024 Proceedings of the 27th Computer Vision Winter Workshop (CVWW). Accepted: 19.1.2024. Published: 16.2.2024. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Code: <a href="https://doi.org/10.5281/zenodo.10581562">this https URL</a> Data: <a href="https://doi.org/10.5281/zenodo.10581278">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 27th Computer Vision Winter Workshop CVWW
  (2024) 29-37
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The goal of this paper is to assess the impact of noise in 3D camera-captured
data by modeling the noise of the imaging process and applying it on synthetic
training data. We compiled a dataset of specifically constructed scenes to
obtain a noise model. We specifically model lateral noise, affecting the
position of captured points in the image plane, and axial noise, affecting the
position along the axis perpendicular to the image plane. The estimated models
can be used to emulate noise in synthetic training data. The added benefit of
adding artificial noise is evaluated in an experiment with rendered data for
object segmentation. We train a series of neural networks with varying levels
of noise in the data and measure their ability to generalize on real data. The
results show that using too little or too much noise can hurt the networks'
performance indicating that obtaining a model of noise from real scanners is
beneficial for synthetic data generation.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16515" title="Abstract">arXiv:2402.16515</a> [<a href="/pdf/2402.16515" title="Download PDF">pdf</a>, <a href="/format/2402.16515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based Privacy Data Augmentation Guided by Knowledge Distillation  with a Distribution Tutor for Medical Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yiping Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Juhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiliang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">As sufficient data are not always publically accessible for model training,
researchers exploit limited data with advanced learning algorithms or expand
the dataset via data augmentation (DA). Conducting DA in private domain
requires private protection approaches (i.e. anonymization and perturbation),
but those methods cannot provide protection guarantees. Differential privacy
(DP) learning methods theoretically bound the protection but are not skilled at
generating pseudo text samples with large models. In this paper, we transfer
DP-based pseudo sample generation task to DP-based generated samples
discrimination task, where we propose a DP-based DA method with a LLM and a
DP-based discriminator for text classification on private domains. We construct
a knowledge distillation model as the DP-based discriminator: teacher models,
accessing private data, teaches students how to select private samples with
calibrated noise to achieve DP. To constrain the distribution of DA's
generation, we propose a DP-based tutor that models the noised private
distribution and controls samples' generation with a low privacy cost. We
theoretically analyze our model's privacy protection and empirically verify our
model.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16516" title="Abstract">arXiv:2402.16516</a> [<a href="/pdf/2402.16516" title="Download PDF">pdf</a>, <a href="/format/2402.16516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Pretrained Hierarchical Transformer for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiqian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Mingyue Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yucong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent efforts have been dedicated to enhancing time series forecasting
accuracy by introducing advanced network architectures and self-supervised
pretraining strategies. Nevertheless, existing approaches still exhibit two
critical drawbacks. Firstly, these methods often rely on a single dataset for
training, limiting the model's generalizability due to the restricted scale of
the training data. Secondly, the one-step generation schema is widely followed,
which necessitates a customized forecasting head and overlooks the temporal
dependencies in the output series, and also leads to increased training costs
under different horizon length settings.
<br />To address these issues, we propose a novel generative pretrained
hierarchical transformer architecture for forecasting, named GPHT. There are
two aspects of key designs in GPHT. On the one hand, we advocate for
constructing a mixed dataset for pretraining our model, comprising various
datasets from diverse data scenarios. This approach significantly expands the
scale of training data, allowing our model to uncover commonalities in time
series data and facilitating improved transfer to specific datasets. On the
other hand, GPHT employs an auto-regressive forecasting approach under the
channel-independent assumption, effectively modeling temporal dependencies in
the output series. Importantly, no customized forecasting head is required,
enabling a single model to forecast at arbitrary horizon settings. We conduct
sufficient experiments on eight datasets with mainstream self-supervised
pretraining models and supervised models. The results demonstrated that GPHT
surpasses the baseline models across various fine-tuning and zero/few-shot
learning settings in the traditional long-term forecasting task, providing
support for verifying the feasibility of pretrained time series large models.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16517" title="Abstract">arXiv:2402.16517</a> [<a href="/pdf/2402.16517" title="Download PDF">pdf</a>, <a href="/format/2402.16517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Artificial Viscosity Models for Discontinuous Galerkin  Approximation of Conservation Laws using Physics-Informed Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caldana%2C+M">Matteo Caldana</a>, 
<a href="/search/math?searchtype=author&query=Antonietti%2C+P+F">Paola F. Antonietti</a>, 
<a href="/search/math?searchtype=author&query=Dede%27%2C+L">Luca Dede&#x27;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Finite element-based high-order solvers of conservation laws offer large
accuracy but face challenges near discontinuities due to the Gibbs phenomenon.
Artificial viscosity is a popular and effective solution to this problem based
on physical insight. In this work, we present a physics-informed machine
learning algorithm to automate the discovery of artificial viscosity models in
a non-supervised paradigm. The algorithm is inspired by reinforcement learning
and trains a neural network acting cell-by-cell (the viscosity model) by
minimizing a loss defined as the difference with respect to a reference
solution thanks to automatic differentiation. This enables a dataset-free
training procedure. We prove that the algorithm is effective by integrating it
into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase
several numerical tests on scalar and vectorial problems, such as Burgers' and
Euler's equations in one and two dimensions. Results demonstrate that the
proposed approach trains a model that is able to outperform classical viscosity
models. Moreover, we show that the learnt artificial viscosity model is able to
generalize across different problems and parameters.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16532" title="Abstract">arXiv:2402.16532</a> [<a href="/pdf/2402.16532" title="Download PDF">pdf</a>, <a href="/format/2402.16532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast, Fair and Truthful Distributed Stable Matching for Common  Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirvonen%2C+J">Juho Hirvonen</a>, 
<a href="/search/cs?searchtype=author&query=Ranjbaran%2C+S">Sara Ranjbaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Stable matching is a fundamental problem studied both in economics and
computer science. The task is to find a matching between two sides of agents
that have preferences over who they want to be matched with. A matching is
stable if no pair of agents prefer each other over their current matches. The
deferred acceptance algorithm of Gale and Shapley solves this problem in
polynomial time. Further, it is a mechanism: the proposing side in the
algorithm is always incentivised to report their preferences truthfully. The
deferred acceptance algorithm has a natural interpretation as a distributed
algorithm (and thus a distributed mechanism). However, the algorithm is slow in
the worst case and it is known that the stable matching problem cannot be
solved efficiently in the distributed setting. In this work we study a natural
special case of the stable matching problem where all agents on one side share
common preferences. We show that in this case the deferred acceptance algorithm
does yield a fast and truthful distributed mechanism for finding a stable
matching. We show how algorithms for sampling random colorings can be used to
break ties fairly and extend the results to fractional stable matching.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16534" title="Abstract">arXiv:2402.16534</a> [<a href="/pdf/2402.16534" title="Download PDF">pdf</a>, <a href="/ps/2402.16534" title="Download PostScript">ps</a>, <a href="/format/2402.16534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak-linearity, globality and in-place update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gramaglia%2C+H">Hector Gramaglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Computational interpretations of linear logic allow static control of memory
resources: the data produced by the program are endowed through its type with
attributes that determine its life cycle. This has promoted numerous
investigations into safe introduction of in-place update. Various type systems
have been proposed for this aim, but linearity and correctness of in-place
update are properties that are not fully compatible. The main achievement of
this work is to establish a simple theoretical framework that will allow us to
clarify the potential (and limits) of linearity to guarantee the process of
transforming a functional program into an imperative one.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16539" title="Abstract">arXiv:2402.16539</a> [<a href="/pdf/2402.16539" title="Download PDF">pdf</a>, <a href="/ps/2402.16539" title="Download PostScript">ps</a>, <a href="/format/2402.16539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Large Language Models with Graphical Session-Based  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+N">Naicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hongwei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qianqiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Linxun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bing Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapid development of Large Language Models (LLMs), various
explorations have arisen to utilize LLMs capability of context understanding on
recommender systems. While pioneering strategies have primarily transformed
traditional recommendation tasks into challenges of natural language
generation, there has been a relative scarcity of exploration in the domain of
session-based recommendation (SBR) due to its specificity. SBR has been
primarily dominated by Graph Neural Networks, which have achieved many
successful outcomes due to their ability to capture both the implicit and
explicit relationships between adjacent behaviors. The structural nature of
graphs contrasts with the essence of natural language, posing a significant
adaptation gap for LLMs. In this paper, we introduce large language models with
graphical Session-Based recommendation, named LLMGR, an effective framework
that bridges the aforementioned gap by harmoniously integrating LLMs with Graph
Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the
complementary strengths of LLMs in natural language understanding and GNNs in
relational data processing, leading to a more powerful session-based
recommender system that can understand and recommend items within a session.
Moreover, to endow the LLM with the capability to empower SBR tasks, we design
a series of prompts for both auxiliary and major instruction tuning tasks.
These prompts are crafted to assist the LLM in understanding graph-structured
data and align textual information with nodes, effectively translating nuanced
user interactions into a format that can be understood and utilized by LLM
architectures. Extensive experiments on three real-world datasets demonstrate
that LLMGR outperforms several competitive baselines, indicating its
effectiveness in enhancing SBR tasks and its potential as a research direction
for future exploration.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16540" title="Abstract">arXiv:2402.16540</a> [<a href="/pdf/2402.16540" title="Download PDF">pdf</a>, <a href="/ps/2402.16540" title="Download PostScript">ps</a>, <a href="/format/2402.16540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi Directed Jonsson Operations Imply Bounded Width (For fo-expansions  of symmetric binary cores with free amalgamation)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wrona%2C+M">Michal Wrona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Every CSP(B) for a finite structure B is either in P or it is NP-complete but
the proofs of the finite-domain CSP dichotomy by Andrei Bulatov and Dimitryi
Zhuk not only show the computational complexity separation but also confirm the
algebraic tractability conjecture stating that tractability origins from a
certain system of operations preserving B. The establishment of the dichotomy
was in fact preceded by a number of similar results for stronger conditions of
this type, i.e. for system of operations covering not necessarily all tractable
finite-domain CSPs.
<br />A similar, infinite-domain algebraic tractability conjecture is known for
first-order reducts of countably infinite finitely bounded homogeneous
structures and is currently wide open. In particular, with an exception of a
quasi near-unanimity operation there are no known systems of operations
implying tractability in this regime. This paper changes the state-of-the-art
and provides a proof that a chain of quasi directed Jonsson operations imply
tractability and bounded width for a large and natural class of infinite
structures.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16542" title="Abstract">arXiv:2402.16542</a> [<a href="/pdf/2402.16542" title="Download PDF">pdf</a>, <a href="/format/2402.16542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboGrind: Intuitive and Interactive Surface Treatment with Industrial  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alt%2C+B">Benjamin Alt</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%B6ckl%2C+F">Florian St&#xf6;ckl</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+S">Silvan M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+C">Christopher Braun</a>, 
<a href="/search/cs?searchtype=author&query=Raible%2C+J">Julian Raible</a>, 
<a href="/search/cs?searchtype=author&query=Alhasan%2C+S">Saad Alhasan</a>, 
<a href="/search/cs?searchtype=author&query=Rettig%2C+O">Oliver Rettig</a>, 
<a href="/search/cs?searchtype=author&query=Ringle%2C+L">Lukas Ringle</a>, 
<a href="/search/cs?searchtype=author&query=Katic%2C+D">Darko Katic</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4kel%2C+R">Rainer J&#xe4;kel</a>, 
<a href="/search/cs?searchtype=author&query=Beetz%2C+M">Michael Beetz</a>, 
<a href="/search/cs?searchtype=author&query=Strand%2C+M">Marcus Strand</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+M+F">Marco F. Huber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Surface treatment tasks such as grinding, sanding or polishing are a vital
step of the value chain in many industries, but are notoriously challenging to
automate. We present RoboGrind, an integrated system for the intuitive,
interactive automation of surface treatment tasks with industrial robots. It
combines a sophisticated 3D perception pipeline for surface scanning and
automatic defect identification, an interactive voice-controlled wizard system
for the AI-assisted bootstrapping and parameterization of robot programs, and
an automatic planning and execution pipeline for force-controlled robotic
surface treatment. RoboGrind is evaluated both under laboratory and real-world
conditions in the context of refabricating fiberglass wind turbine blades.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16544" title="Abstract">arXiv:2402.16544</a> [<a href="/pdf/2402.16544" title="Download PDF">pdf</a>, <a href="/format/2402.16544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Learning Method Based on Tensor Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Quanxue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Cheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+D">Deyan Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multi-view clustering method based on anchor graph has been widely concerned
due to its high efficiency and effectiveness. In order to avoid
post-processing, most of the existing anchor graph-based methods learn
bipartite graphs with connected components. However, such methods have high
requirements on parameters, and in some cases it may not be possible to obtain
bipartite graphs with clear connected components. To end this, we propose a
label learning method based on tensor projection (LLMTP). Specifically, we
project anchor graph into the label space through an orthogonal projection
matrix to obtain cluster labels directly. Considering that the spatial
structure information of multi-view data may be ignored to a certain extent
when projected in different views separately, we extend the matrix projection
transformation to tensor projection, so that the spatial structure information
between views can be fully utilized. In addition, we introduce the tensor
Schatten $p$-norm regularization to make the clustering label matrices of
different views as consistent as possible. Extensive experiments have proved
the effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16546" title="Abstract">arXiv:2402.16546</a> [<a href="/pdf/2402.16546" title="Download PDF">pdf</a>, <a href="/format/2402.16546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep  Learning Projects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sijia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Turhan%2C+B">Burak Turhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaodong Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM Transactions on Software Engineering and Methodology (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep Learning (DL) models have rapidly advanced, focusing on achieving high
performance through testing model accuracy and robustness. However, it is
unclear whether DL projects, as software systems, are tested thoroughly or
functionally correct when there is a need to treat and test them like other
software systems. Therefore, we empirically study the unit tests in open-source
DL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested
DL projects have positive correlation with the open-source project metrics and
have a higher acceptance rate of pull requests, 2) 68% of the sampled DL
projects are not unit tested at all, 3) the layer and utilities (utils) of DL
models have the most unit tests. Based on these findings and previous research
outcomes, we built a mapping taxonomy between unit tests and faults in DL
projects. We discuss the implications of our findings for developers and
researchers and highlight the need for unit testing in open-source DL projects
to ensure their reliability and stability. The study contributes to this
community by raising awareness of the importance of unit testing in DL projects
and encouraging further research in this area.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16547" title="Abstract">arXiv:2402.16547</a> [<a href="/pdf/2402.16547" title="Download PDF">pdf</a>, <a href="/ps/2402.16547" title="Download PostScript">ps</a>, <a href="/format/2402.16547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-Designed Contracts: How to Sell Hidden Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+M">Martino Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the problem faced by a service provider that has to sell services to
a user. In our model the service provider proposes various payment options (a
menu) to the user which may be based, for example, on the quality of the
service. Then, the user chooses one of these options and pays an amount to the
service provider, contingent on the observed final outcome. Users are not able
to observe directly the action performed by the service provide to reach the
final outcome. This might incentivize misconduct. Therefore, we propose a model
that enforces trust through economics incentives. The problem has two crucial
features: i) the service provider is responsible for both formulating the
contract and performing the action for which the user issues payments, and ii)
the user is unaware of the true action carried out by the service provider,
which is hidden. We study this delegation problem through the lens of contract
design, with the overarching goal of enabling the computation of contracts that
guarantee that the user can trust the service provider, even if their action is
hidden.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16548" title="Abstract">arXiv:2402.16548</a> [<a href="/pdf/2402.16548" title="Download PDF">pdf</a>, <a href="/format/2402.16548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point collocation with mollified piecewise polynomial approximants for  high-order partial differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alfarisy%2C+D">Dewangga Alfarisy</a>, 
<a href="/search/math?searchtype=author&query=Zuhal%2C+L">Lavi Zuhal</a>, 
<a href="/search/math?searchtype=author&query=Ortiz%2C+M">Michael Ortiz</a>, 
<a href="/search/math?searchtype=author&query=Cirak%2C+F">Fehmi Cirak</a>, 
<a href="/search/math?searchtype=author&query=Febrianto%2C+E">Eky Febrianto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The solution approximation for partial differential equations (PDEs) can be
substantially improved using smooth basis functions. The recently introduced
mollified basis functions are constructed through mollification, or
convolution, of cell-wise defined piecewise polynomials with a smooth mollifier
of certain characteristics. The properties of the mollified basis functions are
governed by the order of the piecewise functions and the smoothness of the
mollifier. In this work, we exploit the high-order and high-smoothness
properties of the mollified basis functions for solving PDEs through the point
collocation method. The basis functions are evaluated at a set of collocation
points in the domain. In addition, boundary conditions are imposed at a set of
boundary collocation points distributed over the domain boundaries. To ensure
the stability of the resulting linear system of equations, the number of
collocation points is set larger than the total number of basis functions. The
resulting linear system is overdetermined and is solved using the least square
technique. The presented numerical examples confirm the convergence of the
proposed approximation scheme for Poisson, linear elasticity, and biharmonic
problems. We study in particular the influence of the mollifier and the spatial
distribution of the collocation points.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16553" title="Abstract">arXiv:2402.16553</a> [<a href="/pdf/2402.16553" title="Download PDF">pdf</a>, <a href="/ps/2402.16553" title="Download PostScript">ps</a>, <a href="/format/2402.16553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contracts with Inspections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+M">Matteo Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In the classical principal-agent hidden-action model, a principal delegates
the execution of a costly task to an agent for which he can choose among
actions with different costs and different success probabilities to accomplish
the task. To incentivize the agent to exert effort, the principal can commit to
a contract, which is the amount of payment based on the task's success. A
crucial assumption of this model is that the principal can only base the
payment on the outcome but not on the agent's chosen action.
<br />In this work, we relax the hidden-action assumption and introduce a new model
where the principal is allowed to inspect subsets of actions at some cost that
depends on the inspected subset. If the principal discovers that the agent did
not select the agreed-upon action through the inspection, the principal can
withhold payment. This relaxation of the model introduces a broader strategy
space for the principal, who now faces a tradeoff between positive incentives
(increasing payment) and negative incentives (increasing inspection).
<br />We show how to find the best deterministic incentive-compatible inspection
scheme for all monotone inspection cost functions. We then turn to randomized
inspection schemes and show that one can efficiently find the best randomized
incentive-compatible inspection scheme when the inspection cost function is
submodular. We complement this result by showing that it is impossible to
efficiently find the optimal randomized inspection scheme for the more general
case of XOS inspection cost functions.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16557" title="Abstract">arXiv:2402.16557</a> [<a href="/pdf/2402.16557" title="Download PDF">pdf</a>, <a href="/format/2402.16557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A randomized algorithm for simultaneously diagonalizing symmetric  matrices by congruence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=He%2C+H">Haoze He</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A family of symmetric matrices $A_1,\ldots, A_d$ is SDC (simultaneous
diagonalization by congruence) if there is an invertible matrix $X$ such that
every $X^T A_k X$ is diagonal. In this work, a novel randomized SDC (RSDC)
algorithm is proposed that reduces SDC to a generalized eigenvalue problem by
considering two (random) linear combinations of the family. We establish exact
recovery: RSDC achieves diagonalization with probability $1$ if the family is
exactly SDC. Under a mild regularity assumption, robust recovery is also
established: Given a family that is $\epsilon$-close to SDC then RSDC
diagonalizes, with high probability, the family up to an error of norm
$\mathcal{O}(\epsilon)$. Under a positive definiteness assumption, which often
holds in applications, stronger results are established, including a bound on
the condition number of the transformation matrix. For practical use, we
suggest to combine RSDC with an optimization algorithm. The performance of the
resulting method is verified for synthetic data, image separation and EEG
analysis tasks. It turns out that our newly developed method outperforms
existing optimization-based methods in terms of efficiency while achieving a
comparable level of accuracy.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16558" title="Abstract">arXiv:2402.16558</a> [<a href="/pdf/2402.16558" title="Download PDF">pdf</a>, <a href="/format/2402.16558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Your Ears to Take a Look: A State-of-the-Art Report on the  Integration of Sonification and Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enge%2C+K">Kajetan Enge</a>, 
<a href="/search/cs?searchtype=author&query=Elmquist%2C+E">Elias Elmquist</a>, 
<a href="/search/cs?searchtype=author&query=Caiola%2C+V">Valentina Caiola</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6nnberg%2C+N">Niklas R&#xf6;nnberg</a>, 
<a href="/search/cs?searchtype=author&query=Rind%2C+A">Alexander Rind</a>, 
<a href="/search/cs?searchtype=author&query=Iber%2C+M">Michael Iber</a>, 
<a href="/search/cs?searchtype=author&query=Lenzi%2C+S">Sara Lenzi</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+F">Fangfei Lan</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6ldrich%2C+R">Robert H&#xf6;ldrich</a>, 
<a href="/search/cs?searchtype=author&query=Aigner%2C+W">Wolfgang Aigner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 10 figures, submitted to EuroVis 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The research communities studying visualization and sonification for data
display and analysis share exceptionally similar goals, essentially making data
of any kind interpretable to humans. One community does so by using visual
representations of data, the other community does so by employing auditory
(non-speech) representations of data. While the two communities have a lot in
common, they developed mostly in parallel over the course of the last few
decades. With this STAR, we discuss a collection of work that bridges the
borders of the two communities, hence a collection of work that aims to
integrate the two techniques to one form of audiovisual display, which we argue
to be "more than the sum of the two." We introduce and motivate a
classification system applicable to such audiovisual displays and categorize a
corpus of 57 academic publications that appeared between 2011 and 2023 in
categories such as reading level, dataset type, or evaluation system, to
mention a few. The corpus also enables a meta-analysis of the field, including
regularly occurring design patterns such as type of visualization and
sonification techniques, or the use of visual and auditory channels, and the
analysis of a co-author network of the field which shows individual teams
without much interconnection. The body of work covered in this STAR also
relates to three adjacent topics: audiovisual monitoring, accessibility, and
audiovisual data art. These three topics are discussed individually in addition
to the systematically conducted part of this research. The findings of this
report may be used by researchers from both fields to understand the potentials
and challenges of such integrated designs, while inspiring them for future
collaboration with experts from the respective other field.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16562" title="Abstract">arXiv:2402.16562</a> [<a href="/pdf/2402.16562" title="Download PDF">pdf</a>, <a href="/ps/2402.16562" title="Download PostScript">ps</a>, <a href="/format/2402.16562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-FOX Learning: Breaking Tradition in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alqaseer%2C+M">Mahmood Alqaseer</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+Y+H">Yossra H. Ali</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+T+A">Tarik A. Rashid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16565" title="Abstract">arXiv:2402.16565</a> [<a href="/pdf/2402.16565" title="Download PDF">pdf</a>, <a href="/format/2402.16565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Rankings of Optimizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodemann%2C+J">Julian Rodemann</a>, 
<a href="/search/cs?searchtype=author&query=Blocher%2C+H">Hannah Blocher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a framework for benchmarking optimizers according to multiple
criteria over various test functions. Based on a recently introduced union-free
generic depth function for partial orders/rankings, it fully exploits the
ordinal information and allows for incomparability. Our method describes the
distribution of all partial orders/rankings, avoiding the notorious
shortcomings of aggregation. This permits to identify test functions that
produce central or outlying rankings of optimizers and to assess the quality of
benchmarking suites.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16567" title="Abstract">arXiv:2402.16567</a> [<a href="/pdf/2402.16567" title="Download PDF">pdf</a>, <a href="/format/2402.16567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Large Language Models to a Domain-specific Graph Database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuanyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K">Keren Tan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tingyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+W">Wenbiao Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">Graph Databases (Graph DB) are widely applied in various fields, including
finance, social networks, and medicine. However, translating Natural Language
(NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to
be challenging due to its inherent complexity and specialized nature. Some
approaches have sought to utilize Large Language Models (LLMs) to address
analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a
particular domain, the absence of domain-specific NL-GQL data pairs makes it
difficult to establish alignment between LLMs and the graph DB. To address this
challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT
to create NL-GQL data pairs based on the given graph DB with self-instruct.
Then, we use the created data to fine-tune LLMs, thereby achieving alignment
between LLMs and the graph DB. Additionally, during inference, we propose a
method that extracts relevant schema to the queried NL as the input context to
guide LLMs for generating accurate GQLs.We evaluate our method on two
constructed datasets deriving from graph DBs in finance domain and medicine
domain, namely FinGQL and MediGQL. Experimental results demonstrate that our
method significantly outperforms a set of baseline methods, with improvements
of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on
EX, respectively.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16568" title="Abstract">arXiv:2402.16568</a> [<a href="/pdf/2402.16568" title="Download PDF">pdf</a>, <a href="/format/2402.16568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Generative Question Answering on Temporal Knowledge Graph  Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yifu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+L">Linbo Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+Z">Zhigang Kan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhihua Wen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongquan He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal knowledge graph question answering (TKGQA) poses a significant
challenge task, due to the temporal constraints hidden in questions and the
answers sought from dynamic structured knowledge. Although large language
models (LLMs) have made considerable progress in their reasoning ability over
structured data, their application to the TKGQA task is a relatively unexplored
area. This paper first proposes a novel generative temporal knowledge graph
question answering framework, GenTKGQA, which guides LLMs to answer temporal
questions through two phases: Subgraph Retrieval and Answer Generation. First,
we exploit LLM's intrinsic knowledge to mine temporal constraints and
structural links in the questions without extra training, thus narrowing down
the subgraph search space in both temporal and structural dimensions. Next, we
design virtual knowledge indicators to fuse the graph neural network signals of
the subgraph and the text representations of the LLM in a non-shallow way,
which helps the open-source LLM deeply understand the temporal order and
structural dependencies among the retrieved facts through instruction tuning.
Experimental results demonstrate that our model outperforms state-of-the-art
baselines, even achieving 100\% on the metrics for the simple question type.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16569" title="Abstract">arXiv:2402.16569</a> [<a href="/pdf/2402.16569" title="Download PDF">pdf</a>, <a href="/format/2402.16569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrained Visual Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirchhof%2C+M">Michael Kirchhof</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+M">Mark Collier</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate uncertainty estimation is vital to trustworthy machine learning, yet
uncertainties typically have to be learned for each task anew. This work
introduces the first pretrained uncertainty modules for vision models. Similar
to standard pretraining this enables the zero-shot transfer of uncertainties
learned on a large pretraining dataset to specialized downstream datasets. We
enable our large-scale pretraining on ImageNet-21k by solving a gradient
conflict in previous uncertainty modules and accelerating the training by up to
180x. We find that the pretrained uncertainties generalize to unseen datasets.
In scrutinizing the learned uncertainties, we find that they capture aleatoric
uncertainty, disentangled from epistemic components. We demonstrate that this
enables safe retrieval and uncertainty-aware dataset visualization. To
encourage applications to further problems and domains, we release all
pretrained checkpoints and code under https://github.com/mkirchhof/url .
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16570" title="Abstract">arXiv:2402.16570</a> [<a href="/pdf/2402.16570" title="Download PDF">pdf</a>, <a href="/format/2402.16570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Searching a Lightweight Network Architecture for Thermal Infrared  Pedestrian Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ru-Yue Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Manually-designed network architectures for thermal infrared pedestrian
tracking (TIR-PT) require substantial effort from human experts. Neural
networks with ResNet backbones are popular for TIR-PT. However, TIR-PT is a
tracking task and more challenging than classification and detection. This
paper makes an early attempt to search an optimal network architecture for
TIR-PT automatically, employing single-bottom and dual-bottom cells as basic
search units and incorporating eight operation candidates within the search
space. To expedite the search process, a random channel selection strategy is
employed prior to assessing operation candidates. Classification, batch hard
triplet, and center loss are jointly used to retrain the searched architecture.
The outcome is a high-performance network architecture that is both parameter-
and computation-efficient. Extensive experiments proved the effectiveness of
the automated method.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16572" title="Abstract">arXiv:2402.16572</a> [<a href="/pdf/2402.16572" title="Download PDF">pdf</a>, <a href="/ps/2402.16572" title="Download PostScript">ps</a>, <a href="/format/2402.16572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bottom-Left Algorithm for the Strip Packing Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hougardy%2C+S">Stefan Hougardy</a>, 
<a href="/search/cs?searchtype=author&query=Zondervan%2C+B">Bart Zondervan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">The bottom-left algorithm is a simple heuristic for the Strip Packing
Problem. It places the rectangles in the given order at the lowest free
position in the strip, using the left most position in case of ties. Despite
its simplicity, the exact approximation ratio of the bottom-left algorithm
remains unknown. We will improve the more-than-40-year-old value for the lower
bound from $5/4$ to $4/3 - \varepsilon$. Additionally, we will show that this
lower bound holds even in the special case of squares, where the previously
known lower bound was $12/11 -\varepsilon$. These lower bounds apply regardless
of the ordering of the rectangles. When squares are arranged in the worst
possible order, we establish a constant upper bound and a $10/3-\varepsilon$
lower bound for the approximation ratio of the bottom-left algorithm. This
bound also applies to some online setting and yields an almost tight result
there. Finally, we show that the approximation ratio of a local search
algorithm based on permuting rectangles in the ordering of the bottom-left
algorithm is at least~$2$ and that such an algorithm may need an exponential
number of improvement steps to reach a local optimum.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16578" title="Abstract">arXiv:2402.16578</a> [<a href="/pdf/2402.16578" title="Download PDF">pdf</a>, <a href="/format/2402.16578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Bit Distortion-Free Watermarking for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boroujeny%2C+M+K">Massieh Kordi Boroujeny</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Ya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Mark%2C+B">Brian Mark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Methods for watermarking large language models have been proposed that
distinguish AI-generated text from human-generated text by slightly altering
the model output distribution, but they also distort the quality of the text,
exposing the watermark to adversarial detection. More recently, distortion-free
watermarking methods were proposed that require a secret key to detect the
watermark. The prior methods generally embed zero-bit watermarks that do not
provide additional information beyond tagging a text as being AI-generated. We
extend an existing zero-bit distortion-free watermarking method by embedding
multiple bits of meta-information as part of the watermark. We also develop a
computationally efficient decoder that extracts the embedded information from
the watermark with low bit error rate.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16586" title="Abstract">arXiv:2402.16586</a> [<a href="/pdf/2402.16586" title="Download PDF">pdf</a>, <a href="/format/2402.16586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the JPEG-resistance of Adversarial Attacks on Face Recognition  by Interpolation Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kefu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fengfan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Hefei Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Ping Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">JPEG compression can significantly impair the performance of adversarial face
examples, which previous adversarial attacks on face recognition (FR) have not
adequately addressed. Considering this challenge, we propose a novel
adversarial attack on FR that aims to improve the resistance of adversarial
examples against JPEG compression. Specifically, during the iterative process
of generating adversarial face examples, we interpolate the adversarial face
examples into a smaller size. Then we utilize these interpolated adversarial
face examples to create the adversarial examples in the next iteration.
Subsequently, we restore the adversarial face examples to their original size
by interpolating. Throughout the entire process, our proposed method can smooth
the adversarial perturbations, effectively mitigating the presence of
high-frequency signals in the crafted adversarial face examples that are
typically eliminated by JPEG compression. Our experimental results demonstrate
the effectiveness of our proposed method in improving the JPEG-resistance of
adversarial face examples.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16587" title="Abstract">arXiv:2402.16587</a> [<a href="/pdf/2402.16587" title="Download PDF">pdf</a>, <a href="/format/2402.16587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed LSTM-Based Delay Compensation Framework for  Teleoperated UGVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Abubakar%2C+A">Ahmad Abubakar</a>, 
<a href="/search/eess?searchtype=author&query=Zweiri%2C+Y">Yahya Zweiri</a>, 
<a href="/search/eess?searchtype=author&query=Haddad%2C+A">AbdelGafoor Haddad</a>, 
<a href="/search/eess?searchtype=author&query=Yakubu%2C+M">Mubarak Yakubu</a>, 
<a href="/search/eess?searchtype=author&query=Alhammadi%2C+R">Ruqayya Alhammadi</a>, 
<a href="/search/eess?searchtype=author&query=Seneviratne%2C+L">Lakmal Seneviratne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Bilateral teleoperation of low-speed Unmanned Ground Vehicles (UGVs) on soft
terrains is crucial for applications like lunar exploration, offering effective
control of terrain-induced longitudinal slippage. However, latency arising from
transmission delays over a network presents a challenge in maintaining
high-fidelity closed-loop integration, potentially hindering UGV controls and
leading to poor command-tracking performance. To address this challenge, this
paper proposes a novel predictor framework that employs a Physics-informed Long
Short-Term Memory (PiLSTM) network for designing bilateral teleoperator
controls that effectively compensate for large delays. Contrasting with
conventional model-free predictor frameworks, which are limited by their linear
nature in capturing nonlinear and temporal dynamic behaviors, our approach
integrates the LSTM structure with physical constraints for enhanced
performance and better generalization across varied scenarios. Specifically,
four distinct predictors were employed in the framework: two compensate for
forward delays, while the other two compensate for backward delays. Due to
their effectiveness in learning from temporal data, the proposed PiLSTM
framework demonstrates a 26.1\ improvement in delay compensation over the
conventional model-free predictors for large delays in open-loop case studies.
Subsequently, experiments were conducted to validate the efficacy of the
framework in close-loop scenarios, particularly to compensate for the
real-network delays experienced by teleoperated UGVs coupled with longitudinal
slippage. The results confirm the proposed framework is effective in restoring
the fidelity of the closed-loop integration. This improvement is showcased
through improved performance and transparency, which leads to excellent
command-tracking performance.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16589" title="Abstract">arXiv:2402.16589</a> [<a href="/pdf/2402.16589" title="Download PDF">pdf</a>, <a href="/format/2402.16589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isogeometric analysis of the Laplace eigenvalue problem on circular  sectors: Regularity properties, graded meshes &amp; variational crimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Apel%2C+T">Thomas Apel</a>, 
<a href="/search/math?searchtype=author&query=Zilk%2C+P">Philipp Zilk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Laplace eigenvalue problem on circular sectors has eigenfunctions with
corner singularities. Standard methods may produce suboptimal approximation
results. To address this issue, a novel numerical algorithm that enhances
standard isogeometric analysis is proposed in this paper by using a
single-patch graded mesh refinement scheme. Numerical tests demonstrate optimal
convergence rates for both the eigenvalues and eigenfunctions. Furthermore, the
results show that smooth splines possess a superior approximation constant
compared to their $C^0$-continuous counterparts for the lower part of the
Laplace spectrum. This is an extension of previous findings about excellent
spectral approximation properties of smooth splines on rectangular domains to
circular sectors. In addition, graded meshes prove to be particularly
advantageous for an accurate approximation of a limited number of eigenvalues.
The novel algorithm applied here has a drawback in the singularity of the
isogeometric parameterization. It results in some basis functions not belonging
to the solution space of the corresponding weak problem, which is considered a
variational crime. However, the approach proves to be robust. Finally, a
hierarchical mesh structure is presented to avoid anisotropic elements, omit
redundant degrees of freedom and keep the number of basis functions
contributing to the variational crime constant, independent of the mesh size.
Numerical results validate the effectiveness of hierarchical mesh grading for
the simulation of eigenfunctions with and without corner singularities.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16592" title="Abstract">arXiv:2402.16592</a> [<a href="/pdf/2402.16592" title="Download PDF">pdf</a>, <a href="/ps/2402.16592" title="Download PostScript">ps</a>, <a href="/format/2402.16592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech as Interactive Design Material (SIDM): How to design and evaluate  task-tailored synthetic voices?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubiel%2C+M">Mateusz Dubiel</a>, 
<a href="/search/cs?searchtype=author&query=Aylett%2C+M">Matthew Aylett</a>, 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+A">Anuschka Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zilin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+G">Gary Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Wambsganss%2C+T">Thiemo Wambsganss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Authors' preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The aim of this workshop is two-fold. First, it aims to establish a research
community focused on design and evaluation of synthetic speech (TTS) interfaces
that are tailored not only to goal oriented tasks (e.g., food ordering, online
shopping) but also personal growth and resilience promoting applications (e.g.,
coaching, mindful reflection, and tutoring). Second, through discussion and
collaborative efforts, to establish a set of practices and standards that will
help to improve ecological validity of TTS evaluation. In particular, the
workshop will explore the topics such as: interaction design of voice-based
conversational interfaces; the interplay between prosodic aspects (e.g., pitch
variance, loudness, jitter) of TTS and its impact on voice perception. This
workshop will serve as a platform on which to build a community that is better
equipped to tackle the dynamic field of interactive TTS interfaces, which
remains understudied, yet increasingly pertinent to everyday lives of users.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16594" title="Abstract">arXiv:2402.16594</a> [<a href="/pdf/2402.16594" title="Download PDF">pdf</a>, <a href="/format/2402.16594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qixuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hong Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To achieve greater accuracy, hypergraph matching algorithms require
exponential increases in computational resources. Recent kd-tree-based
approximate nearest neighbor (ANN) methods, despite the sparsity of their
compatibility tensor, still require exhaustive calculations for large-scale
graph matching. This work utilizes CUR tensor decomposition and introduces a
novel cascaded second and third-order hypergraph matching framework (CURSOR)
for efficient hypergraph matching. A CUR-based second-order graph matching
algorithm is used to provide a rough match, and then the core of CURSOR, a
fiber-CUR-based tensor generation method, directly calculates entries of the
compatibility tensor by leveraging the initial second-order match result. This
significantly decreases the time complexity and tensor density. A probability
relaxation labeling (PRL)-based matching algorithm, specifically suitable for
sparse tensors, is developed. Experiment results on large-scale synthetic
datasets and widely-adopted benchmark sets demonstrate the superiority of
CURSOR over existing methods. The tensor generation method in CURSOR can be
integrated seamlessly into existing hypergraph matching methods to improve
their performance and lower their computational costs.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16596" title="Abstract">arXiv:2402.16596</a> [<a href="/pdf/2402.16596" title="Download PDF">pdf</a>, <a href="/format/2402.16596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic change detection for Slovene language: a novel dataset and an  approach based on optimal transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pranji%C4%87%2C+M">Marko Pranji&#x107;</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Dobrovoljc%2C+K">Kaja Dobrovoljc</a> (1), 
<a href="/search/cs?searchtype=author&query=Pollak%2C+S">Senja Pollak</a> (1), 
<a href="/search/cs?searchtype=author&query=Martinc%2C+M">Matej Martinc</a> (1) ((1) Jo&#x17e;ef Stefan Institute, Ljubljana, Slovenia, (2) Jo&#x17e;ef Stefan International Postgraduate School, Ljubljana, Slovenia)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we focus on the detection of semantic changes in Slovene, a
less resourced Slavic language with two million speakers. Detecting and
tracking semantic changes provides insights into the evolution of the language
caused by changes in society and culture. Recently, several systems have been
proposed to aid in this study, but all depend on manually annotated gold
standard datasets for evaluation. In this paper, we present the first Slovene
dataset for evaluating semantic change detection systems, which contains
aggregated semantic change scores for 104 target words obtained from more than
3000 manually annotated sentence pairs. We evaluate several existing semantic
change detection methods on this dataset and also propose a novel approach
based on optimal transport that improves on the existing state-of-the-art
systems with an error reduction rate of 22.8%.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16598" title="Abstract">arXiv:2402.16598</a> [<a href="/pdf/2402.16598" title="Download PDF">pdf</a>, <a href="/format/2402.16598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PCR-99: A Practical Method for Point Cloud Registration with 99%  Outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">Seong Hun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Civera%2C+J">Javier Civera</a>, 
<a href="/search/cs?searchtype=author&query=Vandewalle%2C+P">Patrick Vandewalle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We propose a robust method for point cloud registration that can handle both
unknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a
deterministic 3-point sampling approach with two novel mechanisms that
significantly boost the speed: (1) an improved ordering of the samples based on
pairwise scale consistency, prioritizing the point correspondences that are
more likely to be inliers, and (2) an efficient outlier rejection scheme based
on triplet scale consistency, prescreening bad samples and reducing the number
of hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio,
the proposed method achieves comparable performance to the state of the art. At
99% outlier ratio, however, it outperforms the state of the art for both
known-scale and unknown-scale problems. Especially for the latter, we observe a
clear superiority in terms of robustness and speed.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16599" title="Abstract">arXiv:2402.16599</a> [<a href="/pdf/2402.16599" title="Download PDF">pdf</a>, <a href="/format/2402.16599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolution-Agnostic Neural Compression for High-Fidelity Portrait Video  Conferencing via Implicit Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yicong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, accepted by IFTC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Video conferencing has caught much more attention recently. High fidelity and
low bandwidth are two major objectives of video compression for video
conferencing applications. Most pioneering methods rely on classic video
compression codec without high-level feature embedding and thus can not reach
the extremely low bandwidth. Recent works instead employ model-based neural
compression to acquire ultra-low bitrates using sparse representations of each
frame such as facial landmark information, while these approaches can not
maintain high fidelity due to 2D image-based warping. In this paper, we propose
a novel low bandwidth neural compression approach for high-fidelity portrait
video conferencing using implicit radiance fields to achieve both major
objectives. We leverage dynamic neural radiance fields to reconstruct
high-fidelity talking head with expression features, which are represented as
frame substitution for transmission. The overall system employs deep model to
encode expression features at the sender and reconstruct portrait at the
receiver with volume rendering as decoder for ultra-low bandwidth. In
particular, with the characteristic of neural radiance fields based model, our
compression approach is resolution-agnostic, which means that the low bandwidth
achieved by our approach is independent of video resolution, while maintaining
fidelity for higher resolution reconstruction. Experimental results demonstrate
that our novel framework can (1) construct ultra-low bandwidth video
conferencing, (2) maintain high fidelity portrait and (3) have better
performance on high-resolution video compression than previous works.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16602" title="Abstract">arXiv:2402.16602</a> [<a href="/pdf/2402.16602" title="Download PDF">pdf</a>, <a href="/format/2402.16602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Negative Instances for Generative Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuyang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pinzheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zecheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bowen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated impressive capabilities for
generalizing in unseen tasks. In the Named Entity Recognition (NER) task,
recent advancements have seen the remarkable improvement of LLMs in a broad
range of entity domains via instruction tuning, by adopting entity-centric
schema. In this work, we explore the potential enhancement of the existing
methods by incorporating negative instances into training. Our experiments
reveal that negative instances contribute to remarkable improvements by (1)
introducing contextual information, and (2) clearly delineating label
boundaries. Furthermore, we introduce a novel and efficient algorithm named
Hierarchical Matching, which is tailored to transform unstructured predictions
into structured entities. By integrating these components, we present GNER, a
Generative NER system that shows improved zero-shot performance across unseen
entity domains. Our comprehensive evaluation illustrates our system's
superiority, surpassing state-of-the-art (SoTA) methods by 11 $F_1$ score in
zero-shot evaluation.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16606" title="Abstract">arXiv:2402.16606</a> [<a href="/pdf/2402.16606" title="Download PDF">pdf</a>, <a href="/format/2402.16606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence analysis for a fully-discrete finite element approximation  of the unsteady $p(\cdot,\cdot)$-Navier-Stokes equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berselli%2C+L+C">Luigi C. Berselli</a>, 
<a href="/search/math?searchtype=author&query=Kaltenbach%2C+A">Alex Kaltenbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 6 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In the present paper, we establish the well-posedness, stability, and (weak)
convergence of a fully-discrete approximation of the unsteady
$p(\cdot,\cdot)$-Navier-Stokes equations employing an implicit Euler step in
time and a discretely inf-sup-stable finite element approximation in space.
Moreover, numerical experiments are carried out that supplement the theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16607" title="Abstract">arXiv:2402.16607</a> [<a href="/pdf/2402.16607" title="Download PDF">pdf</a>, <a href="/format/2402.16607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Haocheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+E">Errui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents GEA, a novel method for creating expressive 3D avatars
with high-fidelity reconstructions of body and hands based on 3D Gaussians. The
key contributions are twofold. First, we design a two-stage pose estimation
method to obtain an accurate SMPL-X pose from input images, providing a correct
mapping between the pixels of a training image and the SMPL-X model. It uses an
attention-aware network and an optimization scheme to align the normal and
silhouette between the estimated SMPL-X body and the real body in the image.
Second, we propose an iterative re-initialization strategy to handle unbalanced
aggregation and initialization bias faced by Gaussian representation. This
strategy iteratively redistributes the avatar's Gaussian points, making it
evenly distributed near the human body surface by applying meshing, resampling
and re-Gaussian operations. As a result, higher-quality rendering can be
achieved. Extensive experimental analyses validate the effectiveness of the
proposed model, demonstrating that it achieves state-of-the-art performance in
photorealistic novel view synthesis while offering fine-grained control over
the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16608" title="Abstract">arXiv:2402.16608</a> [<a href="/pdf/2402.16608" title="Download PDF">pdf</a>, <a href="/format/2402.16608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAQA: Toward ProActive Open-Retrieval Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erbacher%2C+P">Pierre Erbacher</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>, 
<a href="/search/cs?searchtype=author&query=Preux%2C+P">Philippe Preux</a>, 
<a href="/search/cs?searchtype=author&query=Soulier%2C+L">Laure Soulier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Conversational systems have made significant progress in generating natural
language responses. However, their potential as conversational search systems
is currently limited due to their passive role in the information-seeking
process. One major limitation is the scarcity of datasets that provide labelled
ambiguous questions along with a supporting corpus of documents and relevant
clarifying questions. This work aims to tackle the challenge of generating
relevant clarifying questions by taking into account the inherent ambiguities
present in both user queries and documents. To achieve this, we propose PAQA,
an extension to the existing AmbiNQ dataset, incorporating clarifying
questions. We then evaluate various models and assess how passage retrieval
impacts ambiguity detection and the generation of clarifying questions. By
addressing this gap in conversational search systems, we aim to provide
additional supervision to enhance their active participation in the
information-seeking process and provide users with more accurate results.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16611" title="Abstract">arXiv:2402.16611</a> [<a href="/pdf/2402.16611" title="Download PDF">pdf</a>, <a href="/format/2402.16611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Dataset Practitioners Behind Large Language Model  Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Crystal Qian</a>, 
<a href="/search/cs?searchtype=author&query=Reif%2C+E">Emily Reif</a>, 
<a href="/search/cs?searchtype=author&query=Kahng%2C+M">Minsuk Kahng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">As large language models (LLMs) become more advanced and impactful, it is
increasingly important to scrutinize the data that they rely upon and produce.
What is it to be a dataset practitioner doing this work? We approach this in
two parts: first, we define the role of "dataset practitioner" by performing a
retrospective analysis on the responsibilities of teams contributing to LLM
development at Google. Then, we conduct semi-structured interviews with a
cross-section of these practitioners (N=10). We find that data quality is the
top priority. To evaluate data quality, practitioners either rely on their own
intuition or write custom evaluation logic. There is a lack of consensus across
practitioners on what quality is and how to evaluate it. We discuss potential
reasons for this phenomenon and opportunities for alignment.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16613" title="Abstract">arXiv:2402.16613</a> [<a href="/pdf/2402.16613" title="Download PDF">pdf</a>, <a href="/format/2402.16613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Preserving Operator Learning: Modeling the Collision Operator  of Kinetic Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+J+Y">Jae Yong Lee</a>, 
<a href="/search/math?searchtype=author&query=Schotth%C3%B6fer%2C+S">Steffen Schotth&#xf6;fer</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+T">Tianbai Xiao</a>, 
<a href="/search/math?searchtype=author&query=Krumscheid%2C+S">Sebastian Krumscheid</a>, 
<a href="/search/math?searchtype=author&query=Frank%2C+M">Martin Frank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work explores the application of deep operator learning principles to a
problem in statistical physics. Specifically, we consider the linear kinetic
equation, consisting of a differential advection operator and an integral
collision operator, which is a powerful yet expensive mathematical model for
interacting particle systems with ample applications, e.g., in radiation
transport. We investigate the capabilities of the Deep Operator network
(DeepONet) approach to modelling the high dimensional collision operator of the
linear kinetic equation. This integral operator has crucial analytical
structures that a surrogate model, e.g., a DeepONet, needs to preserve to
enable meaningful physical simulation. We propose several DeepONet
modifications to encapsulate essential structural properties of this integral
operator in a DeepONet model. To be precise, we adapt the architecture of the
trunk-net so the DeepONet has the same collision invariants as the theoretical
kinetic collision operator, thus preserving conserved quantities, e.g., mass,
of the modeled many-particle system. Further, we propose an entropy-inspired
data-sampling method tailored to train the modified DeepONet surrogates without
requiring an excessive expensive simulation-based data generation.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16617" title="Abstract">arXiv:2402.16617</a> [<a href="/pdf/2402.16617" title="Download PDF">pdf</a>, <a href="/format/2402.16617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Context Language Modeling with Parallel Context Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yen%2C+H">Howard Yen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tianyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data are available at <a href="https://github.com/princeton-nlp/CEPE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Extending large language models (LLMs) to process longer inputs is crucial
for numerous applications. However, the considerable computational cost of
transformers, coupled with limited generalization of positional encoding,
restricts the size of their context window. We introduce Context Expansion with
Parallel Encoding (CEPE), a framework that can be applied to any existing
decoder-only LLMs to extend their context window. CEPE adopts a small encoder
to process long inputs chunk by chunk and enables the frozen decoder to
leverage additional contexts via cross-attention. CEPE is efficient,
generalizable, and versatile: trained with 8K-token documents, CEPE extends the
context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only
1/6 of the memory. CEPE yields strong performance on language modeling and
in-context learning. CEPE also excels in retrieval-augmented applications,
while existing long-context models degenerate with retrieved contexts. We
further introduce a CEPE variant that can extend the context window of
instruction-tuned models with only unlabeled data, and showcase its
effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model
that can leverage very long context on downstream tasks.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16618" title="Abstract">arXiv:2402.16618</a> [<a href="/pdf/2402.16618" title="Download PDF">pdf</a>, <a href="/format/2402.16618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IM-based Pilot-assisted Channel Estimation for FTN Signaling HF  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keykhosravi%2C+S">Simin Keykhosravi</a>, 
<a href="/search/cs?searchtype=author&query=Bedeer%2C+E">Ebrahim Bedeer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to IEEE Transactions on Broadcasting
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates doubly-selective (i.e., time- and
frequency-selective) channel estimation in faster-than-Nyquist (FTN) signaling
HF communications. In particular, we propose a novel IM-based channel
estimation algorithm for FTN signaling HF communications including pilot
sequence placement (PSP) and pilot sequence location identification (PSLI)
algorithms. At the transmitter, we propose the PSP algorithm that utilizes the
locations of pilot sequences to carry additional information bits, thereby
improving the SE of HF communications. HF channels have two non-zero
independent fading paths with specific fixed delay spread and frequency spread
characteristics as outlined in the Union Radio communication Sector (ITU-R)
F.1487 and F.520. Having said that, based on the aforementioned properties of
the HF channels and the favorable auto-correlation characteristics of the
optimal pilot sequence, we propose a novel PSLI algorithm that effectively
identifies the pilot sequence location within a given frame at the receiver.
This is achieved by showing that the square of the absolute value of the
cross-correlation between the received symbols and the pilot sequence consists
of a scaled version of the square of the absolute value of the auto-correlation
of the pilot sequence weighted by the gain of the corresponding HF channel
path. Simulation results show very low pilot sequence location identification
errors for HF channels. Our simulation results show a 6 dB improvement in the
MSE of the channel estimation as well as about 3.5 dB BER improvement of FTN
signaling along with an enhancement in SE compared to the method in [1]. We
also achieved an enhancement in SE compared to the work in [2] while
maintaining comparable MSE of the channel estimation and BER performance.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16623" title="Abstract">arXiv:2402.16623</a> [<a href="/pdf/2402.16623" title="Download PDF">pdf</a>, <a href="/format/2402.16623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized sparsity-promoting solvers for Bayesian inverse problems:  Versatile sparsifying transforms and unknown noise variances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lindbloom%2C+J">Jonathan Lindbloom</a>, 
<a href="/search/math?searchtype=author&query=Glaubitz%2C+J">Jan Glaubitz</a>, 
<a href="/search/math?searchtype=author&query=Gelb%2C+A">Anne Gelb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Bayesian hierarchical models can provide efficient algorithms for finding
sparse solutions to ill-posed inverse problems. The models typically comprise a
conditionally Gaussian prior model for the unknown which is augmented by a
generalized gamma hyper-prior model for variance hyper-parameters. This
investigation generalizes these models and their efficient maximum a posterior
(MAP) estimation using the iterative alternating sequential (IAS) algorithm in
two ways: (1) General sparsifying transforms: Diverging from conventional
methods, our approach permits the use of sparsifying transformations with
nontrivial kernels; (2) Unknown noise variances: We treat the noise variance as
a random variable that is estimated during the inference procedure. This is
important in applications where the noise estimate cannot be accurately
estimated a priori. Remarkably, these augmentations neither significantly
burden the computational expense of the algorithm nor compromise its efficacy.
We include convexity and convergence analysis for the method and demonstrate
its efficacy in several numerical experiments.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16627" title="Abstract">arXiv:2402.16627</a> [<a href="/pdf/2402.16627" title="Download PDF">pdf</a>, <a href="/format/2402.16627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Modal Contextualized Diffusion Models for Text-Guided Visual  Generation and Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Ling Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+B">Bin Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Project: <a href="https://github.com/YangLing0818/ContextDiff">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conditional diffusion models have exhibited superior performance in
high-fidelity text-guided visual generation and editing. Nevertheless,
prevailing text-guided visual diffusion models primarily focus on incorporating
text-visual relationships exclusively into the reverse process, often
disregarding their relevance in the forward process. This inconsistency between
forward and reverse processes may limit the precise conveyance of textual
semantics in visual synthesis results. To address this issue, we propose a
novel and general contextualized diffusion model (ContextDiff) by incorporating
the cross-modal context encompassing interactions and alignments between text
condition and visual sample into forward and reverse processes. We propagate
this context to all timesteps in the two processes to adapt their trajectories,
thereby facilitating cross-modal conditional modeling. We generalize our
contextualized diffusion to both DDPMs and DDIMs with theoretical derivations,
and demonstrate the effectiveness of our model in evaluations with two
challenging tasks: text-to-image generation, and text-to-video editing. In each
task, our ContextDiff achieves new state-of-the-art performance, significantly
enhancing the semantic alignment between text condition and generated samples,
as evidenced by quantitative and qualitative evaluations. Our code is available
at https://github.com/YangLing0818/ContextDiff
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16628" title="Abstract">arXiv:2402.16628</a> [<a href="/pdf/2402.16628" title="Download PDF">pdf</a>, <a href="/format/2402.16628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single Neuromorphic Memristor closely Emulates Multiple Synaptic  Mechanisms for Energy Efficient Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weilenmann%2C+C">Christoph Weilenmann</a>, 
<a href="/search/cs?searchtype=author&query=Ziogas%2C+A">Alexandros Ziogas</a>, 
<a href="/search/cs?searchtype=author&query=Zellweger%2C+T">Till Zellweger</a>, 
<a href="/search/cs?searchtype=author&query=Portner%2C+K">Kevin Portner</a>, 
<a href="/search/cs?searchtype=author&query=Mladenovi%C4%87%2C+M">Marko Mladenovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kaniselvan%2C+M">Manasa Kaniselvan</a>, 
<a href="/search/cs?searchtype=author&query=Moraitis%2C+T">Timoleon Moraitis</a>, 
<a href="/search/cs?searchtype=author&query=Luisier%2C+M">Mathieu Luisier</a>, 
<a href="/search/cs?searchtype=author&query=Emboras%2C+A">Alexandros Emboras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall)

</div>
<p class="mathjax">Biological neural networks do not only include long-term memory and weight
multiplication capabilities, as commonly assumed in artificial neural networks,
but also more complex functions such as short-term memory, short-term
plasticity, and meta-plasticity - all collocated within each synapse. Here, we
demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all
these synaptic functions. These memristors operate in a non-filamentary, low
conductance regime, which enables stable and energy efficient operation. They
can act as multi-functional hardware synapses in a class of bio-inspired deep
neural networks (DNN) that make use of both long- and short-term synaptic
dynamics and are capable of meta-learning or "learning-to-learn". The resulting
bio-inspired DNN is then trained to play the video game Atari Pong, a complex
reinforcement learning task in a dynamic environment. Our analysis shows that
the energy consumption of the DNN with multi-functional memristive synapses
decreases by about two orders of magnitude as compared to a pure GPU
implementation. Based on this finding, we infer that memristive devices with a
better emulation of the synaptic functionalities do not only broaden the
applicability of neuromorphic computing, but could also improve the performance
and energy costs of certain artificial intelligence applications.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16629" title="Abstract">arXiv:2402.16629</a> [<a href="/pdf/2402.16629" title="Download PDF">pdf</a>, <a href="/format/2402.16629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting  Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javadi%2C+S">Sepideh Javadi</a>, 
<a href="/search/cs?searchtype=author&query=Faramarzi%2C+S">Sajad Faramarzi</a>, 
<a href="/search/cs?searchtype=author&query=Zeinali%2C+F">Farshad Zeinali</a>, 
<a href="/search/cs?searchtype=author&query=Zarini%2C+H">Hosein Zarini</a>, 
<a href="/search/cs?searchtype=author&query=Mili%2C+M+R">Mohammad Robat Mili</a>, 
<a href="/search/cs?searchtype=author&query=Diamantoulakis%2C+P+D">Panagiotis D. Diamantoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Jorswieck%2C+E">Eduard Jorswieck</a>, 
<a href="/search/cs?searchtype=author&query=Karagiannidis%2C+G+K">George K. Karagiannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Optical wireless communication (OWC) systems with multiple light-emitting
diodes (LEDs) have recently been explored to support energy-limited devices via
simultaneous lightwave information and power transfer (SLIPT). The energy
consumption, however, becomes considerable by increasing the number of
incorporated LEDs. This paper proposes a joint dimming (JD) scheme that lowers
the consumed power of a SLIPT-enabled OWC system by controlling the number of
active LEDs. We further enhance the data rate of this system by utilizing rate
splitting multiple access (RSMA). More specifically, we formulate a data rate
maximization problem to optimize the beamforming design, LED selection and RSMA
rate adaptation that guarantees the power budget of the OWC transmitter, as
well as the quality-of-service (QoS) and an energy harvesting level for users.
We propose a dynamic resource allocation solution based on proximal policy
optimization (PPO) reinforcement learning. In simulations, the optimal dimming
level is determined to initiate a trade-off between the data rate and power
consumption. It is also verified that RSMA significantly improves the data
rate.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16631" title="Abstract">arXiv:2402.16631</a> [<a href="/pdf/2402.16631" title="Download PDF">pdf</a>, <a href="/format/2402.16631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAINet: Enabling Wireless Collective Intelligence via Knowledge  Transfer and Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+H">Hang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qiyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bariah%2C+L">Lina Bariah</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>, 
<a href="/search/cs?searchtype=author&query=Lasaulce%2C+S">Samson Lasaulce</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+F">Faouzi Bader</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Generative artificial intelligence (GenAI) and communication networks are
expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a
wireless network can potentially unleash the power of collective intelligence
and pave the way for artificial general intelligence (AGI). However, current
wireless networks are designed as a "data pipe" and are not suited to
accommodate and leverage the power of GenAI. In this paper, we propose the
GenAINet framework in which distributed GenAI agents communicate knowledge
(high-level concepts or abstracts) to accomplish arbitrary tasks. We first
provide a network architecture integrating GenAI capabilities to manage both
network protocols and applications. Building on this, we investigate effective
communication and reasoning problems by proposing a semantic-native GenAINet.
Specifically, GenAI agents extract semantic concepts from multi-modal raw data,
build a knowledgebase representing their semantic relations, which is retrieved
by GenAI models for planning and reasoning. Under this paradigm, an agent can
learn fast from other agents' experience for making better decisions with
efficient communications. Furthermore, we conduct two case studies where in
wireless device query, we show that extracting and transferring knowledge can
improve query accuracy with reduced communication; and in wireless power
control, we show that distributed agents can improve decisions via
collaborative reasoning. Finally, we address that developing a hierarchical
semantic level Telecom world model is a key path towards network of collective
intelligence.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16632" title="Abstract">arXiv:2402.16632</a> [<a href="/pdf/2402.16632" title="Download PDF">pdf</a>, <a href="/format/2402.16632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Embeddings for Generating Complex Descriptions of Concepts in  Italian Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maisto%2C+A">Alessandro Maisto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this work, we propose a Distributional Semantic resource enriched with
linguistic and lexical information extracted from electronic dictionaries,
designed to address the challenge of bridging the gap between the continuous
semantic values represented by distributional vectors and the discrete
descriptions offered by general semantics theory. Recently, many researchers
have concentrated on the nexus between embeddings and a comprehensive theory of
semantics and meaning. This often involves decoding the representation of word
meanings in Distributional Models into a set of discrete, manually constructed
properties such as semantic primitives or features, using neural decoding
techniques. Our approach introduces an alternative strategy grounded in
linguistic data. We have developed a collection of domain-specific
co-occurrence matrices, derived from two sources: a classification of Italian
nouns categorized into 4 semantic traits and 20 concrete noun sub-categories,
and a list of Italian verbs classified according to their semantic classes. In
these matrices, the co-occurrence values for each word are calculated
exclusively with a defined set of words pertinent to a particular lexical
domain. The resource comprises 21 domain-specific matrices, one comprehensive
matrix, and a Graphical User Interface. Our model facilitates the generation of
reasoned semantic descriptions of concepts by selecting matrices directly
associated with concrete conceptual knowledge, such as a matrix based on
location nouns and the concept of animal habitats. We assessed the utility of
the resource through two experiments, achieving promising outcomes in both: the
automatic classification of animal nouns and the extraction of animal features.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16639" title="Abstract">arXiv:2402.16639</a> [<a href="/pdf/2402.16639" title="Download PDF">pdf</a>, <a href="/format/2402.16639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Particle Filtering using Optimal Placement Resampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Csuzdi%2C+D">Domonkos Csuzdi</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B6r%C5%91%2C+O">Oliv&#xe9;r T&#xf6;r&#x151;</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A9csi%2C+T">Tam&#xe1;s B&#xe9;csi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO)

</div>
<p class="mathjax">Particle filters are a frequent choice for inference tasks in nonlinear and
non-Gaussian state-space models. They can either be used for state inference by
approximating the filtering distribution or for parameter inference by
approximating the marginal data (observation) likelihood. A good proposal
distribution and a good resampling scheme are crucial to obtain low variance
estimates. However, traditional methods like multinomial resampling introduce
nondifferentiability in PF-based loss functions for parameter estimation,
prohibiting gradient-based learning tasks. This work proposes a differentiable
resampling scheme by deterministic sampling from an empirical cumulative
distribution function. We evaluate our method on parameter inference tasks and
proposal learning.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16640" title="Abstract">arXiv:2402.16640</a> [<a href="/pdf/2402.16640" title="Download PDF">pdf</a>, <a href="/ps/2402.16640" title="Download PostScript">ps</a>, <a href="/format/2402.16640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRSI-Net: Dual-Residual Spatial Interaction Network for Multi-Person  Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-person pose estimation (MPPE), which aims to locate keypoints for all
persons in the frames, is an active research branch of computer vision.
Variable human poses and complex scenes make MPPE dependent on both local
details and global structures, and the absence of them may cause keypoint
feature misalignment. In this case, high-order spatial interactions that can
effectively link the local and global information of features are particularly
important. However, most methods do not have spatial interactions, and a few
methods have low-order spatial interactions but they are difficult to achieve a
good balance between accuracy and complexity. To address the above problems, a
Dual-Residual Spatial Interaction Network (DRSI-Net) for MPPE with high
accuracy and low complexity is proposed in this paper. DRSI-Net recursively
performs residual spatial information interactions on neighbor features, so
that more useful spatial information can be retained and more similarities can
be obtained between shallow and deep extracted features. The channel and
spatial dual attention mechanism introduced in the multi-scale feature fusion
also helps the network to adaptively focus on features relevant to target
keypoints and further refine generated poses. At the same time, by optimizing
interactive channel dimensions and dividing gradient flow, the spatial
interaction module is designed to be lightweight, which reduces the complexity
of the network. According to the experimental results on the COCO dataset, the
proposed DRSI-Net outperforms other state-of-the-art methods in both accuracy
and complexity.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16641" title="Abstract">arXiv:2402.16641</a> [<a href="/pdf/2402.16641" title="Download PDF">pdf</a>, <a href="/format/2402.16641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Open-ended Visual Quality Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Erli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaofeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Liang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Annan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenxiu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qiong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Comparative settings (e.g. pairwise choice, listwise ranking) have been
adopted by a wide range of subjective studies for image quality assessment
(IQA), as it inherently standardizes the evaluation criteria across different
observers and offer more clear-cut responses. In this work, we extend the edge
of emerging large multi-modality models (LMMs) to further advance visual
quality comparison into open-ended settings, that 1) can respond to open-range
questions on quality comparison; 2) can provide detailed reasonings beyond
direct answers. To this end, we propose the Co-Instruct. To train this
first-of-its-kind open-source open-ended visual quality comparer, we collect
the Co-Instruct-562K dataset, from two sources: (a) LMM-merged single image
quality description, (b) GPT-4V "teacher" responses on unlabeled data.
Furthermore, to better evaluate this setting, we propose the MICBench, the
first benchmark on multi-image comparison for LMMs. We demonstrate that
Co-Instruct not only achieves 30% higher superior accuracy than
state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher),
on both existing related benchmarks and the proposed MICBench. Our model is
published at https://huggingface.co/q-future/co-instruct.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16645" title="Abstract">arXiv:2402.16645</a> [<a href="/pdf/2402.16645" title="Download PDF">pdf</a>, <a href="/format/2402.16645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Based NMPC Adaptation for Autonomous Driving using Parallelized  Digital Twin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allamaa%2C+J+P">Jean Pierre Allamaa</a>, 
<a href="/search/cs?searchtype=author&query=Patrinos%2C+P">Panagiotis Patrinos</a>, 
<a href="/search/cs?searchtype=author&query=Van+der+Auweraer%2C+H">Herman Van der Auweraer</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+T+D">Tong Duy Son</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this work, we address the problem of transferring an autonomous driving
(AD) module from one domain to another, in particular from simulation to the
real world (Sim2Real). We propose a data-efficient method for online and
on-the-fly learning based adaptation for parametrizable control architectures
such that the target closed-loop performance is optimized under several
uncertainty sources such as model mismatches, environment changes and task
choice. The novelty of the work resides in leveraging black-box optimization
enabled by executable digital twins, with data-driven hyper-parameter tuning
through derivative-free methods to directly adapt in real-time the AD module.
Our proposed method requires a minimal amount of interaction with the
real-world in the randomization and online training phase. Specifically, we
validate our approach in real-world experiments and show the ability to
transfer and safely tune a nonlinear model predictive controller in less than
10 minutes, eliminating the need of day-long manual tuning and hours-long
machine learning training phases. Our results show that the online adapted NMPC
directly compensates for disturbances, avoids overtuning in simulation and for
one specific task, and it generalizes for less than 15cm of tracking accuracy
over a multitude of trajectories, and leads to 83% tracking improvement.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16650" title="Abstract">arXiv:2402.16650</a> [<a href="/pdf/2402.16650" title="Download PDF">pdf</a>, <a href="/ps/2402.16650" title="Download PostScript">ps</a>, <a href="/format/2402.16650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ESG Sentiment Analysis: comparing human and language model performance  including GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Derrick%2C+K">Karim Derrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE); Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper we explore the challenges of measuring sentiment in relation to
Environmental, Social and Governance (ESG) social media. ESG has grown in
importance in recent years with a surge in interest from the financial sector
and the performance of many businesses has become based in part on their ESG
related reputations. The use of sentiment analysis to measure ESG related
reputation has developed and with it interest in the use of machines to do so.
The era of digital media has created an explosion of new media sources, driven
by the growth of social media platforms. This growing data environment has
become an excellent source for behavioural insight studies across many
disciplines that includes politics, healthcare and market research. Our study
seeks to compare human performance with the cutting edge in machine performance
in the measurement of ESG related sentiment. To this end researchers classify
the sentiment of 150 tweets and a reliability measure is made. A gold standard
data set is then established based on the consensus of 3 researchers and this
data set is then used to measure the performance of different machine
approaches: one based on the VADER dictionary approach to sentiment
classification and then multiple language model approaches, including Llama2,
T5, Mistral, Mixtral, FINBERT, GPT3.5 and GPT4.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16651" title="Abstract">arXiv:2402.16651</a> [<a href="/pdf/2402.16651" title="Download PDF">pdf</a>, <a href="/ps/2402.16651" title="Download PostScript">ps</a>, <a href="/format/2402.16651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Belief Rule Base (BRB) Hybrid Expert system:  Bridging Decision Science and Professional Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Derrick%2C+K">Karim Derrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The Belief Rule Base (BRB) system that adopts a hybrid approach integrating
the precision of expert systems with the adaptability of data-driven models.
Characterized by its use of if-then rules to accommodate various types of
uncertainty through belief degrees, BRB adeptly handles fuzziness, randomness,
and ignorance. This semi-quantitative tool excels in processing both numerical
data and linguistic knowledge from diverse sources, making it as an
indispensable resource in modelling complex nonlinear systems. Notably, BRB's
transparent, white-box nature ensures accessibility and clarity for
decision-makers and stakeholders, further enhancing its applicability. With its
growing adoption in fields ranging from decision-making and reliability
evaluation in network security and fault diagnosis, this study aims to explore
the evolution and the multifaceted applications of BRB. By analysing its
development across different domains, we highlight BRB's potential to
revolutionize sectors traditionally resistant to technological disruption, in
particular insurance and law.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16654" title="Abstract">arXiv:2402.16654</a> [<a href="/pdf/2402.16654" title="Download PDF">pdf</a>, <a href="/format/2402.16654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GigaPevt: Multimodal Medical Assistant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blinov%2C+P">Pavel Blinov</a>, 
<a href="/search/cs?searchtype=author&query=Egorov%2C+K">Konstantin Egorov</a>, 
<a href="/search/cs?searchtype=author&query=Sviridov%2C+I">Ivan Sviridov</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+N">Nikolay Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Botman%2C+S">Stepan Botman</a>, 
<a href="/search/cs?searchtype=author&query=Tagin%2C+E">Evgeniy Tagin</a>, 
<a href="/search/cs?searchtype=author&query=Kudin%2C+S">Stepan Kudin</a>, 
<a href="/search/cs?searchtype=author&query=Zubkova%2C+G">Galina Zubkova</a>, 
<a href="/search/cs?searchtype=author&query=Savchenko%2C+A">Andrey Savchenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Building an intelligent and efficient medical assistant is still a
challenging AI problem. The major limitation comes from the data modality
scarceness, which reduces comprehensive patient perception. This demo paper
presents the GigaPevt, the first multimodal medical assistant that combines the
dialog capabilities of large language models with specialized medical models.
Such an approach shows immediate advantages in dialog quality and metric
performance, with a 1.18\% accuracy improvement in the question-answering task.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16655" title="Abstract">arXiv:2402.16655</a> [<a href="/pdf/2402.16655" title="Download PDF">pdf</a>, <a href="/ps/2402.16655" title="Download PostScript">ps</a>, <a href="/format/2402.16655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling robust sensor network design with data processing and  optimization making use of local beehive image and video files
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Namugenyi%2C+E+E">Ephrance Eunice Namugenyi</a> (1), 
<a href="/search/cs?searchtype=author&query=Tugume%2C+D">David Tugume</a> (2), 
<a href="/search/cs?searchtype=author&query=Kigwana%2C+A">Augustine Kigwana</a> (3), 
<a href="/search/cs?searchtype=author&query=Rukundo%2C+B">Benjamin Rukundo</a> (4) ((1) Department of Computer Networks, CoCIS, Makerere University, Uganda AdEMNEA Project)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures, AIBD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">There is an immediate need for creative ways to improve resource ef iciency
given the dynamic nature of robust sensor networks and their increasing
reliance on data-driven approaches.One key challenge faced is ef iciently
managing large data files collected from sensor networks for example optimal
beehive image and video data files. We of er a revolutionary paradigm that uses
cutting-edge edge computing techniques to optimize data transmission and
storage in order to meet this problem. Our approach encompasses data
compression for images and videos, coupled with a data aggregation technique
for numerical data. Specifically, we propose a novel compression algorithm that
performs better than the traditional Bzip2, in terms of data compression ratio
and throughput. We also designed as an addition a data aggregation algorithm
that basically performs very well by reducing on the time to process the
overhead of individual data packets there by reducing on the network traf ic. A
key aspect of our approach is its ability to operate in resource-constrained
environments, such as that typically found in a local beehive farm application
from where we obtained various datasets. To achieve this, we carefully explore
key parameters such as throughput, delay tolerance, compression rate, and data
retransmission. This ensures that our approach can meet the unique requirements
of robust network management while minimizing the impact on resources. Overall,
our study presents and majorly focuses on a holistic solution for optimizing
data transmission and processing across robust sensor networks for specifically
local beehive image and video data files. Our approach has the potential to
significantly improve the ef iciency and ef ectiveness of robust sensor network
management, thereby supporting sustainable practices in various IoT
applications such as in Bee Hive Data Management.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16660" title="Abstract">arXiv:2402.16660</a> [<a href="/pdf/2402.16660" title="Download PDF">pdf</a>, <a href="/format/2402.16660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BOXREC: Recommending a Box of Preferred Outfits in Online Shopping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+D">Debopriyo Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+K+S">Krothapalli Sreenivasa Rao</a>, 
<a href="/search/cs?searchtype=author&query=Sural%2C+S">Shamik Sural</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+N">Niloy Ganguly</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Trans. Intell. Syst. Technol. 11, 6, Article 69 (December
  2020), pages 69:1-69:28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Over the past few years, automation of outfit composition has gained much
attention from the research community. Most of the existing outfit
recommendation systems focus on pairwise item compatibility prediction (using
visual and text features) to score an outfit combination having several items,
followed by recommendation of top-n outfits or a capsule wardrobe having a
collection of outfits based on user's fashion taste. However, none of these
consider user's preference of price-range for individual clothing types or an
overall shopping budget for a set of items. In this paper, we propose a box
recommendation framework - BOXREC - which at first, collects user preferences
across different item types (namely, top-wear, bottom-wear and foot-wear)
including price-range of each type and a maximum shopping budget for a
particular shopping session. It then generates a set of preferred outfits by
retrieving all types of preferred items from the database (according to user
specified preferences including price-ranges), creates all possible
combinations of three preferred items (belonging to distinct item types) and
verifies each combination using an outfit scoring framework - BOXREC-OSF.
Finally, it provides a box full of fashion items, such that different
combinations of the items maximize the number of outfits suitable for an
occasion while satisfying maximum shopping budget. Empirical results show
superior performance of BOXREC-OSF over the baseline methods.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16664" title="Abstract">arXiv:2402.16664</a> [<a href="/pdf/2402.16664" title="Download PDF">pdf</a>, <a href="/format/2402.16664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Assisted Multi-Teacher Continual Learning for Visual Question  Answering in Robotic Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuyang Du</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+T">Tao You</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M">Mobarakol Islam</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yueming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P">Pheng-Ann Heng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accapted by 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Visual question answering (VQA) can be fundamentally crucial for promoting
robotic-assisted surgical education. In practice, the needs of trainees are
constantly evolving, such as learning more surgical types, adapting to
different robots, and learning new surgical instruments and techniques for one
surgery. Therefore, continually updating the VQA system by a sequential data
stream from multiple resources is demanded in robotic surgery to address new
tasks. In surgical scenarios, the storage cost and patient data privacy often
restrict the availability of old data when updating the model, necessitating an
exemplar-free continual learning (CL) setup. However, prior studies overlooked
two vital problems of the surgical domain: i) large domain shifts from diverse
surgical operations collected from multiple departments or clinical centers,
and ii) severe data imbalance arising from the uneven presence of surgical
instruments or activities during surgical procedures. This paper proposes to
address these two problems with a multimodal large language model (LLM) and an
adaptive weight assignment methodology. We first develop a new multi-teacher CL
framework that leverages a multimodal LLM as the additional teacher. The strong
generalization ability of the LLM can bridge the knowledge gap when domain
shifts and data imbalances occur. We then put forth a novel data processing
method that transforms complex LLM embeddings into logits compatible with our
CL framework. We further design an adaptive weight assignment approach that
balances the generalization ability of the LLM and the domain expertise of the
old CL model. We construct a new dataset for surgical VQA tasks, providing
valuable data resources for future research. Extensive experimental results on
three datasets demonstrate the superiority of our method to other advanced CL
models.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16665" title="Abstract">arXiv:2402.16665</a> [<a href="/pdf/2402.16665" title="Download PDF">pdf</a>, <a href="/format/2402.16665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of  Fidelity in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonfert%2C+M">Michael Bonfert</a>, 
<a href="/search/cs?searchtype=author&query=Muender%2C+T">Thomas Muender</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>, 
<a href="/search/cs?searchtype=author&query=Steinicke%2C+F">Frank Steinicke</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+D">Doug Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Malaka%2C+R">Rainer Malaka</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%B6ring%2C+T">Tanja D&#xf6;ring</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages incl. references and appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
<p class="mathjax">Fidelity describes how closely a replication resembles the original. It can
be helpful to analyze how faithful interactions in virtual reality (VR) are to
a reference interaction. In prior research, fidelity has been restricted to the
simulation of reality - also called realism. Our definition includes other
reference interactions, such as superpowers or fiction. Interaction fidelity is
a multilayered concept. Unfortunately, different aspects of fidelity have
either not been distinguished in scientific discourse or referred to with
inconsistent terminology. Therefore, we present the Interaction Fidelity Model
(IntFi Model). Based on the human-computer interaction loop, it systematically
covers all stages of VR interactions. The conceptual model establishes a clear
structure and precise definitions of eight distinct components. It was reviewed
through interviews with fourteen VR experts. We provide guidelines, diverse
examples, and educational material to universally apply the IntFi Model to any
VR experience. We identify common patterns and propose foundational research
opportunities.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16667" title="Abstract">arXiv:2402.16667</a> [<a href="/pdf/2402.16667" title="Download PDF">pdf</a>, <a href="/format/2402.16667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RepoAgent: An LLM-Powered Open-Source Framework for Repository-level  Code Documentation Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qinyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yining Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shihao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaxi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yesai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+X">Xiaoyin Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative models have demonstrated considerable potential in software
engineering, particularly in tasks such as code generation and debugging.
However, their utilization in the domain of code documentation generation
remains underexplored. To this end, we introduce RepoAgent, a large language
model powered open-source framework aimed at proactively generating,
maintaining, and updating code documentation. Through both qualitative and
quantitative evaluations, we have validated the effectiveness of our approach,
showing that RepoAgent excels in generating high-quality repository-level
documentation. The code and results are publicly accessible at
https://github.com/OpenBMB/RepoAgent.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16668" title="Abstract">arXiv:2402.16668</a> [<a href="/pdf/2402.16668" title="Download PDF">pdf</a>, <a href="/format/2402.16668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program-Based Strategy Induction for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Correa%2C+C+G">Carlos G. Correa</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Daw%2C+N+D">Nathaniel D. Daw</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Typical models of learning assume incremental estimation of
continuously-varying decision variables like expected rewards. However, this
class of models fails to capture more idiosyncratic, discrete heuristics and
strategies that people and animals appear to exhibit. Despite recent advances
in strategy discovery using tools like recurrent networks that generalize the
classic models, the resulting strategies are often onerous to interpret, making
connections to cognition difficult to establish. We use Bayesian program
induction to discover strategies implemented by programs, letting the
simplicity of strategies trade off against their effectiveness. Focusing on
bandit tasks, we find strategies that are difficult or unexpected with
classical incremental learning, like asymmetric learning from rewarded and
unrewarded trials, adaptive horizon-dependent random exploration, and discrete
state switching.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16669" title="Abstract">arXiv:2402.16669</a> [<a href="/pdf/2402.16669" title="Download PDF">pdf</a>, <a href="/format/2402.16669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Preserving Numerical Methods for Two Nonlinear Systems of  Dispersive Wave Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lampert%2C+J">Joshua Lampert</a>, 
<a href="/search/math?searchtype=author&query=Ranocha%2C+H">Hendrik Ranocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We use the general framework of summation by parts operators to construct
conservative, entropy-stable and well-balanced semidiscretizations of two
different nonlinear systems of dispersive shallow water equations with varying
bathymetry: (i) a variant of the coupled Benjamin-Bona-Mahony (BBM) equations
and (ii) a recently proposed model by Sv\"ard and Kalisch (2023) with enhanced
dispersive behavior. Both models share the property of being conservative in
terms of a nonlinear invariant, often interpreted as entropy function. This
property is preserved exactly in our novel semidiscretizations. To obtain
fully-discrete entropy-stable schemes, we employ the relaxation method. We
present improved numerical properties of our schemes in some test cases.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16670" title="Abstract">arXiv:2402.16670</a> [<a href="/pdf/2402.16670" title="Download PDF">pdf</a>, <a href="/ps/2402.16670" title="Download PostScript">ps</a>, <a href="/format/2402.16670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pay Attention: a Call to Regulate the Attention Market and Prevent  Algorithmic Emotional Governance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michel%2C+F">Franck Michel</a>, 
<a href="/search/cs?searchtype=author&query=Gandon%2C+F">Fabien Gandon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Over the last 70 years, we, humans, have created an economic market where
attention is being captured and turned into money thanks to advertising. During
the last two decades, leveraging research in psychology, sociology,
neuroscience and other domains, Web platforms have brought the process of
capturing attention to an unprecedented scale. With the initial commonplace
goal of making targeted advertising more effective, the generalization of
attention-capturing techniques and their use of cognitive biases and emotions
have multiple detrimental side effects such as polarizing opinions, spreading
false information and threatening public health, economies and democracies.
This is clearly a case where the Web is not used for the common good and where,
in fact, all its users become a vulnerable population. This paper brings
together contributions from a wide range of disciplines to analyze current
practices and consequences thereof. Through a set of propositions and
principles that could be used do drive further works, it calls for actions
against these practices competing to capture our attention on the Web, as it
would be unsustainable for a civilization to allow attention to be wasted with
impunity on a world-wide scale.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16671" title="Abstract">arXiv:2402.16671</a> [<a href="/pdf/2402.16671" title="Download PDF">pdf</a>, <a href="/format/2402.16671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StructLM: Towards Building Generalist Models for Structured Knowledge  Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+A">Alex Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinrun Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Weiming Ren</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Structured data sources, such as tables, graphs, and databases, are
ubiquitous knowledge sources. Despite the demonstrated capabilities of large
language models (LLMs) on plain text, their proficiency in interpreting and
utilizing structured data remains limited. Our investigation reveals a notable
deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags
behind state-of-the-art (SoTA) model by an average of 35%. To augment the
Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a
comprehensive instruction tuning dataset comprising 1.1 million examples.
Utilizing this dataset, we train a series of models, referred to as StructLM,
based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our
StructLM series surpasses task-specific models on 14 out of 18 evaluated
datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,
StructLM demonstrates exceptional generalization across 6 novel SKG tasks.
Contrary to expectations, we observe that scaling model size offers marginal
benefits, with StructLM-34B showing only slight improvements over StructLM-7B.
This suggests that structured knowledge grounding is still a challenging task
and requires more innovative design to push to a new level.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16674" title="Abstract">arXiv:2402.16674</a> [<a href="/pdf/2402.16674" title="Download PDF">pdf</a>, <a href="/format/2402.16674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConSept: Continual Semantic Segmentation via Adapter-based Vision  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bowen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guanglei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we delve into the realm of vision transformers for continual
semantic segmentation, a problem that has not been sufficiently explored in
previous literature. Empirical investigations on the adaptation of existing
frameworks to vanilla ViT reveal that incorporating visual adapters into ViTs
or fine-tuning ViTs with distillation terms is advantageous for enhancing the
segmentation capability of novel classes. These findings motivate us to propose
Continual semantic Segmentation via Adapter-based ViT, namely ConSept. Within
the simplified architecture of ViT with linear segmentation head, ConSept
integrates lightweight attention-based adapters into vanilla ViTs. Capitalizing
on the feature adaptation abilities of these adapters, ConSept not only retains
superior segmentation ability for old classes, but also attains promising
segmentation quality for novel classes. To further harness the intrinsic
anti-catastrophic forgetting ability of ConSept and concurrently enhance the
segmentation capabilities for both old and new classes, we propose two key
strategies: distillation with a deterministic old-classes boundary for improved
anti-catastrophic forgetting, and dual dice losses to regularize segmentation
maps, thereby improving overall segmentation performance. Extensive experiments
show the effectiveness of ConSept on multiple continual semantic segmentation
benchmarks under overlapped or disjoint settings. Code will be publicly
available at \url{https://github.com/DongSky/ConSept}.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16678" title="Abstract">arXiv:2402.16678</a> [<a href="/pdf/2402.16678" title="Download PDF">pdf</a>, <a href="/format/2402.16678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Diameter on H-free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oostveen%2C+J+J">Jelle J. Oostveen</a>, 
<a href="/search/cs?searchtype=author&query=Paulusma%2C+D">Dani&#xeb;l Paulusma</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+E+J">Erik Jan van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The intensively studied Diameter problem is to find the diameter of a given
connected graph. We investigate, for the first time in a structured manner, the
complexity of Diameter for H-free graphs, that is, graphs that do not contain a
fixed graph H as an induced subgraph. We first show that if H is not a linear
forest with small components, then Diameter cannot be solved in subquadratic
time for H-free graphs under SETH. For some small linear forests, we do show
linear-time algorithms for solving Diameter. For other linear forests H, we
make progress towards linear-time algorithms by considering specific diameter
values. If H is a linear forest, the maximum value of the diameter of any graph
in a connected H-free graph class is some constant dmax dependent only on H. We
give linear-time algorithms for deciding if a connected H-free graph has
diameter dmax, for several linear forests H. In contrast, for one such linear
forest H, Diameter cannot be solved in subquadratic time for H-free graphs
under SETH. Moreover, we even show that, for several other linear forests H,
one cannot decide in subquadratic time if a connected H-free graph has diameter
dmax under SETH.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16681" title="Abstract">arXiv:2402.16681</a> [<a href="/pdf/2402.16681" title="Download PDF">pdf</a>, <a href="/format/2402.16681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Continuous Domain Adaptation with Multi-Path Transfer  Curriculum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanbing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Ye Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Addressing the large distribution gap between training and testing data has
long been a challenge in machine learning, giving rise to fields such as
transfer learning and domain adaptation. Recently, Continuous Domain Adaptation
(CDA) has emerged as an effective technique, closing this gap by utilizing a
series of intermediate domains. This paper contributes a novel CDA method,
W-MPOT, which rigorously addresses the domain ordering and error accumulation
problems overlooked by previous studies. Specifically, we construct a transfer
curriculum over the source and intermediate domains based on Wasserstein
distance, motivated by theoretical analysis of CDA. Then we transfer the source
model to the target domain through multiple valid paths in the curriculum using
a modified version of continuous optimal transport. A bidirectional path
consistency constraint is introduced to mitigate the impact of accumulated
mapping errors during continuous transfer. We extensively evaluate W-MPOT on
multiple datasets, achieving up to 54.1\% accuracy improvement on multi-session
Alzheimer MR image classification and 94.7\% MSE reduction on battery capacity
estimation.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16684" title="Abstract">arXiv:2402.16684</a> [<a href="/pdf/2402.16684" title="Download PDF">pdf</a>, <a href="/ps/2402.16684" title="Download PostScript">ps</a>, <a href="/format/2402.16684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Floodwater Depth Estimation Using Large Multimodal Model for  Rapid Flood Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akinboyewa%2C+T">Temitope Akinboyewa</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+H">Huan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Lessani%2C+M+N">M. Naser Lessani</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenlong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Information on the depth of floodwater is crucial for rapid mapping of areas
affected by floods. However, previous approaches for estimating floodwater
depth, including field surveys, remote sensing, and machine learning
techniques, can be time-consuming and resource-intensive. This paper presents
an automated and fast approach for estimating floodwater depth from on-site
flood photos. A pre-trained large multimodal model, GPT-4 Vision, was used
specifically for estimating floodwater. The input data were flooding photos
that contained referenced objects, such as street signs, cars, people, and
buildings. Using the heights of the common objects as references, the model
returned the floodwater depth as the output. Results show that the proposed
approach can rapidly provide a consistent and reliable estimation of floodwater
depth from flood photos. Such rapid estimation is transformative in flood
inundation mapping and assessing the severity of the flood in near-real time,
which is essential for effective flood response strategies.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16689" title="Abstract">arXiv:2402.16689</a> [<a href="/pdf/2402.16689" title="Download PDF">pdf</a>, <a href="/format/2402.16689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptation of Biomedical and Clinical Pretrained Models to French Long  Documents: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazoge%2C+A">Adrien Bazoge</a>, 
<a href="/search/cs?searchtype=author&query=Morin%2C+E">Emmanuel Morin</a>, 
<a href="/search/cs?searchtype=author&query=Daille%2C+B">Beatrice Daille</a>, 
<a href="/search/cs?searchtype=author&query=Gourraud%2C+P">Pierre-Antoine Gourraud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, pretrained language models based on BERT have been introduced for
the French biomedical domain. Although these models have achieved
state-of-the-art results on biomedical and clinical NLP tasks, they are
constrained by a limited input sequence length of 512 tokens, which poses
challenges when applied to clinical notes. In this paper, we present a
comparative study of three adaptation strategies for long-sequence models,
leveraging the Longformer architecture. We conducted evaluations of these
models on 16 downstream tasks spanning both biomedical and clinical domains.
Our findings reveal that further pre-training an English clinical model with
French biomedical texts can outperform both converting a French biomedical BERT
to the Longformer architecture and pre-training a French biomedical Longformer
from scratch. The results underscore that long-sequence French biomedical
models improve performance across most downstream tasks regardless of sequence
length, but BERT based models remain the most efficient for named entity
recognition tasks.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16690" title="Abstract">arXiv:2402.16690</a> [<a href="/pdf/2402.16690" title="Download PDF">pdf</a>, <a href="/format/2402.16690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROVER: Risk-Aware Swarm Robotics MOtion Planner Using Conditional ValuE  at Risk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuru Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yunze Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Pingping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Ying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The field of swarm robotics has attracted considerable interest for its
capacity to complete intricate and synchronized tasks. Existing methodologies
for motion planning within swarm robotic systems mainly encounter difficulties
in scalability and safety guarantee. To address these two limitations, we
propose a Risk-aware swarm mOtion planner using conditional ValuE at Risk
(ROVER) that systematically modulates the safety and conservativeness and
navigates the swarm to the target area through cluttered environments. Our
approach formulates a finite-time model predictive control (FTMPC) problem
predicated upon the macroscopic state of the robot swarm represented by
Gaussian Mixture Model (GMM) and integrates conditional value-at-risk (CVaR) to
avoid collision. We leverage the linearized Signed Distance Function for the
efficient computation of CVaR concerning the proximity between the robot swarm
to obstacles. The key component of this method is implementing CVaR constraint
under GMM uncertainty in the FTMPC to measure the collision risk that a robot
swarm faces. However, the non-convex constrained FTMPC is nontrival to solve.
To navigate this complexity, we develop a computationally tractable strategy
through 1) an explicit linear approximation of the CVaR constraint; and 2) a
sequential quadratic programming formulation. Simulations and comparisons with
other approaches demonstrate the effectiveness of the proposed method in
flexibility, scalability, and risk mitigation.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16694" title="Abstract">arXiv:2402.16694</a> [<a href="/pdf/2402.16694" title="Download PDF">pdf</a>, <a href="/format/2402.16694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual  Natural Language Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Q">Qiwei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yekun Chai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuhong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Large language models (LLMs) have made significant progress in generating
codes from textual prompts. However, existing benchmarks have mainly
concentrated on translating English prompts to multilingual codes or have been
constrained to very limited natural languages (NLs). These benchmarks have
overlooked the vast landscape of massively multilingual NL to multilingual
code, leaving a critical gap in the evaluation of multilingual LLMs. In
response, we introduce HumanEval-XL, a massively multilingual code generation
benchmark specifically crafted to address this deficiency. HumanEval-XL
establishes connections between 23 NLs and 12 programming languages (PLs), and
comprises of a collection of 22,080 prompts with an average of 8.33 test cases.
By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a
comprehensive evaluation platform for multilingual LLMs, allowing the
assessment of the understanding of different NLs. Our work serves as a
pioneering step towards filling the void in evaluating NL generalization in the
area of multilingual code generation. We make our evaluation code and data
publicly available at \url{https://github.com/FloatAI/HumanEval-XL}.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16696" title="Abstract">arXiv:2402.16696</a> [<a href="/pdf/2402.16696" title="Download PDF">pdf</a>, <a href="/format/2402.16696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look Before You Leap: Towards Decision-Aware and Generalizable  Tool-Usage for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+A">Anchun Gui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Han Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Tool-augmented large language models (LLMs) are attracting widespread
attention when accessing up-to-date knowledge and alleviating hallucination
issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated
surprising tool-usage capabilities through prompting and in-context learning
techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in
manipulating tools, current efforts focus on either template-driven or
token-triggered tool-usage. However, the former hampers LLMs' flexibility to
address diverse user's queries due to constrained tool interactions, while the
latter limits the generalizability when engaging with new tools, since
tool-usage learning is based on task- and tool-specific datasets. To alleviate
these concerns, in this paper, we propose a decision-aware and generalizable
tool-usage framework (DEER). Specifically, we first construct the tool-usage
samples with multiple decision branches via an automatic generation pipeline,
thereby inspiring the decision-making awareness of LLMs under diverse
scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the
generalizability of LLMs over unseen tools. Extensive experiments demonstrate
that our proposed DEER is effective and significantly outperforms baselines
across various datasets.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16699" title="Abstract">arXiv:2402.16699</a> [<a href="/pdf/2402.16699" title="Download PDF">pdf</a>, <a href="/format/2402.16699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yunze Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuru Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kangjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qinghang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Pingping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Swarm robotic systems consisting of large-scale cooperative agents hold
promise for performing autonomous tasks in diverse fields. However, existing
planning strategies for swarm robotic systems often encounter a trade-off
between scalability and solution quality. We introduce here SwarmPRM, a
hierarchical, highly scalable, computationally efficient, and risk-aware
sampling-based motion planning approach for swarm robotic systems, which is
asymptotically optimal under mild assumptions. We employ probability density
functions (PDFs) to represent the swarm's macroscopic state and utilize optimal
mass transport (OMT) theory to measure the swarm's cost to go. A risk-aware
Gaussian roadmap is constructed wherein each node encapsulates a distinct PDF
and conditional-value-at-risk (CVaR) is employed to assess the collision risk,
facilitating the generation of macroscopic PDFs in Wasserstein-GMM space.
Extensive simulations demonstrate that the proposed approach outperforms
state-of-the-art methods in terms of computational efficiency and the average
travelling distance.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16700" title="Abstract">arXiv:2402.16700</a> [<a href="/pdf/2402.16700" title="Download PDF">pdf</a>, <a href="/format/2402.16700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Effective Ensembles for Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Etelis%2C+I">Itay Etelis</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+A">Avi Rosenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Weinberg%2C+A+I">Abraham Itzhak Weinberg</a>, 
<a href="/search/cs?searchtype=author&query=Sarne%2C+D">David Sarne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, transformer models have revolutionized Natural Language
Processing (NLP), achieving exceptional results across various tasks, including
Sentiment Analysis (SA). As such, current state-of-the-art approaches for SA
predominantly rely on transformer models alone, achieving impressive accuracy
levels on benchmark datasets. In this paper, we show that the key for further
improving the accuracy of such ensembles for SA is to include not only
transformers, but also traditional NLP models, despite the inferiority of the
latter compared to transformer models. However, as we empirically show, this
necessitates a change in how the ensemble is constructed, specifically relying
on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our
empirical studies across eight canonical SA datasets reveal that ensembles
incorporating a mix of model types, structured via HEC, significantly
outperform traditional ensembles. Finally, we provide a comparative analysis of
the performance of the HEC and GPT-4, demonstrating that while GPT-4 closely
approaches state-of-the-art SA methods, it remains outperformed by our proposed
ensemble strategy.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16702" title="Abstract">arXiv:2402.16702</a> [<a href="/pdf/2402.16702" title="Download PDF">pdf</a>, <a href="/ps/2402.16702" title="Download PostScript">ps</a>, <a href="/format/2402.16702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On bundle closures of matrix pencils and matrix polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=De+Ter%C3%A1n%2C+F">Fernando De Ter&#xe1;n</a>, 
<a href="/search/math?searchtype=author&query=Dopico%2C+F+M">Froil&#xe1;n M. Dopico</a>, 
<a href="/search/math?searchtype=author&query=Koval%2C+V">Vadym Koval</a>, 
<a href="/search/math?searchtype=author&query=Pagacz%2C+P">Patryk Pagacz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Bundles of matrix polynomials are sets of matrix polynomials with the same
size and grade and the same eigenstructure up to the specific values of the
eigenvalues. It is known that the closure of the bundle of a pencil $L$
(namely, a matrix polynomial of grade $1$), denoted by $\mathcal{B}(L)$, is the
union of $\mathcal{B}(L)$ itself with a finite number of other bundles. The
first main contribution of this paper is to prove that the dimension of each of
these bundles is strictly smaller than the dimension of $\mathcal{B}(L)$. The
second main contribution is to prove that also the closure of the bundle of a
matrix polynomial of grade larger than 1 is the union of the bundle itself with
a finite number of other bundles of smaller dimension. To get these results we
obtain a formula for the (co)dimension of the bundle of a matrix pencil in
terms of the Weyr characteristics of the partial multiplicities of the
eigenvalues and of the (left and right) minimal indices, and we provide a
characterization for the inclusion relationship between the closures of two
bundles of matrix polynomials of the same size and grade.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16705" title="Abstract">arXiv:2402.16705</a> [<a href="/pdf/2402.16705" title="Download PDF">pdf</a>, <a href="/format/2402.16705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelectIT: Selective Instruction Tuning for Large Language Models via  Uncertainty-Aware Self-Reflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning (IT) is crucial to tailoring large language models (LLMs)
towards human-centric interactions. Recent advancements have shown that the
careful selection of a small, high-quality subset of IT data can significantly
enhance the performance of LLMs. Despite this, common approaches often rely on
additional models or data sets, which increases costs and limits widespread
adoption. In this work, we propose a novel approach, termed SelectIT, that
capitalizes on the foundational capabilities of the LLM itself. Specifically,
we exploit the intrinsic uncertainty present in LLMs to more effectively select
high-quality IT data, without the need for extra resources. Furthermore, we
introduce a novel IT dataset, the Selective Alpaca, created by applying
SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT
using Selective Alpaca leads to substantial model ability enhancement. The
robustness of SelectIT has also been corroborated in various foundation models
and domain-specific tasks. Our findings suggest that longer and more
computationally intensive IT data may serve as superior sources of IT, offering
valuable insights for future research in this area. Data, code, and scripts are
freely available at https://github.com/Blue-Raincoat/SelectIT.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16710" title="Abstract">arXiv:2402.16710</a> [<a href="/pdf/2402.16710" title="Download PDF">pdf</a>, <a href="/format/2402.16710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost Aware Best Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanarios%2C+K">Kellen Kanarios</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qining Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lei Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we study a best arm identification problem with dual objects.
In addition to the classic reward, each arm is associated with a cost
distribution and the goal is to identify the largest reward arm using the
minimum expected cost. We call it \emph{Cost Aware Best Arm Identification}
(CABAI), which captures the separation of testing and implementation phases in
product development pipelines and models the objective shift between phases,
i.e., cost for testing and reward for implementation. We first derive an
theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$
to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we
further propose a low-complexity algorithm called CO, based on a square-root
rule, which proves optimal in simplified two-armed models and generalizes
surprisingly well in numerical experiments. Our results show (i) ignoring the
heterogeneous action cost results in sub-optimality in practice, and (ii)
low-complexity algorithms deliver near-optimal performance over a wide range of
problems.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16713" title="Abstract">arXiv:2402.16713</a> [<a href="/pdf/2402.16713" title="Download PDF">pdf</a>, <a href="/ps/2402.16713" title="Download PostScript">ps</a>, <a href="/format/2402.16713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Complexity: Orchestrated Problem Solving with Multi-Agent  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasal%2C+S">Sumedh Rasal</a>, 
<a href="/search/cs?searchtype=author&query=Hauer%2C+E+J">E. J. Hauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable capabilities in
solving various tasks, yet they often struggle with comprehensively addressing
complex and vague problems. Existing approaches, including multi-agent LLM
systems, offer solutions to certain challenges but still require manual setup
and lack scalability. To address this gap, we propose a novel approach
leveraging decomposition to enable LLMs to tackle vague problems effectively.
<br />Our approach involves an orchestrating LLM that interacts with users to
understand the problem and then decomposes it into tangible sub-problems.
Instead of expecting the LLM to solve the entire problem in one go, we train it
to ask follow-up questions to gain a deeper understanding of the user's
requirements. Once the problem is adequately understood, the orchestrating LLM
divides it into smaller, manageable sub-problems. Each sub-problem is then
assigned to specialized LLM agents or non-LLM functions for resolution. These
agents work in parallel to solve their respective sub-problems, with the
orchestrating LLM overseeing the process and compiling the solutions into a
comprehensive answer for the user. By adopting this decomposition approach, we
alleviate the constraints imposed by token limitations on LLM outputs and
empower them to provide nuanced solutions to complex and ambiguous problems.
<br />Through our approach, we aim to enable LLMs to think and operate more like
humans, breaking down complex problems into manageable parts and
collaboratively solving them. This not only enhances the problem-solving
capabilities of LLMs but also offers a scalable and efficient method for
addressing a wide range of real-world challenges.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16717" title="Abstract">arXiv:2402.16717</a> [<a href="/pdf/2402.16717" title="Download PDF">pdf</a>, <a href="/format/2402.16717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeChameleon: Personalized Encryption Framework for Jailbreaking Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Huijie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Caishuang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junjie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Adversarial misuse, particularly through `jailbreaking' that circumvents a
model's safety and ethical protocols, poses a significant challenge for Large
Language Models (LLMs). This paper delves into the mechanisms behind such
successful attacks, introducing a hypothesis for the safety mechanism of
aligned LLMs: intent security recognition followed by response generation.
Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak
framework based on personalized encryption tactics. To elude the intent
security recognition phase, we reformulate tasks into a code completion format,
enabling users to encrypt queries using personalized encryption functions. To
guarantee response generation functionality, we embed a decryption function
within the instructions, which allows the LLM to decrypt and execute the
encrypted queries successfully. We conduct extensive experiments on 7 LLMs,
achieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our
method achieves an 86.6\% ASR on GPT-4-1106.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16720" title="Abstract">arXiv:2402.16720</a> [<a href="/pdf/2402.16720" title="Download PDF">pdf</a>, <a href="/format/2402.16720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think2Drive: Efficient Reinforcement Learning by Thinking in Latent  World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaosong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaobo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Real-world autonomous driving (AD) especially urban driving involves many
corner cases. The lately released AD simulator CARLA v2 adds 39 common events
in the driving scene, and provide more quasi-realistic testbed compared to
CARLA v1. It poses new challenge to the community and so far no literature has
reported any success on the new scenarios in V2 as existing works mostly have
to rely on specific rules for planning yet they cannot cover the more complex
cases in CARLA v2. In this work, we take the initiative of directly training a
planner and the hope is to handle the corner cases flexibly and effectively,
which we believe is also the future of AD. To our best knowledge, we develop
the first model-based RL method named Think2Drive for AD, with a world model to
learn the transitions of the environment, and then it acts as a neural
simulator to train the planner. This paradigm significantly boosts the training
efficiency due to the low dimensional state space and parallel computing of
tensors in the world model. As a result, Think2Drive is able to run in an
expert-level proficiency in CARLA v2 within 3 days of training on a single
A6000 GPU, and to our best knowledge, so far there is no reported success
(100\% route completion)on CARLA v2. We also propose CornerCase-Repository, a
benchmark that supports the evaluation of driving models by scenarios.
Additionally, we propose a new and balanced metric to evaluate the performance
by route completion, infraction number, and scenario density, so that the
driving score could give more information about the actual driving performance.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16726" title="Abstract">arXiv:2402.16726</a> [<a href="/pdf/2402.16726" title="Download PDF">pdf</a>, <a href="/format/2402.16726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Grokked Transformers in Complex Modular Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Gouki%2C+M">Minegishi Gouki</a>, 
<a href="/search/cs?searchtype=author&query=Iwasawa%2C+Y">Yusuke Iwasawa</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/frt03/grok_mod_poly">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Grokking has been actively explored to reveal the mystery of delayed
generalization. Identifying interpretable algorithms inside the grokked models
is a suggestive hint to understanding its mechanism. In this work, beyond the
simplest and well-studied modular addition, we observe the internal circuits
learned through grokking in complex modular arithmetic via interpretable
reverse engineering, which highlights the significant difference in their
dynamics: subtraction poses a strong asymmetry on Transformer; multiplication
requires cosine-biased components at all the frequencies in a Fourier domain;
polynomials often result in the superposition of the patterns from elementary
arithmetic, but clear patterns do not emerge in challenging cases; grokking can
easily occur even in higher-degree formulas with basic symmetric and
alternating expressions. We also introduce the novel progress measure for
modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio,
which not only indicate the late generalization but also characterize
distinctive internal representations of grokked models per modular operation.
Our empirical analysis emphasizes the importance of holistic evaluation among
various combinations.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16728" title="Abstract">arXiv:2402.16728</a> [<a href="/pdf/2402.16728" title="Download PDF">pdf</a>, <a href="/format/2402.16728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto Tuning for OpenMP Dynamic Scheduling applied to FWI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+F+H+S">Felipe H. S. da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+J+B">Jo&#xe3;o B. Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Sardina%2C+I+M">Idalmis M. Sardina</a>, 
<a href="/search/cs?searchtype=author&query=Barros%2C+T">Tiago Barros</a>, 
<a href="/search/cs?searchtype=author&query=Xavier-de-Souza%2C+S">Samuel Xavier-de-Souza</a>, 
<a href="/search/cs?searchtype=author&query=Assis%2C+I+A+S">Italo A. S. Assis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Because Full Waveform Inversion (FWI) works with a massive amount of data,
its execution requires much time and computational resources, being restricted
to large-scale computer systems such as supercomputers. Techniques such as FWI
adapt well to parallel computing and can be parallelized in shared memory
systems using the application programming interface (API) OpenMP. The
management of parallel tasks can be performed through loop schedulers contained
in OpenMP. The dynamic scheduler stands out for distributing predefined
fixed-size chunk sizes to idle processing cores at runtime. It can better adapt
to FWI, where data processing can be irregular. However, the relationship
between the size of the chunk size and the runtime is unknown. Optimization
techniques can employ meta-heuristics to explore the parameter search space,
avoiding testing all possible solutions. Here, we propose a strategy to use the
Parameter Auto Tuning for Shared Memory Algorithms (PATSMA), with Coupled
Simulated Annealing (CSA) as its optimization method, to automatically adjust
the chunk size for the dynamic scheduling of wave propagation, one of the most
expensive steps in FWI. Since testing each candidate chunk size in the complete
FWI is unpractical, our approach consists of running a PATSMA where the
objective function is the runtime of the first time iteration of the first
seismic shot of the first FWI iteration. The resulting chunk size is then
employed in all wave propagations involved in an FWI. We conducted tests to
measure the runtime of an FWI using the proposed autotuning, varying the
problem size and running on different computational environments, such as
supercomputers and cloud computing instances. The results show that applying
the proposed autotuning in an FWI reduces its runtime by up to 70.46% compared
to standard OpenMP schedulers.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16731" title="Abstract">arXiv:2402.16731</a> [<a href="/pdf/2402.16731" title="Download PDF">pdf</a>, <a href="/format/2402.16731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giannoula%2C+C">Christina Giannoula</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Peiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Vega%2C+I+F">Ivan Fernandez Vega</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiacheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+X">Yu Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Luna%2C+J+G">Juan Gomez Luna</a>, 
<a href="/search/cs?searchtype=author&query=Sadrosadati%2C+M">Mohammad Sadrosadati</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Pekhimenko%2C+G">Gennady Pekhimenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16733" title="Abstract">arXiv:2402.16733</a> [<a href="/pdf/2402.16733" title="Download PDF">pdf</a>, <a href="/format/2402.16733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+H">Haneul Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jieun Han</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">So-Yeon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.05191">arXiv:2310.05191</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated essay scoring (AES) is a useful tool in English as a Foreign
Language (EFL) writing education, offering real-time essay scores for students
and instructors. However, previous AES models were trained on essays and scores
irrelevant to the practical scenarios of EFL writing education and usually
provided a single holistic score due to the lack of appropriate datasets. In
this paper, we release DREsS, a large-scale, standard dataset for rubric-based
automated essay scoring. DREsS comprises three sub-datasets: DREsS_New,
DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with
1.7K essays authored by EFL undergraduate students and scored by English
education experts. We also standardize existing rubric-based essay scoring
datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation
strategy for essays, which generates 20K synthetic samples of DREsS_CASE and
improves the baseline results by 45.44%. DREsS will enable further research to
provide a more accurate and practical AES system for EFL writing education.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16735" title="Abstract">arXiv:2402.16735</a> [<a href="/pdf/2402.16735" title="Download PDF">pdf</a>, <a href="/format/2402.16735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multichain Taprootized Atomic Swaps: Introducing Untraceability through  Zero-Knowledge Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurbatov%2C+O">Oleksandr Kurbatov</a>, 
<a href="/search/cs?searchtype=author&query=Zakharov%2C+D">Dmytro Zakharov</a>, 
<a href="/search/cs?searchtype=author&query=Levochko%2C+A">Anton Levochko</a>, 
<a href="/search/cs?searchtype=author&query=Riabov%2C+K">Kyrylo Riabov</a>, 
<a href="/search/cs?searchtype=author&query=Skriabin%2C+B">Bohdan Skriabin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Taprootized Atomic Swaps is an extension for Atomic Swaps that enables the
untraceability of transactions in a particular swap. Based on Schnorr
signatures, Taproot technology, and zero-knowledge proofs, the taprootized
atomic swaps hide swap transactions between regular payments. We propose
several implementation options: single-transaction protocol,
multiple-transaction protocol that splits the receiving amount in an
untraceable way, and multichain swap protocol. Our proposed approach works with
any smart-contract-compatible chain and multiple Taproot-compatible chains. We
describe the concrete implementation of the protocol and release the source
code publically.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16739" title="Abstract">arXiv:2402.16739</a> [<a href="/pdf/2402.16739" title="Download PDF">pdf</a>, <a href="/format/2402.16739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zanjani%2C+F+G">Farhad G. Zanjani</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mirvakhabova%2C+L">Leyla Mirvakhabova</a>, 
<a href="/search/cs?searchtype=author&query=Porikli%2C+F">Fatih Porikli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents Neural Mesh Fusion (NMF), an efficient approach for joint
optimization of polygon mesh from multi-view image observations and
unsupervised 3D planar-surface parsing of the scene. In contrast to implicit
neural representations, NMF directly learns to deform surface triangle mesh and
generate an embedding for unsupervised 3D planar segmentation through
gradient-based optimization directly on the surface mesh. The conducted
experiments show that NMF obtains competitive results compared to
state-of-the-art multi-view planar reconstruction, while not requiring any
ground-truth 3D or planar supervision. Moreover, NMF is significantly more
computationally efficient compared to implicit neural rendering-based scene
reconstruction approaches.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16741" title="Abstract">arXiv:2402.16741</a> [<a href="/pdf/2402.16741" title="Download PDF">pdf</a>, <a href="/format/2402.16741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More Revisit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+N">Nobuko Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+P">Ping Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Multiparty session types (MPST) is a type discipline where a programmer or
architect specifies a whole view of communications as a global protocol, and
each distributed program is locally type-checked against its end-point
projection. After 10 years from the birth of MPST, Scalas and Yoshida have
discovered that the proofs of type safety in the literature which use the
end-point projection with mergeability are flawed. After this paper,
researchers wrongly believe that the end-point projection (with mergeability)
is unsound. We correct this misunderstanding, proposing a new general proof
technique for type soundness of multiparty session $\pi$-calculus, which uses
an association relation between a global type and its end-point projection.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16744" title="Abstract">arXiv:2402.16744</a> [<a href="/pdf/2402.16744" title="Download PDF">pdf</a>, <a href="/format/2402.16744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical foundations of spectral methods for time-dependent PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Iserles%2C+A">Arieh Iserles</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The contention of this paper is that a spectral method for time-dependent
PDEs is basically no more than a choice of an orthonormal basis of the
underlying Hilbert space. This choice is governed by a long list of
considerations: stability, speed of convergence, geometric numerical
integration, fast approximation and efficient linear algebra. We subject
different choices of orthonormal bases, focussing on the real line, to these
considerations. While nothing is likely to improve upon a Fourier basis in the
presence of periodic boundary conditions, the situation is considerably more
interesting in other settings. We introduce two kinds of orthonormal bases,
T-systems and W-systems, and investigate in detail their features. T-systems
are designed to work with Cauchy boundary conditions, while W-systems are
suited to zero Dirichlet boundary conditions.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16746" title="Abstract">arXiv:2402.16746</a> [<a href="/pdf/2402.16746" title="Download PDF">pdf</a>, <a href="/format/2402.16746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic-preserving and energy stable dynamical low-rank approximation  for thermal radiative transfer equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Patwardhan%2C+C">Chinmay Patwardhan</a>, 
<a href="/search/math?searchtype=author&query=Frank%2C+M">Martin Frank</a>, 
<a href="/search/math?searchtype=author&query=Kusch%2C+J">Jonas Kusch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The thermal radiative transfer equations model temperature evolution through
a background medium as a result of radiation. When a large number of particles
are absorbed in a short time scale, the dynamics tend to a non-linear
diffusion-type equation called the Rosseland approximation. The main challenges
for constructing numerical schemes that exhibit the correct limiting behavior
are posed by the solution's high-dimensional phase space and multi-scale
effects. In this work, we propose an asymptotic-preserving and rank-adaptive
dynamical low-rank approximation scheme based on the macro-micro decomposition
of the particle density and a modified augmented basis-update \&amp; Galerkin
integrator. We show that this scheme, for linear particle emission by the
material, dissipates energy over time under a step size restriction that
captures the hyperbolic and parabolic CFL conditions. We demonstrate the
efficacy of the proposed method in a series of numerical experiments.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16748" title="Abstract">arXiv:2402.16748</a> [<a href="/pdf/2402.16748" title="Download PDF">pdf</a>, <a href="/format/2402.16748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Hypergradients Estimation: A Study of Preconditioning and  Reparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhenzhang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Peyr%C3%A9%2C+G">Gabriel Peyr&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>, 
<a href="/search/cs?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bilevel optimization aims to optimize an outer objective function that
depends on the solution to an inner optimization problem. It is routinely used
in Machine Learning, notably for hyperparameter tuning. The conventional method
to compute the so-called hypergradient of the outer problem is to use the
Implicit Function Theorem (IFT). As a function of the error of the inner
problem resolution, we study the error of the IFT method. We analyze two
strategies to reduce this error: preconditioning the IFT formula and
reparameterizing the inner problem. We give a detailed account of the impact of
these two modifications on the error, highlighting the role played by
higher-order derivatives of the functionals at stake. Our theoretical findings
explain when super efficiency, namely reaching an error on the hypergradient
that depends quadratically on the error on the inner problem, is achievable and
compare the two approaches when this is impossible. Numerical evaluations on
hyperparameter tuning for regression problems substantiate our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16749" title="Abstract">arXiv:2402.16749</a> [<a href="/pdf/2402.16749" title="Download PDF">pdf</a>, <a href="/format/2402.16749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large  Multimodal Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guo Lu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Donghui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">With the evolution of storage and communication protocols, ultra-low bitrate
image compression has become a highly demanding topic. However, existing
compression algorithms must sacrifice either consistency with the ground truth
or perceptual quality at ultra-low bitrate. In recent years, the rapid
development of the Large Multimodal Model (LMM) has made it possible to balance
these two goals. To solve this problem, this paper proposes a method called
Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder
for extracting the semantic information of the image, a map encoder to locate
the region corresponding to the semantic, an image encoder generates an
extremely compressed bitstream, and a decoder reconstructs the image based on
the above information. Experimental results show that our proposed MISC is
suitable for compressing both traditional Natural Sense Images (NSIs) and
emerging AI-Generated Images (AIGIs) content. It can achieve optimal
consistency and perception results while saving 50% bitrate, which has strong
potential applications in the next generation of storage and communication. The
code will be released on https://github.com/lcysyzxdxc/MISC.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16751" title="Abstract">arXiv:2402.16751</a> [<a href="/pdf/2402.16751" title="Download PDF">pdf</a>, <a href="/format/2402.16751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value Preferences Estimation and Disambiguation in Hybrid Participatory  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liscio%2C+E">Enrico Liscio</a>, 
<a href="/search/cs?searchtype=author&query=Siebert%2C+L+C">Luciano C. Siebert</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+C+M">Catholijn M. Jonker</a>, 
<a href="/search/cs?searchtype=author&query=Murukannaiah%2C+P+K">Pradeep K. Murukannaiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Understanding citizens' values in participatory systems is crucial for
citizen-centric policy-making. We envision a hybrid participatory system where
participants make choices and provide motivations for those choices, and AI
agents estimate their value preferences by interacting with them. We focus on
situations where a conflict is detected between participants' choices and
motivations, and propose methods for estimating value preferences while
addressing detected inconsistencies by interacting with the participants. We
operationalize the philosophical stance that "valuing is deliberatively
consequential." That is, if a participant's choice is based on a deliberation
of value preferences, the value preferences can be observed in the motivation
the participant provides for the choice. Thus, we propose and compare value
estimation methods that prioritize the values estimated from motivations over
the values estimated from choices alone. Then, we introduce a disambiguation
strategy that addresses the detected inconsistencies between choices and
motivations by directly interacting with the participants. We evaluate the
proposed methods on a dataset of a large-scale survey on energy transition. The
results show that explicitly addressing inconsistencies between choices and
motivations improves the estimation of an individual's value preferences. The
disambiguation strategy does not show substantial improvements when compared to
similar baselines--however, we discuss how the novelty of the approach can open
new research avenues and propose improvements to address the current
limitations.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16755" title="Abstract">arXiv:2402.16755</a> [<a href="/pdf/2402.16755" title="Download PDF">pdf</a>, <a href="/format/2402.16755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Bridging the Gap between Near and Far-Field Characterizations of  the Wireless Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+N">Navneet Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Tohidi%2C+E">Ehsan Tohidi</a>, 
<a href="/search/cs?searchtype=author&query=Cavalcante%2C+R+L+G">Renato L. G. Cavalcante</a>, 
<a href="/search/cs?searchtype=author&query=Sta%C5%84czak%2C+S">S&#x142;awomir Sta&#x144;czak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE International Conference on Communications (ICC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The "near-field" propagation modeling of wireless channels is necessary to
support sixth-generation (6G) technologies, such as intelligent reflecting
surface (IRS), that are enabled by large aperture antennas and higher frequency
carriers. As the conventional far-field model proves inadequate in this
context, there is a pressing need to explore and bridge the gap between near
and far-field propagation models. Although far-field models are simple and
provide computationally efficient solutions for many practical applications,
near-field models provide the most accurate representation of wireless
channels. This paper builds upon the foundations of electromagnetic wave
propagation theory to derive near and far-field models as approximations of the
Green's function (Maxwell's equations). We characterize the near and far-field
models both theoretically and with the help of simulations in a line-of-sight
(LOS)-only scenario. In particular, for two key applications in multiantenna
systems, namely, beamforming and multiple-access, we showcase the advantages of
using the near-field model over the far-field, and present a novel scheduling
scheme for multiple-access in the near-field regime. Our findings offer
insights into the challenge of incorporating near-field models in practical
wireless systems, fostering enhanced performance in future communication
technologies.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16757" title="Abstract">arXiv:2402.16757</a> [<a href="/pdf/2402.16757" title="Download PDF">pdf</a>, <a href="/format/2402.16757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Environmental Preference Based Speech Enhancement For  Individualised Multi-Modal Hearing Aids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirton-Wingate%2C+J">Jasper Kirton-Wingate</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S">Shafique Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Adeel Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Gogate%2C+M">Mandar Gogate</a>, 
<a href="/search/cs?searchtype=author&query=Dashtipour%2C+K">Kia Dashtipour</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jen-Cheng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+T">Tassadaq Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Amir Hussain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This has been submitted to the Trends in Hearing journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Since the advent of Deep Learning (DL), Speech Enhancement (SE) models have
performed well under a variety of noise conditions. However, such systems may
still introduce sonic artefacts, sound unnatural, and restrict the ability for
a user to hear ambient sound which may be of importance. Hearing Aid (HA) users
may wish to customise their SE systems to suit their personal preferences and
day-to-day lifestyle. In this paper, we introduce a preference learning based
SE (PLSE) model for future multi-modal HAs that can contextually exploit audio
information to improve listening comfort, based upon the preferences of the
user. The proposed system estimates the Signal-to-noise ratio (SNR) as a basic
objective speech quality measure which quantifies the relative amount of
background noise present in speech, and directly correlates to the
intelligibility of the signal. Additionally, to provide contextual information
we predict the acoustic scene in which the user is situated. These tasks are
achieved via a multi-task DL model, which surpasses the performance of
inferring the acoustic scene or SNR separately, by jointly leveraging a shared
encoded feature space. These environmental inferences are exploited in a
preference elicitation framework, which linearly learns a set of predictive
functions to determine the target SNR of an AV (Audio-Visual) SE system. By
greatly reducing noise in challenging listening conditions, and by novelly
scaling the output of the SE model, we are able to provide HA users with
contextually individualised SE. Preliminary results suggest an improvement over
the non-individualised baseline model in some participants.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16759" title="Abstract">arXiv:2402.16759</a> [<a href="/pdf/2402.16759" title="Download PDF">pdf</a>, <a href="/format/2402.16759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing  and Data Collection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DuFrene%2C+K">Kyle DuFrene</a>, 
<a href="/search/cs?searchtype=author&query=Strohbehn%2C+L">Luke Strohbehn</a>, 
<a href="/search/cs?searchtype=author&query=Nave%2C+K">Keegan Nave</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+R">Ravi Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Grimm%2C+C">Cindy Grimm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic manipulation in human environments is a challenging problem for
researchers and industry alike. In particular, opening doors/drawers can be
challenging for robots, as the size, shape, actuation and required force is
variable. Because of this, it can be difficult to collect large real-world
datasets and to benchmark different control algorithms on the same hardware. In
this paper we present two automated testbeds, the Door Reset Mechanism (DORM)
and Drawer Reset Mechanism (DWRM), for the purpose of real world testing and
data collection. These devices are low-cost, are sensorized, operate with
customized variable resistance, and come with open source software.
Additionally, we provide a dataset of over 600 grasps using the DORM and DWRM.
We use this dataset to highlight how much variability can exist even with the
same trial on the same hardware. This data can also serve as a source for
real-world noise in simulation environments.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16760" title="Abstract">arXiv:2402.16760</a> [<a href="/pdf/2402.16760" title="Download PDF">pdf</a>, <a href="/format/2402.16760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Dark Pattern Taxonomies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lewis%2C+F">Frank Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Vassileva%2C+J">Julita Vassileva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The problem of ``Dark Patterns" in user interface/user experience (UI/UX)
design has proven a difficult issue to tackle. Malicious and explotitative
design has expanded to multiple domains in the past 10 years and which has in
turn led to multiple taxonomies attempting to describe them. While these
taxonomies holds their own merit, and constitute unique contributions to the
literature, their usefulness as separate entities is limited. We believe that
in order to make meaningful progress in regulating malicious interface design,
we must first form a globally harmonized system (GHS) for the classification
and labeling of Dark Patterns. By leaning on network analysis tools and
methods, this paper synthesizes existing taxonomies and their elements through
as a directed graph. In doing so, the interconnectedness of Dark patterns can
be more clearly revealed via community (cluster) detection. Ultimately, we hope
that this work can serve as the inspiration for the creation of a glyph-based
GHS for the classification of Dark Patterns.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16765" title="Abstract">arXiv:2402.16765</a> [<a href="/pdf/2402.16765" title="Download PDF">pdf</a>, <a href="/format/2402.16765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oscillations-Aware Frequency Security Assessment via Efficient  Worst-Case Frequency Nadir Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Y">Yan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Min%2C+H">Hancheng Min</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Baosen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Frequency security assessment following major disturbances has long been one
of the central tasks in power system operations. The standard approach is to
study the center of inertia frequency, an aggregate signal for an entire
system, to avoid analyzing the frequency signal at individual buses. However,
as the amount of low-inertia renewable resources in a grid increases, the
center of inertia frequency is becoming too coarse to provide reliable
frequency security assessment. In this paper, we propose an efficient algorithm
to determine the worst-case frequency nadir across all buses for bounded power
disturbances, as well as identify the power disturbances leading to that
severest scenario. The proposed algorithm allows oscillations-aware frequency
security assessment without conducting exhaustive simulations and intractable
analysis.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16766" title="Abstract">arXiv:2402.16766</a> [<a href="/pdf/2402.16766" title="Download PDF">pdf</a>, <a href="/ps/2402.16766" title="Download PostScript">ps</a>, <a href="/format/2402.16766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Paradox of Industrial Involvement in Engineering Higher Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Srinjoy Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Raskin%2C+J">Jean-Pierre Raskin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper discusses the importance of reflective and socially conscious
education in engineering schools, particularly within the EE/CS sector. While
most engineering disciplines have historically aligned themselves with the
demands of the technology industry, the lack of critical examination of
industry practices and their impact on justice, equality, and sustainability is
self-evident. Today, the for-profit engineering/technology companies, some of
which are among the largest in the world, also shape the narrative of
engineering education and research in universities. As engineering graduates
form the largest cohorts within STEM disciplines in Western countries, they
become future professionals who will work, lead, or even establish companies in
this industry. Unfortunately, the curriculum within engineering education often
lacks a deep understanding of social realities, an essential component of a
comprehensive university education. Here we establish this unusual connection
with the industry that has driven engineering higher education for several
decades and its obvious negative impacts to society. We analyse this nexus and
highlight the need for engineering schools to hold a more critical viewpoint.
Given the wealth and power of modern technology companies, particularly in the
ICT domain, questioning their techno-solutionism narrative is essential within
the institutes of higher education.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16767" title="Abstract">arXiv:2402.16767</a> [<a href="/pdf/2402.16767" title="Download PDF">pdf</a>, <a href="/format/2402.16767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CorpusBrain++: A Continual Generative Pre-Training Framework for  Knowledge-Intensive Language Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Changjiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangui Chen</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yixing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM Transactions on Information Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Knowledge-intensive language tasks (KILTs) typically require retrieving
relevant documents from trustworthy corpora, e.g., Wikipedia, to produce
specific answers. Very recently, a pre-trained generative retrieval model for
KILTs, named CorpusBrain, was proposed and reached new state-of-the-art
retrieval performance. However, most existing research on KILTs, including
CorpusBrain, has predominantly focused on a static document collection,
overlooking the dynamic nature of real-world scenarios, where new documents are
continuously being incorporated into the source corpus. To address this gap, it
is crucial to explore the capability of retrieval models to effectively handle
the dynamic retrieval scenario inherent in KILTs.
<br />In this work, we first introduce the continual document learning (CDL) task
for KILTs and build a novel benchmark dataset named KILT++ based on the
original KILT dataset for evaluation. Then, we conduct a comprehensive study
over the use of pre-trained CorpusBrain on KILT++. Unlike the promising results
in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in
the dynamic scenario, hence hampering the retrieval performance. To alleviate
this issue, we propose CorpusBrain++, a continual generative pre-training
framework. Empirical results demonstrate the significant effectiveness and
remarkable efficiency of CorpusBrain++ in comparison to both traditional and
generative IR methods.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16769" title="Abstract">arXiv:2402.16769</a> [<a href="/pdf/2402.16769" title="Download PDF">pdf</a>, <a href="/format/2402.16769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Latent and Lexicon Representations for Effective Video-Text  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yaya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chunfeng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Weiming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In video-text retrieval, most existing methods adopt the dual-encoder
architecture for fast retrieval, which employs two individual encoders to
extract global latent representations for videos and texts. However, they face
challenges in capturing fine-grained semantic concepts. In this work, we
propose the UNIFY framework, which learns lexicon representations to capture
fine-grained semantics and combines the strengths of latent and lexicon
representations for video-text retrieval. Specifically, we map videos and texts
into a pre-defined lexicon space, where each dimension corresponds to a
semantic concept. A two-stage semantics grounding approach is proposed to
activate semantically relevant dimensions and suppress irrelevant dimensions.
The learned lexicon representations can thus reflect fine-grained semantics of
videos and texts. Furthermore, to leverage the complementarity between latent
and lexicon representations, we propose a unified learning scheme to facilitate
mutual learning via structure sharing and self-distillation. Experimental
results show our UNIFY framework largely outperforms previous video-text
retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and
DiDeMo respectively.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16774" title="Abstract">arXiv:2402.16774</a> [<a href="/pdf/2402.16774" title="Download PDF">pdf</a>, <a href="/ps/2402.16774" title="Download PostScript">ps</a>, <a href="/format/2402.16774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Based Autism Detection with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serna-Aguilera%2C+M">M. Serna-Aguilera</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+X+B">X. B. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">A. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Rockers%2C+L">L. Rockers</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">S. Park</a>, 
<a href="/search/cs?searchtype=author&query=Neely%2C+L">L. Neely</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">H. Seo</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+K">K. Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Poster Abstract. Accepted into 2024 IEEE Green Technologies Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Autism Spectrum Disorder (ASD) can often make life difficult for children,
therefore early diagnosis is necessary for proper treatment and care. Thus, in
this work, we consider the problem of detecting or classifying ASD in children
to aid medical professionals in early detection. To this end, we develop a deep
learning model that analyzes video clips of children reacting to sensory
stimuli, with the intent on capturing key differences in reactions and behavior
between ASD and non-ASD patients. Unlike many works in ASD classification,
their data consist of MRI data, which requires expensive specialized MRI
equipment, meanwhile our method need only rely on a powerful but relatively
cheaper GPU, a decent computer setup, and a video camera for inference. Results
on our data show that our model can generalize well and can understand key
differences in the distinct movements of the patients. This is despite limited
amounts of data for a deep learning problem, limited temporal information
available to the model as input, and even when there is noise due to movement.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16775" title="Abstract">arXiv:2402.16775</a> [<a href="/pdf/2402.16775" title="Download PDF">pdf</a>, <a href="/format/2402.16775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Quantization Strategies for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Renren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jiangcun Du</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wuwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+J">Jian Luan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 16 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Increasing the number of parameters in large language models (LLMs) usually
improves performance in downstream tasks but raises compute and memory costs,
making deployment difficult in resource-limited settings. Quantization
techniques, which reduce the bits needed for model weights or activations with
minimal performance loss, have become popular due to the rise of LLMs. However,
most quantization studies use pre-trained LLMs, and the impact of quantization
on instruction-tuned LLMs and the relationship between perplexity and benchmark
performance of quantized LLMs are not well understood. Evaluation of quantized
LLMs is often limited to language modeling and a few classification tasks,
leaving their performance on other benchmarks unclear. To address these gaps,
we propose a structured evaluation framework consisting of three critical
dimensions: (1) knowledge \&amp; capacity, (2) alignment, and (3) efficiency, and
conduct extensive experiments across ten diverse benchmarks. Our experimental
results indicate that LLMs with 4-bit quantization can retain performance
comparable to their non-quantized counterparts, and perplexity can serve as a
proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs
with larger parameter scales can outperform smaller LLMs. Despite the memory
savings achieved through quantization, it can also slow down the inference
speed of LLMs. Consequently, substantial engineering efforts and hardware
support are imperative to achieve a balanced optimization of decoding speed and
memory consumption in the context of quantized LLMs.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16777" title="Abstract">arXiv:2402.16777</a> [<a href="/pdf/2402.16777" title="Download PDF">pdf</a>, <a href="/format/2402.16777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Simplet Frequency Distribution for Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beigy%2C+H">Hamid Beigy</a>, 
<a href="/search/cs?searchtype=author&query=Mahini%2C+M">Mohammad Mahini</a>, 
<a href="/search/cs?searchtype=author&query=Qadami%2C+S">Salman Qadami</a>, 
<a href="/search/cs?searchtype=author&query=Saghafian%2C+M">Morteza Saghafian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EuroCG2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computational Complexity (cs.CC); Geometric Topology (math.GT)

</div>
<p class="mathjax">Simplets, constituting elementary units within simplicial complexes (SCs),
serve as foundational elements for the structural analysis of SCs. Previous
efforts have focused on the exact count or approximation of simplet count
rather than their frequencies, with the latter being more practical in
large-scale SCs. This paper enables simplet frequency analysis of SCs by
introducing the Simplet Frequency Distribution (SFD) vector. In addition, we
present a bound on the sample complexity required for accurately approximating
the SFD vector by any uniform sampling-based algorithm. We also present a
simple algorithm for this purpose and justify the theoretical bounds with
experiments on some random simplicial complexes.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16778" title="Abstract">arXiv:2402.16778</a> [<a href="/pdf/2402.16778" title="Download PDF">pdf</a>, <a href="/format/2402.16778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Growth of Mistakes in Differentially Private Online Learning: A  Lower Bound Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dmitriev%2C+D">Daniil Dmitriev</a>, 
<a href="/search/cs?searchtype=author&query=Szab%C3%B3%2C+K">Krist&#xf3;f Szab&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+A">Amartya Sanyal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this paper, we provide lower bounds for Differentially Private (DP) Online
Learning algorithms. Our result shows that, for a broad class of
$(\varepsilon,\delta)$-DP online algorithms, for $T$ such that $\log T\leq O(1
/ \delta)$, the expected number of mistakes incurred by the algorithm grows as
$\Omega(\log \frac{T}{\delta})$. This matches the upper bound obtained by
Golowich and Livni (2021) and is in contrast to non-private online learning
where the number of mistakes is independent of $T$. To the best of our
knowledge, our work is the first result towards settling lower bounds for
DP-Online learning and partially addresses the open question in Sanyal and
Ramponi (2022).
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16785" title="Abstract">arXiv:2402.16785</a> [<a href="/pdf/2402.16785" title="Download PDF">pdf</a>, <a href="/format/2402.16785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARTE: pretraining and transfer for tabular learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M+J">Myung Jun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Grinsztajn%2C+L">L&#xe9;o Grinsztajn</a>, 
<a href="/search/cs?searchtype=author&query=Varoquaux%2C+G">Ga&#xeb;l Varoquaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pretrained deep-learning models are the go-to solution for images or text.
However, for tabular data the standard is still to train tree-based models.
Pre-training or transfer is a huge challenge as in general tables have columns
about different quantities and naming conventions that vary vastly across
sources. Data integration tackles correspondences across multiple sources:
schema matching for columns, and entity matching for entries. We propose a
neural architecture that does not need such matches. As a result, we can
pretrain it on background data that has not been matched. The architecture -
CARTE for Context Aware Representation of Table Entries - uses a graph
representation of tabular (or relational) data to process tables with different
columns, string embeddings of entries and columns names to model an open
vocabulary, and a graph-attentional network to contextualize entries with
column names and neighboring entries. An extensive benchmark shows that CARTE
facilitates learning, outperforming a solid set of baselines including the best
tree-based models. CARTE also enables joint learning across tables with
unmatched columns, enhancing a small table with bigger ones. CARTE opens the
door to large pretrained models embarking information for tabular data.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16786" title="Abstract">arXiv:2402.16786</a> [<a href="/pdf/2402.16786" title="Download PDF">pdf</a>, <a href="/format/2402.16786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Political Compass or Spinning Arrow? Towards More Meaningful Evaluations  for Values and Opinions in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+V">Valentin Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Hinck%2C+M">Musashi Hinck</a>, 
<a href="/search/cs?searchtype=author&query=Kirk%2C+H+R">Hannah Rose Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+D">Dirk Hovy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1 prepared for conference submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Much recent work seeks to evaluate values and opinions in large language
models (LLMs) using multiple-choice surveys and questionnaires. Most of this
work is motivated by concerns around real-world LLM applications. For example,
politically-biased LLMs may subtly influence society when they are used by
millions of people. Such real-world concerns, however, stand in stark contrast
to the artificiality of current evaluations: real users do not typically ask
LLMs survey questions. Motivated by this discrepancy, we challenge the
prevailing constrained evaluation paradigm for values and opinions in LLMs and
explore more realistic unconstrained evaluations. As a case study, we focus on
the popular Political Compass Test (PCT). In a systematic review, we find that
most prior work using the PCT forces models to comply with the PCT's
multiple-choice format. We show that models give substantively different
answers when not forced; that answers change depending on how models are
forced; and that answers lack paraphrase robustness. Then, we demonstrate that
models give different answers yet again in a more realistic open-ended answer
setting. We distill these findings into recommendations and open challenges in
evaluating values and opinions in LLMs.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16788" title="Abstract">arXiv:2402.16788</a> [<a href="/pdf/2402.16788" title="Download PDF">pdf</a>, <a href="/format/2402.16788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Transformers Need Adam: A Hessian Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Congliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziniu Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">SGD performs worse than Adam by a significant margin on Transformers, but the
reason remains unclear. In this work, we provide an explanation of SGD's
failure on Transformers through the lens of Hessian: (i) Transformers are
``heterogeneous'': the Hessian spectrum across parameter blocks vary
dramatically, a phenomenon we call ``block heterogeneity"; (ii) Heterogeneity
hampers SGD: SGD performs badly on problems with block heterogeneity. To
validate that heterogeneity hampers SGD, we check various Transformers, CNNs,
MLPs, and quadratic problems, and find that SGD works well on problems without
block heterogeneity but performs badly when the heterogeneity exists. Our
initial theoretical analysis indicates that SGD fails because it applies one
single learning rate for all blocks, which cannot handle the heterogeneity
among blocks. The failure could be rescued if we could assign different
learning rates across blocks, as designed in Adam.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16790" title="Abstract">arXiv:2402.16790</a> [<a href="/pdf/2402.16790" title="Download PDF">pdf</a>, <a href="/format/2402.16790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Self-learned Attention: Mitigating Attention Bias in  Transformer-based Models Using Attention Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gesi%2C+J">Jiri Gesi</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Iftekhar Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer-based models have demonstrated considerable potential for source
code modeling tasks in software engineering. However, they are limited by their
dependence solely on automatic self-attention weight learning mechanisms.
Previous studies have shown that these models overemphasize delimiters added by
tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential
information in the original input source code. To address this challenge, we
introduce SyntaGuid, a novel approach that utilizes the observation that
attention weights tend to be biased towards specific source code syntax tokens
and abstract syntax tree (AST) elements in fine-tuned language models when they
make correct predictions. SyntaGuid facilitates the guidance of
attention-weight learning, leading to improved model performance on various
software engineering tasks. We evaluate the effectiveness of SyntaGuid on
multiple tasks and demonstrate that it outperforms existing state-of-the-art
models in overall performance without requiring additional data. Experimental
result shows that SyntaGuid can improve overall performance up to 3.25% and fix
up to 28.3% wrong predictions. Our work represents the first attempt to guide
the attention of Transformer-based models towards critical source code tokens
during fine-tuning, highlighting the potential for enhancing Transformer-based
models in software engineering.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16795" title="Abstract">arXiv:2402.16795</a> [<a href="/pdf/2402.16795" title="Download PDF">pdf</a>, <a href="/format/2402.16795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> If in a Crowdsourced Data Annotation Pipeline, a GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zeyu He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chieh-Yang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C+C">Chien-Kuang Cornelia Ding</a>, 
<a href="/search/cs?searchtype=author&query=Rohatgi%2C+S">Shaurya Rohatgi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T+%27">Ting-Hao &#x27;Kenneth&#x27; Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted By CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recent studies indicated GPT-4 outperforms online crowd workers in data
labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).
However, these studies were criticized for deviating from standard
crowdsourcing practices and emphasizing individual workers' performances over
the whole data-annotation process. This paper compared GPT-4 and an ethical and
well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments
from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces
yielded 127,080 labels, which were then used to infer the final labels through
eight label-aggregation algorithms. Our evaluation showed that despite best
practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved
83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected
via an advanced worker interface for aggregation, 2 out of the 8 algorithms
achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested
that, when the crowd's and GPT-4's labeling strengths are complementary,
aggregating them could increase labeling accuracy.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16796" title="Abstract">arXiv:2402.16796</a> [<a href="/pdf/2402.16796" title="Download PDF">pdf</a>, <a href="/format/2402.16796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressive Whole-Body Control for Humanoid Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xuxin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yandong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Ge Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://expressive-humanoid.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Can we enable humanoid robots to generate rich, diverse, and expressive
motions in the real world? We propose to learn a whole-body control policy on a
human-sized robot to mimic human motions as realistic as possible. To train
such a policy, we leverage the large-scale human motion capture data from the
graphics community in a Reinforcement Learning framework. However, directly
performing imitation learning with the motion capture dataset would not work on
the real humanoid robot, given the large gap in degrees of freedom and physical
capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this
problem by encouraging the upper humanoid body to imitate a reference motion,
while relaxing the imitation constraint on its two legs and only requiring them
to follow a given velocity robustly. With training in simulation and Sim2Real
transfer, our policy can control a humanoid robot to walk in different styles,
shake hands with humans, and even dance with a human in the real world. We
conduct extensive studies and comparisons on diverse motions in both simulation
and the real world to show the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16797" title="Abstract">arXiv:2402.16797</a> [<a href="/pdf/2402.16797" title="Download PDF">pdf</a>, <a href="/format/2402.16797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Set the Clock: Temporal Alignment of Pretrained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bowen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Brumbaugh%2C+Z">Zander Brumbaugh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 figures. Our code and data will be available at <a href="https://github.com/yizhongw/llm-temporal-alignment">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models (LMs) are trained on web text originating from many points in
time and, in general, without any explicit temporal grounding. This work
investigates the temporal chaos of pretrained LMs and explores various methods
to align their internal knowledge to a target time, which we call "temporal
alignment." To do this, we first automatically construct a dataset containing
20K time-sensitive questions and their answers for each year from 2000 to 2023.
Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),
despite having a recent pretraining cutoff (e.g., 2022), mostly answer
questions using earlier knowledge (e.g., in 2019). We then develop several
methods, from prompting to finetuning, to align LMs to use their most recent
knowledge when answering questions, and investigate various factors in this
alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost
its performance by up to 62% relatively as measured by that year, even without
mentioning time information explicitly, indicating the possibility of aligning
models' internal sense of time after pretraining. Finally, we find that
alignment to a historical time is also possible, with up to 2.8$\times$ the
performance of the unaligned LM in 2010 if finetuning models to that year.
These findings hint at the sophistication of LMs' internal knowledge
organization and the necessity of tuning them properly.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16799" title="Abstract">arXiv:2402.16799</a> [<a href="/pdf/2402.16799" title="Download PDF">pdf</a>, <a href="/format/2402.16799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element schemes with tangential motion for fourth order geometric  curve evolutions in arbitrary codimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Deckelnick%2C+K">Klaus Deckelnick</a>, 
<a href="/search/math?searchtype=author&query=N%C3%BCrnberg%2C+R">Robert N&#xfc;rnberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We introduce novel finite element schemes for curve diffusion and elastic
flow in arbitrary codimension. The schemes are based on a variational form of a
system that includes a specifically chosen tangential motion. We derive optimal
$L^2$- and $H^1$-error bounds for continuous-in-time semidiscrete finite
element approximations that use piecewise linear elements. In addition, we
consider fully discrete schemes and, in the case of curve diffusion, prove
unconditional stability for it. Finally, we present several numerical
simulations, including some convergence experiments that confirm the derived
error bounds. The presented simulations suggest that the tangential motion
leads to equidistribution in practice.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16801" title="Abstract">arXiv:2402.16801</a> [<a href="/pdf/2402.16801" title="Download PDF">pdf</a>, <a href="/format/2402.16801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matthews%2C+M">Michael Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Beukman%2C+M">Michael Beukman</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+B">Benjamin Ellis</a>, 
<a href="/search/cs?searchtype=author&query=Samvelyan%2C+M">Mikayel Samvelyan</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+M">Matthew Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Coward%2C+S">Samuel Coward</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms. We identify that existing benchmarks
used for research into open-ended learning fall into one of two categories.
Either they are too slow for meaningful research to be performed without
enormous computational resources, like Crafter, NetHack and Minecraft, or they
are not complex enough to pose a significant challenge, like Minigrid and
Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite
of Crafter in JAX that runs up to 250x faster than the Python-native original.
A run of PPO using 1 billion environment interactions finishes in under an hour
using only a single GPU and averages 90% of the optimal reward. To provide a
more compelling challenge we present the main Craftax benchmark, a significant
extension of the Crafter mechanics with elements inspired from NetHack. Solving
Craftax requires deep exploration, long term planning and memory, as well as
continual adaptation to novel situations as more of the world is discovered. We
show that existing methods including global and episodic exploration, as well
as unsupervised environment design fail to make material progress on the
benchmark. We believe that Craftax can for the first time allow researchers to
experiment in a complex, open-ended environment with limited computational
resources.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16803" title="Abstract">arXiv:2402.16803</a> [<a href="/pdf/2402.16803" title="Download PDF">pdf</a>, <a href="/format/2402.16803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A stochastic perturbation approach to nonlinear bifurcating problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gonnella%2C+I+C">Isabella Carla Gonnella</a>, 
<a href="/search/math?searchtype=author&query=Khamlich%2C+M">Moaad Khamlich</a>, 
<a href="/search/math?searchtype=author&query=Pichi%2C+F">Federico Pichi</a>, 
<a href="/search/math?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
<p class="mathjax">Incorporating probabilistic terms in mathematical models is crucial for
capturing and quantifying uncertainties in real-world systems. Indeed,
randomness can have a significant impact on the behavior of the problem's
solution, and a deeper analysis is needed to obtain more realistic and
informative results. On the other hand, the investigation of stochastic models
may require great computational resources due to the importance of generating
numerous realizations of the system to have meaningful statistics. This makes
the development of complexity reduction techniques, such as surrogate models,
essential for enabling efficient and scalable simulations. In this work, we
exploit polynomial chaos (PC) expansion to study the accuracy of surrogate
representations for a bifurcating phenomena in fluid dynamics, namely the
Coanda effect, where the stochastic setting gives a different perspective on
the non-uniqueness of the solution. Then, its inclusion in the finite element
setting is described, arriving to the formulation of the enhanced Spectral
Stochastic Finite Element Method (SSFEM). Moreover, we investigate the
connections between the deterministic bifurcation diagram and the PC
polynomials, underlying their capability in reconstructing the whole solution
manifold.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16806" title="Abstract">arXiv:2402.16806</a> [<a href="/pdf/2402.16806" title="Download PDF">pdf</a>, <a href="/format/2402.16806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Human Mesh Recovery with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zhenzhen Weng</a>, 
<a href="/search/cs?searchtype=author&query=Yeung-Levy%2C+S">Serena Yeung-Levy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conventional approaches to human mesh recovery predominantly employ a
region-based strategy. This involves initially cropping out a human-centered
region as a preprocessing step, with subsequent modeling focused on this
zoomed-in image. While effective for single figures, this pipeline poses
challenges when dealing with images featuring multiple individuals, as
different people are processed separately, often leading to inaccuracies in
relative positioning. Despite the advantages of adopting a whole-image-based
approach to address this limitation, early efforts in this direction have
fallen short in performance compared to recent region-based methods. In this
work, we advocate for this under-explored area of modeling all people at once,
emphasizing its potential for improved accuracy in multi-person scenarios
through considering all individuals simultaneously and leveraging the overall
context and interactions. We introduce a new model with a streamlined
transformer-based design, featuring three critical design choices: multi-scale
feature incorporation, focused attention mechanisms, and relative joint
supervision. Our proposed model demonstrates a significant performance
improvement, surpassing state-of-the-art region-based and whole-image-based
methods on various benchmarks involving multiple individuals.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16810" title="Abstract">arXiv:2402.16810</a> [<a href="/pdf/2402.16810" title="Download PDF">pdf</a>, <a href="/ps/2402.16810" title="Download PostScript">ps</a>, <a href="/format/2402.16810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OncoGPT: A Medical Conversational Model Tailored with Oncology Domain  Expertise on a Large Language Model Meta-AI (LLaMA)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Fujian Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Lixi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiwen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+C">Chunchao Pu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+T">Tunan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mengjiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuanzhi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the past year, there has been a growing trend in applying Large Language
Models (LLMs) to the field of medicine, particularly with the advent of
advanced language models such as ChatGPT developed by OpenAI. However, there is
limited research on LLMs specifically addressing oncology-related queries. The
primary aim of this research was to develop a specialized language model that
demonstrates improved accuracy in providing advice related to oncology. We
performed an extensive data collection of online question-answer interactions
centered around oncology, sourced from reputable doctor-patient platforms.
Following data cleaning and anonymization, a dataset comprising over 180K+
oncology-related conversations was established. The conversations were
categorized and meticulously reviewed by field specialists and clinicians to
ensure precision. Employing the LLaMA model and other selected open-source
datasets, we conducted iterative fine-tuning to enhance the model's proficiency
in basic medical conversation and specialized oncology knowledge. We observed a
substantial enhancement in the model's understanding of genuine patient
inquiries and its reliability in offering oncology-related advice through the
utilization of real online question-answer interactions in the fine-tuning
process. We release database and models to the research community
(https://github.com/OncoGPT1).
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16814" title="Abstract">arXiv:2402.16814</a> [<a href="/pdf/2402.16814" title="Download PDF">pdf</a>, <a href="/format/2402.16814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cut Facets and Cube Facets of Lifted Multicut Polytopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naumann%2C+L+F">Lucas Fabian Naumann</a>, 
<a href="/search/cs?searchtype=author&query=Irmai%2C+J">Jannik Irmai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shengxian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Andres%2C+B">Bjoern Andres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The lifted multicut problem has diverse applications in the field of computer
vision. Exact algorithms based on linear programming require an understanding
of lifted multicut polytopes. Despite recent progress, two fundamental
questions about these polytopes have remained open: Which lower cube
inequalities define facets, and which cut inequalities define facets? In this
article, we answer the first question by establishing conditions that are
necessary, sufficient and efficiently decidable. Toward the second question, we
show that deciding facet-definingness of cut inequalities is NP-hard. This
completes the analysis of canonical facets of lifted multicut polytopes.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16815" title="Abstract">arXiv:2402.16815</a> [<a href="/pdf/2402.16815" title="Download PDF">pdf</a>, <a href="/format/2402.16815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2+2D Texture for Full Positive Parallax Effect
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dias%2C+A+Y+G">Alexandre Yip Gon&#xe7;alves Dias</a>, 
<a href="/search/cs?searchtype=author&query=Zuffo%2C+M+K">Marcelo Kn&#xf6;rich Zuffo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">The representation of parallax on virtual environment is still a problem to
be studied. Common algorithms, such as Bump Mapping, Parallax Mapping and
Displacement Mapping, treats this problem for small disparity between a real
object and a simplified model. This work will introduce a new texture structure
and one possible render algorithm able to display parallax for large
disparities, it is an approach based on the four-dimensional representation of
the Light Field and was thought to positive parallax and to display the
surfaces on the inside of our simplified model. These conditions are imposed to
allow the free movement of an observer, if its movement is restrict, these
conditions may be loosen. It is a high storage low process approach possible to
be used in real time systems. As an example we will develop a scene with
several objects and simplified them by a unique sphere that encloses them all,
our system was able to run this scene with about 180fps.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16817" title="Abstract">arXiv:2402.16817</a> [<a href="/pdf/2402.16817" title="Download PDF">pdf</a>, <a href="/format/2402.16817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Effectiveness of HyperTuning via Gisting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phang%2C+J">Jason Phang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Gisting (Mu et al., 2023) is a simple method for training models to compress
information into fewer token representations using a modified attention mask,
and can serve as an economical approach to training Transformer-based
hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks
built on Llama-2 models that generates task-specific soft prefixes based on
few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol
Tuning datasets, we show that HyperLlama models can effectively compress
information from few-shot examples into soft prefixes. However, they still
underperform multi-task fine-tuned language models with full attention over
few-shot in-context examples. We also show that HyperLlama-generated soft
prefixes can serve as better initializations for further prefix tuning.
Overall, Gisting-based hypernetworks are economical and easy to implement, but
have mixed empirical performance.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16819" title="Abstract">arXiv:2402.16819</a> [<a href="/pdf/2402.16819" title="Download PDF">pdf</a>, <a href="/format/2402.16819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nemotron-4 15B Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parmar%2C+J">Jupinder Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Prabhumoye%2C+S">Shrimai Prabhumoye</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+J">Joseph Jennings</a>, 
<a href="/search/cs?searchtype=author&query=Patwary%2C+M">Mostofa Patwary</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Sandeep Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+D">Deepak Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Jhunjhunwala%2C+A">Aastha Jhunjhunwala</a>, 
<a href="/search/cs?searchtype=author&query=Dattagupta%2C+A">Ayush Dattagupta</a>, 
<a href="/search/cs?searchtype=author&query=Jawa%2C+V">Vibhu Jawa</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mahabaleshwarkar%2C+A">Ameya Mahabaleshwarkar</a>, 
<a href="/search/cs?searchtype=author&query=Nitski%2C+O">Osvald Nitski</a>, 
<a href="/search/cs?searchtype=author&query=Brundyn%2C+A">Annika Brundyn</a>, 
<a href="/search/cs?searchtype=author&query=Maki%2C+J">James Maki</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Miguel Martinez</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiaxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Kamalu%2C+J">John Kamalu</a>, 
<a href="/search/cs?searchtype=author&query=LeGresley%2C+P">Patrick LeGresley</a>, 
<a href="/search/cs?searchtype=author&query=Fridman%2C+D">Denys Fridman</a>, 
<a href="/search/cs?searchtype=author&query=Casper%2C+J">Jared Casper</a>, 
<a href="/search/cs?searchtype=author&query=Aithal%2C+A">Ashwath Aithal</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J">Jonathan Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual
language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates
strong performance when assessed on English, multilingual, and coding tasks: it
outperforms all existing similarly-sized open models on 4 out of 7 downstream
evaluation areas and achieves competitive performance to the leading open
models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best
multilingual capabilities of all similarly-sized models, even outperforming
models over four times larger and those explicitly specialized for multilingual
tasks.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16821" title="Abstract">arXiv:2402.16821</a> [<a href="/pdf/2402.16821" title="Download PDF">pdf</a>, <a href="/format/2402.16821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Analysis on Neural Network Projected Schemes for Approximating  One Dimensional Wasserstein Gradient Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zuo%2C+X">Xinzhe Zuo</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+J">Jiaxi Zhao</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/math?searchtype=author&query=Osher%2C+S">Stanley Osher</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wuchen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We provide a numerical analysis and computation of neural network projected
schemes for approximating one dimensional Wasserstein gradient flows. We
approximate the Lagrangian mapping functions of gradient flows by the class of
two-layer neural network functions with ReLU (rectified linear unit) activation
functions. The numerical scheme is based on a projected gradient method, namely
the Wasserstein natural gradient, where the projection is constructed from the
$L^2$ mapping spaces onto the neural network parameterized mapping space. We
establish theoretical guarantees for the performance of the neural projected
dynamics. We derive a closed-form update for the scheme with well-posedness and
explicit consistency guarantee for a particular choice of network structure.
General truncation error analysis is also established on the basis of the
projective nature of the dynamics. Numerical examples, including gradient drift
Fokker-Planck equations, porous medium equations, and Keller-Segel models,
verify the accuracy and effectiveness of the proposed neural projected
algorithm.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16822" title="Abstract">arXiv:2402.16822</a> [<a href="/pdf/2402.16822" title="Download PDF">pdf</a>, <a href="/format/2402.16822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samvelyan%2C+M">Mikayel Samvelyan</a>, 
<a href="/search/cs?searchtype=author&query=Raparthy%2C+S+C">Sharath Chandra Raparthy</a>, 
<a href="/search/cs?searchtype=author&query=Lupu%2C+A">Andrei Lupu</a>, 
<a href="/search/cs?searchtype=author&query=Hambro%2C+E">Eric Hambro</a>, 
<a href="/search/cs?searchtype=author&query=Markosyan%2C+A+H">Aram H. Markosyan</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+M">Manish Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuning Mao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Raileanu%2C+R">Roberta Raileanu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">As large language models (LLMs) become increasingly prevalent across many
real-world applications, understanding and enhancing their robustness to user
inputs is of paramount importance. Existing methods for identifying adversarial
prompts tend to focus on specific domains, lack diversity, or require extensive
human annotations. To address these limitations, we present Rainbow Teaming, a
novel approach for producing a diverse collection of adversarial prompts.
Rainbow Teaming casts adversarial prompt generation as a quality-diversity
problem, and uses open-ended search to generate prompts that are both effective
and diverse. It can uncover a model's vulnerabilities across a broad range of
domains including, in this paper, safety, question answering, and
cybersecurity. We also demonstrate that fine-tuning on synthetic data generated
by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting
their general capabilities and helpfulness, paving the path to open-ended
self-improvement.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16823" title="Abstract">arXiv:2402.16823</a> [<a href="/pdf/2402.16823" title="Download PDF">pdf</a>, <a href="/format/2402.16823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents as Optimizable Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuge%2C+M">Mingchen Zhuge</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kirsch%2C+L">Louis Kirsch</a>, 
<a href="/search/cs?searchtype=author&query=Faccio%2C+F">Francesco Faccio</a>, 
<a href="/search/cs?searchtype=author&query=Khizbullin%2C+D">Dmitrii Khizbullin</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">Jurgen Schmidhuber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://gptswarm.org">this https URL</a> ; Github Repo: <a href="https://github.com/metauto-ai/gptswarm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Various human-designed prompt engineering techniques have been proposed to
improve problem solvers based on Large Language Models (LLMs), yielding many
disparate code bases. We unify these approaches by describing LLM-based agents
as computational graphs. The nodes implement functions to process multimodal
data or query LLMs, and the edges describe the information flow between
operations. Graphs can be recursively combined into larger composite graphs
representing hierarchies of inter-agent collaboration (where edges connect
operations of different agents). Our novel automatic graph optimizers (1)
refine node-level LLM prompts (node optimization) and (2) improve agent
orchestration by changing graph connectivity (edge optimization). Experiments
demonstrate that our framework can be used to efficiently develop, integrate,
and automatically improve various LLM agents. The code can be found at
https://github.com/metauto-ai/gptswarm.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16825" title="Abstract">arXiv:2402.16825</a> [<a href="/pdf/2402.16825" title="Download PDF">pdf</a>, <a href="/ps/2402.16825" title="Download PostScript">ps</a>, <a href="/format/2402.16825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional  layers for 3D abdominal organ segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenzhao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Albert%2C+S">Steffen Albert</a>, 
<a href="/search/cs?searchtype=author&query=Wichtmann%2C+B+D">Barbara D. Wichtmann</a>, 
<a href="/search/cs?searchtype=author&query=Maurer%2C+A">Angelika Maurer</a>, 
<a href="/search/cs?searchtype=author&query=Attenberger%2C+U">Ulrike Attenberger</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+F+G">Frank G. Z&#xf6;llner</a>, 
<a href="/search/cs?searchtype=author&query=Hesser%2C+J">J&#xfc;rgen Hesser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Filter-decomposition-based group equivariant convolutional neural networks
show promising stability and data efficiency for 3D image feature extraction.
However, the existing filter-decomposition-based 3D group equivariant neural
networks rely on parameter-sharing designs and are mostly limited to rotation
transform groups, where the chosen spherical harmonic filter bases consider
only angular orthogonality. These limitations hamper its application to deep
neural network architectures for medical image segmentation. To address these
issues, this paper describes a non-parameter-sharing affine group equivariant
neural network for 3D medical image segmentation based on an adaptive
aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The
efficiency and flexibility of the adopted non-parameter strategy enable for the
first time an efficient implementation of 3D affine group equivariant
convolutional neural networks for volumetric data. The introduced spherical
Bessel Fourier filter basis combines both angular and radial orthogonality for
better feature extraction. The 3D image segmentation experiments on two
abdominal image sets, BTCV and the NIH Pancreas datasets, show that the
proposed methods excel the state-of-the-art 3D neural networks with high
training stability and data efficiency. The code will be available at
https://github.com/ZhaoWenzhao/WVMS.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16827" title="Abstract">arXiv:2402.16827</a> [<a href="/pdf/2402.16827" title="Download PDF">pdf</a>, <a href="/format/2402.16827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Data Selection for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albalak%2C+A">Alon Albalak</a>, 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Bairu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Haewon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shiyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A major factor in the recent success of large language models is the use of
enormous and ever-growing text datasets for unsupervised pre-training. However,
naively training a model on all available data may not be optimal (or
feasible), as the quality of available text data can vary. Filtering out data
can also decrease the carbon footprint and financial costs of training models
by reducing the amount of training required.
<br />Data selection methods aim to determine which candidate data points to
include in the training dataset and how to appropriately sample from the
selected data points. The promise of improved data selection methods has caused
the volume of research in the area to rapidly expand. However, because deep
learning is mostly driven by empirical evidence and experimentation on
large-scale data is expensive, few organizations have the resources for
extensive data selection research. Consequently, knowledge of effective data
selection practices has become concentrated within a few organizations, many of
which do not openly share their findings and methodologies.
<br />To narrow this gap in knowledge, we present a comprehensive review of
existing literature on data selection methods and related research areas,
providing a taxonomy of existing approaches. By describing the current
landscape of research, this work aims to accelerate progress in data selection
by establishing an entry point for new and established researchers.
Additionally, throughout this review we draw attention to noticeable holes in
the literature and conclude the paper by proposing promising avenues for future
research.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16828" title="Abstract">arXiv:2402.16828</a> [<a href="/pdf/2402.16828" title="Download PDF">pdf</a>, <a href="/format/2402.16828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Neural Networks from Scratch with Parallel Low-Rank Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huh%2C+M">Minyoung Huh</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+B">Brian Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+J">Jeremy Bernstein</a>, 
<a href="/search/cs?searchtype=author&query=Isola%2C+P">Phillip Isola</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The scalability of deep learning models is fundamentally limited by computing
resources, memory, and communication. Although methods like low-rank adaptation
(LoRA) have reduced the cost of model finetuning, its application in model
pre-training remains largely unexplored. This paper explores extending LoRA to
model pre-training, identifying the inherent constraints and limitations of
standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel
bi-level optimization algorithm designed to enable parallel training of
multiple low-rank heads across computing nodes, thereby reducing the need for
frequent synchronization. Our approach includes extensive experimentation on
vision transformers using various vision datasets, demonstrating that LTE is
competitive with standard pre-training.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16829" title="Abstract">arXiv:2402.16829</a> [<a href="/pdf/2402.16829" title="Download PDF">pdf</a>, <a href="/format/2402.16829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GISTEmbed: Guided In-sample Selection of Training Negatives for Text  Embedding Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Solatorio%2C+A+V">Aivin V. Solatorio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GISTEmbed GitHub repository at <a href="https://github.com/avsolatorio/GISTEmbed">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16832" title="Abstract">arXiv:2402.16832</a> [<a href="/pdf/2402.16832" title="Download PDF">pdf</a>, <a href="/format/2402.16832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual  Capabilities Without Richer Cross-Modal Projections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gaurav Verma</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minje Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Kartik Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Watson-Daniels%2C+J">Jamelle Watson-Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sejoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable
general-purpose conversations about images with the language modality. As
off-the-shelf MLLMs may have limited capabilities on images from domains like
dermatology and agriculture, they must be fine-tuned to unlock domain-specific
applications. The prevalent architecture of current open-source MLLMs comprises
two major modules: an image-language (cross-modal) projection network and a
large language model. It is desirable to understand the roles of these two
modules in modeling domain-specific visual attributes to inform the design of
future models and streamline the interpretability efforts on the current
models. To this end, via experiments on 4 datasets and under 2 fine-tuning
settings, we find that as the MLLM is fine-tuned, it indeed gains
domain-specific visual capabilities, but the updates do not lead to the
projection extracting relevant domain-specific visual attributes. Our results
indicate that the domain-specific visual attributes are modeled by the LLM,
even when only the projection is fine-tuned. Through this study, we offer a
potential reinterpretation of the role of cross-modal projections in MLLM
architectures. Projection webpage:
https://claws-lab.github.io/projection-in-MLLMs/
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16835" title="Abstract">arXiv:2402.16835</a> [<a href="/pdf/2402.16835" title="Download PDF">pdf</a>, <a href="/format/2402.16835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eight Methods to Evaluate Robust Unlearning in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lynch%2C+A">Aengus Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Phillip Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ewart%2C+A">Aidan Ewart</a>, 
<a href="/search/cs?searchtype=author&query=Casper%2C+S">Stephen Casper</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield-Menell%2C+D">Dylan Hadfield-Menell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Machine unlearning can be useful for removing harmful capabilities and
memorized text from large language models (LLMs), but there are not yet
standardized methods for rigorously evaluating it. In this paper, we first
survey techniques and limitations of existing unlearning evaluations. Second,
we apply a comprehensive set of tests for the robustness and competitiveness of
unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich
(2023). While WHP's unlearning generalizes well when evaluated with the
"Familiarity" metric from Eldan and Russinovich, we find i)
higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP
performs on par with the original model on Harry Potter Q&amp;A tasks, iii) it
represents latent knowledge comparably to the original model, and iv) there is
collateral unlearning in related domains. Overall, our results highlight the
importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16836" title="Abstract">arXiv:2402.16836</a> [<a href="/pdf/2402.16836" title="Download PDF">pdf</a>, <a href="/format/2402.16836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large  Multimodal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dingkun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yuqi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Robotic grasping is a fundamental aspect of robot functionality, defining how
robots interact with objects. Despite substantial progress, its
generalizability to counter-intuitive or long-tailed scenarios, such as objects
with uncommon materials or shapes, remains a challenge. In contrast, humans can
easily apply their intuitive physics to grasp skillfully and change grasps
efficiently, even for objects they have never seen before.
<br />This work delves into infusing such physical commonsense reasoning into
robotic manipulation. We introduce PhyGrasp, a multimodal large model that
leverages inputs from two modalities: natural language and 3D point clouds,
seamlessly integrated through a bridge module. The language modality exhibits
robust reasoning capabilities concerning the impacts of diverse physical
properties on grasping, while the 3D modality comprehends object shapes and
parts. With these two capabilities, PhyGrasp is able to accurately assess the
physical properties of object parts and determine optimal grasping poses.
Additionally, the model's language comprehension enables human instruction
interpretation, generating grasping poses that align with human preferences. To
train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances
with varying physical properties and human preferences, alongside their
corresponding language descriptions. Extensive experiments conducted in the
simulation and on the real robots demonstrate that PhyGrasp achieves
state-of-the-art performance, particularly in long-tailed cases, e.g., about
10% improvement in success rate over GraspNet. Project page:
https://sites.google.com/view/phygrasp
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16837" title="Abstract">arXiv:2402.16837</a> [<a href="/pdf/2402.16837" title="Download PDF">pdf</a>, <a href="/format/2402.16837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Large Language Models Latently Perform Multi-Hop Reasoning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sohee Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gribovskaya%2C+E">Elena Gribovskaya</a>, 
<a href="/search/cs?searchtype=author&query=Kassner%2C+N">Nora Kassner</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Riedel%2C+S">Sebastian Riedel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We study whether Large Language Models (LLMs) latently perform multi-hop
reasoning with complex prompts such as "The mother of the singer of
'Superstition' is". We look for evidence of a latent reasoning pathway where an
LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder,
the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to
complete the prompt. We analyze these two hops individually and consider their
co-occurrence as indicative of latent multi-hop reasoning. For the first hop,
we test if changing the prompt to indirectly mention the bridge entity instead
of any other entity increases the LLM's internal recall of the bridge entity.
For the second hop, we test if increasing this recall causes the LLM to better
utilize what it knows about the bridge entity. We find strong evidence of
latent multi-hop reasoning for the prompts of certain relation types, with the
reasoning pathway used in more than 80% of the prompts. However, the
utilization is highly contextual, varying across different types of prompts.
Also, on average, the evidence for the second hop and the full multi-hop
traversal is rather moderate and only substantial for the first hop. Moreover,
we find a clear scaling trend with increasing model size for the first hop of
reasoning but not for the second hop. Our experimental findings suggest
potential challenges and opportunities for future development and applications
of LLMs.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16839" title="Abstract">arXiv:2402.16839</a> [<a href="/pdf/2402.16839" title="Download PDF">pdf</a>, <a href="/ps/2402.16839" title="Download PostScript">ps</a>, <a href="/format/2402.16839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can recombination growth lead to scientific breakthroughs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linzhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yilin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Since the 1990s, recombinant growth theory has fascinated academic circles by
proposing that new ideas flourish through reconfiguring existing ones, leading
to accelerated innovation in science and technology. However, after three
decades, a marked decline in scientific breakthroughs challenges this theory.
We explore its potential limitations, suggesting that while it emphasizes
complementarity among ideas, it overlooks the competitive dynamics between them
and how this rivalry fosters major breakthroughs. Examining 20 scientific
breakthroughs nominated by surveyed scientists, we reveal a recurring pattern
where new ideas are intentionally crafted to challenge and replace established
ones. Analyzing 19 million papers spanning a century, we consistently observe a
negative correlation between reference atypicality, which reflects the effort
to recombine more ideas, and paper disruption, indicating the extent to which
this work represents major breakthroughs, across all fields, periods, and team
sizes. Moreover, our analysis of a novel dataset, comparing early and
subsequent versions of 2,461 papers, offers quasi-experimental evidence
suggesting that additional efforts to increase reference atypicality indeed
result in a reduction of disruption for the same paper. In summary, our
analyses challenge recombinant growth theory, suggesting that scientific
breakthroughs originate from a clear purpose to replace established, impactful
ideas.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16840" title="Abstract">arXiv:2402.16840</a> [<a href="/pdf/2402.16840" title="Download PDF">pdf</a>, <a href="/format/2402.16840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thawakar%2C+O">Omkar Thawakar</a>, 
<a href="/search/cs?searchtype=author&query=Vayani%2C+A">Ashmal Vayani</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Cholakal%2C+H">Hisham Cholakal</a>, 
<a href="/search/cs?searchtype=author&query=Anwer%2C+R+M">Rao M. Anwer</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Tim Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric P. Xing</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at : <a href="https://github.com/mbzuai-oryx/MobiLlama">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">"Bigger the better" has been the predominant trend in recent Large Language
Models (LLMs) development. However, LLMs do not suit well for scenarios that
require on-device processing, energy efficiency, low memory footprint, and
response efficiency. These requisites are crucial for privacy, security, and
sustainable deployment. This paper explores the "less is more" paradigm by
addressing the challenge of designing accurate yet efficient Small Language
Models (SLMs) for resource constrained devices. Our primary contribution is the
introduction of an accurate and fully transparent open-source 0.5 billion
(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of
resource-constrained computing with an emphasis on enhanced performance with
reduced resource demands. MobiLlama is a SLM design that initiates from a
larger model and applies a careful parameter sharing scheme to reduce both the
pre-training and the deployment cost. Our work strives to not only bridge the
gap in open-source SLMs but also ensures full transparency, where complete
training data pipeline, training code, model weights, and over 300 checkpoints
along with evaluation codes is available at :
https://github.com/mbzuai-oryx/MobiLlama.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16842" title="Abstract">arXiv:2402.16842</a> [<a href="/pdf/2402.16842" title="Download PDF">pdf</a>, <a href="/format/2402.16842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetry in Low-Rank Adapters of Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Greenewald%2C+K">Kristjan Greenewald</a>, 
<a href="/search/cs?searchtype=author&query=Nadjahi%2C+K">Kimia Nadjahi</a>, 
<a href="/search/cs?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Gabrielsson%2C+R+B">Rickard Br&#xfc;el Gabrielsson</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Parameter-efficient fine-tuning optimizes large, pre-trained foundation
models by updating a subset of parameters; in this class, Low-Rank Adaptation
(LoRA) is particularly effective. Inspired by an effort to investigate the
different roles of LoRA matrices during fine-tuning, this paper characterizes
and leverages unexpected asymmetry in the importance of low-rank adapter
matrices. Specifically, when updating the parameter matrices of a neural
network by adding a product $BA$, we observe that the $B$ and $A$ matrices have
distinct functions: $A$ extracts features from the input, while $B$ uses these
features to create the desired output. Based on this observation, we
demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning
$A$, and that a random untrained $A$ should perform nearly as well as a
fine-tuned one. Using an information-theoretic lens, we also bound the
generalization of low-rank adapters, showing that the parameter savings of
exclusively training $B$ improves the bound. We support our conclusions with
experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16843" title="Abstract">arXiv:2402.16843</a> [<a href="/pdf/2402.16843" title="Download PDF">pdf</a>, <a href="/format/2402.16843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-LoRA Composition for Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yadong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yizhu Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siru Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Donghan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://maszhongming.github.io/Multi-LoRA-Composition/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models
for the accurate rendition of specific elements like distinct characters or
unique styles in generated images. Nonetheless, existing methods face
challenges in effectively composing multiple LoRAs, especially as the number of
LoRAs to be integrated grows, thus hindering the creation of complex imagery.
In this paper, we study multi-LoRA composition through a decoding-centric
perspective. We present two training-free methods: LoRA Switch, which
alternates between different LoRAs at each denoising step, and LoRA Composite,
which simultaneously incorporates all LoRAs to guide more cohesive image
synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new
comprehensive testbed as part of this research. It features a diverse range of
LoRA categories with 480 composition sets. Utilizing an evaluation framework
based on GPT-4V, our findings demonstrate a clear improvement in performance
with our methods over the prevalent baseline, particularly evident when
increasing the number of LoRAs in a composition.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16844" title="Abstract">arXiv:2402.16844</a> [<a href="/pdf/2402.16844" title="Download PDF">pdf</a>, <a href="/format/2402.16844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bergner%2C+B">Benjamin Bergner</a>, 
<a href="/search/cs?searchtype=author&query=Skliar%2C+A">Andrii Skliar</a>, 
<a href="/search/cs?searchtype=author&query=Royer%2C+A">Amelie Royer</a>, 
<a href="/search/cs?searchtype=author&query=Blankevoort%2C+T">Tijmen Blankevoort</a>, 
<a href="/search/cs?searchtype=author&query=Asano%2C+Y">Yuki Asano</a>, 
<a href="/search/cs?searchtype=author&query=Bejnordi%2C+B+E">Babak Ehteshami Bejnordi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have become ubiquitous in practice and are
widely used for generation tasks such as translation, summarization and
instruction following. However, their enormous size and reliance on
autoregressive decoding increase deployment costs and complicate their use in
latency-critical applications. In this work, we propose a hybrid approach that
combines language models of different sizes to increase the efficiency of
autoregressive decoding while maintaining high performance. Our method utilizes
a pretrained frozen LLM that encodes all prompt tokens once in parallel, and
uses the resulting representations to condition and guide a small language
model (SLM), which then generates the response more efficiently. We investigate
the combination of encoder-decoder LLMs with both encoder-decoder and
decoder-only SLMs from different model families and only require fine-tuning of
the SLM. Experiments with various benchmarks show substantial speedups of up to
$4\times$, with minor performance penalties of $1-2\%$ for translation and
summarization tasks compared to the LLM.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16845" title="Abstract">arXiv:2402.16845</a> [<a href="/pdf/2402.16845" title="Download PDF">pdf</a>, <a href="/format/2402.16845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Operators with Localized Integral and Differential Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu-Schiaffini%2C+M">Miguel Liu-Schiaffini</a>, 
<a href="/search/cs?searchtype=author&query=Berner%2C+J">Julius Berner</a>, 
<a href="/search/cs?searchtype=author&query=Bonev%2C+B">Boris Bonev</a>, 
<a href="/search/cs?searchtype=author&query=Kurth%2C+T">Thorsten Kurth</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Neural operators learn mappings between function spaces, which is practical
for learning solution operators of PDEs and other scientific modeling
applications. Among them, the Fourier neural operator (FNO) is a popular
architecture that performs global convolutions in the Fourier space. However,
such global operations are often prone to over-smoothing and may fail to
capture local details. In contrast, convolutional neural networks (CNN) can
capture local features but are limited to training and inference at a single
resolution. In this work, we present a principled approach to operator learning
that can capture local features under two frameworks by learning differential
operators and integral operators with locally supported kernels. Specifically,
inspired by stencil methods, we prove that we obtain differential operators
under an appropriate scaling of the kernel values of CNNs. To obtain local
integral operators, we utilize suitable basis representations for the kernels
based on discrete-continuous convolutions. Both these approaches preserve the
properties of operator learning and, hence, the ability to predict at any
resolution. Adding our layers to FNOs significantly improves their performance,
reducing the relative L2-error by 34-72% in our experiments on turbulent 2D
Navier-Stokes fluid flow and the spherical shallow water equations.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16846" title="Abstract">arXiv:2402.16846</a> [<a href="/pdf/2402.16846" title="Download PDF">pdf</a>, <a href="/format/2402.16846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GROUNDHOG: Grounding Large Language Models to Holistic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiaofeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shakiah%2C+S">Suhaila Shakiah</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qiaozi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://groundhog-mllm.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Most multimodal large language models (MLLMs) learn language-to-object
grounding through causal language modeling where grounded objects are captured
by bounding boxes as sequences of location tokens. This paradigm lacks
pixel-level representations that are important for fine-grained visual
understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM
developed by grounding Large Language Models to holistic segmentation.
GROUNDHOG incorporates a masked feature extractor and converts extracted
features into visual entity tokens for the MLLM backbone, which then connects
groundable phrases to unified grounding masks by retrieving and merging the
entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual
instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by
harvesting a collection of segmentation-grounded datasets with rich
annotations. Our experimental results show that GROUNDHOG achieves superior
performance on various language grounding tasks without task-specific
fine-tuning, and significantly reduces object hallucination. GROUNDHOG also
demonstrates better grounding towards complex forms of visual input and
provides easy-to-understand diagnosis in failure cases.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16847" title="Abstract">arXiv:2402.16847</a> [<a href="/pdf/2402.16847" title="Download PDF">pdf</a>, <a href="/format/2402.16847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Art of Staying Ahead of Deadlines: Improved Algorithms for the  Minimum Tardy Processing Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoian%2C+M">Mihail Stoian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study the fundamental scheduling problem $1\|\sum p_jU_j$. Given a set of
$n$ jobs with processing times $p_j$ and deadlines $d_j$, the problem is to
select a subset of jobs such that the total processing time is maximized
without violating the deadlines. In the midst of a flourishing line of
research, Fischer and Wennmann have recently devised the sought-after
$\widetilde O(P)$-time algorithm, where $P = \sum p_j$ is the total processing
time of all jobs. This running time is optimal as it matches conditional lower
bounds based on popular conjectures.
<br />However, $P$ is not the sole parameter one could parameterize the running
time by. Indeed, they explicitly leave open the question of whether a running
time of $\widetilde O(n + \max d_j)$ or even $\widetilde O(n + \max p_j)$ is
possible. In this work, we show, somewhat surprisingly, that by a refined
implementation of their original algorithm, one can obtain the asked-for
$\widetilde O(n + \max d_j)$-time algorithm.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16848" title="Abstract">arXiv:2402.16848</a> [<a href="/pdf/2402.16848" title="Download PDF">pdf</a>, <a href="/format/2402.16848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterroGate: Learning to Share, Specialize, and Prune Representations  for Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bejnordi%2C+B+E">Babak Ehteshami Bejnordi</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+G">Gaurav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Royer%2C+A">Amelie Royer</a>, 
<a href="/search/cs?searchtype=author&query=Louizos%2C+C">Christos Louizos</a>, 
<a href="/search/cs?searchtype=author&query=Blankevoort%2C+T">Tijmen Blankevoort</a>, 
<a href="/search/cs?searchtype=author&query=Ghafoorian%2C+M">Mohsen Ghafoorian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Jointly learning multiple tasks with a unified model can improve accuracy and
data efficiency, but it faces the challenge of task interference, where
optimizing one task objective may inadvertently compromise the performance of
another. A solution to mitigate this issue is to allocate task-specific
parameters, free from interference, on top of shared features. However,
manually designing such architectures is cumbersome, as practitioners need to
balance between the overall performance across all tasks and the higher
computational cost induced by the newly added parameters. In this work, we
propose \textit{InterroGate}, a novel multi-task learning (MTL) architecture
designed to mitigate task interference while optimizing inference computational
efficiency. We employ a learnable gating mechanism to automatically balance the
shared and task-specific representations while preserving the performance of
all tasks. Crucially, the patterns of parameter sharing and specialization
dynamically learned during training, become fixed at inference, resulting in a
static, optimized MTL architecture. Through extensive empirical evaluations, we
demonstrate SoTA results on three MTL benchmarks using convolutional as well as
transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 27 Feb 24</h3>
<dl>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15087" title="Abstract">arXiv:2402.15087</a> (cross-list from math.GT) [<a href="/pdf/2402.15087" title="Download PDF">pdf</a>, <a href="/format/2402.15087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Software for Triangulating and Simplifying 4-Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burke%2C+R+A">Rhuaidi Antonio Burke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 29 figures. Full version of a paper to appear in a shorter form in the 40th International Symposium on Computational Geometry (SoCG 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geometric Topology (math.GT)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Dimension 4 is the first dimension in which exotic smooth manifold pairs
appear -- manifolds which are topologically the same but for which there is no
smooth deformation of one into the other. Whilst smooth and triangulated
4-manifolds do coincide, comparatively little work has been done towards
gaining an understanding of smooth 4-manifolds from the discrete and
algorithmic perspective. In this paper we introduce new software tools to make
this possible, including a software implementation of an algorithm which
enables us to build triangulations of 4-manifolds from Kirby diagrams, as well
as a new heuristic for simplifying 4-manifold triangulations. Using these
tools, we present new triangulations of several bounded exotic pairs, corks and
plugs (objects responsible for "exoticity"), as well as the smallest known
triangulation of the fundamental K3 surface. The small size of these
triangulations benefit us by revealing fine structural features in 4-manifold
triangulations.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15534" title="Abstract">arXiv:2402.15534</a> (cross-list from eess.IV) [<a href="/pdf/2402.15534" title="Download PDF">pdf</a>, <a href="/format/2402.15534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in  Chest X-Ray Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Parida%2C+A">Abhieet Parida</a>, 
<a href="/search/eess?searchtype=author&query=Capellan-Martin%2C+D">Daniel Capellan-Martin</a>, 
<a href="/search/eess?searchtype=author&query=Atito%2C+S">Sara Atito</a>, 
<a href="/search/eess?searchtype=author&query=Awais%2C+M">Muhammad Awais</a>, 
<a href="/search/eess?searchtype=author&query=Ledesma-Carbayo%2C+M+J">Maria J. Ledesma-Carbayo</a>, 
<a href="/search/eess?searchtype=author&query=Linguraru%2C+M+G">Marius G. Linguraru</a>, 
<a href="/search/eess?searchtype=author&query=Anwar%2C+S+M">Syed Muhammad Anwar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Chest X-Ray (CXR) is a widely used clinical imaging modality and has a
pivotal role in the diagnosis and prognosis of various lung and heart related
conditions. Conventional automated clinical diagnostic tool design strategies
relying on radiology reads and supervised learning, entail the cumbersome
requirement of high quality annotated training data. To address this challenge,
self-supervised pre-training has proven to outperform supervised pre-training
in numerous downstream vision tasks, representing a significant breakthrough in
the field. However, medical imaging pre-training significantly differs from
pre-training with natural images (e.g., ImageNet) due to unique attributes of
clinical images. In this context, we introduce Diverse Concept Modeling
(DiCoM), a novel self-supervised training paradigm that leverages a student
teacher framework for learning diverse concepts and hence effective
representation of the CXR data. Hence, expanding beyond merely modeling a
single primary label within an image, instead, effectively harnessing the
information from all the concepts inherent in the CXR. The pre-trained model is
subsequently fine-tuned to address diverse domain-specific tasks. Our proposed
paradigm consistently demonstrates robust performance across multiple
downstream tasks on multiple datasets, highlighting the success and
generalizability of the pre-training strategy. To establish the efficacy of our
methods we analyze both the power of learned representations and the speed of
convergence (SoC) of our models. For diverse data and tasks, DiCoM is able to
achieve in most cases better results compared to other state-of-the-art
pre-training strategies. This when combined with the higher SoC and
generalization capabilities positions DiCoM to be established as a foundation
model for CXRs, a widely used imaging modality.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15539" title="Abstract">arXiv:2402.15539</a> (cross-list from eess.AS) [<a href="/pdf/2402.15539" title="Download PDF">pdf</a>, <a href="/ps/2402.15539" title="Download PostScript">ps</a>, <a href="/format/2402.15539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards  Automatic Assessment Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Seonwoo Lee</a>, 
<a href="/search/eess?searchtype=author&query=Mun%2C+J">Jihyun Mun</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+S">Sunhee Kim</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+M">Minhwa Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, Accepted for LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the growing demand for digital therapeutics for children with Autism
Spectrum Disorder (ASD), there is currently no speech corpus available for
Korean children with ASD. This paper introduces a speech corpus specifically
designed for Korean children with ASD, aiming to advance speech technologies
such as pronunciation and severity evaluation. Speech recordings from speech
and language evaluation sessions were transcribed, and annotated for
articulatory and linguistic characteristics. Three speech and language
pathologists rated these recordings for social communication severity (SCS) and
pronunciation proficiency (PP) using a 3-point Likert scale. The total number
of participants will be 300 for children with ASD and 50 for typically
developing (TD) children. The paper also analyzes acoustic and linguistic
features extracted from speech data collected and completed for annotation from
73 children with ASD and 9 TD children to investigate the characteristics of
children with ASD and identify significant features that correlate with the
clinical scores. The results reveal some speech and linguistic characteristics
in children with ASD that differ from those in TD children or another subgroup
of ASD categorized by clinical scores, demonstrating the potential for
developing automatic assessment systems for SCS and PP.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15566" title="Abstract">arXiv:2402.15566</a> (cross-list from eess.IV) [<a href="/pdf/2402.15566" title="Download PDF">pdf</a>, <a href="/ps/2402.15566" title="Download PostScript">ps</a>, <a href="/format/2402.15566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the AI generalization gap by adjusting for dermatology condition  distribution differences across clinical settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rikhye%2C+R+V">Rajeev V. Rikhye</a>, 
<a href="/search/eess?searchtype=author&query=Loh%2C+A">Aaron Loh</a>, 
<a href="/search/eess?searchtype=author&query=Hong%2C+G+E">Grace Eunhae Hong</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+P">Preeti Singh</a>, 
<a href="/search/eess?searchtype=author&query=Smith%2C+M+A">Margaret Ann Smith</a>, 
<a href="/search/eess?searchtype=author&query=Muralidharan%2C+V">Vijaytha Muralidharan</a>, 
<a href="/search/eess?searchtype=author&query=Wong%2C+D">Doris Wong</a>, 
<a href="/search/eess?searchtype=author&query=Sayres%2C+R">Rory Sayres</a>, 
<a href="/search/eess?searchtype=author&query=Phung%2C+M">Michelle Phung</a>, 
<a href="/search/eess?searchtype=author&query=Betancourt%2C+N">Nicolas Betancourt</a>, 
<a href="/search/eess?searchtype=author&query=Fong%2C+B">Bradley Fong</a>, 
<a href="/search/eess?searchtype=author&query=Sahasrabudhe%2C+R">Rachna Sahasrabudhe</a>, 
<a href="/search/eess?searchtype=author&query=Nasim%2C+K">Khoban Nasim</a>, 
<a href="/search/eess?searchtype=author&query=Eschholz%2C+A">Alec Eschholz</a>, 
<a href="/search/eess?searchtype=author&query=Mustafa%2C+B">Basil Mustafa</a>, 
<a href="/search/eess?searchtype=author&query=Freyberg%2C+J">Jan Freyberg</a>, 
<a href="/search/eess?searchtype=author&query=Spitz%2C+T">Terry Spitz</a>, 
<a href="/search/eess?searchtype=author&query=Matias%2C+Y">Yossi Matias</a>, 
<a href="/search/eess?searchtype=author&query=Corrado%2C+G+S">Greg S. Corrado</a>, 
<a href="/search/eess?searchtype=author&query=Chou%2C+K">Katherine Chou</a>, 
<a href="/search/eess?searchtype=author&query=Webster%2C+D+R">Dale R. Webster</a>, 
<a href="/search/eess?searchtype=author&query=Bui%2C+P">Peggy Bui</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Ko%2C+J">Justin Ko</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+S">Steven Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, there has been great progress in the ability of artificial
intelligence (AI) algorithms to classify dermatological conditions from
clinical photographs. However, little is known about the robustness of these
algorithms in real-world settings where several factors can lead to a loss of
generalizability. Understanding and overcoming these limitations will permit
the development of generalizable AI that can aid in the diagnosis of skin
conditions across a variety of clinical settings. In this retrospective study,
we demonstrate that differences in skin condition distribution, rather than in
demographics or image capture mode are the main source of errors when an AI
algorithm is evaluated on data from a previously unseen source. We demonstrate
a series of steps to close this generalization gap, requiring progressively
more information about the new source, ranging from the condition distribution
to training data enriched for data less frequently seen during training. Our
results also suggest comparable performance from end-to-end fine tuning versus
fine tuning solely the classification layer on top of a frozen embedding model.
Our approach can inform the adaptation of AI algorithms to new settings, based
on the information and resources available.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15569" title="Abstract">arXiv:2402.15569</a> (cross-list from eess.AS) [<a href="/pdf/2402.15569" title="Download PDF">pdf</a>, <a href="/format/2402.15569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Fully Self-Supervised Multi-Pitch Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cwitkowitz%2C+F">Frank Cwitkowitz</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+Z">Zhiyao Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Multi-pitch estimation is a decades-long research problem involving the
detection of pitch activity associated with concurrent musical events within
multi-instrument mixtures. Supervised learning techniques have demonstrated
solid performance on more narrow characterizations of the task, but suffer from
limitations concerning the shortage of large-scale and diverse polyphonic music
datasets with multi-pitch annotations. We present a suite of self-supervised
learning objectives for multi-pitch estimation, which encourage the
concentration of support around harmonics, invariance to timbral
transformations, and equivariance to geometric transformations. These
objectives are sufficient to train an entirely convolutional autoencoder to
produce multi-pitch salience-grams directly, without any fine-tuning. Despite
training exclusively on a collection of synthetic single-note audio samples,
our fully self-supervised framework generalizes to polyphonic music mixtures,
and achieves performance comparable to supervised models trained on
conventional multi-pitch datasets.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15592" title="Abstract">arXiv:2402.15592</a> (cross-list from math.OC) [<a href="/pdf/2402.15592" title="Download PDF">pdf</a>, <a href="/format/2402.15592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural optimal controller for stochastic systems via pathwise HJB  operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiao%2C+Z">Zhe Jiao</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+X">Xiaoyan Luo</a>, 
<a href="/search/math?searchtype=author&query=Yi%2C+X">Xinlei Yi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The aim of this work is to develop deep learning-based algorithms for
high-dimensional stochastic control problems based on physics-informed learning
and dynamic programming. Unlike classical deep learning-based methods relying
on a probabilistic representation of the solution to the
Hamilton--Jacobi--Bellman (HJB) equation, we introduce a pathwise operator
associated with the HJB equation so that we can define a problem of
physics-informed learning. According to whether the optimal control has an
explicit representation, two numerical methods are proposed to solve the
physics-informed learning problem. We provide an error analysis on how the
truncation, approximation and optimization errors affect the accuracy of these
methods. Numerical results on various applications are presented to illustrate
the performance of the proposed algorithms.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15602" title="Abstract">arXiv:2402.15602</a> (cross-list from math.ST) [<a href="/pdf/2402.15602" title="Download PDF">pdf</a>, <a href="/format/2402.15602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimax Optimality of Score-based Diffusion Models: Beyond the Density  Lower Bound Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+K">Kaihong Zhang</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+H">Heqi Yin</a>, 
<a href="/search/math?searchtype=author&query=Liang%2C+F">Feng Liang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jingbo Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the asymptotic error of score-based diffusion model sampling in
large-sample scenarios from a non-parametric statistics perspective. We show
that a kernel-based score estimator achieves an optimal mean square error of
$\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$
for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and
$d$ represent the sample size and the dimension, $t$ is bounded above and below
by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As
a consequence, this yields an $\widetilde{O}\left(n^{-1/2}
t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the
distribution of the sample generated by the diffusion model under a mere
sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric
family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early
stopping strategy, we obtain that the diffusion model is nearly (up to log
factors) minimax optimal. This removes the crucial lower bound assumption on
$p_0$ in previous proofs of the minimax optimality of the diffusion model for
nonparametric families.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15611" title="Abstract">arXiv:2402.15611</a> (cross-list from math.OC) [<a href="/pdf/2402.15611" title="Download PDF">pdf</a>, <a href="/format/2402.15611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data/moment-driven approaches for fast predictive control of collective  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Albi%2C+G">Giacomo Albi</a>, 
<a href="/search/math?searchtype=author&query=Bicego%2C+S">Sara Bicego</a>, 
<a href="/search/math?searchtype=author&query=Herty%2C+M">Michael Herty</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yuyang Huang</a>, 
<a href="/search/math?searchtype=author&query=Kalise%2C+D">Dante Kalise</a>, 
<a href="/search/math?searchtype=author&query=Segala%2C+C">Chiara Segala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Feedback control synthesis for large-scale particle systems is reviewed in
the framework of model predictive control (MPC). The high-dimensional character
of collective dynamics hampers the performance of traditional MPC algorithms
based on fast online dynamic optimization at every time step. Two alternatives
to MPC are proposed. First, the use of supervised learning techniques for the
offline approximation of optimal feedback laws is discussed. Then, a procedure
based on sequential linearization of the dynamics based on macroscopic
quantities of the particle ensemble is reviewed. Both approaches circumvent the
online solution of optimal control problems enabling fast, real-time, feedback
synthesis for large-scale particle systems. Numerical experiments assess the
performance of the proposed algorithms.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15615" title="Abstract">arXiv:2402.15615</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.15615" title="Download PDF">pdf</a>, <a href="/format/2402.15615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attached and separated rotating flow over a finite height ridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Frei%2C+S">Stefan Frei</a>, 
<a href="/search/physics?searchtype=author&query=Burman%2C+E">Erik Burman</a>, 
<a href="/search/physics?searchtype=author&query=Johnson%2C+E+R">Edward R Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper discusses the effect of rotation on the boundary layer in high
Reynolds number flow over a ridge using a numerical method based on stabilised
finite elements that captures steady solutions up to Reynolds number of order
$10^6$. The results are validated against boundary layer computations in
shallow flows and for deep flows against experimental observations reported in
Machicoane et al. (Phys. Rev. Fluids, 2018). In all cases considered the
boundary layer remains attached, even at large Reynolds numbers, provided the
Rossby number of the flow is sufficiently small. At any fixed Rossby number the
flow detaches at sufficiently high Reynolds number to form a steady
recirculating region in the lee of the ridge. At even higher Reynolds numbers
no steady flow is found. This disappearance of steady solutions closely
reproduces the transition to unsteadiness seen in the laboratory.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15625" title="Abstract">arXiv:2402.15625</a> (cross-list from stat.ML) [<a href="/pdf/2402.15625" title="Download PDF">pdf</a>, <a href="/format/2402.15625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Cyclic Causal Models from Incomplete Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sethuraman%2C+M+G">Muralikrishnna G. Sethuraman</a>, 
<a href="/search/stat?searchtype=author&query=Fekri%2C+F">Faramarz Fekri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Causal learning is a fundamental problem in statistics and science, offering
insights into predicting the effects of unseen treatments on a system. Despite
recent advances in this topic, most existing causal discovery algorithms
operate under two key assumptions: (i) the underlying graph is acyclic, and
(ii) the available data is complete. These assumptions can be problematic as
many real-world systems contain feedback loops (e.g., biological systems), and
practical scenarios frequently involve missing data. In this work, we propose a
novel framework, named MissNODAGS, for learning cyclic causal graphs from
partially missing data. Under the additive noise model, MissNODAGS learns the
causal graph by alternating between imputing the missing data and maximizing
the expected log-likelihood of the visible part of the data in each training
step, following the principles of the expectation-maximization (EM) framework.
Through synthetic experiments and real-world single-cell perturbation data, we
demonstrate improved performance when compared to using state-of-the-art
imputation techniques followed by causal learning on partially missing
interventional data.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15649" title="Abstract">arXiv:2402.15649</a> (cross-list from math.AG) [<a href="/pdf/2402.15649" title="Download PDF">pdf</a>, <a href="/ps/2402.15649" title="Download PostScript">ps</a>, <a href="/format/2402.15649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some Lower Bounds on the Reach of an Algebraic Variety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=La+Valle%2C+C">Chris La Valle</a>, 
<a href="/search/math?searchtype=author&query=Tonelli-Cueto%2C+J">Josu&#xe9; Tonelli-Cueto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages in double column
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Computational Geometry (cs.CG); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">Separation bounds are a fundamental measure of the complexity of solving a
zero-dimensional system as it measures how difficult it is to separate its
zeroes. In the positive dimensional case, the notion of reach takes its place.
In this paper, we provide bounds on the reach of a smooth algebraic variety in
terms of several invariants of interest: the condition number, Smale's $\gamma$
and the bit-size. We also provide probabilistic bounds for random algebraic
varieties under some general assumptions.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15686" title="Abstract">arXiv:2402.15686</a> (cross-list from quant-ph) [<a href="/pdf/2402.15686" title="Download PDF">pdf</a>, <a href="/format/2402.15686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower bounds for quantum-inspired classical algorithms via communication  complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mande%2C+N+S">Nikhil S. Mande</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shao%2C+C">Changpeng Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Quantum-inspired classical algorithms provide us with a new way to understand
the computational power of quantum computers for practically-relevant problems,
especially in machine learning. In the past several years, numerous efficient
algorithms for various tasks have been found, while an analysis of lower bounds
is still missing. Using communication complexity, in this work we propose the
first method to study lower bounds for these tasks. We mainly focus on lower
bounds for solving linear regressions, supervised clustering, principal
component analysis, recommendation systems, and Hamiltonian simulations. More
precisely, we show that for linear regressions, in the row-sparse case, the
lower bound is quadratic in the Frobenius norm of the underlying matrix, which
is tight. In the dense case, with an extra assumption on the accuracy we obtain
that the lower bound is quartic in the Frobenius norm, which matches the upper
bound. For supervised clustering, we obtain a tight lower bound that is quartic
in the Frobenius norm. For the other three tasks, we obtain a lower bound that
is quadratic in the Frobenius norm, and the known upper bound is quartic in the
Frobenius norm. Through this research, we find that large quantum speedup can
exist for sparse, high-rank, well-conditioned matrix-related problems. Finally,
we extend our method to study lower bounds analysis of quantum query algorithms
for matrix-related problems. Some applications are given.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15704" title="Abstract">arXiv:2402.15704</a> (cross-list from eess.IV) [<a href="/pdf/2402.15704" title="Download PDF">pdf</a>, <a href="/format/2402.15704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Heterogeneous Dynamic Convolutional Neural Network for Image  Super-resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+C">Chunwei Tian</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xuanyu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+J">Jia Ren</a>, 
<a href="/search/eess?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+C">Chia-Wen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Convolutional neural networks can automatically learn features via deep
network architectures and given input samples. However, robustness of obtained
models may have challenges in varying scenes. Bigger differences of a network
architecture are beneficial to extract more complementary structural
information to enhance robustness of an obtained super-resolution model. In
this paper, we present a heterogeneous dynamic convolutional network in image
super-resolution (HDSRNet). To capture more information, HDSRNet is implemented
by a heterogeneous parallel network. The upper network can facilitate more
contexture information via stacked heterogeneous blocks to improve effects of
image super-resolution. Each heterogeneous block is composed of a combination
of a dilated, dynamic, common convolutional layers, ReLU and residual learning
operation. It can not only adaptively adjust parameters, according to different
inputs, but also prevent long-term dependency problem. The lower network
utilizes a symmetric architecture to enhance relations of different layers to
mine more structural information, which is complementary with a upper network
for image super-resolution. The relevant experimental results show that the
proposed HDSRNet is effective to deal with image resolving. The code of HDSRNet
can be obtained at https://github.com/hellloxiaotian/HDSRNet.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15718" title="Abstract">arXiv:2402.15718</a> (cross-list from stat.ML) [<a href="/pdf/2402.15718" title="Download PDF">pdf</a>, <a href="/format/2402.15718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Long%2C+J">Jihao Long</a>, 
<a href="/search/stat?searchtype=author&query=Peng%2C+X">Xiaojun Peng</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+L">Lei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we conduct a comprehensive analysis of generalization
properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario
crucial to scientific computing, where data are often generated via computer
simulations. We prove that KRR can attain the minimax optimal rate, which
depends on both the eigenvalue decay of the associated kernel and the relative
smoothness of target functions. Particularly, when the eigenvalue decays
exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence
rate faster than any polynomial. Moreover, the numerical experiments well
corroborate our theoretical findings. Our proof leverages a novel extension of
the duality framework introduced by Chen et al. (2023), which could be useful
in analyzing kernel-based methods beyond the scope of this work.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15724" title="Abstract">arXiv:2402.15724</a> (cross-list from math.OC) [<a href="/pdf/2402.15724" title="Download PDF">pdf</a>, <a href="/format/2402.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Learning of Decision Functions in Multiplayer Games with  Expectation Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yuanhanqing Huang</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jianghai Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We explore a class of stochastic multiplayer games where each player in the
game aims to optimize its objective under uncertainty and adheres to some
expectation constraints. The study employs an offline learning paradigm,
leveraging a pre-existing dataset containing auxiliary features. While prior
research in deterministic and stochastic multiplayer games primarily explored
vector-valued decisions, this work departs by considering function-valued
decisions that incorporate auxiliary features as input. We leverage the law of
large deviations and degree theory to establish the almost sure convergence of
the offline learning solution to the true solution as the number of data
samples increases. Finally, we demonstrate the validity of our method via a
multi-account portfolio optimization problem.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15735" title="Abstract">arXiv:2402.15735</a> (cross-list from eess.AS) [<a href="/pdf/2402.15735" title="Download PDF">pdf</a>, <a href="/ps/2402.15735" title="Download PostScript">ps</a>, <a href="/format/2402.15735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A circular microphone array with virtual microphones based on  acoustics-informed neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+S">Sipei Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+F">Fei Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to JASA on 24/02/2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Acoustic beamforming aims to focus acoustic signals to a specific direction
and suppress undesirable interferences from other directions. Despite its
flexibility and steerability, beamforming with circular microphone arrays
suffers from significant performance degradation at frequencies corresponding
to zeros of the Bessel functions. To conquer this constraint, baffled or
concentric circular microphone arrays have been studied; however, the former
needs a bulky baffle that interferes with the original sound field whereas the
latter requires more microphones that increase the complexity and cost, both of
which are undesirable in practical applications. To tackle this challenge, this
paper proposes a circular microphone array equipped with virtual microphones,
which resolves the performance degradation commonly associated with circular
microphone arrays without resorting to physical modifications. The sound
pressures at the virtual microphones are predicted from those measured by the
physical microphones based on an acoustics-informed neural network, and then
the sound pressures measured by the physical microphones and those predicted at
the virtual microphones are integrated to design the beamformer. Experimental
results demonstrate that the proposed approach not only eliminates the
performance degradation but also suppresses spatial aliasing at high
frequencies, thereby underscoring its promising potential.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15744" title="Abstract">arXiv:2402.15744</a> (cross-list from eess.IV) [<a href="/pdf/2402.15744" title="Download PDF">pdf</a>, <a href="/format/2402.15744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traditional Transformation Theory Guided Model for Learned Image  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhiyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+C">Chenyang Ge</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures, accepted by ICCE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, many deep image compression methods have been proposed and achieved
remarkable performance. However, these methods are dedicated to optimizing the
compression performance and speed at medium and high bitrates, while research
on ultra low bitrates is limited. In this work, we propose a ultra low bitrates
enhanced invertible encoding network guided by traditional transformation
theory, experiments show that our codec outperforms existing methods in both
compression and reconstruction performance. Specifically, we introduce the
Block Discrete Cosine Transformation to model the sparsity of features and
employ traditional Haar transformation to improve the reconstruction
performance of the model without increasing the bitstream cost.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15827" title="Abstract">arXiv:2402.15827</a> (cross-list from quant-ph) [<a href="/pdf/2402.15827" title="Download PDF">pdf</a>, <a href="/format/2402.15827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Analysis of Termination Problems for Nondeterministic  Quantum Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Fu%2C+J">Jianling Fu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jiang%2C+H">Hui Jiang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+M">Ming Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Deng%2C+Y">Yuxin Deng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Z">Zhi-Bin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">We consider the two categories of termination problems of quantum programs
with nondeterminism: 1) Is an input of a program terminating with probability
one under all schedulers? If not, how can a scheduler be synthesized to
evidence the nontermination? 2) Are all inputs terminating with probability one
under their respective schedulers? If yes, a further question asks whether
there is a scheduler that forces all inputs to be terminating with probability
one together with how to synthesize it; otherwise, how can an input be provided
to refute the universal termination?
<br />For the effective verification of the first category, we over-approximate the
reachable set of quantum program states by the reachable subspace, whose
algebraic structure is a linear space. On the other hand, we study the set of
divergent states from which the program terminates with probability zero under
some scheduler. The divergent set has an explicit algebraic structure.
Exploiting them, we address the decision problem by a necessary and sufficient
condition, i.e. the disjointness of the reachable subspace and the divergent
set. Furthermore, the scheduler synthesis is completed in exponential time.
<br />For the second category, we reduce the decision problem to the existence of
invariant subspace, from which the program terminates with probability zero
under all schedulers. The invariant subspace is characterized by linear
equations. The states on that invariant subspace are evidence of the
nontermination. Furthermore, the scheduler synthesis is completed by seeking a
pattern of finite schedulers that forces all inputs to be terminating with
positive probability. The repetition of that pattern yields the desired
universal scheduler that forces all inputs to be terminating with probability
one. All the problems in the second category are shown to be solved in
polynomial time.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15855" title="Abstract">arXiv:2402.15855</a> (cross-list from quant-ph) [<a href="/pdf/2402.15855" title="Download PDF">pdf</a>, <a href="/format/2402.15855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protocols for Quantum Weak Coin Flipping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Arora%2C+A+S">Atul Singh Arora</a>, 
<a href="/search/quant-ph?searchtype=author&query=Roland%2C+J">J&#xe9;r&#xe9;mie Roland</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vlachou%2C+C">Chrysoula Vlachou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Weis%2C+S">Stephan Weis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages (+ 9 appendix), 12 figures. This is a self-contained, concise version of our main results in <a href="/abs/1811.02984">arXiv:1811.02984</a> (STOC '19) and <a href="/abs/1911.13283">arXiv:1911.13283v2</a> (SODA '21). The Cryptology ePrint 2022/1101 is the comprehensive version, subsuming the above
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Weak coin flipping is an important cryptographic primitive -- it is the
strongest known secure two-party computation primitive that classically becomes
secure only under certain assumptions (e.g. computational hardness), while
quantumly there exist protocols that achieve arbitrarily close to perfect
security. This breakthrough result was established by Mochon in 2007
[<a href="/abs/0711.4114">arXiv:0711.4114</a>]. However, his proof relied on the existence of certain
unitary operators which was established by a non-constructive argument.
Consequently, explicit protocols have remained elusive. In this work, we give
exact constructions of related unitary operators. These, together with a new
formalism, yield a family of protocols approaching perfect security thereby
also simplifying Mochon's proof of existence. We illustrate the construction of
explicit weak coin flipping protocols by considering concrete examples (from
the aforementioned family of protocols) that are more secure than all
previously known protocols.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15890" title="Abstract">arXiv:2402.15890</a> (cross-list from econ.TH) [<a href="/pdf/2402.15890" title="Download PDF">pdf</a>, <a href="/ps/2402.15890" title="Download PostScript">ps</a>, <a href="/format/2402.15890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimality of weighted contracts for multi-agent contract design with a  budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Goel%2C+S">Sumit Goel</a>, 
<a href="/search/econ?searchtype=author&query=Hann-Caruthers%2C+W">Wade Hann-Caruthers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study a contract design problem between a principal and multiple agents.
Each agent participates in an independent task with binary outcomes (success or
failure), in which it may exert costly effort towards improving its probability
of success, and the principal has a fixed budget which it can use to provide
outcome-dependent rewards to the agents. Crucially, we assume the principal
cares only about maximizing the agents' probabilities of success, not how much
of the budget it expends. We first show that a contract is optimal for some
objective if and only if it is a successful-get-everything contract. An
immediate consequence of this result is that piece-rate contracts and
bonus-pool contracts are never optimal in this setting. We then show that for
any objective, there is an optimal priority-based weighted contract, which
assigns positive weights and priority levels to the agents, and splits the
budget among the highest-priority successful agents, with each such agent
receiving a fraction of the budget proportional to her weight. This result
provides a significant reduction in the dimensionality of the principal's
optimal contract design problem and gives an interpretable and easily
implementable optimal contract. Finally, we discuss an application of our
results to the design of optimal contracts with two agents and quadratic costs.
In this context, we find that the optimal contract assigns a higher weight to
the agent whose success it values more, irrespective of the heterogeneity in
the agents' cost parameters. This suggests that the structure of the optimal
contract depends primarily on the bias in the principal's objective and is, to
some extent, robust to the heterogeneity in the agents' cost functions.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15892" title="Abstract">arXiv:2402.15892</a> (cross-list from math.ST) [<a href="/pdf/2402.15892" title="Download PDF">pdf</a>, <a href="/format/2402.15892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Konczer%2C+J">Jozsef Konczer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 129 pages, 51 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Theoretical Economics (econ.TH); Machine Learning (stat.ML)

</div>
<p class="mathjax">This work contains the mathematical exploration of a few prototypical games
in which central concepts from statistics and probability theory naturally
emerge. The first two kinds of games are termed Fisher and Bayesian games,
which are connected to Frequentist and Bayesian statistics, respectively.
Later, a more general type of game is introduced, termed Statistical game, in
which a further parameter, the players' relative risk aversion, can be set. In
this work, we show that Fisher and Bayesian games can be viewed as limiting
cases of Statistical games. Therefore, Statistical games can be viewed as a
unified framework, incorporating both Frequentist and Bayesian statistics.
Furthermore, a philosophical framework is (re-)presented -- often referred to
as minimax regret criterion -- as a general approach to decision making.
<br />The main motivation for this work was to embed Bayesian statistics into a
broader decision-making framework, where, based on collected data, actions with
consequences have to be made, which can be translated to utilities (or
rewards/losses) of the decision-maker. The work starts with the simplest
possible toy model, related to hypothesis testing and statistical inference.
This choice has two main benefits: i.) it allows us to determine (conjecture)
the behaviour of the equilibrium strategies in various limiting cases ii.) this
way, we can introduce Statistical games without requiring additional stochastic
parameters. The work contains game theoretical methods related to two-player,
non-cooperative games to determine and prove equilibrium strategies of Fisher,
Bayesian and Statistical games. It also relies on analytical tools for
derivations concerning various limiting cases.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15904" title="Abstract">arXiv:2402.15904</a> (cross-list from econ.TH) [<a href="/pdf/2402.15904" title="Download PDF">pdf</a>, <a href="/format/2402.15904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Budget Aggregation with Single-Peaked Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Brandt%2C+F">Felix Brandt</a>, 
<a href="/search/econ?searchtype=author&query=Greger%2C+M">Matthias Greger</a>, 
<a href="/search/econ?searchtype=author&query=Segal-Halevi%2C+E">Erel Segal-Halevi</a>, 
<a href="/search/econ?searchtype=author&query=Suksompong%2C+W">Warut Suksompong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study the problem of aggregating distributions, such as budget proposals,
into a collective distribution. An ideal aggregation mechanism would be Pareto
efficient, strategyproof, and fair. Most previous work assumes that agents
evaluate budgets according to the $\ell_1$ distance to their ideal budget. We
investigate and compare different models from the larger class of star-shaped
utility functions - a multi-dimensional generalization of single-peaked
preferences. For the case of two alternatives, we extend existing results by
proving that under very general assumptions, the uniform phantom mechanism is
the only strategyproof mechanism that satisfies proportionality - a minimal
notion of fairness introduced by Freeman et al. (2021). Moving to the case of
more than two alternatives, we establish sweeping impossibilities for $\ell_1$
and $\ell_\infty$ disutilities: no mechanism satisfies efficiency,
strategyproofness, and proportionality. We then propose a new kind of
star-shaped utilities based on evaluating budgets by the ratios of shares
between a given budget and an ideal budget. For these utilities, efficiency,
strategyproofness, and fairness become compatible. In particular, we prove that
the mechanism that maximizes the Nash product of individual utilities is
characterized by group-strategyproofness and a core-based fairness condition.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15908" title="Abstract">arXiv:2402.15908</a> (cross-list from math.CO) [<a href="/pdf/2402.15908" title="Download PDF">pdf</a>, <a href="/ps/2402.15908" title="Download PostScript">ps</a>, <a href="/format/2402.15908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the finiteness of $k$-vertex-critical $2P_2$-free graphs with  forbidden induced squids or bulls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Adekanye%2C+M">Melvin Adekanye</a>, 
<a href="/search/math?searchtype=author&query=Bury%2C+C">Christopher Bury</a>, 
<a href="/search/math?searchtype=author&query=Cameron%2C+B">Ben Cameron</a>, 
<a href="/search/math?searchtype=author&query=Knodel%2C+T">Thaler Knodel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IWOCA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A graph is $k$-vertex-critical if $\chi(G)=k$ but $\chi(G-v)&lt;k$ for all $v\in
V(G)$ and $(G,H)$-free if it contains no induced subgraph isomorphic to $G$ or
$H$. We show that there are only finitely many $k$-vertex-critical
$(2P_2,H)$-free graphs for all $k$ when $H$ is isomorphic to any of the
following graphs of order $5$: $bull$, $chair$, $claw+P_1$, or
$\overline{diamond+P_1}$.
<br />The latter three are corollaries of more general results where $H$ is
isomorphic to $(m, \ell)$-$squid$ for $m=3,4$ and any $\ell\ge 1$ where an
$(m,\ell)$-$squid$ is the graph obtained from an $m$-cycle by attaching $\ell$
leaves to a single vertex of the cycle. For each of the graphs $H$ above and
any fixed $k$, our results imply the existence of polynomial-time certifying
algorithms for deciding the $k$-colourability problem for $(2P_2,H)$-free
graphs. Further, our structural classifications allow us to exhaustively
generate, with aid of computer search, all $k$-vertex-critical $(2P_2,H)$-free
graphs for $k\le 7$ when $H=bull$ or $H=(4,1)$-$squid$ (also known as
$banner$).
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15919" title="Abstract">arXiv:2402.15919</a> (cross-list from eess.IV) [<a href="/pdf/2402.15919" title="Download PDF">pdf</a>, <a href="/format/2402.15919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle  Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+X">Xiaopeng Peng</a>, 
<a href="/search/eess?searchtype=author&query=Fleet%2C+E+F">Erin F. Fleet</a>, 
<a href="/search/eess?searchtype=author&query=Watnik%2C+A+T">Abbie T. Watnik</a>, 
<a href="/search/eess?searchtype=author&query=Swartzlander%2C+G+A">Grover A. Swartzlander</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)

</div>
<p class="mathjax">Conventional camera systems are susceptible to the adverse effects of laser
dazzle, which may over-saturate an image or cause permanent damage to pixels.
To address this problem, we developed an approach combining point spread
function engineering whereby a wavefront-coded mask in the pupil plane blurs
both the laser and scene, together with a deep neural sandwich network. In
addition to protecting the sensor, our approach jointly removes the laser from
the scene and reconstructs a satisfactory deblurred image. Image recovery is
achieved by wrapping two generative adversarial networks (GANs) around a
learnable non-blind image deconvolution module. We trained the Sandwich GAN
(SGAN) to suppress the peak laser irradiance as high as $10^6$ times the sensor
saturation threshold - the point at which the bare system without the phase
mask may exhibit damage. The end-to-end training includes physics-based
modeling of the imaging system whereby a laser having an arbitrary angle of
incidence is superimposed on images from a large publicly available library.
The trained system was validated in the laboratory for laser strengths up to
$10^4$ times the saturation value. The proposed image restoration model
quantitatively and qualitatively outperforms other methods for a wide range of
scene contents, illumination conditions, laser strengths, and noise
characteristics.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15924" title="Abstract">arXiv:2402.15924</a> (cross-list from quant-ph) [<a href="/pdf/2402.15924" title="Download PDF">pdf</a>, <a href="/ps/2402.15924" title="Download PostScript">ps</a>, <a href="/format/2402.15924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive-Proximity Bit-Flipping for Decoding Surface Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pacenti%2C+M">Michele Pacenti</a>, 
<a href="/search/quant-ph?searchtype=author&query=Flanagan%2C+M+F">Mark F. Flanagan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chytas%2C+D">Dimitris Chytas</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vasic%2C+B">Bane Vasic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Topological quantum codes, such as toric and surface codes, are excellent
candidates for hardware implementation due to their robustness against errors
and their local interactions between qubits. However, decoding these codes
efficiently remains a challenge: existing decoders often fall short of meeting
requirements such as having low computational complexity (ideally linear in the
code's blocklength), low decoding latency, and low power consumption. In this
paper we propose a novel bit-flipping (BF) decoder tailored for toric and
surface codes. We introduce the proximity vector as a heuristic metric for
flipping bits, and we develop a new subroutine for correcting a particular
class of harmful degenerate errors. Our algorithm achieves linear complexity
growth and it can be efficiently implemented as it only involves simple
operations such as bit-wise additions, quasi-cyclic permutations and
vector-matrix multiplications. The proposed decoder shows a decoding threshold
of 7.5% for the 2D toric code and 7% for the rotated planar code over the
binary symmetric channel.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15939" title="Abstract">arXiv:2402.15939</a> (cross-list from eess.IV) [<a href="/pdf/2402.15939" title="Download PDF">pdf</a>, <a href="/ps/2402.15939" title="Download PostScript">ps</a>, <a href="/format/2402.15939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+M">Min Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chengyan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+N">Naiming Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yi Li</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+Y">Yiwen Gong</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+S">Shufu Chang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yinyin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Liuhong Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianjun Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+C">Congbo Cai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Dynamic magnetic resonance imaging (MRI) plays an indispensable role in
cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled
but the image reconstruction poses a great challenge of high-dimensional
processing. This challenge leads to necessitate extensive training data in many
deep learning reconstruction methods. This work proposes a novel and efficient
approach, leveraging a dimension-reduced separable learning scheme that excels
even with highly limited training data. We further integrate it with
spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning
network (DeepSSL), which unrolls an iteration process of a reconstruction model
with both temporal low-rankness and spatial sparsity. Intermediate outputs are
visualized to provide insights into the network's behavior and enhance its
interpretability. Extensive results on cardiac cine datasets show that the
proposed DeepSSL is superior to the state-of-the-art methods visually and
quantitatively, while reducing the demand for training cases by up to 75%. And
its preliminary adaptability to cardiac patients has been verified through
experienced radiologists' and cardiologists' blind reader study. Additionally,
DeepSSL also benefits for achieving the downstream task of cardiac segmentation
with higher accuracy and shows robustness in prospective real-time cardiac MRI.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15942" title="Abstract">arXiv:2402.15942</a> (cross-list from math.OC) [<a href="/pdf/2402.15942" title="Download PDF">pdf</a>, <a href="/format/2402.15942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum energy density steering of linear systems with  Gromov-Wasserstein terminal cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Morimoto%2C+K">Kohei Morimoto</a>, 
<a href="/search/math?searchtype=author&query=Kashima%2C+K">Kenji Kashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this study, we address optimal control problems focused on steering the
probabilistic distribution of state variables in linear dynamical systems.
Specifically, we address the problem of controlling the structural properties
of Gaussian state distributions to predefined targets at terminal times. This
task is not yet explored in existing works that primarily aim to exactly match
state distributions. By employing the Gromov-Wasserstein (GW) distance as the
terminal cost, we formulate a problem that seeks to align the structure of the
state density with that of a desired distribution. This approach allows us to
extend the control objectives to capture the distribution's shape. We
demonstrate that this complex problem can be reduced to a Difference of Convex
(DC) programming, which is efficiently solvable through the DC algorithm.
Through numerical experiments, we confirm that the terminal distribution indeed
gets closer to the desired structural properties of the target distribution.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15948" title="Abstract">arXiv:2402.15948</a> (cross-list from math.OC) [<a href="/pdf/2402.15948" title="Download PDF">pdf</a>, <a href="/format/2402.15948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Criticality measure-based error estimates for infinite dimensional  optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+D">Danlin Li</a>, 
<a href="/search/math?searchtype=author&query=Milz%2C+J">Johannes Milz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Motivated by optimization with differential equations, we consider
optimization problems with Hilbert spaces as decision spaces. As a consequence
of their infinite dimensionality, the numerical solution necessitates finite
dimensional approximations and discretizations. We develop an approximation
framework and demonstrate criticality measure-based error estimates. We
consider criticality measures inspired by those used within optimization
methods, such as semismooth Newton and (conditional) gradient methods.
Furthermore, we show that our error estimates are order-optimal. Our findings
augment existing distance-based error estimates, but do not rely on strong
convexity or second-order sufficient optimality conditions. Moreover, our error
estimates can be used for code verification and validation. We illustrate our
theoretical convergence rates on linear, semilinear, and bilinear
PDE-constrained optimization.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15955" title="Abstract">arXiv:2402.15955</a> (cross-list from math.AP) [<a href="/pdf/2402.15955" title="Download PDF">pdf</a>, <a href="/format/2402.15955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct and Inverse scattering in a three-dimensional planar waveguide
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chang%2C+Y">Yan Chang</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+Y">Yukun Guo</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we study the direct and inverse scattering of the
Schr\"odinger equation in a three-dimensional planar waveguide. For the direct
problem, we derive a resonance-free region and resolvent estimates for the
resolvent of the Schr\"odinger operator in such a geometry. Based on the
analysis of the resolvent, several inverse problems are investigated. First,
given the potential function, we prove the uniqueness of the inverse source
problem with multi-frequency data. We also develop a Fourier-based method to
reconstruct the source function. The capability of this method is numerically
illustrated by examples. Second, the uniqueness and increased stability of an
inverse potential problem from data generated by incident waves are achieved in
the absence of the source function. To derive the stability estimate, we use an
argument of quantitative analytic continuation in complex theory. Third, we
prove the uniqueness of simultaneously determining the source and potential by
active boundary data generated by incident waves. In these inverse problems, we
only use the limited lateral Dirichlet boundary data at multiple wavenumbers
within a finite interval.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15994" title="Abstract">arXiv:2402.15994</a> (cross-list from q-fin.CP) [<a href="/pdf/2402.15994" title="Download PDF">pdf</a>, <a href="/ps/2402.15994" title="Download PostScript">ps</a>, <a href="/format/2402.15994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Portfolio Management and Risk Assessment in Digital Assets  Using Deep Learning for Predictive Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Cheng%2C+Q">Qishuo Cheng</a>, 
<a href="/search/q-fin?searchtype=author&query=Yang%2C+L">Le Yang</a>, 
<a href="/search/q-fin?searchtype=author&query=Zheng%2C+J">Jiajian Zheng</a>, 
<a href="/search/q-fin?searchtype=author&query=Tian%2C+M">Miao Tian</a>, 
<a href="/search/q-fin?searchtype=author&query=Xin%2C+D">Duan Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Portfolio management issues have been extensively studied in the field of
artificial intelligence in recent years, but existing deep learning-based
quantitative trading methods have some areas where they could be improved.
First of all, the prediction mode of stocks is singular; often, only one
trading expert is trained by a model, and the trading decision is solely based
on the prediction results of the model. Secondly, the data source used by the
model is relatively simple, and only considers the data of the stock itself,
ignoring the impact of the whole market risk on the stock. In this paper, the
DQN algorithm is introduced into asset management portfolios in a novel and
straightforward way, and the performance greatly exceeds the benchmark, which
fully proves the effectiveness of the DRL algorithm in portfolio management.
This also inspires us to consider the complexity of financial problems, and the
use of algorithms should be fully combined with the problems to adapt. Finally,
in this paper, the strategy is implemented by selecting the assets and actions
with the largest Q value. Since different assets are trained separately as
environments, there may be a phenomenon of Q value drift among different assets
(different assets have different Q value distribution areas), which may easily
lead to incorrect asset selection. Consider adding constraints so that the Q
values of different assets share a Q value distribution to improve results.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16047" title="Abstract">arXiv:2402.16047</a> (cross-list from math.CO) [<a href="/pdf/2402.16047" title="Download PDF">pdf</a>, <a href="/ps/2402.16047" title="Download PostScript">ps</a>, <a href="/format/2402.16047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> List Coloring of some Cayley graphs using Kernel perfections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=S%2C+P">Prajnanaswaroopa S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, short paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this paper, we try to determine exact or bounds on the choosability, or
list chromatic numbers of some Cayley graphs, typically some Unitary Cayley
graphs and Cayley graphs on Dihedral groups.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16059" title="Abstract">arXiv:2402.16059</a> (cross-list from stat.ML) [<a href="/pdf/2402.16059" title="Download PDF">pdf</a>, <a href="/format/2402.16059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-enhanced deep Gaussian processes for multifidelity modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bone%2C+V">Viv Bone</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Heide%2C+C">Chris van der Heide</a>, 
<a href="/search/stat?searchtype=author&query=Mackle%2C+K">Kieran Mackle</a>, 
<a href="/search/stat?searchtype=author&query=Jahn%2C+I+H+J">Ingo H.J. Jahn</a>, 
<a href="/search/stat?searchtype=author&query=Dower%2C+P+M">Peter M. Dower</a>, 
<a href="/search/stat?searchtype=author&query=Manzie%2C+C">Chris Manzie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multifidelity models integrate data from multiple sources to produce a single
approximator for the underlying process. Dense low-fidelity samples are used to
reduce interpolation error, while sparse high-fidelity samples are used to
compensate for bias or noise in the low-fidelity samples. Deep Gaussian
processes (GPs) are attractive for multifidelity modelling as they are
non-parametric, robust to overfitting, perform well for small datasets, and,
critically, can capture nonlinear and input-dependent relationships between
data of different fidelities. Many datasets naturally contain gradient data,
especially when they are generated by computational models that are compatible
with automatic differentiation or have adjoint solutions. Principally, this
work extends deep GPs to incorporate gradient data. We demonstrate this method
on an analytical test problem and a realistic partial differential equation
problem, where we predict the aerodynamic coefficients of a hypersonic flight
vehicle over a range of flight conditions and geometries. In both examples, the
gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and
their non-gradient-enhanced counterparts.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16158" title="Abstract">arXiv:2402.16158</a> (cross-list from stat.ML) [<a href="/pdf/2402.16158" title="Download PDF">pdf</a>, <a href="/format/2402.16158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Free Fair Federated Learning with Small Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yin%2C+Q">Qichuan Yin</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+J">Junzhou Huang</a>, 
<a href="/search/stat?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+L">Linjun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">As federated learning gains increasing importance in real-world applications
due to its capacity for decentralized data training, addressing fairness
concerns across demographic groups becomes critically important. However, most
existing machine learning algorithms for ensuring fairness are designed for
centralized data environments and generally require large-sample and
distributional assumptions, underscoring the urgent need for fairness
techniques adapted for decentralized and heterogeneous systems with
finite-sample and distribution-free guarantees. To address this issue, this
paper introduces FedFaiREE, a post-processing algorithm developed specifically
for distribution-free fair learning in decentralized settings with small
samples. Our approach accounts for unique challenges in decentralized
environments, such as client heterogeneity, communication costs, and small
sample sizes. We provide rigorous theoretical guarantees for both fairness and
accuracy, and our experimental results further provide robust empirical
validation for our proposed method.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16208" title="Abstract">arXiv:2402.16208</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.16208" title="Download PDF">pdf</a>, <a href="/ps/2402.16208" title="Download PostScript">ps</a>, <a href="/format/2402.16208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New approach method for solving nonlinear differential equations of  blood flow with nanoparticle in presence of magnetic field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pahnehkolaei%2C+S+M+H">Seyed Morteza Hamzeh Pahnehkolaei</a>, 
<a href="/search/physics?searchtype=author&query=Kachabi%2C+A">Amirreza Kachabi</a>, 
<a href="/search/physics?searchtype=author&query=Sipey%2C+M+H">Milad Heydari Sipey</a>, 
<a href="/search/physics?searchtype=author&query=Ganji%2C+D+D">Davood Domiri Ganji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages with 17 figures, International Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, effect of physical parameters in presence of magnetic field on
heat transfer and flow of third grade non-Newtonian Nanofluid in a porous
medium with annular cross sectional analytically has been investigated. The
viscosity of Nanofluid categorized in 3 model include constant model and
variable models with temperature that in variable category Reynolds Model and
Vogel's Model has been used to determine the effect of viscosity in flow filed.
analytically solution for velocity, temperature, and nanoparticle concentration
are developed by Akbari-Ganji's Method (AGM) that has high proximity with
numerical solution (Runge-Kutta 4th-order). Physical parameters that used for
extract result for non dimensional variables of nonlinear equations are
pressure gradient, Brownian motion parameter, thermophoresis parameter,
magnetic field intensity and Grashof number. The results show that the increase
in the pressure gradient and Thermophoresis parameter and decrease in the
Brownian motion parameter cause the rise in the velocity profile. Also the
increase in the Grashof number and decrease in MHD parameter cause the rise in
the velocity profile. Furthermore, either increase in Thermophoresis or
decrease in Brownian motion parameters results in enhancement in nanoparticle
concentration. The highest value of velocity is observed when the Vogel's Model
is used for viscosity.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16215" title="Abstract">arXiv:2402.16215</a> (cross-list from math.CO) [<a href="/pdf/2402.16215" title="Download PDF">pdf</a>, <a href="/ps/2402.16215" title="Download PostScript">ps</a>, <a href="/format/2402.16215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branch-depth is minor closure of contraction-deletion-depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bria%C5%84ski%2C+M">Marcin Bria&#x144;ski</a>, 
<a href="/search/math?searchtype=author&query=Kr%C3%A1%C4%BE%2C+D">Daniel Kr&#xe1;&#x13e;</a>, 
<a href="/search/math?searchtype=author&query=Pek%C3%A1rkov%C3%A1%2C+K">Krist&#xfd;na Pek&#xe1;rkov&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The notion of branch-depth for matroids was introduced by DeVos, Kwon and Oum
as the matroid analogue of the tree-depth of graphs. The
contraction-deletion-depth, another tree-depth like parameter of matroids, is
the number of recursive steps needed to decompose a matroid by contractions and
deletions to single elements. Any matroid with contraction-deletion-depth at
most d has branch-depth at most d. However, the two notions are not
functionally equivalent as contraction-deletion-depth of matroids with
branch-depth two can be arbitrarily large.
<br />We show that the two notions are functionally equivalent for representable
matroids when minor closures are considered. Namely, an F-representable matroid
has small branch-depth if and only if it is a minor of an F-representable
matroid with small contraction-deletion-depth. This implies that any class of
F-representable matroids has bounded branch-depth if and only if it is a
subclass of the minor closure of a class of F-representable matroids with
bounded contraction-deletion-depth.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16221" title="Abstract">arXiv:2402.16221</a> (cross-list from eess.IV) [<a href="/pdf/2402.16221" title="Download PDF">pdf</a>, <a href="/format/2402.16221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Preprocessing Methods and Convolutional Neural Networks for  Effective Tumor Detection in Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vu%2C+H+A">Ha Anh Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, utilizing convolutional neural networks and preprocessing methods for tumor detection in MRI images, featuring a detailed methodology section on image preprocessing, segmentation, and model training, with a comprehensive evaluation of model performance on the Figshare dataset using IEEE template
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This research presents a machine-learning approach for tumor detection in
medical images using convolutional neural networks (CNNs). The study focuses on
preprocessing techniques to enhance image features relevant to tumor detection,
followed by developing and training a CNN model for accurate classification.
Various image processing techniques, including Gaussian smoothing, bilateral
filtering, and K-means clustering, are employed to preprocess the input images
and highlight tumor regions. The CNN model is trained and evaluated on a
dataset of medical images, with augmentation and data generators utilized to
enhance model generalization. Experimental results demonstrate the
effectiveness of the proposed approach in accurately detecting tumors in
medical images, paving the way for improved diagnostic tools in healthcare.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16225" title="Abstract">arXiv:2402.16225</a> (cross-list from eess.SP) [<a href="/pdf/2402.16225" title="Download PDF">pdf</a>, <a href="/ps/2402.16225" title="Download PostScript">ps</a>, <a href="/format/2402.16225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Fourier Transform Approximations Based on the Cooley-Tukey  Radix-2 Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Coelho%2C+D+F+G">D. F. G. Coelho</a>, 
<a href="/search/eess?searchtype=author&query=Cintra%2C+R+J">R. J. Cintra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Originally written in Oct 2016. Contains 55 pages, 17 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Numerical Analysis (math.NA); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">This report elaborates on approximations for the discrete Fourier transform
by means of replacing the exact Cooley-Tukey algorithm twiddle-factors by
low-complexity integers, such as $0, \pm \frac{1}{2}, \pm 1$.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16285" title="Abstract">arXiv:2402.16285</a> (cross-list from hep-ex) [<a href="/pdf/2402.16285" title="Download PDF">pdf</a>, <a href="/format/2402.16285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison of Deep Learning Models for Proton Background Rejection  with the AMS Electromagnetic Calorimeter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Hashmani%2C+R+K">Raheem Karim Hashmani</a>, 
<a href="/search/hep-ex?searchtype=author&query=Akba%C5%9F%2C+E">Emre Akba&#x15f;</a>, 
<a href="/search/hep-ex?searchtype=author&query=Demirk%C3%B6z%2C+M+B">Melahat Bilge Demirk&#xf6;z</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector
onboard the International Space Station containing six different subdetectors.
The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are
used to separate electrons/positrons from the abundant cosmic-ray proton
background.
<br />The positron flux measured in space by AMS falls with a power law which
unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several
theoretical models try to explain these phenomena, and a purer measurement of
positrons at higher energies is needed to help test them. The currently used
methods to reject the proton background at high energies involve extrapolating
shower features from the ECAL to use as inputs for boosted decision tree and
likelihood classifiers. We present a new approach for particle identification
with the AMS ECAL using deep learning (DL). By taking the energy deposition
within all the ECAL cells as an input and treating them as pixels in an
image-like format, we train an MLP, a CNN, and multiple ResNets and
Convolutional vision Transformers (CvTs) as shower classifiers.
<br />Proton rejection performance is evaluated using Monte Carlo (MC) events and
ISS data separately. For MC, using events with a reconstructed energy between
0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT
model is more than 5 times that of the other DL models. Similarly, for ISS data
with a reconstructed energy between 50 - 70 GeV, the proton rejection power of
our CvT model is more than 2.5 times that of the other DL models.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16326" title="Abstract">arXiv:2402.16326</a> (cross-list from stat.ML) [<a href="/pdf/2402.16326" title="Download PDF">pdf</a>, <a href="/format/2402.16326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Provably Accurate Randomized Sampling Algorithm for Logistic  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chowdhury%2C+A">Agniva Chowdhury</a>, 
<a href="/search/stat?searchtype=author&query=Ramuhalli%2C+P">Pradeep Ramuhalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the proceedings of AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">In statistics and machine learning, logistic regression is a widely-used
supervised learning technique primarily employed for binary classification
tasks. When the number of observations greatly exceeds the number of predictor
variables, we present a simple, randomized sampling-based algorithm for
logistic regression problem that guarantees high-quality approximations to both
the estimated probabilities and the overall discrepancy of the model. Our
analysis builds upon two simple structural conditions that boil down to
randomized matrix multiplication, a fundamental and well-understood primitive
of randomized numerical linear algebra. We analyze the properties of estimated
probabilities of logistic regression when leverage scores are used to sample
observations, and prove that accurate approximations can be achieved with a
sample whose size is much smaller than the total number of observations. To
further validate our theoretical findings, we conduct comprehensive empirical
evaluations. Overall, our work sheds light on the potential of using randomized
sampling approaches to efficiently approximate the estimated probabilities in
logistic regression, offering a practical and computationally efficient
solution for large-scale datasets.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16337" title="Abstract">arXiv:2402.16337</a> (cross-list from math.OC) [<a href="/pdf/2402.16337" title="Download PDF">pdf</a>, <a href="/ps/2402.16337" title="Download PostScript">ps</a>, <a href="/format/2402.16337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Triggered Parameterized Control of Nonlinear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rajan%2C+A">Anusree Rajan</a>, 
<a href="/search/math?searchtype=author&query=Tallapragada%2C+P">Pavankumar Tallapragada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper deals with event-triggered parameterized control (ETPC) of
nonlinear systems with external disturbances. In this control method, between
two successive events, each control input to the plant is a linear combination
of a set of linearly independent scalar functions. At each event, the
controller updates the coefficients of the parameterized control input so as to
minimize the error in approximating a continuous time control signal and
communicates the same to the actuator. We design an event-triggering rule that
guarantees global uniform ultimate boundedness of trajectories of the closed
loop system. We also ensure the absence of Zeno behavior by showing the
existence of a uniform positive lower bound on the inter-event times. We
illustrate our results through numerical examples, which indicate that the
proposed control method leads to a significant improvement in average
inter-event time and minimum inter-event time compared to the event-triggered
zero-order-hold control.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16353" title="Abstract">arXiv:2402.16353</a> (cross-list from quant-ph) [<a href="/pdf/2402.16353" title="Download PDF">pdf</a>, <a href="/ps/2402.16353" title="Download PostScript">ps</a>, <a href="/format/2402.16353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimal tradeoff between entanglement and copy complexity for state  tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+S">Sitan Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+J">Jerry Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+A">Allen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at STOC 2024. Abstract shortened to meet arXiv requirement. 36 pages, comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">There has been significant interest in understanding how practical
constraints on contemporary quantum devices impact the complexity of quantum
learning. For the classic question of tomography, recent work tightly
characterized the copy complexity for any protocol that can only measure one
copy of the unknown state at a time, showing it is polynomially worse than if
one can make fully-entangled measurements. While we now have a fairly complete
picture of the rates for such tasks in the near-term and fault-tolerant
regimes, it remains poorly understood what the landscape in between looks like.
<br />In this work, we study tomography in the natural setting where one can make
measurements of $t$ copies at a time. For sufficiently small $\epsilon$, we
show that for any $t \le d^2$,
$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$ copies are necessary and
sufficient to learn an unknown $d$-dimensional state $\rho$ to trace distance
$\epsilon$. This gives a smooth and optimal interpolation between the known
rates for single-copy and fully-entangled measurements.
<br />To our knowledge, this is the first smooth entanglement-copy tradeoff known
for any quantum learning task, and for tomography, no intermediate point on
this curve was known, even at $t = 2$. An important obstacle is that unlike the
optimal single-copy protocol, the optimal fully-entangled protocol is
inherently biased and thus precludes naive batching approaches. Instead, we
devise a novel two-stage procedure that uses Keyl's algorithm to refine a crude
estimate for $\rho$ based on single-copy measurements. A key insight is to use
Schur-Weyl sampling not to estimate the spectrum of $\rho$, but to estimate the
deviation of $\rho$ from the maximally mixed state. When $\rho$ is far from the
maximally mixed state, we devise a novel quantum splitting procedure that
reduces to the case where $\rho$ is close to maximally mixed.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16368" title="Abstract">arXiv:2402.16368</a> (cross-list from eess.IV) [<a href="/pdf/2402.16368" title="Download PDF">pdf</a>, <a href="/format/2402.16368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images  using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=M%C3%B6ller%2C+H">Hendrik M&#xf6;ller</a>, 
<a href="/search/eess?searchtype=author&query=Graf%2C+R">Robert Graf</a>, 
<a href="/search/eess?searchtype=author&query=Schmitt%2C+J">Joachim Schmitt</a>, 
<a href="/search/eess?searchtype=author&query=Keinert%2C+B">Benjamin Keinert</a>, 
<a href="/search/eess?searchtype=author&query=Atad%2C+M">Matan Atad</a>, 
<a href="/search/eess?searchtype=author&query=Sekuboyina%2C+A">Anjany Sekuboyina</a>, 
<a href="/search/eess?searchtype=author&query=Streckenbach%2C+F">Felix Streckenbach</a>, 
<a href="/search/eess?searchtype=author&query=Sch%C3%B6n%2C+H">Hanna Sch&#xf6;n</a>, 
<a href="/search/eess?searchtype=author&query=Kofler%2C+F">Florian Kofler</a>, 
<a href="/search/eess?searchtype=author&query=Kroencke%2C+T">Thomas Kroencke</a>, 
<a href="/search/eess?searchtype=author&query=Bette%2C+S">Stefanie Bette</a>, 
<a href="/search/eess?searchtype=author&query=Willich%2C+S">Stefan Willich</a>, 
<a href="/search/eess?searchtype=author&query=Keil%2C+T">Thomas Keil</a>, 
<a href="/search/eess?searchtype=author&query=Niendorf%2C+T">Thoralf Niendorf</a>, 
<a href="/search/eess?searchtype=author&query=Pischon%2C+T">Tobias Pischon</a>, 
<a href="/search/eess?searchtype=author&query=Endemann%2C+B">Beate Endemann</a>, 
<a href="/search/eess?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/eess?searchtype=author&query=Kirschke%2C+J+S">Jan S. Kirschke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/Hendrik-code/spineps">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Purpose. To present SPINEPS, an open-source deep learning approach for
semantic and instance segmentation of 14 spinal structures (ten vertebra
substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in
whole body T2w MRI.
<br />Methods. During this HIPPA-compliant, retrospective study, we utilized the
public SPIDER dataset (218 subjects, 63% female) and a subset of the German
National Cohort (1423 subjects, mean age 53, 49% female) for training and
evaluation. We combined CT and T2w segmentations to train models that segment
14 spinal structures in T2w sagittal scans both semantically and instance-wise.
Performance evaluation metrics included Dice similarity coefficient, average
symmetrical surface distance, panoptic quality, segmentation quality, and
recognition quality. Statistical significance was assessed using the Wilcoxon
signed-rank test. An in-house dataset was used to qualitatively evaluate
out-of-distribution samples.
<br />Results. On the public dataset, our approach outperformed the baseline
(instance-wise vertebra dice score 0.929 vs. 0.907, p-value&lt;0.001). Training on
auto-generated annotations and evaluating on manually corrected test data from
the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for
intervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER
dataset during training increased these scores to 0.920, 0.967, 0.958,
respectively.
<br />Conclusions. The proposed segmentation approach offers robust segmentation of
14 spinal structures in T2w sagittal images, including the spinal cord, spinal
canal, intervertebral discs, endplate, sacrum, and vertebrae. The approach
yields both a semantic and instance mask as output, thus being easy to utilize.
This marks the first publicly available algorithm for whole spine segmentation
in sagittal T2w MR imaging.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16377" title="Abstract">arXiv:2402.16377</a> (cross-list from math.AP) [<a href="/pdf/2402.16377" title="Download PDF">pdf</a>, <a href="/ps/2402.16377" title="Download PostScript">ps</a>, <a href="/format/2402.16377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation and perturbations of stable solutions to a stationary mean  field game system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berry%2C+J">Jules Berry</a> (IRMAR, INSA Rennes, UR), 
<a href="/search/math?searchtype=author&query=Ley%2C+O">Olivier Ley</a> (IRMAR), 
<a href="/search/math?searchtype=author&query=Silva%2C+F+J">Francisco J Silva</a> (XLIM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This work introduces a new general approach for the numerical analysis of
stable equilibria to second order mean field games systems in cases where the
uniqueness of solutions may fail. For the sake of simplicity, we focus on a
simple stationary case. We propose an abstract framework to study these
solutions by reformulating the mean field game system as an abstract equation
in a Banach space. In this context, stable equilibria turn out to be regular
solutions to this equation, meaning that the linearized system is well-posed.
We provide three applications of this property: we study the sensitivity
analysis of stable solutions, establish error estimates for their finite
element approximations, and prove the local converge of Newton's method in
infinite dimensions.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16380" title="Abstract">arXiv:2402.16380</a> (cross-list from eess.AS) [<a href="/pdf/2402.16380" title="Download PDF">pdf</a>, <a href="/format/2402.16380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Automated End-to-End Open-Source Software for High-Quality  Text-to-Speech Dataset Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gunduz%2C+A">Ahmet Gunduz</a>, 
<a href="/search/eess?searchtype=author&query=Yuksel%2C+K+A">Kamer Ali Yuksel</a>, 
<a href="/search/eess?searchtype=author&query=Darwish%2C+K">Kareem Darwish</a>, 
<a href="/search/eess?searchtype=author&query=Javadi%2C+G">Golara Javadi</a>, 
<a href="/search/eess?searchtype=author&query=Minazzi%2C+F">Fabio Minazzi</a>, 
<a href="/search/eess?searchtype=author&query=Sobieski%2C+N">Nicola Sobieski</a>, 
<a href="/search/eess?searchtype=author&query=Bratieres%2C+S">Sebastien Bratieres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 6 Figures, 4 Tables, LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data availability is crucial for advancing artificial intelligence
applications, including voice-based technologies. As content creation,
particularly in social media, experiences increasing demand, translation and
text-to-speech (TTS) technologies have become essential tools. Notably, the
performance of these TTS technologies is highly dependent on the quality of the
training data, emphasizing the mutual dependence of data availability and
technological progress. This paper introduces an end-to-end tool to generate
high-quality datasets for text-to-speech (TTS) models to address this critical
need for high-quality data. The contributions of this work are manifold and
include: the integration of language-specific phoneme distribution into sample
selection, automation of the recording process, automated and human-in-the-loop
quality assurance of recordings, and processing of recordings to meet specified
formats. The proposed application aims to streamline the dataset creation
process for TTS models through these features, thereby facilitating
advancements in voice-based technologies.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16384" title="Abstract">arXiv:2402.16384</a> (cross-list from cond-mat.supr-con) [<a href="/pdf/2402.16384" title="Download PDF">pdf</a>, <a href="/format/2402.16384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Superconductor Neuron with Ternary Synaptic Connections for  Ultra-Fast SNN Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ucpinar%2C+B+Z">Beyza Zeynep Ucpinar</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fayyazi%2C+A">Arash Fayyazi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kamal%2C+M">Mehdi Kamal</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pedram%2C+M">Massoud Pedram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Superconductivity (cond-mat.supr-con)</span>; Emerging Technologies (cs.ET); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">A novel high-fan-in differential superconductor neuron structure designed for
ultra-high-performance Spiking Neural Network (SNN) accelerators is presented.
Utilizing a high-fan-in neuron structure allows us to design SNN accelerators
with more synaptic connections, enhancing the overall network capabilities. The
proposed neuron design is based on superconductor electronics fabric,
incorporating multiple superconducting loops, each with two Josephson
Junctions. This arrangement enables each input data branch to have positive and
negative inductive coupling, supporting excitatory and inhibitory synaptic
data. Compatibility with synaptic devices and thresholding operation is
achieved using a single flux quantum (SFQ) pulse-based logic style. The neuron
design, along with ternary synaptic connections, forms the foundation for a
superconductor-based SNN inference. To demonstrate the capabilities of our
design, we train the SNN using snnTorch, augmenting the PyTorch framework.
After pruning, the demonstrated SNN inference achieves an impressive 96.1%
accuracy on MNIST images. Notably, the network exhibits a remarkable throughput
of 8.92 GHz while consuming only 1.5 nJ per inference, including the energy
consumption associated with cooling to 4K. These results underscore the
potential of superconductor electronics in developing high-performance and
ultra-energy-efficient neural network accelerator architectures.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16388" title="Abstract">arXiv:2402.16388</a> (cross-list from stat.ML) [<a href="/pdf/2402.16388" title="Download PDF">pdf</a>, <a href="/format/2402.16388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification in Anomaly Detection with Cross-Conformal  $p$-Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hennh%C3%B6fer%2C+O">Oliver Hennh&#xf6;fer</a>, 
<a href="/search/stat?searchtype=author&query=Preisach%2C+C">Christine Preisach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Given the growing significance of reliable, trustworthy, and explainable
machine learning, the requirement of uncertainty quantification for anomaly
detection systems has become increasingly important. In this context,
effectively controlling Type I error rates ($\alpha$) without compromising the
statistical power ($1-\beta$) of these systems can build trust and reduce costs
related to false discoveries, particularly when follow-up procedures are
expensive. Leveraging the principles of conformal prediction emerges as a
promising approach for providing respective statistical guarantees by
calibrating a model's uncertainty. This work introduces a novel framework for
anomaly detection, termed cross-conformal anomaly detection, building upon
well-known cross-conformal methods designed for prediction tasks. With that, it
addresses a natural research gap by extending previous works in the context of
inductive conformal anomaly detection, relying on the split-conformal approach
for model calibration. Drawing on insights from conformal prediction, we
demonstrate that the derived methods for calculating cross-conformal $p$-values
strike a practical compromise between statistical efficiency (full-conformal)
and computational efficiency (split-conformal) for uncertainty-quantified
anomaly detection on benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16394" title="Abstract">arXiv:2402.16394</a> (cross-list from eess.AS) [<a href="/pdf/2402.16394" title="Download PDF">pdf</a>, <a href="/format/2402.16394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based  Contextual Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hussain%2C+T">Tassadaq Hussain</a>, 
<a href="/search/eess?searchtype=author&query=Dashtipour%2C+K">Kia Dashtipour</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Hussain%2C+A">Amir Hussain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In real-world environments, background noise significantly degrades the
intelligibility and clarity of human speech. Audio-visual speech enhancement
(AVSE) attempts to restore speech quality, but existing methods often fall
short, particularly in dynamic noise conditions. This study investigates the
inclusion of emotion as a novel contextual cue within AVSE, hypothesizing that
incorporating emotional understanding can improve speech enhancement
performance. We propose a novel emotion-aware AVSE system that leverages both
auditory and visual information. It extracts emotional features from the facial
landmarks of the speaker and fuses them with corresponding audio and visual
modalities. This enriched data serves as input to a deep UNet-based
encoder-decoder network, specifically designed to orchestrate the fusion of
multimodal information enhanced with emotion. The network iteratively refines
the enhanced speech representation through an encoder-decoder architecture,
guided by perceptually-inspired loss functions for joint learning and
optimization. We train and evaluate the model on the CMU Multimodal Opinion
Sentiment and Emotion Intensity (CMU-MOSEI) dataset, a rich repository of
audio-visual recordings with annotated emotions. Our comprehensive evaluation
demonstrates the effectiveness of emotion as a contextual cue for AVSE. By
integrating emotional features, the proposed system achieves significant
improvements in both objective and subjective assessments of speech quality and
intelligibility, especially in challenging noise environments. Compared to
baseline AVSE and audio-only speech enhancement systems, our approach exhibits
a noticeable increase in PESQ and STOI, indicating higher perceptual quality
and intelligibility. Large-scale listening tests corroborate these findings,
suggesting improved human understanding of enhanced speech.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16408" title="Abstract">arXiv:2402.16408</a> (cross-list from stat.ML) [<a href="/pdf/2402.16408" title="Download PDF">pdf</a>, <a href="/format/2402.16408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Training of Normalizing Flows for High-dimensional Variational  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andrade%2C+D">Daniel Andrade</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Variational inference with normalizing flows (NFs) is an increasingly popular
alternative to MCMC methods. In particular, NFs based on coupling layers (Real
NVPs) are frequently used due to their good empirical performance. In theory,
increasing the depth of normalizing flows should lead to more accurate
posterior approximations. However, in practice, training deep normalizing flows
for approximating high-dimensional posterior distributions is often infeasible
due to the high variance of the stochastic gradients. In this work, we show
that previous methods for stabilizing the variance of stochastic gradient
descent can be insufficient to achieve stable training of Real NVPs. As the
source of the problem, we identify that, during training, samples often exhibit
unusual high values. As a remedy, we propose a combination of two methods: (1)
soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log
transformation of the samples. We evaluate these and other previously proposed
modification on several challenging target distributions, including a
high-dimensional horseshoe logistic regression model. Our experiments show that
with our modifications, stable training of Real NVPs for posteriors with
several thousand dimensions is possible, allowing for more accurate marginal
likelihood estimation via importance sampling. Moreover, we evaluate several
common training techniques and architecture choices and provide practical
advise for training NFs for high-dimensional variational inference.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16513" title="Abstract">arXiv:2402.16513</a> (cross-list from physics.optics) [<a href="/pdf/2402.16513" title="Download PDF">pdf</a>, <a href="/ps/2402.16513" title="Download PostScript">ps</a>, <a href="/format/2402.16513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Photonic Neural Network Fabricated on Thin Film Lithium Niobate for  High-Fidelity and Power-Efficient Matrix Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zheng%2C+Y">Yong Zheng</a>, 
<a href="/search/physics?searchtype=author&query=Wu%2C+R">Rongbo Wu</a>, 
<a href="/search/physics?searchtype=author&query=Ren%2C+Y">Yuan Ren</a>, 
<a href="/search/physics?searchtype=author&query=Bao%2C+R">Rui Bao</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ma%2C+Y">Yu Ma</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+M">Min Wang</a>, 
<a href="/search/physics?searchtype=author&query=Cheng%2C+Y">Ya Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages,10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Photonic neural networks (PNNs) have emerged as a promising platform to
address the energy consumption issue that comes with the advancement of
artificial intelligence technology, and thin film lithium niobate (TFLN) offers
an attractive solution as a material platform mainly for its combined
characteristics of low optical loss and large electro-optic (EO) coefficients.
Here, we present the first implementation of an EO tunable PNN based on the
TFLN platform. Our device features ultra-high fidelity, high computation speed,
and exceptional power efficiency. We benchmark the performance of our device
with several deep learning missions including in-situ training of Circle and
Moons nonlinear datasets classification, Iris flower species recognition, and
handwriting digits recognition. Our work paves the way for sustainable
up-scaling of high-speed, energy-efficient PNNs.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16541" title="Abstract">arXiv:2402.16541</a> (cross-list from quant-ph) [<a href="/pdf/2402.16541" title="Download PDF">pdf</a>, <a href="/format/2402.16541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integer Programming Using A Single Atom
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Goswami%2C+K">Kapil Goswami</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schmelcher%2C+P">Peter Schmelcher</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mukherjee%2C+R">Rick Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Optimization and Control (math.OC); Atomic Physics (physics.atom-ph)

</div>
<p class="mathjax">Integer programming (IP), as the name suggests is an integer-variable-based
approach commonly used to formulate real-world optimization problems with
constraints. Currently, quantum algorithms reformulate the IP into an
unconstrained form through the use of binary variables, which is an indirect
and resource-consuming way of solving it. We develop an algorithm that maps and
solves an IP problem in its original form to any quantum system that possesses
a large number of accessible internal degrees of freedom which can be
controlled with sufficient accuracy. Using a single Rydberg atom as an example,
we associate the integer values to electronic states belonging to different
manifolds and implement a selective superposition of these different states to
solve the full IP problem. The optimal solution is found within 2-40{\mu}s for
a few prototypical IP problems with up to eight variables and up to four
constraints including a non-linear IP problem, which is usually harder to solve
with classical algorithms when compared with linear IP problems. Our algorithm
for solving IP is benchmarked using the Branch &amp; Bound approach and it
outperforms the classical algorithm in terms of the number of steps needed to
converge and carries the potential to improve the bounds provided by the
classical algorithm for larger problems.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16543" title="Abstract">arXiv:2402.16543</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.16543" title="Download PDF">pdf</a>, <a href="/format/2402.16543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based deep reinforcement learning for accelerated learning from  flow simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Weiner%2C+A">Andre Weiner</a>, 
<a href="/search/physics?searchtype=author&query=Geise%2C+J">Janis Geise</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, deep reinforcement learning has emerged as a technique to
solve closed-loop flow control problems. Employing simulation-based
environments in reinforcement learning enables a priori end-to-end optimization
of the control system, provides a virtual testbed for safety-critical control
applications, and allows to gain a deep understanding of the control
mechanisms. While reinforcement learning has been applied successfully in a
number of rather simple flow control benchmarks, a major bottleneck toward
real-world applications is the high computational cost and turnaround time of
flow simulations. In this contribution, we demonstrate the benefits of
model-based reinforcement learning for flow control applications. Specifically,
we optimize the policy by alternating between trajectories sampled from flow
simulations and trajectories sampled from an ensemble of environment models.
The model-based learning reduces the overall training time by up to $85\%$ for
the fluidic pinball test case. Even larger savings are expected for more
demanding flow simulations.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16609" title="Abstract">arXiv:2402.16609</a> (cross-list from q-fin.PM) [<a href="/pdf/2402.16609" title="Download PDF">pdf</a>, <a href="/ps/2402.16609" title="Download PostScript">ps</a>, <a href="/format/2402.16609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Transformer based Deep Reinforcement Learning with  Black-Litterman Model for Portfolio Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a> (1), 
<a href="/search/q-fin?searchtype=author&query=Stefanidis%2C+A">Angelos Stefanidis</a> (2), 
<a href="/search/q-fin?searchtype=author&query=Jiang%2C+Z">Zhengyong Jiang</a> (2), 
<a href="/search/q-fin?searchtype=author&query=Su%2C+J">Jionglong Su</a> (2) ((1) Xi&#x27;an Jiaotong-Liverpool University, School of Mathematics and Physics, Department of Financial and Actuarial Mathematics (2) Xi&#x27;an Jiaotong-Liverpool University Entrepreneur College (Taicang), School of AI and Advanced Computing (1))
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As a model-free algorithm, deep reinforcement learning (DRL) agent learns and
makes decisions by interacting with the environment in an unsupervised way. In
recent years, DRL algorithms have been widely applied by scholars for portfolio
optimization in consecutive trading periods, since the DRL agent can
dynamically adapt to market changes and does not rely on the specification of
the joint dynamics across the assets. However, typical DRL agents for portfolio
optimization cannot learn a policy that is aware of the dynamic correlation
between portfolio asset returns. Since the dynamic correlations among portfolio
assets are crucial in optimizing the portfolio, the lack of such knowledge
makes it difficult for the DRL agent to maximize the return per unit of risk,
especially when the target market permits short selling (i.e., the US stock
market). In this research, we propose a hybrid portfolio optimization model
combining the DRL agent and the Black-Litterman (BL) model to enable the DRL
agent to learn the dynamic correlation between the portfolio asset returns and
implement an efficacious long/short strategy based on the correlation.
Essentially, the DRL agent is trained to learn the policy to apply the BL model
to determine the target portfolio weights. To test our DRL agent, we construct
the portfolio based on all the Dow Jones Industrial Average constitute stocks.
Empirical results of the experiments conducted on real-world United States
stock market data demonstrate that our DRL agent significantly outperforms
various comparison portfolio choice strategies and alternative DRL frameworks
by at least 42% in terms of accumulated return. In terms of the return per unit
of risk, our DRL agent significantly outperforms various comparative portfolio
choice strategies and alternative strategies based on other machine learning
frameworks.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16619" title="Abstract">arXiv:2402.16619</a> (cross-list from eess.IV) [<a href="/pdf/2402.16619" title="Download PDF">pdf</a>, <a href="/ps/2402.16619" title="Download PostScript">ps</a>, <a href="/format/2402.16619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magnetic resonance delta radiomics to track radiation response in lung  tumors receiving stereotactic MRI-guided radiotherapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zha%2C+Y">Yining Zha</a> (1 and 2 and 3), 
<a href="/search/eess?searchtype=author&query=Kann%2C+B+H">Benjamin H. Kann</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Ye%2C+Z">Zezhong Ye</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Zapaishchykova%2C+A">Anna Zapaishchykova</a> (1 and 2 and 4), 
<a href="/search/eess?searchtype=author&query=He%2C+J">John He</a> (2), 
<a href="/search/eess?searchtype=author&query=Hsu%2C+S">Shu-Hui Hsu</a> (2), 
<a href="/search/eess?searchtype=author&query=Leeman%2C+J+E">Jonathan E. Leeman</a> (2), 
<a href="/search/eess?searchtype=author&query=Fitzgerald%2C+K+J">Kelly J. Fitzgerald</a> (2), 
<a href="/search/eess?searchtype=author&query=Kozono%2C+D+E">David E. Kozono</a> (2), 
<a href="/search/eess?searchtype=author&query=Mak%2C+R+H">Raymond H. Mak</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Aerts%2C+H+J+W+L">Hugo J.W.L. Aerts</a> (1 and 2 and 4 and 5) ((1) Artificial Intelligence in Medicine Program, Mass General Brigham, Harvard Medical School, Boston, MA, USA, (2) Department of Radiation Oncology, Dana-Farber Cancer Institute and Brigham and Women&#x27;s Hospital, Harvard Medical School, Boston, MA, USA, (3) Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA, USA, (4) Radiology and Nuclear Medicine, CARIM &amp; GROW, Maastricht University, Maastricht, the Netherlands, (5) Department of Radiology, Brigham and Women&#x27;s Hospital, Dana-Farber Cancer Institute, Harvard Medical School, Boston, MA, USA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Introduction: Lung cancer is a leading cause of cancer-related mortality, and
stereotactic body radiotherapy (SBRT) has become a standard treatment for
early-stage lung cancer. However, the heterogeneous response to radiation at
the tumor level poses challenges. Currently, standardized dosage regimens lack
adaptation based on individual patient or tumor characteristics. Thus, we
explore the potential of delta radiomics from on-treatment magnetic resonance
(MR) imaging to track radiation dose response, inform personalized radiotherapy
dosing, and predict outcomes. Methods: A retrospective study of 47 MR-guided
lung SBRT treatments for 39 patients was conducted. Radiomic features were
extracted using Pyradiomics, and stability was evaluated temporally and
spatially. Delta radiomics were correlated with radiation dose delivery and
assessed for associations with tumor control and survival with Cox regressions.
Results: Among 107 features, 49 demonstrated temporal stability, and 57 showed
spatial stability. Fifteen stable and non-collinear features were analyzed.
Median Skewness and surface to volume ratio decreased with radiation dose
fraction delivery, while coarseness and 90th percentile values increased.
Skewness had the largest relative median absolute changes (22%-45%) per
fraction from baseline and was associated with locoregional failure (p=0.012)
by analysis of covariance. Skewness, Elongation, and Flatness were
significantly associated with local recurrence-free survival, while tumor
diameter and volume were not. Conclusions: Our study establishes the
feasibility and stability of delta radiomics analysis for MR-guided lung SBRT.
Findings suggest that MR delta radiomics can capture short-term radiographic
manifestations of intra-tumoral radiation effect.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16634" title="Abstract">arXiv:2402.16634</a> (cross-list from eess.IV) [<a href="/pdf/2402.16634" title="Download PDF">pdf</a>, <a href="/format/2402.16634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Skull-Stripping Performance for Pediatric Brain Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kelley%2C+W">William Kelley</a>, 
<a href="/search/eess?searchtype=author&query=Ngo%2C+N">Nathan Ngo</a>, 
<a href="/search/eess?searchtype=author&query=Dalca%2C+A+V">Adrian V. Dalca</a>, 
<a href="/search/eess?searchtype=author&query=Fischl%2C+B">Bruce Fischl</a>, 
<a href="/search/eess?searchtype=author&query=Z%C3%B6llei%2C+L">Lilla Z&#xf6;llei</a>, 
<a href="/search/eess?searchtype=author&query=Hoffmann%2C+M">Malte Hoffmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, 1 table, skull-stripping, brain extraction, newborn, infant, toddler, pediatric MRI, machine learning, accepted by the IEEE International Symposium on Biomedical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Skull-stripping is the removal of background and non-brain anatomical
features from brain images. While many skull-stripping tools exist, few target
pediatric populations. With the emergence of multi-institutional pediatric data
acquisition efforts to broaden the understanding of perinatal brain
development, it is essential to develop robust and well-tested tools ready for
the relevant data processing. However, the broad range of neuroanatomical
variation in the developing brain, combined with additional challenges such as
high motion levels, as well as shoulder and chest signal in the images, leaves
many adult-specific tools ill-suited for pediatric skull-stripping. Building on
an existing framework for robust and accurate skull-stripping, we propose
developmental SynthStrip (d-SynthStrip), a skull-stripping model tailored to
pediatric images. This framework exposes networks to highly variable images
synthesized from label maps. Our model substantially outperforms pediatric
baselines across scan types and age cohorts. In addition, the &lt;1-minute runtime
of our tool compares favorably to the fastest baselines. We distribute our
model at https://w3id.org/synthstrip.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16643" title="Abstract">arXiv:2402.16643</a> (cross-list from math.CO) [<a href="/pdf/2402.16643" title="Download PDF">pdf</a>, <a href="/format/2402.16643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-projective two-weight codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kurz%2C+S">Sascha Kurz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">It has been known since the 1970's that the difference of the non-zero
weights of a projective $\mathbb{F}_q$-linear two-weight has to be a power of
the characteristic of the underlying field. Here we study non-projective
two-weight codes and e.g.\ show the same result under mild extra conditions.
For small dimensions we give exhaustive enumerations of the feasible parameters
in the binary case.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16658" title="Abstract">arXiv:2402.16658</a> (cross-list from eess.IV) [<a href="/pdf/2402.16658" title="Download PDF">pdf</a>, <a href="/format/2402.16658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Learning for Deformable Image Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Grewal%2C+M">Monika Grewal</a>, 
<a href="/search/eess?searchtype=author&query=Westerveld%2C+H">Henrike Westerveld</a>, 
<a href="/search/eess?searchtype=author&query=Bosman%2C+P+A+N">Peter A. N. Bosman</a>, 
<a href="/search/eess?searchtype=author&query=Alderliesten%2C+T">Tanja Alderliesten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deformable image registration (DIR) involves optimization of multiple
conflicting objectives, however, not many existing DIR algorithms are
multi-objective (MO). Further, while there has been progress in the design of
deep learning algorithms for DIR, there is no work in the direction of MO DIR
using deep learning. In this paper, we fill this gap by combining a recently
proposed approach for MO training of neural networks with a well-known deep
neural network for DIR and create a deep learning based MO DIR approach. We
evaluate the proposed approach for DIR of pelvic magnetic resonance imaging
(MRI) scans. We experimentally demonstrate that the proposed MO DIR approach --
providing multiple registration outputs for each patient that each correspond
to a different trade-off between the objectives -- has additional desirable
properties from a clinical use point-of-view as compared to providing a single
DIR output. The experiments also show that the proposed MO DIR approach
provides a better spread of DIR outputs across the entire trade-off front than
simply training multiple neural networks with weights for each objective
sampled from a grid of possible values.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16661" title="Abstract">arXiv:2402.16661</a> (cross-list from stat.ML) [<a href="/pdf/2402.16661" title="Download PDF">pdf</a>, <a href="/format/2402.16661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penalized Generative Variable Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+J">Jian Huang</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+S">Shuangge Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Deep networks are increasingly applied to a wide variety of data, including
data with high-dimensional predictors. In such analysis, variable selection can
be needed along with estimation/model building. Many of the existing deep
network studies that incorporate variable selection have been limited to
methodological and numerical developments. In this study, we consider
modeling/estimation using the conditional Wasserstein Generative Adversarial
networks. Group Lasso penalization is applied for variable selection, which may
improve model estimation/prediction, interpretability, stability, etc.
Significantly advancing from the existing literature, the analysis of censored
survival data is also considered. We establish the convergence rate for
variable selection while considering the approximation error, and obtain a more
efficient distribution estimation. Simulations and the analysis of real
experimental data demonstrate satisfactory practical utility of the proposed
analysis.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16663" title="Abstract">arXiv:2402.16663</a> (cross-list from eess.IV) [<a href="/pdf/2402.16663" title="Download PDF">pdf</a>, <a href="/format/2402.16663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Q">Qing Xu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+Y">Yixuan Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In digital pathology, precise nuclei segmentation is pivotal yet challenged
by the diversity of tissue types, staining protocols, and imaging conditions.
Recently, the segment anything model (SAM) revealed overwhelming performance in
natural scenarios and impressive adaptation to medical imaging. Despite these
advantages, the reliance of labor-intensive manual annotation as segmentation
prompts severely hinders their clinical applicability, especially for nuclei
image analysis containing massive cells where dense manual prompts are
impractical. To overcome the limitations of current SAM methods while retaining
the advantages, we propose the Universal prompt-free SAM framework for Nuclei
segmentation (UN-SAM), by providing a fully automated solution with remarkable
generalization capabilities. Specifically, to eliminate the labor-intensive
requirement of per-nuclei annotations for prompt, we devise a multi-scale
Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by
automatically generating high-quality mask hints to guide the segmentation
tasks. Moreover, to unleash the generalization capability of SAM across a
variety of nuclei images, we devise a Domain-adaptive Tuning Encoder
(DT-Encoder) to seamlessly harmonize visual features with domain-common and
domain-specific knowledge, and further devise a Domain Query-enhanced Decoder
(DQ-Decoder) by leveraging learnable domain queries for segmentation decoding
in different nuclei domains. Extensive experiments prove that UN-SAM with
exceptional performance surpasses state-of-the-arts in nuclei instance and
semantic segmentation, especially the generalization capability in zero-shot
scenarios. The source code is available at
https://github.com/CUHK-AIM-Group/UN-SAM.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16683" title="Abstract">arXiv:2402.16683</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.16683" title="Download PDF">pdf</a>, <a href="/format/2402.16683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library  for Gaussian Processes and Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Edenhofer%2C+G">Gordian Edenhofer</a>, 
<a href="/search/astro-ph?searchtype=author&query=Frank%2C+P">Philipp Frank</a>, 
<a href="/search/astro-ph?searchtype=author&query=Roth%2C+J">Jakob Roth</a>, 
<a href="/search/astro-ph?searchtype=author&query=Leike%2C+R+H">Reimar H. Leike</a>, 
<a href="/search/astro-ph?searchtype=author&query=Guerdi%2C+M">Massin Guerdi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Scheel-Platz%2C+L+I">Lukas I. Scheel-Platz</a>, 
<a href="/search/astro-ph?searchtype=author&query=Guardiani%2C+M">Matteo Guardiani</a>, 
<a href="/search/astro-ph?searchtype=author&query=Eberle%2C+V">Vincent Eberle</a>, 
<a href="/search/astro-ph?searchtype=author&query=Westerkamp%2C+M">Margret Westerkamp</a>, 
<a href="/search/astro-ph?searchtype=author&query=En%C3%9Flin%2C+T+A">Torsten A. En&#xdf;lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Imaging is the process of transforming noisy, incomplete data into a space
that humans can interpret. NIFTy is a Bayesian framework for imaging and has
already successfully been applied to many fields in astrophysics. Previous
design decisions held the performance and the development of methods in NIFTy
back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the
modeling principle, extends the inference strategies, and outsources much of
the heavy lifting to JAX. The rewrite dramatically accelerates models written
in NIFTy, lays the foundation for new types of inference machineries, improves
maintainability, and enables interoperability between NIFTy and the JAX machine
learning ecosystem.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16688" title="Abstract">arXiv:2402.16688</a> (cross-list from stat.ML) [<a href="/pdf/2402.16688" title="Download PDF">pdf</a>, <a href="/ps/2402.16688" title="Download PostScript">ps</a>, <a href="/format/2402.16688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the connection between Noise-Contrastive Estimation and Contrastive  Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Olmin%2C+A">Amanda Olmin</a>, 
<a href="/search/stat?searchtype=author&query=Lindqvist%2C+J">Jakob Lindqvist</a>, 
<a href="/search/stat?searchtype=author&query=Svensson%2C+L">Lennart Svensson</a>, 
<a href="/search/stat?searchtype=author&query=Lindsten%2C+F">Fredrik Lindsten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Noise-contrastive estimation (NCE) is a popular method for estimating
unnormalised probabilistic models, such as energy-based models, which are
effective for modelling complex data distributions. Unlike classical maximum
likelihood (ML) estimation that relies on importance sampling (resulting in
ML-IS) or MCMC (resulting in contrastive divergence, CD), NCE uses a proxy
criterion to avoid the need for evaluating an often intractable normalisation
constant.
<br />Despite apparent conceptual differences, we show that two NCE criteria,
ranking NCE (RNCE) and conditional NCE (CNCE), can be viewed as ML estimation
methods. Specifically, RNCE is equivalent to ML estimation combined with
conditional importance sampling, and both RNCE and CNCE are special cases of
CD. These findings bridge the gap between the two method classes and allow us
to apply techniques from the ML-IS and CD literature to NCE, offering several
advantageous extensions.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16706" title="Abstract">arXiv:2402.16706</a> (cross-list from astro-ph.SR) [<a href="/pdf/2402.16706" title="Download PDF">pdf</a>, <a href="/format/2402.16706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance of high-order Godunov-type methods in simulations of  astrophysical low Mach number flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Leidi%2C+G">G. Leidi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Andrassy%2C+R">R. Andrassy</a>, 
<a href="/search/astro-ph?searchtype=author&query=Barsukow%2C+W">W. Barsukow</a>, 
<a href="/search/astro-ph?searchtype=author&query=Higl%2C+J">J. Higl</a>, 
<a href="/search/astro-ph?searchtype=author&query=Edelmann%2C+P+V+F">P. V. F. Edelmann</a>, 
<a href="/search/astro-ph?searchtype=author&query=R%C3%B6pke%2C+F+K">F. K. R&#xf6;pke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Numerical Analysis (math.NA); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">High-order Godunov methods for gas dynamics have become a standard tool for
simulating different classes of astrophysical flows. Their accuracy is mostly
determined by the spatial interpolant used to reconstruct the pair of Riemann
states at cell interfaces and by the Riemann solver that computes the interface
fluxes. In most Godunov-type methods, these two steps can be treated
independently, so that many different schemes can in principle be built from
the same numerical framework. In this work, we use our fully compressible
Seven-League Hydro (SLH) code to test the accuracy of six reconstruction
methods and three approximate Riemann solvers on two- and three-dimensional (2D
and 3D) problems involving subsonic flows only. We consider Mach numbers in the
range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin--Helmholtz
instability problem and a 3D turbulent convection zone that excites internal
gravity waves in an overlying stable layer. We find that (i) there is a spread
of almost four orders of magnitude in computational cost per fixed accuracy
between the methods tested in this study, with the most performant method being
a combination of a "low-dissipation" Riemann solver and a sextic reconstruction
scheme, (ii) the low-dissipation solver always outperforms conventional Riemann
solvers on a fixed grid when the reconstruction scheme is kept the same, (iii)
in simulations of turbulent flows, increasing the order of spatial
reconstruction reduces the characteristic dissipation length scale achieved on
a given grid even if the overall scheme is only second order accurate, (iv)
reconstruction methods based on slope-limiting techniques tend to generate
artificial, high-frequency acoustic waves during the evolution of the flow, (v)
unlimited reconstruction methods introduce oscillations in the thermal
stratification near the convective boundary, where the entropy gradient is
steep.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16712" title="Abstract">arXiv:2402.16712</a> (cross-list from stat.ML) [<a href="/pdf/2402.16712" title="Download PDF">pdf</a>, <a href="/format/2402.16712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Robust Sparse Principal Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ling%2C+X">Xiao Ling</a>, 
<a href="/search/stat?searchtype=author&query=Brooks%2C+P">Paul Brooks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this work, we propose an optimization framework for estimating a sparse
robust one-dimensional subspace. Our objective is to minimize both the
representation error and the penalty, in terms of the l1-norm criterion. Given
that the problem is NP-hard, we introduce a linear relaxation-based approach.
Additionally, we present a novel fitting procedure, utilizing simple ratios and
sorting techniques. The proposed algorithm demonstrates a worst-case time
complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global
optimality for the sparse robust subspace, thereby exhibiting polynomial time
efficiency. Compared to extant methodologies, the proposed algorithm finds the
subspace with the lowest discordance, offering a smoother trade-off between
sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold
improvement in computational speeds for matrices of 2000x2000 over CPU version.
Furthermore, this method is distinguished by several advantages, including its
independence from initialization and deterministic and replicable procedures.
Furthermore, this method is distinguished by several advantages, including its
independence from initialization and deterministic and replicable procedures.
The real-world example demonstrates the effectiveness of algorithm in achieving
meaningful sparsity, underscoring its precise and useful application across
various domains.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16714" title="Abstract">arXiv:2402.16714</a> (cross-list from quant-ph) [<a href="/pdf/2402.16714" title="Download PDF">pdf</a>, <a href="/format/2402.16714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum linear algebra is all you need for Transformer architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Guo%2C+N">Naixu Guo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yu%2C+Z">Zhan Yu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Agrawal%2C+A">Aman Agrawal</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rebentrost%2C+P">Patrick Rebentrost</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures, 2 tables, comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Generative machine learning methods such as large-language models are
revolutionizing the creation of text and images. While these models are
powerful they also harness a large amount of computational resources. The
transformer is a key component in large language models that aims to generate a
suitable completion of a given partial sequence. In this work, we investigate
transformer architectures under the lens of fault-tolerant quantum computing.
The input model is one where pre-trained weight matrices are given as block
encodings to construct the query, key, and value matrices for the transformer.
As a first step, we show how to prepare a block encoding of the self-attention
matrix, with a row-wise application of the softmax function using the Hadamard
product. In addition, we combine quantum subroutines to construct important
building blocks in the transformer, the residual connection, layer
normalization, and the feed-forward neural network. Our subroutines prepare an
amplitude encoding of the transformer output, which can be measured to obtain a
prediction. We discuss the potential and challenges for obtaining a quantum
advantage.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16718" title="Abstract">arXiv:2402.16718</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.16718" title="Download PDF">pdf</a>, <a href="/ps/2402.16718" title="Download PostScript">ps</a>, <a href="/format/2402.16718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Overview of the Development of Stereotactic Body Radiation Therapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zong%2C+Y">Yanqi Zong</a>, 
<a href="/search/physics?searchtype=author&query=Cui%2C+Z">Zhengrong Cui</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+L">Luqi Lin</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+S">Sihao Wang</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Y">Yizhi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stereotactic body radiation therapy (SBRT) refers to focusing high-energy
rays in three-dimensional space on the tumor lesion area, reducing the dose
received by surrounding normal tissues, which can effectively improve the local
control rate of the tumor and reduce the probability of complications. With the
comprehensive development of medical imaging, radiation biology and other
disciplines, this less-fractional, high-dose radiotherapy method has been
increasingly developed and applied in clinical practice. The background,
radio-biological basis, key technologies and main equipment of SBRT are
discussed, and its future development direction is prospected.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16719" title="Abstract">arXiv:2402.16719</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2402.16719" title="Download PDF">pdf</a>, <a href="/ps/2402.16719" title="Download PostScript">ps</a>, <a href="/format/2402.16719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix denoising: Bayes-optimal estimators via low-degree polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Semerjian%2C+G">Guilhem Semerjian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">We consider the additive version of the matrix denoising problem, where a
random symmetric matrix $S$ of size $n$ has to be inferred from the observation
of $Y=S+Z$, with $Z$ an independent random matrix modeling a noise. For prior
distributions of $S$ and $Z$ that are invariant under conjugation by orthogonal
matrices we determine, using results from first and second order free
probability theory, the Bayes-optimal (in terms of the mean square error)
polynomial estimators of degree at most $D$, asymptotically in $n$, and show
that as $D$ increases they converge towards the estimator introduced by Bun,
Allez, Bouchaud and Potters in [IEEE Transactions on Information Theory {\bf
62}, 7475 (2016)]. We conjecture that this optimality holds beyond strictly
orthogonally invariant priors, and provide partial evidences of this
universality phenomenon when $S$ is an arbitrary Wishart matrix and $Z$ is
drawn from the Gaussian Orthogonal Ensemble, a case motivated by the related
extensive rank matrix factorization problem.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16734" title="Abstract">arXiv:2402.16734</a> (cross-list from eess.IV) [<a href="/pdf/2402.16734" title="Download PDF">pdf</a>, <a href="/format/2402.16734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Robustness of Vision Transformers against Label Noise  in Medical Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khanal%2C+B">Bidur Khanal</a>, 
<a href="/search/eess?searchtype=author&query=Shrestha%2C+P">Prashant Shrestha</a>, 
<a href="/search/eess?searchtype=author&query=Amgain%2C+S">Sanskar Amgain</a>, 
<a href="/search/eess?searchtype=author&query=Khanal%2C+B">Bishesh Khanal</a>, 
<a href="/search/eess?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>, 
<a href="/search/eess?searchtype=author&query=Linte%2C+C+A">Cristian A. Linte</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Label noise in medical image classification datasets significantly hampers
the training of supervised deep learning methods, undermining their
generalizability. The test performance of a model tends to decrease as the
label noise rate increases. Over recent years, several methods have been
proposed to mitigate the impact of label noise in medical image classification
and enhance the robustness of the model. Predominantly, these works have
employed CNN-based architectures as the backbone of their classifiers for
feature extraction. However, in recent years, Vision Transformer (ViT)-based
backbones have replaced CNNs, demonstrating improved performance and a greater
ability to learn more generalizable features, especially when the dataset is
large. Nevertheless, no prior work has rigorously investigated how
transformer-based backbones handle the impact of label noise in medical image
classification. In this paper, we investigate the architectural robustness of
ViT against label noise and compare it to that of CNNs. We use two medical
image classification datasets -- COVID-DU-Ex, and NCT-CRC-HE-100K -- both
corrupted by injecting label noise at various rates. Additionally, we show that
pretraining is crucial for ensuring ViT's improved robustness against label
noise in supervised training.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16747" title="Abstract">arXiv:2402.16747</a> (cross-list from physics.bio-ph) [<a href="/pdf/2402.16747" title="Download PDF">pdf</a>, <a href="/ps/2402.16747" title="Download PostScript">ps</a>, <a href="/format/2402.16747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent progress in the physical principles of dynamic ground  self-righting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+C">Chen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in review at Integrative &amp; Comparative Biology. arXiv admin note: text overlap with <a href="/abs/2304.04603">arXiv:2304.04603</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biological Physics (physics.bio-ph)</span>; Systems and Control (eess.SY); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Animals and robots must be able to self-right on the ground after flipping
over to survive or operate. Biology research has described various strategies
and measured motor control patterns in many species. Robotics research has
designed many mechanisms to enable ground self-righting. However, the physical
principles governing ground self-righting transitions are relatively less
known, except limited understanding in turtles with rigid shell in two
dimensions. Here I review recent progress which I have led in advancing this
understanding using cockroaches as model organisms, by integrating biology
experiments, robotic modeling, and physics modeling.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16763" title="Abstract">arXiv:2402.16763</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.16763" title="Download PDF">pdf</a>, <a href="/format/2402.16763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order from chaos: Interplay of development and learning in recurrent  networks of structured neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kriener%2C+L">Laura Kriener</a>, 
<a href="/search/q-bio?searchtype=author&query=V%C3%B6lk%2C+K">Kristin V&#xf6;lk</a>, 
<a href="/search/q-bio?searchtype=author&query=von+H%C3%BCnerbein%2C+B">Ben von H&#xfc;nerbein</a>, 
<a href="/search/q-bio?searchtype=author&query=Benitez%2C+F">Federico Benitez</a>, 
<a href="/search/q-bio?searchtype=author&query=Senn%2C+W">Walter Senn</a>, 
<a href="/search/q-bio?searchtype=author&query=Petrovici%2C+M+A">Mihai A. Petrovici</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Behavior can be described as a temporal sequence of actions driven by neural
activity. To learn complex sequential patterns in neural networks, memories of
past activities need to persist on significantly longer timescales than
relaxation times of single-neuron activity. While recurrent networks can
produce such long transients, training these networks in a biologically
plausible way is challenging. One approach has been reservoir computing, where
only weights from a recurrent network to a readout are learned. Other models
achieve learning of recurrent synaptic weights using propagated errors.
However, their biological plausibility typically suffers from issues with
locality, resource allocation or parameter scales and tuning. We suggest that
many of these issues can be alleviated by considering dendritic information
storage and computation. By applying a fully local, always-on plasticity rule
we are able to learn complex sequences in a recurrent network comprised of two
populations. Importantly, our model is resource-efficient, enabling the
learning of complex sequences using only a small number of neurons. We
demonstrate these features in a mock-up of birdsong learning, in which our
networks first learn a long, non-Markovian sequence that they can then
reproduce robustly despite external disturbances.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16764" title="Abstract">arXiv:2402.16764</a> (cross-list from math.ST) [<a href="/pdf/2402.16764" title="Download PDF">pdf</a>, <a href="/format/2402.16764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiased LASSO under Poisson-Gauss Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abdalla%2C+P">Pedro Abdalla</a>, 
<a href="/search/math?searchtype=author&query=Kur%2C+G">Gil Kur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
<p class="mathjax">Quantifying uncertainty in high-dimensional sparse linear regression is a
fundamental task in statistics that arises in various applications. One of the
most successful methods for quantifying uncertainty is the debiased LASSO,
which has a solid theoretical foundation but is restricted to settings where
the noise is purely additive. Motivated by real-world applications, we study
the so-called Poisson inverse problem with additive Gaussian noise and propose
a debiased LASSO algorithm that only requires $n \gg s\log^2p$ samples, which
is optimal up to a logarithmic factor.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16770" title="Abstract">arXiv:2402.16770</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.16770" title="Download PDF">pdf</a>, <a href="/format/2402.16770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Population Geometry and Optimal Coding of Tasks with Shared  Latent Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wakhloo%2C+A+J">Albert J. Wakhloo</a>, 
<a href="/search/q-bio?searchtype=author&query=Slatton%2C+W">Will Slatton</a>, 
<a href="/search/q-bio?searchtype=author&query=Chung%2C+S">SueYeon Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 Pages and 7 figures in main text. 15 Pages and 7 figures in supplemental material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Humans and animals can recognize latent structures in their environment and
apply this information to efficiently navigate the world. Several recent works
argue that the brain supports these abilities by forming neural representations
that encode such latent structures in flexible, generalizable ways. However, it
remains unclear what aspects of neural population activity are contributing to
these computational capabilities. Here, we develop an analytical theory linking
the mesoscopic statistics of a neural population's activity to generalization
performance on a multi-task learning problem. To do this, we rely on a
generative model in which different tasks depend on a common, unobserved latent
structure and predictions are formed from a linear readout of a neural
population's activity. We show that three geometric measures of the population
activity determine generalization performance in these settings. Using this
theory, we find that experimentally observed factorized (or disentangled)
representations naturally emerge as an optimal solution to the multi-task
learning problem. We go on to show that when data is scarce, optimal codes
compress less informative latent variables, and when data is abundant, optimal
codes expand this information in the state space. We validate predictions from
our theory using biological and artificial neural network data. Our results
therefore tie neural population geometry to the multi-task learning problem and
make normative predictions of the structure of population activity in these
settings.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16771" title="Abstract">arXiv:2402.16771</a> (cross-list from econ.TH) [<a href="/pdf/2402.16771" title="Download PDF">pdf</a>, <a href="/format/2402.16771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wisdom and Foolishness of Noisy Matching Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Peng%2C+K">Kenny Peng</a>, 
<a href="/search/econ?searchtype=author&query=Garg%2C+N">Nikhil Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Probability (math.PR)

</div>
<p class="mathjax">We consider a many-to-one matching market where colleges share true
preferences over students but make decisions using only independent noisy
rankings. Each student has a true value $v$, but each college $c$ ranks the
student according to an independently drawn estimated value $v + X_c$ for
$X_c\sim \mathcal{D}.$ We ask a basic question about the resulting stable
matching: How noisy is the set of matched students? Two striking effects can
occur in large markets (i.e., with a continuum of students and a large number
of colleges). When $\mathcal{D}$ is light-tailed, noise is fully attenuated:
only the highest-value students are matched. When $\mathcal{D}$ is long-tailed,
noise is fully amplified: students are matched uniformly at random. These
results hold for any distribution of student preferences over colleges, and
extend to when only subsets of colleges agree on true student valuations
instead of the entire market. More broadly, our framework provides a tractable
approach to analyze implications of imperfect preference formation in large
markets.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16782" title="Abstract">arXiv:2402.16782</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.16782" title="Download PDF">pdf</a>, <a href="/format/2402.16782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiplex measures for higher-order networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lotito%2C+Q+F">Quintino Francesco Lotito</a>, 
<a href="/search/physics?searchtype=author&query=Montresor%2C+A">Alberto Montresor</a>, 
<a href="/search/physics?searchtype=author&query=Battiston%2C+F">Federico Battiston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">A wide variety of complex systems are characterized by interactions of
different types involving varying numbers of units. Multiplex hypergraphs serve
as a tool to describe such structures, capturing distinct types of higher-order
interactions among a collection of units. In this work, we introduce a
comprehensive set of measures to describe structural connectivity patterns in
multiplex hypergraphs, considering scales from node and hyperedge levels to the
system's mesoscale. We validate our measures with three real-world datasets:
scientific co-authorship in physics, movie collaborations, and high school
interactions. This validation reveals new collaboration patterns, identifies
trends within and across movie subfields, and provides insights into daily
interaction dynamics. Our framework aims to offer a more nuanced
characterization of real-world systems marked by both multiplex and
higher-order interactions.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16792" title="Abstract">arXiv:2402.16792</a> (cross-list from stat.ML) [<a href="/pdf/2402.16792" title="Download PDF">pdf</a>, <a href="/format/2402.16792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Optimal Rank Aggregation with Private Pairwise Rankings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+S">Shirong Xu</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+W+W">Will Wei Sun</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In various real-world scenarios like recommender systems and political
surveys, pairwise rankings are commonly collected and utilized for rank
aggregation to obtain an overall ranking of items. However, preference rankings
can reveal individuals' personal preferences, underscoring the need to protect
them before releasing for downstream analysis. In this paper, we address the
challenge of preserving privacy while ensuring the utility of rank aggregation
based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model.
Using the randomized response mechanism to perturb raw pairwise rankings is a
common privacy protection strategy used in practice, but a critical challenge
arises because the privatized rankings no longer adhere to the BTL model,
resulting in significant bias in downstream rank aggregation tasks. Motivated
from this, we propose a debiased randomized response mechanism to protect the
raw pairwise rankings, ensuring consistent estimation of true preferences and
rankings in downstream rank aggregation. Theoretically, we offer insights into
the relationship between overall privacy guarantees and estimation errors from
private ranking data, and establish minimax rates for estimation errors. This
enables the determination of optimal privacy guarantees that balance
consistency in rank aggregation with robust privacy protection. We also
investigate convergence rates of expected ranking errors for partial and full
ranking recovery, quantifying how privacy protection influences the
specification of top-$K$ item sets and complete rankings. Our findings are
validated through extensive simulations and a real application.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16793" title="Abstract">arXiv:2402.16793</a> (cross-list from math.ST) [<a href="/pdf/2402.16793" title="Download PDF">pdf</a>, <a href="/format/2402.16793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Failures and Successes of Cross-Validation for Early-Stopped Gradient  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Patil%2C+P">Pratik Patil</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+Y">Yuchen Wu</a>, 
<a href="/search/math?searchtype=author&query=Tibshirani%2C+R+J">Ryan J. Tibshirani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 76 pages, 27 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We analyze the statistical properties of generalized cross-validation (GCV)
and leave-one-out cross-validation (LOOCV) applied to early-stopped gradient
descent (GD) in high-dimensional least squares regression. We prove that GCV is
generically inconsistent as an estimator of the prediction risk of
early-stopped GD, even for a well-specified linear model with isotropic
features. In contrast, we show that LOOCV converges uniformly along the GD
trajectory to the prediction risk. Our theory requires only mild assumptions on
the data distribution and does not require the underlying regression function
to be linear. Furthermore, by leveraging the individual LOOCV errors, we
construct consistent estimators for the entire prediction error distribution
along the GD trajectory and consistent estimators for a wide class of error
functionals. This in particular enables the construction of pathwise prediction
intervals based on GD iterates that have asymptotically correct nominal
coverage conditional on the training data.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16811" title="Abstract">arXiv:2402.16811</a> (cross-list from stat.ML) [<a href="/pdf/2402.16811" title="Download PDF">pdf</a>, <a href="/format/2402.16811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stopping Bayesian Optimization with Probabilistic Regret Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wilson%2C+J+T">James T. Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian optimization is a popular framework for efficiently finding
high-quality solutions to difficult problems based on limited prior
information. As a rule, these algorithms operate by iteratively choosing what
to try next until some predefined budget has been exhausted. We investigate
replacing this de facto stopping rule with an $(\epsilon, \delta)$-criterion:
stop when a solution has been found whose value is within $\epsilon &gt; 0$ of the
optimum with probability at least $1 - \delta$ under the model. Given access to
the prior distribution of problems, we show how to verify this condition in
practice using a limited number of draws from the posterior. For Gaussian
process priors, we prove that Bayesian optimization with the proposed criterion
stops in finite time and returns a point that satisfies the $(\epsilon,
\delta)$-criterion under mild assumptions. These findings are accompanied by
extensive empirical results which demonstrate the strengths and weaknesses of
this approach.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16830" title="Abstract">arXiv:2402.16830</a> (cross-list from eess.AS) [<a href="/pdf/2402.16830" title="Download PDF">pdf</a>, <a href="/format/2402.16830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SKILL: Similarity-aware Knowledge distILLation for Speech  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zampierin%2C+L">Luca Zampierin</a>, 
<a href="/search/eess?searchtype=author&query=Hacene%2C+G+B">Ghouthi Boukli Hacene</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+B">Bac Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Self-supervision in Audio, Speech and Beyond (SASB) Workshop at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Self-supervised learning (SSL) has achieved remarkable success across various
speech-processing tasks. To enhance its efficiency, previous works often
leverage the use of compression techniques. A notable recent attempt is
DPHuBERT, which applies joint knowledge distillation (KD) and structured
pruning to learn a significantly smaller SSL model. In this paper, we
contribute to this research domain by introducing SKILL, a novel method that
conducts distillation across groups of layers instead of distilling individual
arbitrarily selected layers within the teacher network. The identification of
the layers to distill is achieved through a hierarchical clustering procedure
applied to layer similarity measures. Extensive experiments demonstrate that
our distilled version of WavLM Base+ not only outperforms DPHuBERT but also
achieves state-of-the-art results in the 30M parameters model class across
several SUPERB tasks.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 27 Feb 24</h3>
<dl>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.02310" title="Abstract">arXiv:1904.02310</a> (replaced) [<a href="/pdf/1904.02310" title="Download PDF">pdf</a>, <a href="/ps/1904.02310" title="Download PostScript">ps</a>, <a href="/format/1904.02310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steiner systems $S(2,4,2^m)$ supported by a family of extended cyclic  codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Q">Qi Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Mathematics of Communications, Vol. 16, No. 4, pp.
  1011-1022, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.02872" title="Abstract">arXiv:1907.02872</a> (replaced) [<a href="/pdf/1907.02872" title="Download PDF">pdf</a>, <a href="/format/1907.02872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anteater: Interactive Visualization of Program Execution Values in  Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faust%2C+R">Rebecca Faust</a>, 
<a href="/search/cs?searchtype=author&query=Isaacs%2C+K">Katherine Isaacs</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+W+Z">William Z. Bernstein</a>, 
<a href="/search/cs?searchtype=author&query=Sharp%2C+M">Michael Sharp</a>, 
<a href="/search/cs?searchtype=author&query=Scheidegger%2C+C">Carlos Scheidegger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 9 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.08600" title="Abstract">arXiv:1908.08600</a> (replaced) [<a href="/pdf/1908.08600" title="Download PDF">pdf</a>, <a href="/format/1908.08600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Causal Inference for Advertising in Real-Time Bidding Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waisman%2C+C">Caio Waisman</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+H+S">Harikesh S. Nair</a>, 
<a href="/search/cs?searchtype=author&query=Carrion%2C+C">Carlos Carrion</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.10596" title="Abstract">arXiv:1910.10596</a> (replaced) [<a href="/pdf/1910.10596" title="Download PDF">pdf</a>, <a href="/format/1910.10596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Orthogonal Variational Inference for Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shi%2C+J">Jiaxin Shi</a>, 
<a href="/search/stat?searchtype=author&query=Titsias%2C+M+K">Michalis K. Titsias</a>, 
<a href="/search/stat?searchtype=author&query=Mnih%2C+A">Andriy Mnih</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.07325" title="Abstract">arXiv:2005.07325</a> (replaced) [<a href="/pdf/2005.07325" title="Download PDF">pdf</a>, <a href="/format/2005.07325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Origin of Quantum Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Adami%2C+C">Christoph Adami</a> (Michigan State University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages including an appendix. 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.10125" title="Abstract">arXiv:2006.10125</a> (replaced) [<a href="/e-print/2006.10125" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sustainable Recreational Fishing Using a Novel Electrical Muscle  Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and  Release Survivability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haverinen%2C+P">Petteri Haverinen</a>, 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+K">Krithik Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nathan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This was a high school hackathon project that although interesting, lacks sufficient rigor and data as a research paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.05943" title="Abstract">arXiv:2007.05943</a> (replaced) [<a href="/pdf/2007.05943" title="Download PDF">pdf</a>, <a href="/format/2007.05943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the generalization of Tanimoto-type kernels to real valued functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szedmak%2C+S">Sandor Szedmak</a> (1)
<a href="/search/cs?searchtype=author&query=Bach%2C+E">Eric Bach</a> (1) ((1) Department of Computer Science, Aalto University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pages 12, 3 PDF figures, uses arxiv.sty
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.10303" title="Abstract">arXiv:2009.10303</a> (replaced) [<a href="/pdf/2009.10303" title="Download PDF">pdf</a>, <a href="/format/2009.10303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the representation and learning of monotone triangular transport maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Baptista%2C+R">Ricardo Baptista</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>, 
<a href="/search/stat?searchtype=author&query=Zahm%2C+O">Olivier Zahm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 9 figures, 3 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Foundations of Computational Mathematics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05616" title="Abstract">arXiv:2012.05616</a> (replaced) [<a href="/pdf/2012.05616" title="Download PDF">pdf</a>, <a href="/format/2012.05616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Human Pose Estimation in Ancient Vase Paintings via  Perceptually-grounded Style Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhu%2C+P">Prathmesh Madhu</a>, 
<a href="/search/cs?searchtype=author&query=Villar-Corrales%2C+A">Angel Villar-Corrales</a>, 
<a href="/search/cs?searchtype=author&query=Kosti%2C+R">Ronak Kosti</a>, 
<a href="/search/cs?searchtype=author&query=Bendschus%2C+T">Torsten Bendschus</a>, 
<a href="/search/cs?searchtype=author&query=Reinhardt%2C+C">Corinna Reinhardt</a>, 
<a href="/search/cs?searchtype=author&query=Bell%2C+P">Peter Bell</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+A">Andreas Maier</a>, 
<a href="/search/cs?searchtype=author&query=Christlein%2C+V">Vincent Christlein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Link to the repository containing the code to reproduce the experiments. For further details, please read the README. Link: <a href="https://anonymous.4open.science/r/3b1bd8ac-bd3a-4df6-8671-56d4f9bdbd8d/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Comput. Cult. Herit. 16, 1, Article 16 (March 2023), 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.05844" title="Abstract">arXiv:2101.05844</a> (replaced) [<a href="/pdf/2101.05844" title="Download PDF">pdf</a>, <a href="/format/2101.05844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling the Convex Barrier with Sparse Dual Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Palma%2C+A">Alessandro De Palma</a>, 
<a href="/search/cs?searchtype=author&query=Behl%2C+H+S">Harkirat Singh Behl</a>, 
<a href="/search/cs?searchtype=author&query=Bunel%2C+R">Rudy Bunel</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M+P">M. Pawan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of Machine Learning Research, 2024 (extension of ICLR 2021 paper in [v1])
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.01295" title="Abstract">arXiv:2102.01295</a> (replaced) [<a href="/pdf/2102.01295" title="Download PDF">pdf</a>, <a href="/format/2102.01295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaze-based dual resolution deep imitation learning for high-precision  dexterous robot manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heecheol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ohmura%2C+Y">Yoshiyuki Ohmura</a>, 
<a href="/search/cs?searchtype=author&query=Kuniyoshi%2C+Y">Yasuo Kuniyoshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. The supplementary video can be found at: <a href="https://www.youtube.com/watch?v=ytpChcFqD5g">this https URL</a> Published in IEEE Robotics and Automation Letters. Replaced to add video url in the manuscript
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters, Vol. 6, No. 2, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06448" title="Abstract">arXiv:2102.06448</a> (replaced) [<a href="/pdf/2102.06448" title="Download PDF">pdf</a>, <a href="/format/2102.06448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The MSR-Video to Text Dataset with Clean Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianmin Li</a>, 
<a href="/search/cs?searchtype=author&query=Frintrop%2C+S">Simone Frintrop</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaolin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is under consideration at Computer Vision and Image Understanding
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Vision and Image Understanding, 225, p.103581 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.11513" title="Abstract">arXiv:2102.11513</a> (replaced) [<a href="/pdf/2102.11513" title="Download PDF">pdf</a>, <a href="/format/2102.11513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Policy Gradient: off-policy reinforcement learning driven jointly  by data and model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bo Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.07020" title="Abstract">arXiv:2103.07020</a> (replaced) [<a href="/pdf/2103.07020" title="Download PDF">pdf</a>, <a href="/ps/2103.07020" title="Download PostScript">ps</a>, <a href="/format/2103.07020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Max-Linear Regression by Convex Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+S">Seonho Kim</a>, 
<a href="/search/stat?searchtype=author&query=Bahmani%2C+S">Sohail Bahmani</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+K">Kiryung Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.09320" title="Abstract">arXiv:2103.09320</a> (replaced) [<a href="/pdf/2103.09320" title="Download PDF">pdf</a>, <a href="/ps/2103.09320" title="Download PostScript">ps</a>, <a href="/format/2103.09320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Pseudorandomness and Classical Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kretschmer%2C+W">William Kretschmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages. V2: added a new result about Haar random oracles (Corollary 5); various writing improvements. V3: added a new section about t-designs and corrected some proofs involving the use of t-designs. V4: corrected Lemma 25
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 16th Conference on the Theory of Quantum Computation,
  Communication and Cryptography (TQC 2021), Leibniz International Proceedings
  in Informatics (LIPIcs) 197, pp. 2:1-2:20 (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14726" title="Abstract">arXiv:2103.14726</a> (replaced) [<a href="/pdf/2103.14726" title="Download PDF">pdf</a>, <a href="/format/2103.14726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random line graphs and edge-attributed network inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lubberts%2C+Z">Zachary Lubberts</a>, 
<a href="/search/cs?searchtype=author&query=Athreya%2C+A">Avanti Athreya</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Youngser Park</a>, 
<a href="/search/cs?searchtype=author&query=Priebe%2C+C+E">Carey E. Priebe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages total, including supplementary material; 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03673" title="Abstract">arXiv:2104.03673</a> (replaced) [<a href="/pdf/2104.03673" title="Download PDF">pdf</a>, <a href="/format/2104.03673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Byzantine Reliable Broadcast on Partially Connected Networks  (Extended version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonomi%2C+S">Silvia Bonomi</a>, 
<a href="/search/cs?searchtype=author&query=Decouchant%2C+J">J&#xe9;r&#xe9;mie Decouchant</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Giovanni Farina</a>, 
<a href="/search/cs?searchtype=author&query=Rahli%2C+V">Vincent Rahli</a>, 
<a href="/search/cs?searchtype=author&query=Tixeuil%2C+S">S&#xe9;bastien Tixeuil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of a paper that appeared at the IEEE ICDCS 2021 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.01161" title="Abstract">arXiv:2105.01161</a> (replaced) [<a href="/pdf/2105.01161" title="Download PDF">pdf</a>, <a href="/format/2105.01161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sketching approximability of all finite CSPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+C">Chi-Ning Chou</a>, 
<a href="/search/cs?searchtype=author&query=Golovnev%2C+A">Alexander Golovnev</a>, 
<a href="/search/cs?searchtype=author&query=Sudan%2C+M">Madhu Sudan</a>, 
<a href="/search/cs?searchtype=author&query=Velusamy%2C+S">Santhoshini Velusamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version to appear in JACM arXiv admin note: text overlap with <a href="/abs/2102.12351">arXiv:2102.12351</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03425" title="Abstract">arXiv:2105.03425</a> (replaced) [<a href="/pdf/2105.03425" title="Download PDF">pdf</a>, <a href="/format/2105.03425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel Two-Sample Tests for Manifold Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.14077" title="Abstract">arXiv:2106.14077</a> (replaced) [<a href="/pdf/2106.14077" title="Download PDF">pdf</a>, <a href="/format/2106.14077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Contextual Information in Best Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>, 
<a href="/search/cs?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM); Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.11246" title="Abstract">arXiv:2107.11246</a> (replaced) [<a href="/pdf/2107.11246" title="Download PDF">pdf</a>, <a href="/format/2107.11246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chance Constrained Economic Dispatch Considering the Capability of  Network Flexibility Against Renewable Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+Y">Yue Song</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hill%2C+D+J">David J. Hill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Power Systems, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00385" title="Abstract">arXiv:2108.00385</a> (replaced) [<a href="/pdf/2108.00385" title="Download PDF">pdf</a>, <a href="/format/2108.00385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based deep imitation learning for dual-arm robot  manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heecheol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ohmura%2C+Y">Yoshiyuki Ohmura</a>, 
<a href="/search/cs?searchtype=author&query=Kuniyoshi%2C+Y">Yasuo Kuniyoshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Accepted in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.12965" title="Abstract">arXiv:2109.12965</a> (replaced) [<a href="/pdf/2109.12965" title="Download PDF">pdf</a>, <a href="/format/2109.12965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-based Person Search in Full Images via Semantic-Driven Proposal  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shizhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">De Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wenlong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yinghui Xing</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+D">Duo Long</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+K">Kai Niu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guoqiang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.02181" title="Abstract">arXiv:2110.02181</a> (replaced) [<a href="/pdf/2110.02181" title="Download PDF">pdf</a>, <a href="/ps/2110.02181" title="Download PostScript">ps</a>, <a href="/format/2110.02181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Decentralized Multi-Robot Exploration  With Macro Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+A+H">Aaron Hao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Bejarano%2C+F+P">Federico Pizarro Bejarano</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuhan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Richard Ren</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+G">Goldie Nejat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.02168" title="Abstract">arXiv:2111.02168</a> (replaced) [<a href="/pdf/2111.02168" title="Download PDF">pdf</a>, <a href="/format/2111.02168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Klarna Product Page Dataset: Web Element Nomination with Graph  Neural Networks and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hotti%2C+A">Alexandra Hotti</a>, 
<a href="/search/cs?searchtype=author&query=Risuleo%2C+R+S">Riccardo Sven Risuleo</a>, 
<a href="/search/cs?searchtype=author&query=Magureanu%2C+S">Stefan Magureanu</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+A">Aref Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Lagergren%2C+J">Jens Lagergren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, 3 tables, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08492" title="Abstract">arXiv:2111.08492</a> (replaced) [<a href="/pdf/2111.08492" title="Download PDF">pdf</a>, <a href="/format/2111.08492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time 3D human action recognition based on Hyperpoint sequence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xing Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhijian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhenjie Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianjin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Z">Zhuang Miao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been published in IEEE Transactions on Industrial Informatics. [1]Li X, Huang Q, Wang Z, et al. Real-Time 3D Human Action Recognition Based on Hyperpoint Sequence[J]. IEEE Transactions on Industrial Informatics, 2022. The code of this paper has been made public at <a href="https://github.com/XingLi1012/SequentialPointNet.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07082" title="Abstract">arXiv:2202.07082</a> (replaced) [<a href="/pdf/2202.07082" title="Download PDF">pdf</a>, <a href="/format/2202.07082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Graphs with Heterophily: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07881" title="Abstract">arXiv:2202.07881</a> (replaced) [<a href="/pdf/2202.07881" title="Download PDF">pdf</a>, <a href="/format/2202.07881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The addition of temporal neighborhood makes the logic of prefixes and  sub-intervals EXPSPACE-complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozzelli%2C+L">L. Bozzelli</a>, 
<a href="/search/cs?searchtype=author&query=Montanari%2C+A">A. Montanari</a>, 
<a href="/search/cs?searchtype=author&query=Peron%2C+A">A. Peron</a>, 
<a href="/search/cs?searchtype=author&query=Sala%2C+P">P. Sala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2109.08320">arXiv:2109.08320</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09574" title="Abstract">arXiv:2202.09574</a> (replaced) [<a href="/pdf/2202.09574" title="Download PDF">pdf</a>, <a href="/format/2202.09574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Robots without Robots: Deep Imitation Learning for  Master-to-Robot Policy Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heecheol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ohmura%2C+Y">Yoshiyuki Ohmura</a>, 
<a href="/search/cs?searchtype=author&query=Nagakubo%2C+A">Akihiko Nagakubo</a>, 
<a href="/search/cs?searchtype=author&query=Kuniyoshi%2C+Y">Yasuo Kuniyoshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters 8.5 (2023): 2906-2913
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12797" title="Abstract">arXiv:2202.12797</a> (replaced) [<a href="/pdf/2202.12797" title="Download PDF">pdf</a>, <a href="/format/2202.12797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement  Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shuang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+B">Boxiang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Q">Qinglin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor Revision for JMLR. The first three authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03179" title="Abstract">arXiv:2203.03179</a> (replaced) [<a href="/pdf/2203.03179" title="Download PDF">pdf</a>, <a href="/format/2203.03179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting data-driven robust statistical arbitrage strategies with deep  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/q-fin?searchtype=author&query=Sester%2C+J">Julian Sester</a>, 
<a href="/search/q-fin?searchtype=author&query=Yin%2C+D">Daiying Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Machine Learning (cs.LG); Mathematical Finance (q-fin.MF); Statistical Finance (q-fin.ST); Trading and Market Microstructure (q-fin.TR)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.05653" title="Abstract">arXiv:2204.05653</a> (replaced) [<a href="/pdf/2204.05653" title="Download PDF">pdf</a>, <a href="/ps/2204.05653" title="Download PostScript">ps</a>, <a href="/format/2204.05653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free Monads, Intrinsic Scoping, and Higher-Order Preunification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudasov%2C+N">Nikolai Kudasov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.08381" title="Abstract">arXiv:2204.08381</a> (replaced) [<a href="/pdf/2204.08381" title="Download PDF">pdf</a>, <a href="/format/2204.08381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple-environment Self-adaptive Network for Aerial-view  Geo-localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhedong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yaoqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenggang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.11602" title="Abstract">arXiv:2204.11602</a> (replaced) [<a href="/pdf/2204.11602" title="Download PDF">pdf</a>, <a href="/format/2204.11602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Broad Recommender System: An Efficient Nonlinear Collaborative Filtering  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Ling Huang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+C">Can-Rong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhen-Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuefang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yingjie Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chang-Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+L+P">C. L. Philip Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.12243" title="Abstract">arXiv:2204.12243</a> (replaced) [<a href="/pdf/2204.12243" title="Download PDF">pdf</a>, <a href="/ps/2204.12243" title="Download PostScript">ps</a>, <a href="/format/2204.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a Spatially Correlated Vehicular Network Assisted by  Cox-distributed Vehicle Relays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Choi%2C+C">Chang-Sik Choi</a>, 
<a href="/search/eess?searchtype=author&query=Baccelli%2C+F">Fran&#xe7;ois Baccelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10316" title="Abstract">arXiv:2205.10316</a> (replaced) [<a href="/pdf/2205.10316" title="Download PDF">pdf</a>, <a href="/format/2205.10316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex behavior from intrinsic motivation to occupy action-state path  space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ram%C3%ADrez-Ruiz%2C+J">Jorge Ram&#xed;rez-Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Grytskyy%2C+D">Dmytro Grytskyy</a>, 
<a href="/search/cs?searchtype=author&query=Mastrogiuseppe%2C+C">Chiara Mastrogiuseppe</a>, 
<a href="/search/cs?searchtype=author&query=Habib%2C+Y">Yamen Habib</a>, 
<a href="/search/cs?searchtype=author&query=Moreno-Bote%2C+R">Rub&#xe9;n Moreno-Bote</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended results, main ones: high dimensional, continuous control, experiment from Gymnasium; and detailed comparison with Empowerment and Free Energy Principle. Updated all main figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00395" title="Abstract">arXiv:2206.00395</a> (replaced) [<a href="/pdf/2206.00395" title="Download PDF">pdf</a>, <a href="/format/2206.00395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization with Access to Auxiliary Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chayti%2C+E+M">El Mahdi Chayti</a>, 
<a href="/search/cs?searchtype=author&query=Karimireddy%2C+S+P">Sai Praneeth Karimireddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02217" title="Abstract">arXiv:2206.02217</a> (replaced) [<a href="/pdf/2206.02217" title="Download PDF">pdf</a>, <a href="/format/2206.02217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Mesh Motion Techniques with Application to Fluid-Structure  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Haubner%2C+J">Johannes Haubner</a>, 
<a href="/search/math?searchtype=author&query=Hellan%2C+O">Ottar Hellan</a>, 
<a href="/search/math?searchtype=author&query=Zeinhofer%2C+M">Marius Zeinhofer</a>, 
<a href="/search/math?searchtype=author&query=Kuchta%2C+M">Miroslav Kuchta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04044" title="Abstract">arXiv:2206.04044</a> (replaced) [<a href="/pdf/2206.04044" title="Download PDF">pdf</a>, <a href="/ps/2206.04044" title="Download PostScript">ps</a>, <a href="/format/2206.04044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuling Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianqing Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Operations Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07751" title="Abstract">arXiv:2206.07751</a> (replaced) [<a href="/pdf/2206.07751" title="Download PDF">pdf</a>, <a href="/format/2206.07751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Identifiability of Nonlinear ICA: Sparsity and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yujia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+I">Ignavier Ng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.02760" title="Abstract">arXiv:2207.02760</a> (replaced) [<a href="/pdf/2207.02760" title="Download PDF">pdf</a>, <a href="/format/2207.02760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TREE-G: Decision Trees Contesting Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bechler-Speicher%2C+M">Maya Bechler-Speicher</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>, 
<a href="/search/cs?searchtype=author&query=Gilad-Bachrach%2C+R">Ran Gilad-Bachrach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05072" title="Abstract">arXiv:2207.05072</a> (replaced) [<a href="/pdf/2207.05072" title="Download PDF">pdf</a>, <a href="/ps/2207.05072" title="Download PostScript">ps</a>, <a href="/format/2207.05072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An On-demand Photonic Ising Machine with Simplified Hamiltonian  Calculation by Phase encoding and Intensity Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+J">Jiayi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yuxuan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiyao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deyang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xue Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaowen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kaiyu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yidong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08271" title="Abstract">arXiv:2207.08271</a> (replaced) [<a href="/pdf/2207.08271" title="Download PDF">pdf</a>, <a href="/format/2207.08271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Importance Markov Chain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andral%2C+C">Charly Andral</a>, 
<a href="/search/stat?searchtype=author&query=Douc%2C+R">Randal Douc</a>, 
<a href="/search/stat?searchtype=author&query=Marival%2C+H">Hugo Marival</a>, 
<a href="/search/stat?searchtype=author&query=Robert%2C+C+P">Christian P. Robert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14016" title="Abstract">arXiv:2207.14016</a> (replaced) [<a href="/pdf/2207.14016" title="Download PDF">pdf</a>, <a href="/format/2207.14016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascades towards noise-induced transitions on networks revealed using  information flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Elteren%2C+C">Casper van Elteren</a>, 
<a href="/search/cs?searchtype=author&query=Quax%2C+R">Rick Quax</a>, 
<a href="/search/cs?searchtype=author&query=Sloot%2C+P">Peter Sloot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Should contain color
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10256" title="Abstract">arXiv:2208.10256</a> (replaced) [<a href="/e-print/2208.10256" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal  Transport: A Theory for Multi-Agent Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuchan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The assumption at the beginning of the main results that "X^n is i.i.d. if and only if it is in the typical set" is a huge mistake. This makes the subsequent proofs invalid. This is corrected in a recent paper motivated in a quantum setting
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01410" title="Abstract">arXiv:2209.01410</a> (replaced) [<a href="/pdf/2209.01410" title="Download PDF">pdf</a>, <a href="/format/2209.01410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Loop View of the Regulation of AI: Equal Impact across Repeated  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Quan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+R">Ramen Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Shorten%2C+R">Robert Shorten</a>, 
<a href="/search/cs?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05946" title="Abstract">arXiv:2209.05946</a> (replaced) [<a href="/pdf/2209.05946" title="Download PDF">pdf</a>, <a href="/format/2209.05946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmDet: Large-scale vision-language multi-dataset pre-training with  multimodal detection network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tiancheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyusong Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IET CV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08411" title="Abstract">arXiv:2209.08411</a> (replaced) [<a href="/pdf/2209.08411" title="Download PDF">pdf</a>, <a href="/format/2209.08411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynaConF: Dynamic Forecasting of Non-Stationary Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lehrmann%2C+A">Andreas Lehrmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Transactions on Machine Learning Research (TMLR), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12457" title="Abstract">arXiv:2209.12457</a> (replaced) [<a href="/pdf/2209.12457" title="Download PDF">pdf</a>, <a href="/format/2209.12457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Observer-Centric Approach for Detecting Faults in Islanded AC  Microgrids with Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Intriago%2C+G">Gabriel Intriago</a>, 
<a href="/search/eess?searchtype=author&query=Intriago%2C+A">Andres Intriago</a>, 
<a href="/search/eess?searchtype=author&query=Konstantinou%2C+C">Charalambos Konstantinou</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12881" title="Abstract">arXiv:2209.12881</a> (replaced) [<a href="/pdf/2209.12881" title="Download PDF">pdf</a>, <a href="/format/2209.12881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Evaluation of 3D Keypoint Detectors and Descriptors on  Coloured Point Clouds in Subsea Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyungmin Jung</a>, 
<a href="/search/cs?searchtype=author&query=Hitchcox%2C+T">Thomas Hitchcox</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10544" title="Abstract">arXiv:2210.10544</a> (replaced) [<a href="/pdf/2210.10544" title="Download PDF">pdf</a>, <a href="/ps/2210.10544" title="Download PostScript">ps</a>, <a href="/format/2210.10544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtractive random forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Broutin%2C+N">Nicolas Broutin</a>, 
<a href="/search/math?searchtype=author&query=Devroye%2C+L">Luc Devroye</a>, 
<a href="/search/math?searchtype=author&query=Lugosi%2C+G">Gabor Lugosi</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+R+I">Roberto Imbuzeiro Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16140" title="Abstract">arXiv:2210.16140</a> (replaced) [<a href="/pdf/2210.16140" title="Download PDF">pdf</a>, <a href="/format/2210.16140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localized Randomized Smoothing for Collective Robustness Certification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schuchardt%2C+J">Jan Schuchardt</a>, 
<a href="/search/cs?searchtype=author&query=Wollschl%C3%A4ger%2C+T">Tom Wollschl&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Bojchevski%2C+A">Aleksandar Bojchevski</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02567" title="Abstract">arXiv:2211.02567</a> (replaced) [<a href="/pdf/2211.02567" title="Download PDF">pdf</a>, <a href="/format/2211.02567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VAID: Indexing View Designs in Visual Analytics System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lu Ying</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Aoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haotian Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zikun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+J">Ji Lan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+D">Dazhen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05269" title="Abstract">arXiv:2211.05269</a> (replaced) [<a href="/pdf/2211.05269" title="Download PDF">pdf</a>, <a href="/format/2211.05269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Networks for Weakly Supervised Generation and  Evaluation of Brain Tumor Segmentations on MR Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yoo%2C+J+J">Jay J. Yoo</a>, 
<a href="/search/eess?searchtype=author&query=Namdar%2C+K">Khashayar Namdar</a>, 
<a href="/search/eess?searchtype=author&query=Wagner%2C+M+W">Matthias W. Wagner</a>, 
<a href="/search/eess?searchtype=author&query=Nobre%2C+L">Liana Nobre</a>, 
<a href="/search/eess?searchtype=author&query=Tabori%2C+U">Uri Tabori</a>, 
<a href="/search/eess?searchtype=author&query=Hawkins%2C+C">Cynthia Hawkins</a>, 
<a href="/search/eess?searchtype=author&query=Ertl-Wagner%2C+B+B">Birgit B. Ertl-Wagner</a>, 
<a href="/search/eess?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07843" title="Abstract">arXiv:2211.07843</a> (replaced) [<a href="/pdf/2211.07843" title="Download PDF">pdf</a>, <a href="/format/2211.07843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error-Robust Retrieval for Chinese Spelling Check
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xunjian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08405" title="Abstract">arXiv:2211.08405</a> (replaced) [<a href="/pdf/2211.08405" title="Download PDF">pdf</a>, <a href="/format/2211.08405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Generative Models for Bankruptcy Prediction Using Textual  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Mancisidor%2C+R+A">Rogelio A. Mancisidor</a>, 
<a href="/search/q-fin?searchtype=author&query=Aas%2C+K">Kjersti Aas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09887" title="Abstract">arXiv:2211.09887</a> (replaced) [<a href="/pdf/2211.09887" title="Download PDF">pdf</a>, <a href="/format/2211.09887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spherical convolutional neural networks can improve brain microstructure  estimation from diffusion MRI data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kerkel%C3%A4%2C+L">Leevi Kerkel&#xe4;</a>, 
<a href="/search/eess?searchtype=author&query=Seunarine%2C+K">Kiran Seunarine</a>, 
<a href="/search/eess?searchtype=author&query=Szczepankiewicz%2C+F">Filip Szczepankiewicz</a>, 
<a href="/search/eess?searchtype=author&query=Clark%2C+C+A">Chris A. Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10769" title="Abstract">arXiv:2211.10769</a> (replaced) [<a href="/pdf/2211.10769" title="Download PDF">pdf</a>, <a href="/format/2211.10769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Opportunities of SYCL for Biological Sequence Alignment on  GPU-based Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costanzo%2C+M">Manuel Costanzo</a>, 
<a href="/search/cs?searchtype=author&query=Rucci%2C+E">Enzo Rucci</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez%2C+C+G">Carlos Garc&#xed;a S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Naiouf%2C+M">Marcelo Naiouf</a>, 
<a href="/search/cs?searchtype=author&query=Prieto-Mat%C3%ADas%2C+M">Manuel Prieto-Mat&#xed;as</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J Supercomput (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11338" title="Abstract">arXiv:2211.11338</a> (replaced) [<a href="/pdf/2211.11338" title="Download PDF">pdf</a>, <a href="/format/2211.11338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation in the extended functional tensor train format
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Str%C3%B6ssner%2C+C">Christoph Str&#xf6;ssner</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+B">Bonan Sun</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14396" title="Abstract">arXiv:2211.14396</a> (replaced) [<a href="/pdf/2211.14396" title="Download PDF">pdf</a>, <a href="/ps/2211.14396" title="Download PostScript">ps</a>, <a href="/format/2211.14396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-invasive Liver Fibrosis Screening on CT Images using Radiomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J+J">Jay J. Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Namdar%2C+K">Khashayar Namdar</a>, 
<a href="/search/cs?searchtype=author&query=Carey%2C+S">Sean Carey</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+S+E">Sandra E. Fischer</a>, 
<a href="/search/cs?searchtype=author&query=McIntosh%2C+C">Chris McIntosh</a>, 
<a href="/search/cs?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>, 
<a href="/search/cs?searchtype=author&query=Rogalla%2C+P">Patrik Rogalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04218" title="Abstract">arXiv:2212.04218</a> (replaced) [<a href="/pdf/2212.04218" title="Download PDF">pdf</a>, <a href="/format/2212.04218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Reductions and Stutter Sensitive Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paviot-Adet%2C+E">Emmanuel Paviot-Adet</a>, 
<a href="/search/cs?searchtype=author&query=Poitrenaud%2C+D">Denis Poitrenaud</a>, 
<a href="/search/cs?searchtype=author&query=Renault%2C+E">Etienne Renault</a>, 
<a href="/search/cs?searchtype=author&query=Thierry-Mieg%2C+Y">Yann Thierry-Mieg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, extended version of FORTE'22 paper "LTL under reductions with weaker conditions than stutter invariance" <a href="/abs/2111.03342">arXiv:2111.03342</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11920" title="Abstract">arXiv:2212.11920</a> (replaced) [<a href="/pdf/2212.11920" title="Download PDF">pdf</a>, <a href="/format/2212.11920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond SOT: Tracking Multiple Generic Objects at Once
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayer%2C+C">Christoph Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Danelljan%2C+M">Martin Danelljan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+V">Vittorio Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+A">Alina Kuznetsova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by WACV'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12259" title="Abstract">arXiv:2212.12259</a> (replaced) [<a href="/pdf/2212.12259" title="Download PDF">pdf</a>, <a href="/format/2212.12259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite interpolation with retractions on manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=S%C3%A9guin%2C+A">Axel S&#xe9;guin</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG)

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13925" title="Abstract">arXiv:2212.13925</a> (replaced) [<a href="/pdf/2212.13925" title="Download PDF">pdf</a>, <a href="/format/2212.13925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality at the Tail of Machine Learning Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wanling Gao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chunjie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+F">Fei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xu Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+J">Jianfeng Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05872" title="Abstract">arXiv:2301.05872</a> (replaced) [<a href="/pdf/2301.05872" title="Download PDF">pdf</a>, <a href="/format/2301.05872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEDAS: A Compressed Decentralized Stochastic Gradient Method with  Improved Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+K">Kun Huang</a>, 
<a href="/search/math?searchtype=author&query=Pu%2C+S">Shi Pu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08807" title="Abstract">arXiv:2301.08807</a> (replaced) [<a href="/pdf/2301.08807" title="Download PDF">pdf</a>, <a href="/format/2301.08807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4-clique Network Minor Embedding for Quantum Annealers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Emerging Technologies (cs.ET); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12334" title="Abstract">arXiv:2301.12334</a> (replaced) [<a href="/pdf/2301.12334" title="Download PDF">pdf</a>, <a href="/format/2301.12334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Play Favorites: Minority Guidance for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Um%2C+S">Soobin Um</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suhyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00105" title="Abstract">arXiv:2302.00105</a> (replaced) [<a href="/pdf/2302.00105" title="Download PDF">pdf</a>, <a href="/format/2302.00105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier series weight in quantum machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Atchade-Adelomou%2C+P">Parfait Atchade-Adelomou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Larson%2C+K">Kent Larson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 14 figures and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03660" title="Abstract">arXiv:2302.03660</a> (replaced) [<a href="/pdf/2302.03660" title="Download PDF">pdf</a>, <a href="/format/2302.03660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow Matching on General Geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lipman%2C+Y">Yaron Lipman</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04379" title="Abstract">arXiv:2302.04379</a> (replaced) [<a href="/pdf/2302.04379" title="Download PDF">pdf</a>, <a href="/format/2302.04379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Et Tu Certifications: Robustness Certificates Yield Better Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cullen%2C+A+C">Andrew C. Cullen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shijie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Montague%2C+P">Paul Montague</a>, 
<a href="/search/cs?searchtype=author&query=Erfani%2C+S+M">Sarah M. Erfani</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P">Benjamin I.P. Rubinstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04658" title="Abstract">arXiv:2302.04658</a> (replaced) [<a href="/pdf/2302.04658" title="Download PDF">pdf</a>, <a href="/ps/2302.04658" title="Download PostScript">ps</a>, <a href="/format/2302.04658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sample Complexity of Approximate Rejection Sampling with  Applications to Smoothed Online Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Block%2C+A">Adam Block</a>, 
<a href="/search/stat?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected mistake in proof of Lemma 27 from the COLT 2023 version of this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06433" title="Abstract">arXiv:2302.06433</a> (replaced) [<a href="/pdf/2302.06433" title="Download PDF">pdf</a>, <a href="/format/2302.06433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-efficient Time Series Representation Learning: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eldele%2C+E">Emadeldeen Eldele</a>, 
<a href="/search/cs?searchtype=author&query=Ragab%2C+M">Mohamed Ragab</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kwoh%2C+C">Chee-Keong Kwoh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09357" title="Abstract">arXiv:2302.09357</a> (replaced) [<a href="/pdf/2302.09357" title="Download PDF">pdf</a>, <a href="/format/2302.09357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Online Instrumental Variable Regression: Regrets for  Endogeneity and Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Della+Vecchia%2C+R">Riccardo Della Vecchia</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+D">Debabrota Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10419" title="Abstract">arXiv:2302.10419</a> (replaced) [<a href="/e-print/2302.10419" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Heuristic Autonomous Exploration Method Based on Environmental  Information Gain During Quadrotor Flight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiajie Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+M">Minghui Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript will be improved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12491" title="Abstract">arXiv:2302.12491</a> (replaced) [<a href="/pdf/2302.12491" title="Download PDF">pdf</a>, <a href="/format/2302.12491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Learning of Blind Super-Resolution and Crack Segmentation for  Realistic Degraded Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondo%2C+Y">Yuki Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Ukita%2C+N">Norimichi Ukita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Instrumentation and Measurement (TIM) 2024. The project page is located at <a href="https://yuki-11.github.io/CSBSR-project-page/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12822" title="Abstract">arXiv:2302.12822</a> (replaced) [<a href="/pdf/2302.12822" title="Download PDF">pdf</a>, <a href="/format/2302.12822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Prompt Augmentation and Selection with Chain-of-Thought from  Labeled Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shum%2C+K">KaShun Shum</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13376" title="Abstract">arXiv:2302.13376</a> (replaced) [<a href="/pdf/2302.13376" title="Download PDF">pdf</a>, <a href="/format/2302.13376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Ensemble for Multimodal Punctuation Restoration using  Time-Delay Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X+Y">Xing Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Beigi%2C+H">Homayoon Beigi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, 5 tables, paper at IMCOM 2024, technical report at Recognition Technologies, Inc
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 18th International Conference on Ubiquitous Information
  Management and Communication (IMCOM), 2024, pp. 1-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13991" title="Abstract">arXiv:2302.13991</a> (replaced) [<a href="/pdf/2302.13991" title="Download PDF">pdf</a>, <a href="/format/2302.13991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generalize towards Unseen Domains via a Content-Aware Style  Invariant Model for Disease Detection from Chest X-rays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zunaed%2C+M">Mohammad Zunaed</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+A">Md. Aynal Haque</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+T">Taufiq Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14123" title="Abstract">arXiv:2302.14123</a> (replaced) [<a href="/pdf/2302.14123" title="Download PDF">pdf</a>, <a href="/format/2302.14123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Blotto: Viewpoint Competition with Polarized Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donahue%2C+K">Kate Donahue</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14702" title="Abstract">arXiv:2302.14702</a> (replaced) [<a href="/pdf/2302.14702" title="Download PDF">pdf</a>, <a href="/format/2302.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Limits of a Deep Learning-Enabled Text Semantic  Communication under Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Getu%2C+T+M">Tilahun M. Getu</a>, 
<a href="/search/eess?searchtype=author&query=Saad%2C+W">Walid Saad</a>, 
<a href="/search/eess?searchtype=author&query=Kaddoum%2C+G">Georges Kaddoum</a>, 
<a href="/search/eess?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14753" title="Abstract">arXiv:2302.14753</a> (replaced) [<a href="/pdf/2302.14753" title="Download PDF">pdf</a>, <a href="/format/2302.14753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hidden Markov Models Using Conditional Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S+M">Sham M. Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Akshay Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+G">Gaurav Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cyril Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05445" title="Abstract">arXiv:2303.05445</a> (replaced) [<a href="/pdf/2303.05445" title="Download PDF">pdf</a>, <a href="/format/2303.05445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flooding with Absorption: An Efficient Protocol for Heterogeneous  Bandits over Complex Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+L">Laura Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures. Accepted to the 27th International Conference on Principles of Distributed Systems (OPODIS 2023) - Best Student Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06045" title="Abstract">arXiv:2303.06045</a> (replaced) [<a href="/pdf/2303.06045" title="Download PDF">pdf</a>, <a href="/format/2303.06045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-based identification using Lebesgue-sampled data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gonz%C3%A1lez%2C+R+A">Rodrigo A. Gonz&#xe1;lez</a>, 
<a href="/search/eess?searchtype=author&query=Tiels%2C+K">Koen Tiels</a>, 
<a href="/search/eess?searchtype=author&query=Oomen%2C+T">Tom Oomen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06200" title="Abstract">arXiv:2303.06200</a> (replaced) [<a href="/pdf/2303.06200" title="Download PDF">pdf</a>, <a href="/format/2303.06200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monte Carlo Grid Dynamic Programming: Almost Sure Convergence and  Probability Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ramadan%2C+M+S">Mohammad S. Ramadan</a>, 
<a href="/search/eess?searchtype=author&query=Al-Tawaha%2C+A">Ahmad Al-Tawaha</a>, 
<a href="/search/eess?searchtype=author&query=Shouman%2C+M">Mohamed Shouman</a>, 
<a href="/search/eess?searchtype=author&query=Atallah%2C+A">Ahmed Atallah</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+M">Ming Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06440" title="Abstract">arXiv:2303.06440</a> (replaced) [<a href="/pdf/2303.06440" title="Download PDF">pdf</a>, <a href="/format/2303.06440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Xformer: Hybrid X-Shaped Transformer for Image Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiale Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Linghe Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Code and models are available at <a href="https://github.com/gladzhang/Xformer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06965" title="Abstract">arXiv:2303.06965</a> (replaced) [<a href="/pdf/2303.06965" title="Download PDF">pdf</a>, <a href="/format/2303.06965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap between Chemical Reaction Pretraining and Conditional  Molecule Generation with a Unified Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiang%2C+B">Bo Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiran Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ningfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Song Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangren Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10796" title="Abstract">arXiv:2303.10796</a> (replaced) [<a href="/pdf/2303.10796" title="Download PDF">pdf</a>, <a href="/format/2303.10796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Driven Bottleneck Attention U-net for Organ at Risk  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nazib%2C+A">Abdullah Nazib</a>, 
<a href="/search/eess?searchtype=author&query=Hassan%2C+R">Riad Hassan</a>, 
<a href="/search/eess?searchtype=author&query=Islam%2C+Z">Zahidul Islam</a>, 
<a href="/search/eess?searchtype=author&query=Fookes%2C+C">Clinton Fookes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISBI(2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11196" title="Abstract">arXiv:2303.11196</a> (replaced) [<a href="/pdf/2303.11196" title="Download PDF">pdf</a>, <a href="/ps/2303.11196" title="Download PostScript">ps</a>, <a href="/format/2303.11196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Global Divide in AI Regulation: A Proposal for a  Contextual, Coherent, and Commensurable Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangchul Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33(2) Wash. Int'l L.J. _ (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11916" title="Abstract">arXiv:2303.11916</a> (replaced) [<a href="/pdf/2303.11916" title="Download PDF">pdf</a>, <a href="/format/2303.11916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+G">Geonmo Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chun%2C+S">Sanghyuk Chun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+H">HeeJae Jun</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yoohoon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally; 28 pages, 6.2MB
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12446" title="Abstract">arXiv:2303.12446</a> (replaced) [<a href="/pdf/2303.12446" title="Download PDF">pdf</a>, <a href="/ps/2303.12446" title="Download PostScript">ps</a>, <a href="/format/2303.12446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Externalities in Chore Division
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanpui%2C+M+A">Mohammad Azharuddin Sanpui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15702" title="Abstract">arXiv:2303.15702</a> (replaced) [<a href="/pdf/2303.15702" title="Download PDF">pdf</a>, <a href="/ps/2303.15702" title="Download PostScript">ps</a>, <a href="/format/2303.15702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Graph Embedding with Information-Oriented Random Walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+P">Peng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arijit Khan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siqiang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Dan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenli Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuchao Cao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 49th International Conference on Very Large Data Bases (VLDB
  2023), Vancouver, Canada - August 28 to September 1, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00899" title="Abstract">arXiv:2304.00899</a> (replaced) [<a href="/pdf/2304.00899" title="Download PDF">pdf</a>, <a href="/format/2304.00899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Load Balancing with Job-Size Testing: Performance Improvement or  Degradation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anselmi%2C+J">Jonatha Anselmi</a>, 
<a href="/search/cs?searchtype=author&query=Doncel%2C+J">Josu Doncel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03431" title="Abstract">arXiv:2304.03431</a> (replaced) [<a href="/pdf/2304.03431" title="Download PDF">pdf</a>, <a href="/format/2304.03431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization In Robust Invariant Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gauri Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kapila%2C+R">Ritvik Kapila</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+K">Keshav Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Raskar%2C+R">Ramesh Raskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, ICLR 2023 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03516" title="Abstract">arXiv:2304.03516</a> (replaced) [<a href="/pdf/2304.03516" title="Download PDF">pdf</a>, <a href="/format/2304.03516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Recommendation: Towards Next-generation Recommender Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04326" title="Abstract">arXiv:2304.04326</a> (replaced) [<a href="/pdf/2304.04326" title="Download PDF">pdf</a>, <a href="/format/2304.04326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation  for Decentralized Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+D">Deepak Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+G">Gobinda Saha</a>, 
<a href="/search/cs?searchtype=author&query=Aketi%2C+S+A">Sai Aparna Aketi</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06607" title="Abstract">arXiv:2304.06607</a> (replaced) [<a href="/pdf/2304.06607" title="Download PDF">pdf</a>, <a href="/format/2304.06607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> False Claims against Model Ownership Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Szyller%2C+S">Sebastian Szyller</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kui Ren</a>, 
<a href="/search/cs?searchtype=author&query=Asokan%2C+N">N.Asokan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13pages,3 figures. To appear in the 33rd USENIX Security Symposium (USENIX Security '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07472" title="Abstract">arXiv:2304.07472</a> (replaced) [<a href="/pdf/2304.07472" title="Download PDF">pdf</a>, <a href="/format/2304.07472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Convex Algorithms for Universal Kernel Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Talitckii%2C+A">Aleksandr Talitckii</a>, 
<a href="/search/stat?searchtype=author&query=Colbert%2C+B+K">Brendon K. Colbert</a>, 
<a href="/search/stat?searchtype=author&query=Peet%2C+M+M">Matthew M. Peet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08146" title="Abstract">arXiv:2304.08146</a> (replaced) [<a href="/pdf/2304.08146" title="Download PDF">pdf</a>, <a href="/ps/2304.08146" title="Download PostScript">ps</a>, <a href="/format/2304.08146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2D Forward Looking Sonar Simulation with Ground Echo Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chujie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yonghoon Ji</a>, 
<a href="/search/cs?searchtype=author&query=Tsuchiya%2C+H">Hiroshi Tsuchiya</a>, 
<a href="/search/cs?searchtype=author&query=Asama%2C+H">Hajime Asama</a>, 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+A">Atsushi Yamashita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version of UR2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12008" title="Abstract">arXiv:2304.12008</a> (replaced) [<a href="/pdf/2304.12008" title="Download PDF">pdf</a>, <a href="/format/2304.12008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peipeng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiahan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zhihua Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14491" title="Abstract">arXiv:2304.14491</a> (replaced) [<a href="/pdf/2304.14491" title="Download PDF">pdf</a>, <a href="/format/2304.14491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Electrical Impedance Tomography reconstruction using Learned  Half-Quadratic Splitting Networks with Anderson Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guixian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huihui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qingping Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8figures, 6 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14614" title="Abstract">arXiv:2304.14614</a> (replaced) [<a href="/pdf/2304.14614" title="Download PDF">pdf</a>, <a href="/format/2304.14614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhiyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hongjun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">James Liang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shiwei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanhong Tao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongfang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zuzak%2C+M">Michael Zuzak</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR'2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00959" title="Abstract">arXiv:2305.00959</a> (replaced) [<a href="/pdf/2305.00959" title="Download PDF">pdf</a>, <a href="/ps/2305.00959" title="Download PostScript">ps</a>, <a href="/format/2305.00959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skeleton Integral Equations for Acoustic Transmission Problems with  Varying Coefficients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Florian%2C+F">Francesco Florian</a>, 
<a href="/search/math?searchtype=author&query=Hiptmair%2C+R">Ralf Hiptmair</a>, 
<a href="/search/math?searchtype=author&query=Sauter%2C+S+A">Stefan A. Sauter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01181" title="Abstract">arXiv:2305.01181</a> (replaced) [<a href="/pdf/2305.01181" title="Download PDF">pdf</a>, <a href="/format/2305.01181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Paradigm Shift: The Future of Machine Translation Lies with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Chenyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zefeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jitao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yitao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Minghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lynn%2C+T">Teresa Lynn</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02759" title="Abstract">arXiv:2305.02759</a> (replaced) [<a href="/pdf/2305.02759" title="Download PDF">pdf</a>, <a href="/format/2305.02759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Contrastive Collaborative Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiashu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a SIGIR'23 full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03374" title="Abstract">arXiv:2305.03374</a> (replaced) [<a href="/pdf/2305.03374" title="Download PDF">pdf</a>, <a href="/format/2305.03374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Simin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+X">Xuguang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuwei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenwu Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03514" title="Abstract">arXiv:2305.03514</a> (replaced) [<a href="/pdf/2305.03514" title="Download PDF">pdf</a>, <a href="/format/2305.03514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Transform Computational Social Science?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziems%2C+C">Caleb Ziems</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+W">William Held</a>, 
<a href="/search/cs?searchtype=author&query=Shaikh%2C+O">Omar Shaikh</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhehao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in "Computational Linguistics" (CL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04847" title="Abstract">arXiv:2305.04847</a> (replaced) [<a href="/pdf/2305.04847" title="Download PDF">pdf</a>, <a href="/format/2305.04847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buhmann%2C+E">Erik Buhmann</a>, 
<a href="/search/physics?searchtype=author&query=Diefenbacher%2C+S">Sascha Diefenbacher</a>, 
<a href="/search/physics?searchtype=author&query=Eren%2C+E">Engin Eren</a>, 
<a href="/search/physics?searchtype=author&query=Gaede%2C+F">Frank Gaede</a>, 
<a href="/search/physics?searchtype=author&query=Kasieczka%2C+G">Gregor Kasieczka</a>, 
<a href="/search/physics?searchtype=author&query=Korol%2C+A">Anatolii Korol</a>, 
<a href="/search/physics?searchtype=author&query=Korcari%2C+W">William Korcari</a>, 
<a href="/search/physics?searchtype=author&query=Kr%C3%BCger%2C+K">Katja Kr&#xfc;ger</a>, 
<a href="/search/physics?searchtype=author&query=McKeown%2C+P">Peter McKeown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JINST 18 (2023) 11, P11025
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05215" title="Abstract">arXiv:2305.05215</a> (replaced) [<a href="/pdf/2305.05215" title="Download PDF">pdf</a>, <a href="/format/2305.05215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel Synthetic Data Tool for Data-Driven Cardboard Box Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gajdo%C5%A1ech%2C+L">Luk&#xe1;&#x161; Gajdo&#x161;ech</a>, 
<a href="/search/cs?searchtype=author&query=Krav%C3%A1r%2C+P">Peter Krav&#xe1;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract Published in 2023 Artificial Neural Networks and Machine Learning (ICANN). Published version copyrighted by Springer Nature Switzerland. Accepted: 29.6.2023. Published: 22.9.2023. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Code: <a href="https://doi.org/10.5281/zenodo.10649535">this https URL</a> Data: <a href="https://doi.org/10.5281/zenodo.10650158">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Neural Networks and Machine Learning ICANN (2023)
  565-569
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05496" title="Abstract">arXiv:2305.05496</a> (replaced) [<a href="/pdf/2305.05496" title="Download PDF">pdf</a>, <a href="/format/2305.05496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Pseudo Image Captions for Multimodal Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Rui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jinan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACL2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07764" title="Abstract">arXiv:2305.07764</a> (replaced) [<a href="/pdf/2305.07764" title="Download PDF">pdf</a>, <a href="/format/2305.07764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Term Value of Exploration: Measurements, Findings and Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yi Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+E+Y">Elaine Ya Le</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuening Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haokai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lipshitz%2C+B">Benjamin Lipshitz</a>, 
<a href="/search/cs?searchtype=author&query=Badam%2C+S">Sriraj Badam</a>, 
<a href="/search/cs?searchtype=author&query=Heldt%2C+L">Lukasz Heldt</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Shuchao Bi</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E">Ed Chi</a>, 
<a href="/search/cs?searchtype=author&query=Goodrow%2C+C">Cristos Goodrow</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Su-Lin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Baugher%2C+L">Lexi Baugher</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minmin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08144" title="Abstract">arXiv:2305.08144</a> (replaced) [<a href="/pdf/2305.08144" title="Download PDF">pdf</a>, <a href="/format/2305.08144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Danyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongshen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Ruisheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09601" title="Abstract">arXiv:2305.09601</a> (replaced) [<a href="/pdf/2305.09601" title="Download PDF">pdf</a>, <a href="/format/2305.09601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operationalizing content moderation &quot;accuracy&quot; in the Digital Services  Act
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J+T">Johnny Tian-Zheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zufall%2C+F">Frederike Zufall</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09859" title="Abstract">arXiv:2305.09859</a> (replaced) [<a href="/pdf/2305.09859" title="Download PDF">pdf</a>, <a href="/format/2305.09859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smaller Language Models are Better Black-box Machine-Generated Text  Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Mattern%2C+J">Justus Mattern</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11854" title="Abstract">arXiv:2305.11854</a> (replaced) [<a href="/pdf/2305.11854" title="Download PDF">pdf</a>, <a href="/format/2305.11854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Web Navigation with Instruction-Finetuned Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Nachum%2C+O">Ofir Nachum</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S+S">Shixiang Shane Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Website: <a href="https://sites.google.com/view/mm-webnav/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12474" title="Abstract">arXiv:2305.12474</a> (replaced) [<a href="/pdf/2305.12474" title="Download PDF">pdf</a>, <a href="/format/2305.12474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Performance of Large Language Models on GAOKAO Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaotian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yi Zong</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+Z">Zhengyu Ying</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12650" title="Abstract">arXiv:2305.12650</a> (replaced) [<a href="/pdf/2305.12650" title="Download PDF">pdf</a>, <a href="/format/2305.12650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Federated Recommendation Meets Cold-Start Problem: Separating Item  Attributes and User Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunxu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guodong Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+P">Peng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a regular paper of WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13385" title="Abstract">arXiv:2305.13385</a> (replaced) [<a href="/pdf/2305.13385" title="Download PDF">pdf</a>, <a href="/format/2305.13385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Communication Approach for Metadata Exchange in Geo-Distributed  Fog Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kruber%2C+M">Marvin Kruber</a>, 
<a href="/search/cs?searchtype=author&query=Pfandzelter%2C+T">Tobias Pfandzelter</a>, 
<a href="/search/cs?searchtype=author&query=Bermbach%2C+D">David Bermbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14651" title="Abstract">arXiv:2305.14651</a> (replaced) [<a href="/pdf/2305.14651" title="Download PDF">pdf</a>, <a href="/format/2305.14651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisit and Outstrip Entity Alignment: A Perspective of Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14965" title="Abstract">arXiv:2305.14965</a> (replaced) [<a href="/pdf/2305.14965" title="Download PDF">pdf</a>, <a href="/format/2305.14965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting  Jailbreaks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Abhinav Rao</a>, 
<a href="/search/cs?searchtype=author&query=Vashistha%2C+S">Sachin Vashistha</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Atharva Naik</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15030" title="Abstract">arXiv:2305.15030</a> (replaced) [<a href="/pdf/2305.15030" title="Download PDF">pdf</a>, <a href="/format/2305.15030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Lossy Compression Meaningful for Low-Light Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shilv Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liqun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Sheng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Luxin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiahuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xu Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15033" title="Abstract">arXiv:2305.15033</a> (replaced) [<a href="/pdf/2305.15033" title="Download PDF">pdf</a>, <a href="/format/2305.15033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmartTrim: Adaptive Tokens and Attention Pruning for Efficient  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zekun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingchang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haichao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiafeng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+L">Liping Shan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> COLING-LREC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15163" title="Abstract">arXiv:2305.15163</a> (replaced) [<a href="/pdf/2305.15163" title="Download PDF">pdf</a>, <a href="/format/2305.15163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast and accurate domain-decomposition nonlinear manifold reduced  order model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Diaz%2C+A+N">Alejandro N. Diaz</a>, 
<a href="/search/math?searchtype=author&query=Choi%2C+Y">Youngsoo Choi</a>, 
<a href="/search/math?searchtype=author&query=Heinkenschloss%2C+M">Matthias Heinkenschloss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15196" title="Abstract">arXiv:2305.15196</a> (replaced) [<a href="/pdf/2305.15196" title="Download PDF">pdf</a>, <a href="/format/2305.15196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-aligned N-BEATS with Sinkhorn divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joonhun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+M">Myeongho Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kyunghyun Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15596" title="Abstract">arXiv:2305.15596</a> (replaced) [<a href="/pdf/2305.15596" title="Download PDF">pdf</a>, <a href="/format/2305.15596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Online Rollout for Multivehicle Routing in Unmapped  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber%2C+J+W">Jamison W. Weber</a>, 
<a href="/search/cs?searchtype=author&query=Giriyan%2C+D+R">Dhanush R. Giriyan</a>, 
<a href="/search/cs?searchtype=author&query=Parkar%2C+D+R">Devendra R. Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Bertsekas%2C+D+P">Dimitri P. Bertsekas</a>, 
<a href="/search/cs?searchtype=author&query=Richa%2C+A+W">Andr&#xe9;a W. Richa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16203" title="Abstract">arXiv:2305.16203</a> (replaced) [<a href="/pdf/2305.16203" title="Download PDF">pdf</a>, <a href="/format/2305.16203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computing Universal Plans for Partially Observable Multi-Agent Path  Finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fengming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fangzhen Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16882" title="Abstract">arXiv:2305.16882</a> (replaced) [<a href="/pdf/2305.16882" title="Download PDF">pdf</a>, <a href="/ps/2305.16882" title="Download PostScript">ps</a>, <a href="/format/2305.16882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link Residual Closeness of Harary Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dangalchev%2C+C">Ch. Dangalchev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19277" title="Abstract">arXiv:2305.19277</a> (replaced) [<a href="/pdf/2305.19277" title="Download PDF">pdf</a>, <a href="/ps/2305.19277" title="Download PostScript">ps</a>, <a href="/format/2305.19277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrigendum to &quot;On the monophonic rank of a graph&quot; [Discrete Math.  Theor. Comput. Sci. 24:2 (2022) #3]
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dourado%2C+M+C">Mitre C. Dourado</a>, 
<a href="/search/math?searchtype=author&query=Ponciano%2C+V+S">Vitor S. Ponciano</a>, 
<a href="/search/math?searchtype=author&query=da+Silva%2C+R+L+O">R&#xf4;mulo L. O. da Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00742" title="Abstract">arXiv:2306.00742</a> (replaced) [<a href="/pdf/2306.00742" title="Download PDF">pdf</a>, <a href="/format/2306.00742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Galerkin method beats Graph-Based Approaches for Spectral Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+F">Francis Bach</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01727" title="Abstract">arXiv:2306.01727</a> (replaced) [<a href="/pdf/2306.01727" title="Download PDF">pdf</a>, <a href="/format/2306.01727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Broadcasting in random recursive dags
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Briend%2C+S">Simon Briend</a>, 
<a href="/search/stat?searchtype=author&query=Devroye%2C+L">Luc Devroye</a>, 
<a href="/search/stat?searchtype=author&query=Lugosi%2C+G">Gabor Lugosi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02031" title="Abstract">arXiv:2306.02031</a> (replaced) [<a href="/pdf/2306.02031" title="Download PDF">pdf</a>, <a href="/format/2306.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DOS: Diverse Outlier Sampling for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingcai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chongjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02742" title="Abstract">arXiv:2306.02742</a> (replaced) [<a href="/pdf/2306.02742" title="Download PDF">pdf</a>, <a href="/format/2306.02742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Robust Motion Control based on Unknown System Dynamics  Estimator for Robot Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xinyu Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kaixin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yongping Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haoyong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, accepted by IEEE International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03346" title="Abstract">arXiv:2306.03346</a> (replaced) [<a href="/pdf/2306.03346" title="Download PDF">pdf</a>, <a href="/format/2306.03346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from  Offline Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>, 
<a href="/search/cs?searchtype=author&query=Walke%2C+H">Homer Walke</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+P">Patrick Yin</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight (&amp;lt; 5%). Website (<a href="https://chongyi-zheng.github.io/stable_contrastive_rl">this https URL</a>) and code (<a href="https://github.com/chongyi-zheng/stable_contrastive_rl">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03530" title="Abstract">arXiv:2306.03530</a> (replaced) [<a href="/pdf/2306.03530" title="Download PDF">pdf</a>, <a href="/format/2306.03530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLtools: A Fast, Portable Deep Reinforcement Learning Library for  Continuous Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eschmann%2C+J">Jonas Eschmann</a>, 
<a href="/search/cs?searchtype=author&query=Albani%2C+D">Dario Albani</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://rl.tools">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06112" title="Abstract">arXiv:2306.06112</a> (replaced) [<a href="/pdf/2306.06112" title="Download PDF">pdf</a>, <a href="/format/2306.06112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ModelObfuscator: Obfuscating Model Information to Protect Deployed  ML-based Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06490" title="Abstract">arXiv:2306.06490</a> (replaced) [<a href="/pdf/2306.06490" title="Download PDF">pdf</a>, <a href="/format/2306.06490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Code Editing with Search-Generate-Modify
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changshu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cetin%2C+P">Pelin Cetin</a>, 
<a href="/search/cs?searchtype=author&query=Patodia%2C+Y">Yogesh Patodia</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yangruibo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06794" title="Abstract">arXiv:2306.06794</a> (replaced) [<a href="/pdf/2306.06794" title="Download PDF">pdf</a>, <a href="/format/2306.06794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A blind spot for large language models: Supradiegetic linguistic  information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+J+W">Julia Witte Zimmerman</a>, 
<a href="/search/cs?searchtype=author&query=Hudon%2C+D">Denis Hudon</a>, 
<a href="/search/cs?searchtype=author&query=Cramer%2C+K">Kathryn Cramer</a>, 
<a href="/search/cs?searchtype=author&query=Onge%2C+J+S">Jonathan St. Onge</a>, 
<a href="/search/cs?searchtype=author&query=Fudolig%2C+M">Mikaela Fudolig</a>, 
<a href="/search/cs?searchtype=author&query=Trujillo%2C+M+Z">Milo Z. Trujillo</a>, 
<a href="/search/cs?searchtype=author&query=Danforth%2C+C+M">Christopher M. Danforth</a>, 
<a href="/search/cs?searchtype=author&query=Dodds%2C+P+S">Peter Sheridan Dodds</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures, 3 tables. arXiv admin note: text overlap with <a href="/abs/2206.02608">arXiv:2206.02608</a>, <a href="/abs/2303.12712">arXiv:2303.12712</a>, <a href="/abs/2305.10601">arXiv:2305.10601</a>, <a href="/abs/2305.06424">arXiv:2305.06424</a>, <a href="/abs/1908.08530">arXiv:1908.08530</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08887" title="Abstract">arXiv:2306.08887</a> (replaced) [<a href="/pdf/2306.08887" title="Download PDF">pdf</a>, <a href="/format/2306.08887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SplatFlow: Learning Multi-frame Optical Flow via Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhenping Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dewen Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09222" title="Abstract">arXiv:2306.09222</a> (replaced) [<a href="/pdf/2306.09222" title="Download PDF">pdf</a>, <a href="/format/2306.09222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ramnath Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Majmundar%2C+K">Kushal Majmundar</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Suggala%2C+A+S">Arun Sai Suggala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10698" title="Abstract">arXiv:2306.10698</a> (replaced) [<a href="/pdf/2306.10698" title="Download PDF">pdf</a>, <a href="/format/2306.10698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning with Task-Adaptive Retrieval via  Hypernetwork
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yonggang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Liuyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14142" title="Abstract">arXiv:2306.14142</a> (replaced) [<a href="/pdf/2306.14142" title="Download PDF">pdf</a>, <a href="/format/2306.14142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Policy Effects in a Social Network with Independent Set  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ang%2C+E">Eugene Ang</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+P">Prasanta Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+A">Andrew Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16504" title="Abstract">arXiv:2306.16504</a> (replaced) [<a href="/pdf/2306.16504" title="Download PDF">pdf</a>, <a href="/format/2306.16504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Benefits Non-IID Federated Learning Simply and Provably
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Ziheng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinmeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16838" title="Abstract">arXiv:2306.16838</a> (replaced) [<a href="/pdf/2306.16838" title="Download PDF">pdf</a>, <a href="/format/2306.16838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Kernel Ridge Regression with Gradient-Based Optimization Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Allerbo%2C+O">Oskar Allerbo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Article <a href="/abs/2306.16838">arXiv:2306.16838v1</a> has been updated and split into two articles: this article and <a href="/abs/2311.01762">arXiv:2311.01762</a>. Thus, some of the content in <a href="/abs/2306.16838">arXiv:2306.16838v1</a> is not a part of <a href="/abs/2306.16838">arXiv:2306.16838v2</a>, but of <a href="/abs/2311.01762">arXiv:2311.01762</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17777" title="Abstract">arXiv:2306.17777</a> (replaced) [<a href="/pdf/2306.17777" title="Download PDF">pdf</a>, <a href="/ps/2306.17777" title="Download PostScript">ps</a>, <a href="/format/2306.17777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Canonizing Graphs of Bounded Rank-Width in Parallel via  Weisfeiler--Leman
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levet%2C+M">Michael Levet</a>, 
<a href="/search/cs?searchtype=author&query=Rombach%2C+P">Puck Rombach</a>, 
<a href="/search/cs?searchtype=author&query=Sieger%2C+N">Nicholas Sieger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Logic in Computer Science (cs.LO); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00014" title="Abstract">arXiv:2307.00014</a> (replaced) [<a href="/pdf/2307.00014" title="Download PDF">pdf</a>, <a href="/format/2307.00014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inertial Navigation Meets Deep Learning: A Survey of Current Trends and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Nadav Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00743" title="Abstract">arXiv:2307.00743</a> (replaced) [<a href="/pdf/2307.00743" title="Download PDF">pdf</a>, <a href="/ps/2307.00743" title="Download PostScript">ps</a>, <a href="/format/2307.00743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Power Allocation and Beamforming for Active IRS-aided Directional  Modulation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Rongen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+F">Feng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongzhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongpeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00897" title="Abstract">arXiv:2307.00897</a> (replaced) [<a href="/pdf/2307.00897" title="Download PDF">pdf</a>, <a href="/format/2307.00897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixing confirmation bias in feature attribution methods via semantic  match
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cin%C3%A0%2C+G">Giovanni Cin&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Llaneza%2C+D">Daniel Fernandez-Llaneza</a>, 
<a href="/search/cs?searchtype=author&query=Deponte%2C+L">Ludovico Deponte</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+N">Nishant Mishra</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ber%2C+T+E">Tabea E. R&#xf6;ber</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>, 
<a href="/search/cs?searchtype=author&query=Calixto%2C+I">Iacer Calixto</a>, 
<a href="/search/cs?searchtype=author&query=Goedhart%2C+R">Rob Goedhart</a>, 
<a href="/search/cs?searchtype=author&query=Birbil%2C+%C5%9E+%C4%B0">&#x15e;. &#x130;lker Birbil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02031" title="Abstract">arXiv:2307.02031</a> (replaced) [<a href="/pdf/2307.02031" title="Download PDF">pdf</a>, <a href="/format/2307.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Automatic Parallel Training via Balanced Memory Workload  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Youhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+X">Xupeng Miao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+F">Fangcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shenhan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xiaonan Nie</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Yaofeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+B">Bin Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2211.13878">arXiv:2211.13878</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02496" title="Abstract">arXiv:2307.02496</a> (replaced) [<a href="/pdf/2307.02496" title="Download PDF">pdf</a>, <a href="/ps/2307.02496" title="Download PostScript">ps</a>, <a href="/format/2307.02496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to reconstruct the bubble distribution with conductivity maps  using Invertible Neural Networks and Error Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kumar%2C+N">Nishant Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Krause%2C+L">Lukas Krause</a>, 
<a href="/search/eess?searchtype=author&query=Wondrak%2C+T">Thomas Wondrak</a>, 
<a href="/search/eess?searchtype=author&query=Eckert%2C+S">Sven Eckert</a>, 
<a href="/search/eess?searchtype=author&query=Eckert%2C+K">Kerstin Eckert</a>, 
<a href="/search/eess?searchtype=author&query=Gumhold%2C+S">Stefan Gumhold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Oral presentation at WCIPT11 (11th World Congress on Industrial Process Tomography)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03137" title="Abstract">arXiv:2307.03137</a> (replaced) [<a href="/pdf/2307.03137" title="Download PDF">pdf</a>, <a href="/format/2307.03137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed  Tomography Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ozcelik%2C+S">Seher Ozcelik</a>, 
<a href="/search/eess?searchtype=author&query=Unver%2C+S">Sinan Unver</a>, 
<a href="/search/eess?searchtype=author&query=Gurses%2C+I+A">Ilke Ali Gurses</a>, 
<a href="/search/eess?searchtype=author&query=Turkay%2C+R">Rustu Turkay</a>, 
<a href="/search/eess?searchtype=author&query=Gunduz-Demir%2C+C">Cigdem Gunduz-Demir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04661" title="Abstract">arXiv:2307.04661</a> (replaced) [<a href="/pdf/2307.04661" title="Download PDF">pdf</a>, <a href="/ps/2307.04661" title="Download PostScript">ps</a>, <a href="/format/2307.04661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the power of graph neural networks and the role of the activation  function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Amitabh Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05551" title="Abstract">arXiv:2307.05551</a> (replaced) [<a href="/pdf/2307.05551" title="Download PDF">pdf</a>, <a href="/format/2307.05551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks as an Enabler of Terahertz-based Flow-guided  Nanoscale Localization over Highly Erroneous Raw Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartra%2C+G+C">Gerard Calvo Bartra</a>, 
<a href="/search/cs?searchtype=author&query=Lemic%2C+F">Filip Lemic</a>, 
<a href="/search/cs?searchtype=author&query=Pascual%2C+G">Guillem Pascual</a>, 
<a href="/search/cs?searchtype=author&query=Rodas%2C+A+P">Aina P&#xe9;rez Rodas</a>, 
<a href="/search/cs?searchtype=author&query=Struye%2C+J">Jakob Struye</a>, 
<a href="/search/cs?searchtype=author&query=Delgado%2C+C">Carmen Delgado</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+X+C">Xavier Costa P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 16 figures, 6 tables, 45 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08504" title="Abstract">arXiv:2307.08504</a> (replaced) [<a href="/pdf/2307.08504" title="Download PDF">pdf</a>, <a href="/format/2307.08504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up  Patch Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Bin Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Songfang Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09697" title="Abstract">arXiv:2307.09697</a> (replaced) [<a href="/pdf/2307.09697" title="Download PDF">pdf</a>, <a href="/format/2307.09697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel well-balanced continuous interior penalty stabilizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Micalizzi%2C+L">Lorenzo Micalizzi</a>, 
<a href="/search/math?searchtype=author&query=Ricchiuto%2C+M">Mario Ricchiuto</a>, 
<a href="/search/math?searchtype=author&query=Abgrall%2C+R">R&#xe9;mi Abgrall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09913" title="Abstract">arXiv:2307.09913</a> (replaced) [<a href="/pdf/2307.09913" title="Download PDF">pdf</a>, <a href="/format/2307.09913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Non-Regular Extensions of Propositional Dynamic Logic with  Description-Logics Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bednarczyk%2C+B">Bartosz Bednarczyk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of our JELIA 2023 paper, accepted for publication to Logical Methods in Computer Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10489" title="Abstract">arXiv:2307.10489</a> (replaced) [<a href="/pdf/2307.10489" title="Download PDF">pdf</a>, <a href="/format/2307.10489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Basic Mechanical and Geometric Framework for Quasi-Static Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Campolo%2C+D">Domenico Campolo</a>, 
<a href="/search/math?searchtype=author&query=Cardin%2C+F">Franco Cardin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11589" title="Abstract">arXiv:2307.11589</a> (replaced) [<a href="/pdf/2307.11589" title="Download PDF">pdf</a>, <a href="/format/2307.11589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-based system representations from irregularly measured data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alsalti%2C+M">Mohammad Alsalti</a>, 
<a href="/search/eess?searchtype=author&query=Markovsky%2C+I">Ivan Markovsky</a>, 
<a href="/search/eess?searchtype=author&query=Lopez%2C+V+G">Victor G. Lopez</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+M+A">Matthias A. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12455" title="Abstract">arXiv:2307.12455</a> (replaced) [<a href="/pdf/2307.12455" title="Download PDF">pdf</a>, <a href="/format/2307.12455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A monolithic space-time temporal multirate finite element framework for  interface and volume coupled problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Roth%2C+J">Julian Roth</a>, 
<a href="/search/math?searchtype=author&query=Soszy%C5%84ska%2C+M">Martyna Soszy&#x144;ska</a>, 
<a href="/search/math?searchtype=author&query=Richter%2C+T">Thomas Richter</a>, 
<a href="/search/math?searchtype=author&query=Wick%2C+T">Thomas Wick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 18 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12856" title="Abstract">arXiv:2307.12856</a> (replaced) [<a href="/pdf/2307.12856" title="Download PDF">pdf</a>, <a href="/format/2307.12856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Real-World WebAgent with Planning, Long Context Understanding, and  Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Austin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Safdari%2C+M">Mustafa Safdari</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Eck%2C+D">Douglas Eck</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14264" title="Abstract">arXiv:2307.14264</a> (replaced) [<a href="/pdf/2307.14264" title="Download PDF">pdf</a>, <a href="/ps/2307.14264" title="Download PostScript">ps</a>, <a href="/format/2307.14264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A tight Monte-Carlo algorithm for Steiner Tree parameterized by  clique-width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bojikian%2C+N">Narek Bojikian</a>, 
<a href="/search/cs?searchtype=author&query=Kratsch%2C+S">Stefan Kratsch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14281" title="Abstract">arXiv:2307.14281</a> (replaced) [<a href="/pdf/2307.14281" title="Download PDF">pdf</a>, <a href="/ps/2307.14281" title="Download PostScript">ps</a>, <a href="/format/2307.14281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moments of Autocorrelation Demerit Factors of Binary Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katz%2C+D+J">Daniel J. Katz</a>, 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+M+E">Miriam E. Ramirez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM); Signal Processing (eess.SP); Combinatorics (math.CO); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15054" title="Abstract">arXiv:2307.15054</a> (replaced) [<a href="/pdf/2307.15054" title="Download PDF">pdf</a>, <a href="/format/2307.15054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometric Notion of Causal Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guerner%2C+C">Cl&#xe9;ment Guerner</a>, 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Warstadt%2C+A">Alexander Warstadt</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16506" title="Abstract">arXiv:2307.16506</a> (replaced) [<a href="/pdf/2307.16506" title="Download PDF">pdf</a>, <a href="/format/2307.16506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Equivariant Neural Networks for Particle Physics: PELICAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Bogatskiy%2C+A">Alexander Bogatskiy</a>, 
<a href="/search/hep-ph?searchtype=author&query=Hoffman%2C+T">Timothy Hoffman</a>, 
<a href="/search/hep-ph?searchtype=author&query=Miller%2C+D+W">David W. Miller</a>, 
<a href="/search/hep-ph?searchtype=author&query=Offermann%2C+J+T">Jan T. Offermann</a>, 
<a href="/search/hep-ph?searchtype=author&query=Liu%2C+X">Xiaoyang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 34 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16830" title="Abstract">arXiv:2307.16830</a> (replaced) [<a href="/pdf/2307.16830" title="Download PDF">pdf</a>, <a href="/format/2307.16830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Optimal Power Flow with GPUs: SIMD Abstraction of Nonlinear  Programs and Condensed-Space Interior-Point Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shin%2C+S">Sungho Shin</a>, 
<a href="/search/math?searchtype=author&query=Pacaud%2C+F">Fran&#xe7;ois Pacaud</a>, 
<a href="/search/math?searchtype=author&query=Anitescu%2C+M">Mihai Anitescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00637" title="Abstract">arXiv:2308.00637</a> (replaced) [<a href="/pdf/2308.00637" title="Download PDF">pdf</a>, <a href="/format/2308.00637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Krylov Solvers for Interior Point Methods with Applications in Radiation  Therapy and Support Vector Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+F">Felix Liu</a>, 
<a href="/search/math?searchtype=author&query=Fredriksson%2C+A">Albin Fredriksson</a>, 
<a href="/search/math?searchtype=author&query=Markidis%2C+S">Stefano Markidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01037" title="Abstract">arXiv:2308.01037</a> (replaced) [<a href="/pdf/2308.01037" title="Download PDF">pdf</a>, <a href="/format/2308.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast Monte Carlo algorithm for evaluating matrix functions with  application in complex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guidotti%2C+N+L">Nicolas L. Guidotti</a>, 
<a href="/search/cs?searchtype=author&query=Acebr%C3%B3n%2C+J+A">Juan A. Acebr&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+J">Jos&#xe9; Monteiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Journal of Scientific Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02041" title="Abstract">arXiv:2308.02041</a> (replaced) [<a href="/pdf/2308.02041" title="Download PDF">pdf</a>, <a href="/ps/2308.02041" title="Download PostScript">ps</a>, <a href="/format/2308.02041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulating AI: Applying insights from behavioural economics and  psychology to the application of article 5 of the EU AI Act
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Huixin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neill%2C+E">Eamonn O&#x27;Neill</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+J+A">Janina A. Hoffmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted for publication by AAAI 2024 paper on December of 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02514" title="Abstract">arXiv:2308.02514</a> (replaced) [<a href="/pdf/2308.02514" title="Download PDF">pdf</a>, <a href="/format/2308.02514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language models as master equation solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuanbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Proceedings of the National Academy of Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03075" title="Abstract">arXiv:2308.03075</a> (replaced) [<a href="/pdf/2308.03075" title="Download PDF">pdf</a>, <a href="/ps/2308.03075" title="Download PostScript">ps</a>, <a href="/format/2308.03075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knapsack with Small Items in Near-Quadratic Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+K">Karl Bringmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, accepted at STOC'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03475" title="Abstract">arXiv:2308.03475</a> (replaced) [<a href="/pdf/2308.03475" title="Download PDF">pdf</a>, <a href="/format/2308.03475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPA: Efficient Vision-Language Pre-training Through Collaborative  Object- and Patch-Text Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Bin Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on ACM MM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04666" title="Abstract">arXiv:2308.04666</a> (replaced) [<a href="/pdf/2308.04666" title="Download PDF">pdf</a>, <a href="/format/2308.04666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speaker Recognition Using Isomorphic Graph Attention Network Based  Pooling on Self-Supervised Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Z">Zirui Ge</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinzhou Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haiyan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tingting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04941" title="Abstract">arXiv:2308.04941</a> (replaced) [<a href="/pdf/2308.04941" title="Download PDF">pdf</a>, <a href="/format/2308.04941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating large language models and active inference to understand eye  movements in reading and dyslexia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Donnarumma%2C+F">Francesco Donnarumma</a>, 
<a href="/search/q-bio?searchtype=author&query=Frosolone%2C+M">Mirco Frosolone</a>, 
<a href="/search/q-bio?searchtype=author&query=Pezzulo%2C+G">Giovanni Pezzulo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 Appendix, 11 Tables, 9 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04945" title="Abstract">arXiv:2308.04945</a> (replaced) [<a href="/pdf/2308.04945" title="Download PDF">pdf</a>, <a href="/format/2308.04945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalvi%2C+F">Fahim Dalvi</a>, 
<a href="/search/cs?searchtype=author&query=Hasanain%2C+M">Maram Hasanain</a>, 
<a href="/search/cs?searchtype=author&query=Boughorbel%2C+S">Sabri Boughorbel</a>, 
<a href="/search/cs?searchtype=author&query=Mousi%2C+B">Basel Mousi</a>, 
<a href="/search/cs?searchtype=author&query=Abdaljalil%2C+S">Samir Abdaljalil</a>, 
<a href="/search/cs?searchtype=author&query=Nazar%2C+N">Nizi Nazar</a>, 
<a href="/search/cs?searchtype=author&query=Abdelali%2C+A">Ahmed Abdelali</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+A">Shammur Absar Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Mubarak%2C+H">Hamdy Mubarak</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Ahmed Ali</a>, 
<a href="/search/cs?searchtype=author&query=Hawasly%2C+M">Majd Hawasly</a>, 
<a href="/search/cs?searchtype=author&query=Durrani%2C+N">Nadir Durrani</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a demo paper at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06013" title="Abstract">arXiv:2308.06013</a> (replaced) [<a href="/pdf/2308.06013" title="Download PDF">pdf</a>, <a href="/format/2308.06013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Telecom: Forthcoming Impact on the Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maatouk%2C+A">Ali Maatouk</a>, 
<a href="/search/cs?searchtype=author&query=Piovesan%2C+N">Nicola Piovesan</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+F">Fadhel Ayed</a>, 
<a href="/search/cs?searchtype=author&query=De+Domenico%2C+A">Antonio De Domenico</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07426" title="Abstract">arXiv:2308.07426</a> (replaced) [<a href="/pdf/2308.07426" title="Download PDF">pdf</a>, <a href="/format/2308.07426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehui Wang</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6pken%2C+W">Wolfram H&#xf6;pken</a>, 
<a href="/search/cs?searchtype=author&query=Jannach%2C+D">Dietmar Jannach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07728" title="Abstract">arXiv:2308.07728</a> (replaced) [<a href="/pdf/2308.07728" title="Download PDF">pdf</a>, <a href="/format/2308.07728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+S">Seokhyeon Ha</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+S">Sunbeom Jung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungwoo Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08079" title="Abstract">arXiv:2308.08079</a> (replaced) [<a href="/pdf/2308.08079" title="Download PDF">pdf</a>, <a href="/ps/2308.08079" title="Download PostScript">ps</a>, <a href="/format/2308.08079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rigid Transformations for Stabilized Lower Dimensional Space to Support  Subsurface Uncertainty Quantification and Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mabadeje%2C+A+O">Ademide O. Mabadeje</a>, 
<a href="/search/cs?searchtype=author&query=Pyrcz%2C+M+J">Michael J. Pyrcz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08632" title="Abstract">arXiv:2308.08632</a> (replaced) [<a href="/pdf/2308.08632" title="Download PDF">pdf</a>, <a href="/format/2308.08632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Repetitive Action Counting: Joint-Based PoseRAC Model  With Improved Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haodong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Leu%2C+M+C">Ming C. Leu</a>, 
<a href="/search/cs?searchtype=author&query=Moniruzzaman%2C+M">Md Moniruzzaman</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhaozheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Hajmohammadi%2C+S">Solmaz Hajmohammadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08758" title="Abstract">arXiv:2308.08758</a> (replaced) [<a href="/pdf/2308.08758" title="Download PDF">pdf</a>, <a href="/format/2308.08758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Prompt Compression with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hoyoun Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyung-Joong Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09238" title="Abstract">arXiv:2308.09238</a> (replaced) [<a href="/pdf/2308.09238" title="Download PDF">pdf</a>, <a href="/format/2308.09238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Buoy Detection with Deep Transfer Learning for Mussel Farm  Automation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Carl McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junhong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Bing Xue</a>, 
<a href="/search/cs?searchtype=author&query=Vennell%2C+R">Ross Vennell</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengjie Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, presented at 2023 38th International Conference on Image and Vision Computing New Zealand (IVCNZ)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IVCNZ 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09990" title="Abstract">arXiv:2308.09990</a> (replaced) [<a href="/pdf/2308.09990" title="Download PDF">pdf</a>, <a href="/ps/2308.09990" title="Download PostScript">ps</a>, <a href="/format/2308.09990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Textureless-aware Segmentation and Correlative Refinement Guided  Multi-View Stereo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhenlong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiakai Cao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10385" title="Abstract">arXiv:2308.10385</a> (replaced) [<a href="/pdf/2308.10385" title="Download PDF">pdf</a>, <a href="/format/2308.10385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engaged and Affective Virtual Agents: Their Impact on Social Presence,  Trustworthiness, and Decision-Making in the Group Discussion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hanseob Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jieun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Syawaludin%2C+M+F">Muhammad Firdaus Syawaludin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G+J">Gerard Jounghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jae-In Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10430" title="Abstract">arXiv:2308.10430</a> (replaced) [<a href="/pdf/2308.10430" title="Download PDF">pdf</a>, <a href="/format/2308.10430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling of electronic dynamics in twisted bilayer graphene
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math-ph?searchtype=author&query=Kong%2C+T">Tianyu Kong</a>, 
<a href="/search/math-ph?searchtype=author&query=Liu%2C+D">Diyi Liu</a>, 
<a href="/search/math-ph?searchtype=author&query=Luskin%2C+M">Mitchell Luskin</a>, 
<a href="/search/math-ph?searchtype=author&query=Watson%2C+A+B">Alexander B. Watson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Physics (math-ph)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Analysis of PDEs (math.AP); Numerical Analysis (math.NA); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10457" title="Abstract">arXiv:2308.10457</a> (replaced) [<a href="/pdf/2308.10457" title="Download PDF">pdf</a>, <a href="/format/2308.10457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALI-DPFL: Differentially Private Federated Learning with Adaptive Local  Iterations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">Xinpeng Ling</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuncan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhili Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11464" title="Abstract">arXiv:2308.11464</a> (replaced) [<a href="/pdf/2308.11464" title="Download PDF">pdf</a>, <a href="/format/2308.11464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Internal Cross-layer Gradients for Extending Homogeneity to  Heterogeneity in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+Y">Yun-Hin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Rui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Running Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ngai%2C+E+C+-">Edith C.-H. Ngai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. 29 pages. Source codes: <a href="https://github.com/ChanYunHin/InCo-Aggregation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11721" title="Abstract">arXiv:2308.11721</a> (replaced) [<a href="/pdf/2308.11721" title="Download PDF">pdf</a>, <a href="/format/2308.11721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Are Two Lists Better than One?: Benefits and Harms in Joint  Decision-making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donahue%2C+K">Kate Donahue</a>, 
<a href="/search/cs?searchtype=author&query=Gollapudi%2C+S">Sreenivas Gollapudi</a>, 
<a href="/search/cs?searchtype=author&query=Kollias%2C+K">Kostas Kollias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11838" title="Abstract">arXiv:2308.11838</a> (replaced) [<a href="/pdf/2308.11838" title="Download PDF">pdf</a>, <a href="/format/2308.11838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark Study on Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+L">Linwei Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Younan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haolan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Minjing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14296" title="Abstract">arXiv:2308.14296</a> (replaced) [<a href="/pdf/2308.14296" title="Download PDF">pdf</a>, <a href="/format/2308.14296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecMind: Large Language Model Powered Agent For Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yancheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingxue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunah Cho</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaojiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yanbin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yingzhen Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14525" title="Abstract">arXiv:2308.14525</a> (replaced) [<a href="/pdf/2308.14525" title="Download PDF">pdf</a>, <a href="/format/2308.14525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Learning for Visual Bird&#x27;s Eye View Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Junyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lina Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+F">Feng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wanlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15613" title="Abstract">arXiv:2308.15613</a> (replaced) [<a href="/pdf/2308.15613" title="Download PDF">pdf</a>, <a href="/format/2308.15613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Variational Flows for Discrete Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Diluvi%2C+G+C">Gian Carlo Diluvi</a>, 
<a href="/search/stat?searchtype=author&query=Bloem-Reddy%2C+B">Benjamin Bloem-Reddy</a>, 
<a href="/search/stat?searchtype=author&query=Campbell%2C+T">Trevor Campbell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15666" title="Abstract">arXiv:2308.15666</a> (replaced) [<a href="/pdf/2308.15666" title="Download PDF">pdf</a>, <a href="/format/2308.15666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of non-linear diagonal frame filtering for regularizing  inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ebner%2C+A">Andrea Ebner</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01551" title="Abstract">arXiv:2309.01551</a> (replaced) [<a href="/pdf/2309.01551" title="Download PDF">pdf</a>, <a href="/format/2309.01551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Your Learned Query Optimizer Behaving As You Expect? A Machine  Learning Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+C">Claude Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Sulimov%2C+P">Pavel Sulimov</a>, 
<a href="/search/cs?searchtype=author&query=Stockinger%2C+K">Kurt Stockinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03919" title="Abstract">arXiv:2309.03919</a> (replaced) [<a href="/pdf/2309.03919" title="Download PDF">pdf</a>, <a href="/format/2309.03919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hybrid quantum-classical fusion neural network to improve  protein-ligand binding affinity predictions for drug discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Domingo%2C+L">L. Domingo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chehimi%2C+M">M. Chehimi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Banerjee%2C+S">S. Banerjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuxun%2C+S+H">S. He Yuxun</a>, 
<a href="/search/quant-ph?searchtype=author&query=Konakanchi%2C+S">S. Konakanchi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ogunfowora%2C+L">L. Ogunfowora</a>, 
<a href="/search/quant-ph?searchtype=author&query=Roy%2C+S">S. Roy</a>, 
<a href="/search/quant-ph?searchtype=author&query=Selvaras%2C+S">S. Selvaras</a>, 
<a href="/search/quant-ph?searchtype=author&query=Djukic%2C+M">M. Djukic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Johnson%2C+C">C. Johnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04332" title="Abstract">arXiv:2309.04332</a> (replaced) [<a href="/pdf/2309.04332" title="Download PDF">pdf</a>, <a href="/format/2309.04332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks Use Graphs When They Shouldn&#x27;t
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bechler-Speicher%2C+M">Maya Bechler-Speicher</a>, 
<a href="/search/cs?searchtype=author&query=Amos%2C+I">Ido Amos</a>, 
<a href="/search/cs?searchtype=author&query=Gilad-Bachrach%2C+R">Ran Gilad-Bachrach</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04877" title="Abstract">arXiv:2309.04877</a> (replaced) [<a href="/pdf/2309.04877" title="Download PDF">pdf</a>, <a href="/format/2309.04877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Gentle Introduction to Gradient-Based Optimization and Variational  Inequalities for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wadia%2C+N+S">Neha S. Wadia</a>, 
<a href="/search/cs?searchtype=author&query=Dandi%2C+Y">Yatin Dandi</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 7 figures; minor corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05366" title="Abstract">arXiv:2309.05366</a> (replaced) [<a href="/pdf/2309.05366" title="Download PDF">pdf</a>, <a href="/ps/2309.05366" title="Download PostScript">ps</a>, <a href="/format/2309.05366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incipient Slip-Based Rotation Measurement via Visuotactile Sensing  During In-Hand Object Pivoting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y+H">Yen Hang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tiemin Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yao Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures, Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05442" title="Abstract">arXiv:2309.05442</a> (replaced) [<a href="/pdf/2309.05442" title="Download PDF">pdf</a>, <a href="/format/2309.05442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Spreading Behavior in Networks with Arbitrary Topologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Modanese%2C+A">Augusto Modanese</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 4 figures, fixed and improved content in Sect. 4, added text and diagram to introduction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05704" title="Abstract">arXiv:2309.05704</a> (replaced) [<a href="/pdf/2309.05704" title="Download PDF">pdf</a>, <a href="/format/2309.05704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular  Calorimeter Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buhmann%2C+E">Erik Buhmann</a>, 
<a href="/search/physics?searchtype=author&query=Gaede%2C+F">Frank Gaede</a>, 
<a href="/search/physics?searchtype=author&query=Kasieczka%2C+G">Gregor Kasieczka</a>, 
<a href="/search/physics?searchtype=author&query=Korol%2C+A">Anatolii Korol</a>, 
<a href="/search/physics?searchtype=author&query=Korcari%2C+W">William Korcari</a>, 
<a href="/search/physics?searchtype=author&query=Kr%C3%BCger%2C+K">Katja Kr&#xfc;ger</a>, 
<a href="/search/physics?searchtype=author&query=McKeown%2C+P">Peter McKeown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 7 figures, 3 tables, code available at <a href="https://github.com/FLC-QU-hep/CaloClouds-2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08254" title="Abstract">arXiv:2309.08254</a> (replaced) [<a href="/pdf/2309.08254" title="Download PDF">pdf</a>, <a href="/format/2309.08254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous and Human-Driven Vehicles Interacting in a Roundabout: A  Quantitative and Qualitative Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrarotti%2C+L">Laura Ferrarotti</a>, 
<a href="/search/cs?searchtype=author&query=Luca%2C+M">Massimiliano Luca</a>, 
<a href="/search/cs?searchtype=author&query=Santin%2C+G">Gabriele Santin</a>, 
<a href="/search/cs?searchtype=author&query=Previati%2C+G">Giorgio Previati</a>, 
<a href="/search/cs?searchtype=author&query=Mastinu%2C+G">Gianpiero Mastinu</a>, 
<a href="/search/cs?searchtype=author&query=Gobbi%2C+M">Massimiliano Gobbi</a>, 
<a href="/search/cs?searchtype=author&query=Campi%2C+E">Elena Campi</a>, 
<a href="/search/cs?searchtype=author&query=Uccello%2C+L">Lorenzo Uccello</a>, 
<a href="/search/cs?searchtype=author&query=Albanese%2C+A">Antonino Albanese</a>, 
<a href="/search/cs?searchtype=author&query=Zalaya%2C+P">Praveen Zalaya</a>, 
<a href="/search/cs?searchtype=author&query=Roccasalva%2C+A">Alessandro Roccasalva</a>, 
<a href="/search/cs?searchtype=author&query=Lepri%2C+B">Bruno Lepri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08849" title="Abstract">arXiv:2309.08849</a> (replaced) [<a href="/pdf/2309.08849" title="Download PDF">pdf</a>, <a href="/ps/2309.08849" title="Download PostScript">ps</a>, <a href="/format/2309.08849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Stable Dynamic System with a Lyapunov Energy Function for  Demonstratives Using Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yongxiang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiuze Xia</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Long Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08957" title="Abstract">arXiv:2309.08957</a> (replaced) [<a href="/pdf/2309.08957" title="Download PDF">pdf</a>, <a href="/format/2309.08957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jeongtaek Oh</a>, 
<a href="/search/cs?searchtype=author&query=Rim%2C+J">Jaesung Rim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+M">Kyoung Mu Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/taekkii/ExBluRF/tree/main">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09737" title="Abstract">arXiv:2309.09737</a> (replaced) [<a href="/pdf/2309.09737" title="Download PDF">pdf</a>, <a href="/format/2309.09737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhijun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Fangqiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hantao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C+X">Chris Xiaoxuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024. 8 pages, 4 figures. Co-first authorship for Zhijun Pan, Fangqiang Ding and Hantao Zhong, listed randomly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09752" title="Abstract">arXiv:2309.09752</a> (replaced) [<a href="/pdf/2309.09752" title="Download PDF">pdf</a>, <a href="/format/2309.09752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Initial State Buffer for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Messikommer%2C+N">Nico Messikommer</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunlong Song</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09814" title="Abstract">arXiv:2309.09814</a> (replaced) [<a href="/pdf/2309.09814" title="Download PDF">pdf</a>, <a href="/ps/2309.09814" title="Download PostScript">ps</a>, <a href="/format/2309.09814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Deep Kernel Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Milsom%2C+E">Edward Milsom</a>, 
<a href="/search/stat?searchtype=author&query=Anson%2C+B">Ben Anson</a>, 
<a href="/search/stat?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11156" title="Abstract">arXiv:2309.11156</a> (replaced) [<a href="/pdf/2309.11156" title="Download PDF">pdf</a>, <a href="/ps/2309.11156" title="Download PostScript">ps</a>, <a href="/format/2309.11156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNN-based local features for navigation near an asteroid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knuuttila%2C+O">Olli Knuuttila</a>, 
<a href="/search/cs?searchtype=author&query=Kestil%C3%A4%2C+A">Antti Kestil&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Kallio%2C+E">Esa Kallio</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 12, pp. 16652-16672, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11599" title="Abstract">arXiv:2309.11599</a> (replaced) [<a href="/pdf/2309.11599" title="Download PDF">pdf</a>, <a href="/format/2309.11599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Inclusion, Control, and Ownership in Workplace AI-Mediated  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadoma%2C+K">Kowe Kadoma</a>, 
<a href="/search/cs?searchtype=author&query=Quere%2C+M+A+L">Marianne Aubin Le Quere</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jenny Fu</a>, 
<a href="/search/cs?searchtype=author&query=Munsch%2C+C">Christin Munsch</a>, 
<a href="/search/cs?searchtype=author&query=Metaxa%2C+D">Danae Metaxa</a>, 
<a href="/search/cs?searchtype=author&query=Naaman%2C+M">Mor Naaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12235" title="Abstract">arXiv:2309.12235</a> (replaced) [<a href="/pdf/2309.12235" title="Download PDF">pdf</a>, <a href="/format/2309.12235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Conjugate Gradient Method via Conjugate Direction Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shorinwa%2C+O">Ola Shorinwa</a>, 
<a href="/search/math?searchtype=author&query=Schwager%2C+M">Mac Schwager</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12368" title="Abstract">arXiv:2309.12368</a> (replaced) [<a href="/pdf/2309.12368" title="Download PDF">pdf</a>, <a href="/format/2309.12368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Human-AI Collaboration in Complex Medical Decision Making: A  Case Study in Sepsis Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jianing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Changchang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tory%2C+M">Melanie Tory</a>, 
<a href="/search/cs?searchtype=author&query=Padilla%2C+L+M">Lace M. Padilla</a>, 
<a href="/search/cs?searchtype=author&query=Caterino%2C+J">Jeffrey Caterino</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CHI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12546" title="Abstract">arXiv:2309.12546</a> (replaced) [<a href="/pdf/2309.12546" title="Download PDF">pdf</a>, <a href="/ps/2309.12546" title="Download PostScript">ps</a>, <a href="/format/2309.12546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Answerability Evaluation for Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Funakoshi%2C+K">Kotaro Funakoshi</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12646" title="Abstract">arXiv:2309.12646</a> (replaced) [<a href="/pdf/2309.12646" title="Download PDF">pdf</a>, <a href="/ps/2309.12646" title="Download PostScript">ps</a>, <a href="/format/2309.12646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Emotional Experiences in Dyadic Conversations of Married  Couples: Leveraging Semantic Similarity through Sentence Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chen-Wei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Lotsos%2C+A+N">Alexandros N. Lotsos</a>, 
<a href="/search/cs?searchtype=author&query=Haase%2C+C+M">Claudia M. Haase</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13672" title="Abstract">arXiv:2309.13672</a> (replaced) [<a href="/pdf/2309.13672" title="Download PDF">pdf</a>, <a href="/format/2309.13672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image-to-Image Translation with Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Ziwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chengming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13850" title="Abstract">arXiv:2309.13850</a> (replaced) [<a href="/pdf/2309.13850" title="Download PDF">pdf</a>, <a href="/format/2309.13850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Perspective of Top-K Sparse Softmax Gating Mixture of  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Akbarian%2C+P">Pedram Akbarian</a>, 
<a href="/search/stat?searchtype=author&query=Yan%2C+F">Fanqi Yan</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024, 38 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14556" title="Abstract">arXiv:2309.14556</a> (replaced) [<a href="/pdf/2309.14556" title="Download PDF">pdf</a>, <a href="/format/2309.14556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Art or Artifice? Large Language Models and the False Promise of  Creativity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+T">Tuhin Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Laban%2C+P">Philippe Laban</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+D">Divyansh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14655" title="Abstract">arXiv:2309.14655</a> (replaced) [<a href="/pdf/2309.14655" title="Download PDF">pdf</a>, <a href="/format/2309.14655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic 3D Multi-Object Cooperative Tracking for Autonomous  Driving via Differentiable Multi-Sensor Kalman Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiu%2C+H">Hsu-kuang Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chien-Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min-Hung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S+F">Stephen F. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Robotics and Automation (ICRA), 2024. Code: <a href="https://github.com/eddyhkchiu/DMSTrack/">this https URL</a> Video: <a href="https://eddyhkchiu.github.io/dmstrack.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16354" title="Abstract">arXiv:2309.16354</a> (replaced) [<a href="/pdf/2309.16354" title="Download PDF">pdf</a>, <a href="/format/2309.16354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-VQ: Linear-Time Transformers via Vector Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lingle%2C+L+D">Lucas D. Lingle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16869" title="Abstract">arXiv:2309.16869</a> (replaced) [<a href="/pdf/2309.16869" title="Download PDF">pdf</a>, <a href="/format/2309.16869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vidaptive: Efficient and Responsive Rate Control for Real-Time Video on  Variable Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karimi%2C+P">Pantea Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Fouladi%2C+S">Sadjad Fouladi</a>, 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+V">Vibhaalakshmi Sivaraman</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17264" title="Abstract">arXiv:2309.17264</a> (replaced) [<a href="/pdf/2309.17264" title="Download PDF">pdf</a>, <a href="/format/2309.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Foundation Model for General Moving Object Segmentation in Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhongnuo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tong Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiongquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenlong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00386" title="Abstract">arXiv:2310.00386</a> (replaced) [<a href="/pdf/2310.00386" title="Download PDF">pdf</a>, <a href="/format/2310.00386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order-Preserving GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yihang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mauch%2C+L">Lukas Mauch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00771" title="Abstract">arXiv:2310.00771</a> (replaced) [<a href="/pdf/2310.00771" title="Download PDF">pdf</a>, <a href="/format/2310.00771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training with Synthetic Data Helps Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zecheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Che Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zixuan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+K">Keith Ross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00797" title="Abstract">arXiv:2310.00797</a> (replaced) [<a href="/pdf/2310.00797" title="Download PDF">pdf</a>, <a href="/format/2310.00797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Miss Out on Novelty: Importance of Novel Features for Deep Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01061" title="Abstract">arXiv:2310.01061</a> (replaced) [<a href="/pdf/2310.01061" title="Download PDF">pdf</a>, <a href="/format/2310.01061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning on Graphs: Faithful and Interpretable Large Language Model  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01107" title="Abstract">arXiv:2310.01107</a> (replaced) [<a href="/pdf/2310.01107" title="Download PDF">pdf</a>, <a href="/format/2310.01107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Hyeonho Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024, Project Page: <a href="http://ground-a-video.github.io">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01132" title="Abstract">arXiv:2310.01132</a> (replaced) [<a href="/pdf/2310.01132" title="Download PDF">pdf</a>, <a href="/format/2310.01132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Evaluation of Classroom Instructional Support with LLMs and  BoWs: Connecting Global Predictions to Specific Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Whitehill%2C+J">Jacob Whitehill</a>, 
<a href="/search/cs?searchtype=author&query=LoCasale-Crouch%2C+J">Jennifer LoCasale-Crouch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02124" title="Abstract">arXiv:2310.02124</a> (replaced) [<a href="/pdf/2310.02124" title="Download PDF">pdf</a>, <a href="/format/2310.02124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology  View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jintian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. 61 pages (8 main), 67 figures, 37 tables. Blog: \url{<a href="https://zjunlp.github.io/project/MachineSoM">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02174" title="Abstract">arXiv:2310.02174</a> (replaced) [<a href="/pdf/2310.02174" title="Download PDF">pdf</a>, <a href="/format/2310.02174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Again, Then Fail: Large Language Models&#x27; Vacillations in Judgement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zengzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update mitigation results of fine-tuning the model on synthesized high-quality preference data with DPO algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02687" title="Abstract">arXiv:2310.02687</a> (replaced) [<a href="/pdf/2310.02687" title="Download PDF">pdf</a>, <a href="/format/2310.02687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingzhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Bangyan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peidong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03617" title="Abstract">arXiv:2310.03617</a> (replaced) [<a href="/pdf/2310.03617" title="Download PDF">pdf</a>, <a href="/format/2310.03617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RouteKG: A knowledge graph-based framework for route prediction on road  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yihong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weipeng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shuyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuebing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhenliang Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03986" title="Abstract">arXiv:2310.03986</a> (replaced) [<a href="/pdf/2310.03986" title="Download PDF">pdf</a>, <a href="/format/2310.03986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Multimodal Learning with Missing Modalities via  Parameter-Efficient Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reza%2C+M+K">Md Kaykobad Reza</a>, 
<a href="/search/cs?searchtype=author&query=Prater-Bennette%2C+A">Ashley Prater-Bennette</a>, 
<a href="/search/cs?searchtype=author&query=Asif%2C+M+S">M. Salman Asif</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 3 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04361" title="Abstract">arXiv:2310.04361</a> (replaced) [<a href="/pdf/2310.04361" title="Download PDF">pdf</a>, <a href="/format/2310.04361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SADMoE: Exploiting Activation Sparsity with Dynamic-k Gating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szatkowski%2C+F">Filip Szatkowski</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B3jcik%2C+B">Bartosz W&#xf3;jcik</a>, 
<a href="/search/cs?searchtype=author&query=Pi%C3%B3rczy%C5%84ski%2C+M">Miko&#x142;aj Pi&#xf3;rczy&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Adamczewski%2C+K">Kamil Adamczewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04395" title="Abstract">arXiv:2310.04395</a> (replaced) [<a href="/pdf/2310.04395" title="Download PDF">pdf</a>, <a href="/format/2310.04395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Self-Consistency for Data-Efficient Amortized Bayesian  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+M">Marvin Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Ivanova%2C+D+R">Desi R. Ivanova</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+D">Daniel Habermann</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6the%2C+U">Ullrich K&#xf6;the</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCrkner%2C+P">Paul-Christian B&#xfc;rkner</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+S+T">Stefan T. Radev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> previously published as an extended abstract at NeurIPS UniReps 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04668" title="Abstract">arXiv:2310.04668</a> (replaced) [<a href="/pdf/2310.04668" title="Download PDF">pdf</a>, <a href="/format/2310.04668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-free Node Classification on Graphs with Large Language Models  (LLMS)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available via <a href="https://github.com/CurryTang/LLMGNN">this https URL</a>; ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05885" title="Abstract">arXiv:2310.05885</a> (replaced) [<a href="/pdf/2310.05885" title="Download PDF">pdf</a>, <a href="/format/2310.05885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation  for Tree Policy Planning in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Karkus%2C+P">Peter Karkus</a>, 
<a href="/search/cs?searchtype=author&query=Ivanovic%2C+B">Boris Ivanovic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chen Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 IEEE International Conference on Robotics and Automation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06213" title="Abstract">arXiv:2310.06213</a> (replaced) [<a href="/pdf/2310.06213" title="Download PDF">pdf</a>, <a href="/format/2310.06213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoLLM: Extracting Geospatial Knowledge from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manvi%2C+R">Rohin Manvi</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Samar Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M">Marshall Burke</a>, 
<a href="/search/cs?searchtype=author&query=Lobell%2C+D">David Lobell</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06272" title="Abstract">arXiv:2310.06272</a> (replaced) [<a href="/pdf/2310.06272" title="Download PDF">pdf</a>, <a href="/format/2310.06272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let Models Speak Ciphers: Multiagent Debate through Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Chau Pham</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yingxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06286" title="Abstract">arXiv:2310.06286</a> (replaced) [<a href="/pdf/2310.06286" title="Download PDF">pdf</a>, <a href="/format/2310.06286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suppressing Overestimation in Q-Learning through Adversarial Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">HyeAnn Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06707" title="Abstract">arXiv:2310.06707</a> (replaced) [<a href="/pdf/2310.06707" title="Download PDF">pdf</a>, <a href="/format/2310.06707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Aware Translation Models: Efficient Generation and Quality  Estimation in a Single Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tomani%2C+C">Christian Tomani</a>, 
<a href="/search/cs?searchtype=author&query=Vilar%2C+D">David Vilar</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Cherry%2C+C">Colin Cherry</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+S">Subhajit Naskar</a>, 
<a href="/search/cs?searchtype=author&query=Finkelstein%2C+M">Mara Finkelstein</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+X">Xavier Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07088" title="Abstract">arXiv:2310.07088</a> (replaced) [<a href="/pdf/2310.07088" title="Download PDF">pdf</a>, <a href="/format/2310.07088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity of Thought Improves Reasoning Abilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naik%2C+R">Ranjita Naik</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Yuksekgonul%2C+M">Mert Yuksekgonul</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Nushi%2C+B">Besmira Nushi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07968" title="Abstract">arXiv:2310.07968</a> (replaced) [<a href="/pdf/2310.07968" title="Download PDF">pdf</a>, <a href="/format/2310.07968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think, Act, and Ask: Open-World Interactive Personalized Robot  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yinpei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Run Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video URL: <a href="https://www.youtube.com/watch?v=QW6rMHVpxUY">this https URL</a> Code URL: <a href="https://github.com/sled-group/navchat">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08337" title="Abstract">arXiv:2310.08337</a> (replaced) [<a href="/pdf/2310.08337" title="Download PDF">pdf</a>, <a href="/format/2310.08337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartosh%2C+G">Grigory Bartosh</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>, 
<a href="/search/cs?searchtype=author&query=Naesseth%2C+C+A">Christian A. Naesseth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08915" title="Abstract">arXiv:2310.08915</a> (replaced) [<a href="/pdf/2310.08915" title="Download PDF">pdf</a>, <a href="/format/2310.08915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lirui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Mingbao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yunyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yiwu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xingjia Han</a>, 
<a href="/search/cs?searchtype=author&query=Tanner%2C+J">Jared Tanner</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09017" title="Abstract">arXiv:2310.09017</a> (replaced) [<a href="/pdf/2310.09017" title="Download PDF">pdf</a>, <a href="/format/2310.09017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dont Add, dont Miss: Effective Content Preserving Generation from  Pre-Selected Text Spans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slobodkin%2C+A">Aviv Slobodkin</a>, 
<a href="/search/cs?searchtype=author&query=Caciularu%2C+A">Avi Caciularu</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+E">Eran Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Dagan%2C+I">Ido Dagan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09266" title="Abstract">arXiv:2310.09266</a> (replaced) [<a href="/pdf/2310.09266" title="Download PDF">pdf</a>, <a href="/format/2310.09266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Inference Attacks on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kandpal%2C+N">Nikhil Kandpal</a>, 
<a href="/search/cs?searchtype=author&query=Pillutla%2C+K">Krishna Pillutla</a>, 
<a href="/search/cs?searchtype=author&query=Oprea%2C+A">Alina Oprea</a>, 
<a href="/search/cs?searchtype=author&query=Kairouz%2C+P">Peter Kairouz</a>, 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C+A">Christopher A. Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zheng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 contains experiments on additional datasets and differential privacy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09291" title="Abstract">arXiv:2310.09291</a> (replaced) [<a href="/pdf/2310.09291" title="Download PDF">pdf</a>, <a href="/format/2310.09291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-by-Language for Training-Free Compositional Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karthik%2C+S">Shyamgopal Karthik</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+K">Karsten Roth</a>, 
<a href="/search/cs?searchtype=author&query=Mancini%2C+M">Massimiliano Mancini</a>, 
<a href="/search/cs?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10117" title="Abstract">arXiv:2310.10117</a> (replaced) [<a href="/pdf/2310.10117" title="Download PDF">pdf</a>, <a href="/format/2310.10117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Convex Global and Local Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chuan He</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Le Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Ju Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10640" title="Abstract">arXiv:2310.10640</a> (replaced) [<a href="/pdf/2310.10640" title="Download PDF">pdf</a>, <a href="/format/2310.10640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Blueprint: Enabling Text-to-Image Generation with Complex and  Detailed Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gani%2C+H">Hanan Gani</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S+F">Shariq Farooq Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Wonka%2C+P">Peter Wonka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11044" title="Abstract">arXiv:2310.11044</a> (replaced) [<a href="/pdf/2310.11044" title="Download PDF">pdf</a>, <a href="/ps/2310.11044" title="Download PostScript">ps</a>, <a href="/format/2310.11044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tutorial on Near-Field XL-MIMO Communications Towards 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haiquan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Changsheng You</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhenjun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xiaohu You</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11447" title="Abstract">arXiv:2310.11447</a> (replaced) [<a href="/pdf/2310.11447" title="Download PDF">pdf</a>, <a href="/ps/2310.11447" title="Download PostScript">ps</a>, <a href="/format/2310.11447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nondango is NP-Complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruangwises%2C+S">Suthee Ruangwises</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11762" title="Abstract">arXiv:2310.11762</a> (replaced) [<a href="/pdf/2310.11762" title="Download PDF">pdf</a>, <a href="/format/2310.11762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quasi-Wasserstein Loss for Learning Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minjie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12491" title="Abstract">arXiv:2310.12491</a> (replaced) [<a href="/pdf/2310.12491" title="Download PDF">pdf</a>, <a href="/format/2310.12491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hiding Access-pattern is Not Enough! Veil: A Storage and Communication  Efficient Volume-Hiding Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+V">Vishal Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Goodrich%2C+M">Michael Goodrich</a>, 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+S">Sharad Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shantanu Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12615" title="Abstract">arXiv:2310.12615</a> (replaced) [<a href="/pdf/2310.12615" title="Download PDF">pdf</a>, <a href="/format/2310.12615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Abstractions for Characterizing Communication Requirements in  Asynchronous Distributed Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galeana%2C+H+R">Hugo Rincon Galeana</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+U">Ulrich Schmid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Discrete Mathematics (cs.DM); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12809" title="Abstract">arXiv:2310.12809</a> (replaced) [<a href="/pdf/2310.12809" title="Download PDF">pdf</a>, <a href="/format/2310.12809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Forecasting at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprangers%2C+O">Olivier Sprangers</a>, 
<a href="/search/cs?searchtype=author&query=Wadman%2C+W">Wander Wadman</a>, 
<a href="/search/cs?searchtype=author&query=Schelter%2C+S">Sebastian Schelter</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12831" title="Abstract">arXiv:2310.12831</a> (replaced) [<a href="/pdf/2310.12831" title="Download PDF">pdf</a>, <a href="/format/2310.12831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Metric Imitation Learning for Stable Motion Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Dattari%2C+R">Rodrigo P&#xe9;rez-Dattari</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12934" title="Abstract">arXiv:2310.12934</a> (replaced) [<a href="/pdf/2310.12934" title="Download PDF">pdf</a>, <a href="/format/2310.12934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Flow Networks as Entropy-Regularized RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+N">Nikita Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13653" title="Abstract">arXiv:2310.13653</a> (replaced) [<a href="/pdf/2310.13653" title="Download PDF">pdf</a>, <a href="/format/2310.13653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport for Measures with Noisy Tree Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Le%2C+T">Tam Le</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+T">Truyen Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Fukumizu%2C+K">Kenji Fukumizu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in AISTATS'2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13995" title="Abstract">arXiv:2310.13995</a> (replaced) [<a href="/pdf/2310.13995" title="Download PDF">pdf</a>, <a href="/format/2310.13995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Bilingual Lexicon Induction with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaoyiran Li</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 9577-9599
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14053" title="Abstract">arXiv:2310.14053</a> (replaced) [<a href="/pdf/2310.14053" title="Download PDF">pdf</a>, <a href="/format/2310.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Accuracy: Evaluating Self-Consistency of Code Large Language  Models with IdentityChain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+M+J">Marcus J. Min</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yangruibo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Buratti%2C+L">Luca Buratti</a>, 
<a href="/search/cs?searchtype=author&query=Pujar%2C+S">Saurabh Pujar</a>, 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+G">Gail Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+S">Suman Jana</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14403" title="Abstract">arXiv:2310.14403</a> (replaced) [<a href="/pdf/2310.14403" title="Download PDF">pdf</a>, <a href="/format/2310.14403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> O3D: Offline Data-driven Discovery and Distillation for Sequential  Decision-Making with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yuchen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Madhushani%2C+U">Udari Madhushani</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+J">Jared Vann</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+D">Deepeka Garg</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+S">Sumitra Ganesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14592" title="Abstract">arXiv:2310.14592</a> (replaced) [<a href="/pdf/2310.14592" title="Download PDF">pdf</a>, <a href="/format/2310.14592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Training LiDAR-Based 3D Object Detectors Through Colorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+T">Tai-Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianle Chen</a>, 
<a href="/search/cs?searchtype=author&query=Phoo%2C+C+P">Cheng Perng Phoo</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K+Z">Katie Z Luo</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yurong You</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+M">Mark Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Hariharan%2C+B">Bharath Hariharan</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15213" title="Abstract">arXiv:2310.15213</a> (replaced) [<a href="/pdf/2310.15213" title="Download PDF">pdf</a>, <a href="/format/2310.15213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Function Vectors in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Todd%2C+E">Eric Todd</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+L">Millicent L. Li</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A+S">Arnab Sen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+A">Aaron Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. 52 pages, 30 figures, 23 tables. Code and data at <a href="https://functions.baulab.info">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15950" title="Abstract">arXiv:2310.15950</a> (replaced) [<a href="/pdf/2310.15950" title="Download PDF">pdf</a>, <a href="/format/2310.15950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning with Large Language Models for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lixin Su</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Suqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a WWW'24 full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16592" title="Abstract">arXiv:2310.16592</a> (replaced) [<a href="/pdf/2310.16592" title="Download PDF">pdf</a>, <a href="/format/2310.16592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-the-air Federated Policy Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huiwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lingying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Subhrakanti Dey</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Ling Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16603" title="Abstract">arXiv:2310.16603</a> (replaced) [<a href="/pdf/2310.16603" title="Download PDF">pdf</a>, <a href="/format/2310.16603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certifying Bimanual RRT Motion Plans in a Second
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amice%2C+A">Alexandre Amice</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+P">Peter Werner</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17653" title="Abstract">arXiv:2310.17653</a> (replaced) [<a href="/pdf/2310.17653" title="Download PDF">pdf</a>, <a href="/format/2310.17653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fantastic Gains and Where to Find Them: On the Existence and Prospect of  General Knowledge Transfer between Any Pretrained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+K">Karsten Roth</a>, 
<a href="/search/cs?searchtype=author&query=Thede%2C+L">Lukas Thede</a>, 
<a href="/search/cs?searchtype=author&query=Koepke%2C+A+S">Almut Sophia Koepke</a>, 
<a href="/search/cs?searchtype=author&query=Vinyals%2C+O">Oriol Vinyals</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9naff%2C+O">Olivier H&#xe9;naff</a>, 
<a href="/search/cs?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17832" title="Abstract">arXiv:2310.17832</a> (replaced) [<a href="/pdf/2310.17832" title="Download PDF">pdf</a>, <a href="/format/2310.17832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlled transport of fluid particles by microrotors in a Stokes flow  using linear transfer operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buzhardt%2C+J">Jake Buzhardt</a>, 
<a href="/search/physics?searchtype=author&query=Tallapragada%2C+P">Phanindra Tallapragada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures. Accepted to Physics of Fluids
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18285" title="Abstract">arXiv:2310.18285</a> (replaced) [<a href="/pdf/2310.18285" title="Download PDF">pdf</a>, <a href="/format/2310.18285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Prompt-Tuning in Bridging Generalized and  Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wenlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18306" title="Abstract">arXiv:2310.18306</a> (replaced) [<a href="/pdf/2310.18306" title="Download PDF">pdf</a>, <a href="/format/2310.18306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised and Penalized Baseline Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andries%2C+E">Erik Andries</a>, 
<a href="/search/stat?searchtype=author&query=Nikzad-Langerodi%2C+R">Ramin Nikzad-Langerodi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; 9 figures; 2 tables; fixed typos; additional sanity checks for grammar and syntax; streamlined text and made minor cosmetic changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18629" title="Abstract">arXiv:2310.18629</a> (replaced) [<a href="/pdf/2310.18629" title="Download PDF">pdf</a>, <a href="/ps/2310.18629" title="Download PostScript">ps</a>, <a href="/format/2310.18629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach  with High Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenlong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Porte-Agel%2C+F">Fernando Porte-Agel</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiannong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bak-Jensen%2C+B">Birgitte Bak-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+G">Guangchun Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18884" title="Abstract">arXiv:2310.18884</a> (replaced) [<a href="/pdf/2310.18884" title="Download PDF">pdf</a>, <a href="/format/2310.18884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Asymmetric Graph Contrastive Learning without Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huaisheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19253" title="Abstract">arXiv:2310.19253</a> (replaced) [<a href="/pdf/2310.19253" title="Download PDF">pdf</a>, <a href="/format/2310.19253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow-based Distributionally Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Journal on Selected Areas in Information Theory (JSAIT). Accepted. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19852" title="Abstract">arXiv:2310.19852</a> (replaced) [<a href="/pdf/2310.19852" title="Download PDF">pdf</a>, <a href="/format/2310.19852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Alignment: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+H">Hantao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yawen Duan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhonghao He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+K+Y">Kwan Yee Ng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=O%27Gara%2C+A">Aidan O&#x27;Gara</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yingshan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tse%2C+B">Brian Tse</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S">Stephen McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wen Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Continually updated, including weak-to-strong generalization and socio-technical thinking. 58 pages (excluding bibliography), 801 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20054" title="Abstract">arXiv:2310.20054</a> (replaced) [<a href="/pdf/2310.20054" title="Download PDF">pdf</a>, <a href="/format/2310.20054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Hierarchical Monte Carlo Belief-State Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamgochian%2C+A">Arec Jamgochian</a>, 
<a href="/search/cs?searchtype=author&query=Buurmeijer%2C+H">Hugo Buurmeijer</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+K+H">Kyle H. Wray</a>, 
<a href="/search/cs?searchtype=author&query=Corso%2C+A">Anthony Corso</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20141" title="Abstract">arXiv:2310.20141</a> (replaced) [<a href="/pdf/2310.20141" title="Download PDF">pdf</a>, <a href="/format/2310.20141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Difference Predictive Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Website (<a href="https://chongyi-zheng.github.io/td_infonce">this https URL</a>) and code (<a href="https://github.com/chongyi-zheng/td_infonce">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20181" title="Abstract">arXiv:2310.20181</a> (replaced) [<a href="/pdf/2310.20181" title="Download PDF">pdf</a>, <a href="/format/2310.20181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An explicit and symmetric exponential wave integrator for the nonlinear  Schr&#xf6;dinger equation with low regularity potential and nonlinearity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bao%2C+W">Weizhu Bao</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chushan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20290" title="Abstract">arXiv:2310.20290</a> (replaced) [<a href="/pdf/2310.20290" title="Download PDF">pdf</a>, <a href="/format/2310.20290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Rayleigh Quotient Iteration for Dual Quaternion Hermitian Eigenvalue  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duan%2C+S">Shan-Qi Duan</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Q">Qing-Wen Wang</a>, 
<a href="/search/math?searchtype=author&query=Duan%2C+X">Xue-Feng Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2111.12211">arXiv:2111.12211</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01200" title="Abstract">arXiv:2311.01200</a> (replaced) [<a href="/pdf/2311.01200" title="Download PDF">pdf</a>, <a href="/format/2311.01200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning Under Language Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gogoulou%2C+E">Evangelia Gogoulou</a>, 
<a href="/search/cs?searchtype=author&query=Lesort%2C+T">Timoth&#xe9;e Lesort</a>, 
<a href="/search/cs?searchtype=author&query=Boman%2C+M">Magnus Boman</a>, 
<a href="/search/cs?searchtype=author&query=Nivre%2C+J">Joakim Nivre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01270" title="Abstract">arXiv:2311.01270</a> (replaced) [<a href="/pdf/2311.01270" title="Download PDF">pdf</a>, <a href="/format/2311.01270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> People Make Better Edits: Measuring the Efficacy of LLM-Generated  Counterfactually Augmented Data for Harmful Language Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+I">Indira Sen</a>, 
<a href="/search/cs?searchtype=author&query=Assenmacher%2C+D">Dennis Assenmacher</a>, 
<a href="/search/cs?searchtype=author&query=Samory%2C+M">Mattia Samory</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aalst%2C+W">Wil van der Aalst</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+C">Claudia Wagner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of EMNLP'23 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02084" title="Abstract">arXiv:2311.02084</a> (replaced) [<a href="/pdf/2311.02084" title="Download PDF">pdf</a>, <a href="/format/2311.02084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ITEm: Unsupervised Image-Text Embedding Learning for eCommerce
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Kozielski%2C+M">Michael Kozielski</a>, 
<a href="/search/cs?searchtype=author&query=Hewavitharana%2C+S">Sanjika Hewavitharana</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiangbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Khadivi%2C+S">Shahram Khadivi</a>, 
<a href="/search/cs?searchtype=author&query=Lancewicki%2C+T">Tomer Lancewicki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02667" title="Abstract">arXiv:2311.02667</a> (replaced) [<a href="/pdf/2311.02667" title="Download PDF">pdf</a>, <a href="/format/2311.02667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Race Against the Machine: a Fully-annotated, Open-design Dataset of  Autonomous and Piloted High-speed Flight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bosello%2C+M">Michael Bosello</a>, 
<a href="/search/cs?searchtype=author&query=Aguiari%2C+D">Davide Aguiari</a>, 
<a href="/search/cs?searchtype=author&query=Keuter%2C+Y">Yvo Keuter</a>, 
<a href="/search/cs?searchtype=author&query=Pallotta%2C+E">Enrico Pallotta</a>, 
<a href="/search/cs?searchtype=author&query=Kiade%2C+S">Sara Kiade</a>, 
<a href="/search/cs?searchtype=author&query=Caminati%2C+G">Gyordan Caminati</a>, 
<a href="/search/cs?searchtype=author&query=Pinzarrone%2C+F">Flavio Pinzarrone</a>, 
<a href="/search/cs?searchtype=author&query=Halepota%2C+J">Junaid Halepota</a>, 
<a href="/search/cs?searchtype=author&query=Panerati%2C+J">Jacopo Panerati</a>, 
<a href="/search/cs?searchtype=author&query=Pau%2C+G">Giovanni Pau</a> (Technology Innovation Institute)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03628" title="Abstract">arXiv:2311.03628</a> (replaced) [<a href="/pdf/2311.03628" title="Download PDF">pdf</a>, <a href="/format/2311.03628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Twinning: from digital twins to model-based reinforcement  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Schena%2C+L">Lorenzo Schena</a>, 
<a href="/search/eess?searchtype=author&query=Marques%2C+P">Pedro Marques</a>, 
<a href="/search/eess?searchtype=author&query=Poletti%2C+R">Romain Poletti</a>, 
<a href="/search/eess?searchtype=author&query=Ahizi%2C+S">Samuel Ahizi</a>, 
<a href="/search/eess?searchtype=author&query=Van+den+Berghe%2C+J">Jan Van den Berghe</a>, 
<a href="/search/eess?searchtype=author&query=Mendez%2C+M+A">Miguel A. Mendez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted Journal of Computational Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03732" title="Abstract">arXiv:2311.03732</a> (replaced) [<a href="/pdf/2311.03732" title="Download PDF">pdf</a>, <a href="/format/2311.03732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Learn for Few-shot Continual Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+S">Stella Ho</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Longxiang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the pre-print version which has not been fully undergone peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04924" title="Abstract">arXiv:2311.04924</a> (replaced) [<a href="/pdf/2311.04924" title="Download PDF">pdf</a>, <a href="/ps/2311.04924" title="Download PostScript">ps</a>, <a href="/format/2311.04924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning-less Object Naming with a Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lucny%2C+A">Andrej Lucny</a>, 
<a href="/search/cs?searchtype=author&query=Petrovic%2C+P">Pavel Petrovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was funded (or co-funded) by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338 World Symposium on Digital Intelligence for Systems and Machines (DISA2023), Kosice, September 21-22, 2023 citations: <a href="https://ieeexplore.ieee.org/document/10308905">this https URL</a> codes: <a href="https://github.com/andylucny/whatisthis">this https URL</a>, <a href="https://doi.org/10.5281/zenodo.10702868">this https URL</a> 7 pages, 9 figures, 0 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 World Symposium on Digital Intelligence for Systems and
  Machines (DISA) https://ieeexplore.ieee.org/xpl/conhome/10308901/proceeding
  pages 154-160
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05462" title="Abstract">arXiv:2311.05462</a> (replaced) [<a href="/pdf/2311.05462" title="Download PDF">pdf</a>, <a href="/format/2311.05462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT and Other Large Language Models for Cybersecurity of Smart Grid  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaboli%2C+A">Aydin Zaboli</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S+L">Seong Lok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tai-Jin Song</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Junho Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, Accepted, 2024 IEEE Power &amp; Energy Society General Meeting (PESGM), Seattle, WA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05928" title="Abstract">arXiv:2311.05928</a> (replaced) [<a href="/pdf/2311.05928" title="Download PDF">pdf</a>, <a href="/format/2311.05928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Shape of Learning: Anisotropy and Intrinsic Dimensions in  Transformer-Based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razzhigaev%2C+A">Anton Razzhigaev</a>, 
<a href="/search/cs?searchtype=author&query=Mikhalchuk%2C+M">Matvey Mikhalchuk</a>, 
<a href="/search/cs?searchtype=author&query=Goncharova%2C+E">Elizaveta Goncharova</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>, 
<a href="/search/cs?searchtype=author&query=Dimitrov%2C+D">Denis Dimitrov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+A">Andrey Kuznetsov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); General Topology (math.GN)

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06056" title="Abstract">arXiv:2311.06056</a> (replaced) [<a href="/pdf/2311.06056" title="Download PDF">pdf</a>, <a href="/format/2311.06056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual  Categorization Targeting Limited Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Ziye Fang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zechao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Publication in TCSVT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06156" title="Abstract">arXiv:2311.06156</a> (replaced) [<a href="/pdf/2311.06156" title="Download PDF">pdf</a>, <a href="/format/2311.06156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triad: Trusted Timestamps in Untrusted Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+G+P">Gabriel P. Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Brito%2C+A">Andrey Brito</a>, 
<a href="/search/cs?searchtype=author&query=Fetzer%2C+C">Christof Fetzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06918" title="Abstract">arXiv:2311.06918</a> (replaced) [<a href="/pdf/2311.06918" title="Download PDF">pdf</a>, <a href="/format/2311.06918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource-Aware Hierarchical Federated Learning for Video Caching in  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pervej%2C+M+F">Md Ferdous Pervej</a>, 
<a href="/search/cs?searchtype=author&query=Molisch%2C+A+F">Andreas F Molisch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE ICC 2024. \c{opyright} 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06956" title="Abstract">arXiv:2311.06956</a> (replaced) [<a href="/pdf/2311.06956" title="Download PDF">pdf</a>, <a href="/format/2311.06956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegReg: Segmenting OARs by Registering MR Images and CT Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xuyin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Biao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hien Le</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+B">Bora Jeong</a>, 
<a href="/search/cs?searchtype=author&query=To%2C+M">Minh-Son To</a>, 
<a href="/search/cs?searchtype=author&query=Hartley%2C+R">Richard Hartley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07390" title="Abstract">arXiv:2311.07390</a> (replaced) [<a href="/pdf/2311.07390" title="Download PDF">pdf</a>, <a href="/format/2311.07390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Significance of Outdoor Advertising from Driver&#x27;s  Perspective Using Computer Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C4%8Cernekov%C3%A1%2C+Z">Zuzana &#x10c;ernekov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Haladov%C3%A1%2C+Z+B">Zuzana Berger Haladov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0pirka%2C+J">J&#xe1;n &#x160;pirka</a>, 
<a href="/search/cs?searchtype=author&query=Kocur%2C+V">Viktor Kocur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pubslished in 2023 World Symposium on Digital Intelligence for Systems and Machines (DISA) Proceedings. Published version copyrighted by IEEE. Accepted: 7.8.2023. Published: 15.11.2023. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Code: <a href="https://doi.org/10.5281/zenodo.10689666">this https URL</a> Data: <a href="https://doi.org/10.5281/zenodo.10689664">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07398" title="Abstract">arXiv:2311.07398</a> (replaced) [<a href="/pdf/2311.07398" title="Download PDF">pdf</a>, <a href="/format/2311.07398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Processing and Segmentation of Human Teeth from 2D Images using Weakly  Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunzo%2C+T">Tom&#xe1;&#x161; Kunzo</a>, 
<a href="/search/cs?searchtype=author&query=Kocur%2C+V">Viktor Kocur</a>, 
<a href="/search/cs?searchtype=author&query=Gajdo%C5%A1ech%2C+L">Luk&#xe1;&#x161; Gajdo&#x161;ech</a>, 
<a href="/search/cs?searchtype=author&query=Madaras%2C+M">Martin Madaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pubslished in 2023 World Symposium on Digital Intelligence for Systems and Machines (DISA) Proceedings. Published version copyrighted by IEEE. Accepted: 7.8.2023. Published: 15.11.2023. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Code: <a href="https://doi.org/10.5281/zenodo.10688264">this https URL</a> Data: <a href="https://doi.org/10.5281/zenodo.10688365">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07432" title="Abstract">arXiv:2311.07432</a> (replaced) [<a href="/pdf/2311.07432" title="Download PDF">pdf</a>, <a href="/format/2311.07432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supersampling of Data from Structured-light Scanner with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melicher%C4%8D%C3%ADk%2C+M">Martin Melicher&#x10d;&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Gajdo%C5%A1ech%2C+L">Luk&#xe1;&#x161; Gajdo&#x161;ech</a>, 
<a href="/search/cs?searchtype=author&query=Kocur%2C+V">Viktor Kocur</a>, 
<a href="/search/cs?searchtype=author&query=Madaras%2C+M">Martin Madaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pubslished in 2023 World Symposium on Digital Intelligence for Systems and Machines (DISA) Proceedings. Published version copyrighted by IEEE. Accepted: 7.8.2023. Published: 15.11.2023. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Code:<a href="https://doi.org/10.5281/zenodo.10688235">this https URL</a> Data: <a href="https://doi.org/10.5281/zenodo.10688199">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07564" title="Abstract">arXiv:2311.07564</a> (replaced) [<a href="/pdf/2311.07564" title="Download PDF">pdf</a>, <a href="/format/2311.07564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Authorship Attribution Models Distinguish Speakers in Speech  Transcripts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggazzotti%2C+C">Cristina Aggazzotti</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+N">Nicholas Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+E+A">Elizabeth Allyn Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added more baselines, improved fine-tuning results, added pre-training domain experiment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07829" title="Abstract">arXiv:2311.07829</a> (replaced) [<a href="/pdf/2311.07829" title="Download PDF">pdf</a>, <a href="/ps/2311.07829" title="Download PostScript">ps</a>, <a href="/format/2311.07829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Coding Scheme for Unresponsive and Byzantine Server Resilient Quantum  $X$-Secure $T$-Private Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxiang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jafar%2C+S+A">Syed A. Jafar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08600" title="Abstract">arXiv:2311.08600</a> (replaced) [<a href="/pdf/2311.08600" title="Download PDF">pdf</a>, <a href="/format/2311.08600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivation of sixth-order exponential Runge--Kutta methods for stiff  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luan%2C+V+T">Vu Thai Luan</a>, 
<a href="/search/math?searchtype=author&query=Alhsmy%2C+T">Trky Alhsmy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Applied Mathematics Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08941" title="Abstract">arXiv:2311.08941</a> (replaced) [<a href="/pdf/2311.08941" title="Download PDF">pdf</a>, <a href="/format/2311.08941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning over Description Logic-based Contexts with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poulis%2C+A">Angelos Poulis</a>, 
<a href="/search/cs?searchtype=author&query=Tsalapati%2C+E">Eleni Tsalapati</a>, 
<a href="/search/cs?searchtype=author&query=Koubarakis%2C+M">Manolis Koubarakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08996" title="Abstract">arXiv:2311.08996</a> (replaced) [<a href="/pdf/2311.08996" title="Download PDF">pdf</a>, <a href="/format/2311.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasic%2C+F">Faruk Pasic</a>, 
<a href="/search/cs?searchtype=author&query=Hofer%2C+M">Markus Hofer</a>, 
<a href="/search/cs?searchtype=author&query=Mussbah%2C+M">Mariam Mussbah</a>, 
<a href="/search/cs?searchtype=author&query=Caban%2C+S">Sebastian Caban</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+S">Stefan Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Zemen%2C+T">Thomas Zemen</a>, 
<a href="/search/cs?searchtype=author&query=Mecklenbr%C3%A4uker%2C+C+F">Christoph F. Mecklenbr&#xe4;uker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to International Conference on Smart Applications, Communications and Networking (SmartNets), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09101" title="Abstract">arXiv:2311.09101</a> (replaced) [<a href="/pdf/2311.09101" title="Download PDF">pdf</a>, <a href="/format/2311.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Unified View of Answer Calibration for Multi-Step Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Oo%2C+N">Nay Oo</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09106" title="Abstract">arXiv:2311.09106</a> (replaced) [<a href="/pdf/2311.09106" title="Download PDF">pdf</a>, <a href="/format/2311.09106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;We Demand Justice!&quot;: Towards Social Context Grounding of Political  Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pujari%2C+R">Rajkumar Pujari</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chengfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Goldwasser%2C+D">Dan Goldwasser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Was accepted to and withdrawn from Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09114" title="Abstract">arXiv:2311.09114</a> (replaced) [<a href="/pdf/2311.09114" title="Download PDF">pdf</a>, <a href="/format/2311.09114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ever: Mitigating Hallucination in Large Language Models through  Real-Time Verification and Rectification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Haoqiang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Juntong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09393" title="Abstract">arXiv:2311.09393</a> (replaced) [<a href="/pdf/2311.09393" title="Download PDF">pdf</a>, <a href="/format/2311.09393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic  Oblivious Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qianchuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Delaware%2C+B">Benjamin Delaware</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09522" title="Abstract">arXiv:2311.09522</a> (replaced) [<a href="/pdf/2311.09522" title="Download PDF">pdf</a>, <a href="/format/2311.09522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reversed Indexes $\approx$ Values in Wavelet Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiangjun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This prerpint was rejected from ACM STOC 2024, and updated with the review comments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10666" title="Abstract">arXiv:2311.10666</a> (replaced) [<a href="/pdf/2311.10666" title="Download PDF">pdf</a>, <a href="/ps/2311.10666" title="Download PostScript">ps</a>, <a href="/format/2311.10666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A tight lower bound on the minimal dispersion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tr%C3%B6dler%2C+M">Mat&#x11b;j Tr&#xf6;dler</a>, 
<a href="/search/math?searchtype=author&query=Volec%2C+J">Jan Volec</a>, 
<a href="/search/math?searchtype=author&query=Vyb%C3%ADral%2C+J">Jan Vyb&#xed;ral</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10671" title="Abstract">arXiv:2311.10671</a> (replaced) [<a href="/pdf/2311.10671" title="Download PDF">pdf</a>, <a href="/format/2311.10671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+M">Marvin Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+S+T">Stefan T. Radev</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCrkner%2C+P">Paul-Christian B&#xfc;rkner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11157" title="Abstract">arXiv:2311.11157</a> (replaced) [<a href="/pdf/2311.11157" title="Download PDF">pdf</a>, <a href="/format/2311.11157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualizing Internet Memes Across Social Media Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Saurav Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11463" title="Abstract">arXiv:2311.11463</a> (replaced) [<a href="/pdf/2311.11463" title="Download PDF">pdf</a>, <a href="/format/2311.11463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing monitoring strategies for deployed machine learning  algorithms: navigating performativity through a causal lens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jean Feng</a>, 
<a href="/search/cs?searchtype=author&query=Subbaswamy%2C+A">Adarsh Subbaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Gossmann%2C+A">Alexej Gossmann</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harvineet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sahiner%2C+B">Berkman Sahiner</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mi-Ok Kim</a>, 
<a href="/search/cs?searchtype=author&query=Pennello%2C+G">Gene Pennello</a>, 
<a href="/search/cs?searchtype=author&query=Petrick%2C+N">Nicholas Petrick</a>, 
<a href="/search/cs?searchtype=author&query=Pirracchio%2C+R">Romain Pirracchio</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fan Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12351" title="Abstract">arXiv:2311.12351</a> (replaced) [<a href="/pdf/2311.12351" title="Download PDF">pdf</a>, <a href="/format/2311.12351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Transformer Architecture in Long-Context Large Language  Models: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunpeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+J">Junyu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zixu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Taolue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zenan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lijuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shupeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Penghao Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12882" title="Abstract">arXiv:2311.12882</a> (replaced) [<a href="/pdf/2311.12882" title="Download PDF">pdf</a>, <a href="/ps/2311.12882" title="Download PostScript">ps</a>, <a href="/format/2311.12882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs-Healthcare : Current Applications and Challenges of Large Language  Models in various Medical Specialties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mumtaz%2C+U">Ummara Mumtaz</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Awais Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Mumtaz%2C+S">Summaya Mumtaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages and one figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13226" title="Abstract">arXiv:2311.13226</a> (replaced) [<a href="/pdf/2311.13226" title="Download PDF">pdf</a>, <a href="/format/2311.13226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot at the Mirror: Learning to Imitate via Associating Self-supervised  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lucny%2C+A">Andrej Lucny</a>, 
<a href="/search/cs?searchtype=author&query=Malinovska%2C+K">Kristina Malinovska</a>, 
<a href="/search/cs?searchtype=author&query=Farkas%2C+I">Igor Farkas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was funded (or co-funded) by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338, 32nd International Conference on Artificial Neural Networks, Heraklion, Greece, September 26-29, 2023, citations: <a href="https://link.springer.com/chapter/10.1007/978-3-031-44207-0_39">this https URL</a>, codes: <a href="https://github.com/andylucny/learningImitation/tree/main/mirror">this https URL</a>, 12 pages, 3 figures, 0 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Neural Networks and Machine Learning - ICANN 2023
  https://link.springer.com/chapter/10.1007/978-3-031-44207-0_39 pages 471-482
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13541" title="Abstract">arXiv:2311.13541</a> (replaced) [<a href="/pdf/2311.13541" title="Download PDF">pdf</a>, <a href="/format/2311.13541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Log-Normal Attention with Unbiased Concentration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nahshan%2C+Y">Yury Nahshan</a>, 
<a href="/search/cs?searchtype=author&query=Kampeas%2C+J">Joseph Kampeas</a>, 
<a href="/search/cs?searchtype=author&query=Haleva%2C+E">Emir Haleva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 20 figures, 5 tables, submitted to ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14153" title="Abstract">arXiv:2311.14153</a> (replaced) [<a href="/pdf/2311.14153" title="Download PDF">pdf</a>, <a href="/format/2311.14153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC  using Tube-Guided Data Augmentation and NeRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tagliabue%2C+A">Andrea Tagliabue</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video: <a href="https://youtu.be/_W5z33ZK1m4.">this https URL</a> Evolved paper from our previous work: <a href="/abs/2210.10127">arXiv:2210.10127</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14530" title="Abstract">arXiv:2311.14530</a> (replaced) [<a href="/pdf/2311.14530" title="Download PDF">pdf</a>, <a href="/ps/2311.14530" title="Download PostScript">ps</a>, <a href="/format/2311.14530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Translation for Ge&#x27;ez Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wassie%2C+A+K">Aman Kassahun Wassie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14986" title="Abstract">arXiv:2311.14986</a> (replaced) [<a href="/pdf/2311.14986" title="Download PDF">pdf</a>, <a href="/format/2311.14986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAME++: A Self-supervised Anatomical eMbeddings Enhanced medical image  registration framework using stable sampling and regularized transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Lin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fengze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiaoyu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jia Ge</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xianghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Daikai Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15156" title="Abstract">arXiv:2311.15156</a> (replaced) [<a href="/pdf/2311.15156" title="Download PDF">pdf</a>, <a href="/format/2311.15156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> xTrimoGene: An Efficient and Scalable Representation Learner for  Single-Cell RNA-Seq Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jing Gong</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+M">Minsheng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xingyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuegong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Taifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Le Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Genomics (q-bio.GN)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15443" title="Abstract">arXiv:2311.15443</a> (replaced) [<a href="/pdf/2311.15443" title="Download PDF">pdf</a>, <a href="/format/2311.15443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCRA: A Distributed Chiplet-based Reconfigurable Architecture for  Irregular Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orenes-Vera%2C+M">Marcelo Orenes-Vera</a>, 
<a href="/search/cs?searchtype=author&query=Tureci%2C+E">Esin Tureci</a>, 
<a href="/search/cs?searchtype=author&query=Martonosi%2C+M">Margaret Martonosi</a>, 
<a href="/search/cs?searchtype=author&query=Wentzlaff%2C+D">David Wentzlaff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18426" title="Abstract">arXiv:2311.18426</a> (replaced) [<a href="/pdf/2311.18426" title="Download PDF">pdf</a>, <a href="/format/2311.18426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis of Fractional Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aggarwal%2C+A">Ashwani Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 4 figures. Added additional results showing advantage of fractional gradient descent on quadratic functions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18659" title="Abstract">arXiv:2311.18659</a> (replaced) [<a href="/pdf/2311.18659" title="Download PDF">pdf</a>, <a href="/format/2311.18659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Autoscaling Frameworks for Containerised  Machine-Learning-Applications in a Local and Cloud Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schroeder%2C+C">Christian Schroeder</a>, 
<a href="/search/cs?searchtype=author&query=Boehm%2C+R">Rene Boehm</a>, 
<a href="/search/cs?searchtype=author&query=Lampe%2C+A">Alexander Lampe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01036" title="Abstract">arXiv:2312.01036</a> (replaced) [<a href="/pdf/2312.01036" title="Download PDF">pdf</a>, <a href="/format/2312.01036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Clifford Initial States for Ising Hamiltonians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bhattacharyya%2C+B">Bikrant Bhattacharyya</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ravi%2C+G+S">Gokul Subramanian Ravi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at The 8th Annual IEEE International Conference on Rebooting Computing (ICRC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01921" title="Abstract">arXiv:2312.01921</a> (replaced) [<a href="/pdf/2312.01921" title="Download PDF">pdf</a>, <a href="/format/2312.01921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Approach Towards SKILL Code Autocompletion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dehaerne%2C+E">Enrique Dehaerne</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+B">Bappaditya Dey</a>, 
<a href="/search/cs?searchtype=author&query=Meert%2C+W">Wannes Meert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for SPIE Advanced Lithography + Patterning, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02554" title="Abstract">arXiv:2312.02554</a> (replaced) [<a href="/pdf/2312.02554" title="Download PDF">pdf</a>, <a href="/format/2312.02554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ULMA: Unified Language Model Alignment with Human Demonstration and  Point-wise Preference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianchi Cai</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xierui Song</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+F">Fei Teng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02592" title="Abstract">arXiv:2312.02592</a> (replaced) [<a href="/pdf/2312.02592" title="Download PDF">pdf</a>, <a href="/format/2312.02592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FRAPP&#xc9;: A Group Fairness Framework for Post-Processing Everything
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%A2ifrea%2C+A">Alexandru &#x162;ifrea</a>, 
<a href="/search/cs?searchtype=author&query=Lahoti%2C+P">Preethi Lahoti</a>, 
<a href="/search/cs?searchtype=author&query=Packer%2C+B">Ben Packer</a>, 
<a href="/search/cs?searchtype=author&query=Halpern%2C+Y">Yoni Halpern</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Prost%2C+F">Flavien Prost</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presubmission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03004" title="Abstract">arXiv:2312.03004</a> (replaced) [<a href="/pdf/2312.03004" title="Download PDF">pdf</a>, <a href="/format/2312.03004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Bei Hui</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+C">Chong Mu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Ling Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03262" title="Abstract">arXiv:2312.03262</a> (replaced) [<a href="/pdf/2312.03262" title="Download PDF">pdf</a>, <a href="/format/2312.03262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Cost High-Power Membership Inference Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zarifzadeh%2C+S">Sajjad Zarifzadeh</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+P">Philippe Liu</a>, 
<a href="/search/stat?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05063" title="Abstract">arXiv:2312.05063</a> (replaced) [<a href="/pdf/2312.05063" title="Download PDF">pdf</a>, <a href="/format/2312.05063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Individualizing Glioma Radiotherapy Planning by Optimization of Data and  Physics-Informed Discrete Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Balcerak%2C+M">Michal Balcerak</a>, 
<a href="/search/physics?searchtype=author&query=Weidner%2C+J">Jonas Weidner</a>, 
<a href="/search/physics?searchtype=author&query=Karnakov%2C+P">Petr Karnakov</a>, 
<a href="/search/physics?searchtype=author&query=Ezhov%2C+I">Ivan Ezhov</a>, 
<a href="/search/physics?searchtype=author&query=Litvinov%2C+S">Sergey Litvinov</a>, 
<a href="/search/physics?searchtype=author&query=Koumoutsakos%2C+P">Petros Koumoutsakos</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+R+Z">Ray Zirui Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Lowengrub%2C+J+S">John S. Lowengrub</a>, 
<a href="/search/physics?searchtype=author&query=Wiestler%2C+B">Bene Wiestler</a>, 
<a href="/search/physics?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, 1 table. Associated GitHub: <a href="https://github.com/m1balcerak/GliODIL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Numerical Analysis (math.NA); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06968" title="Abstract">arXiv:2312.06968</a> (replaced) [<a href="/pdf/2312.06968" title="Download PDF">pdf</a>, <a href="/format/2312.06968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hallucination Augmented Contrastive Learning for Multimodal Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Mengfan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07159" title="Abstract">arXiv:2312.07159</a> (replaced) [<a href="/pdf/2312.07159" title="Download PDF">pdf</a>, <a href="/ps/2312.07159" title="Download PostScript">ps</a>, <a href="/format/2312.07159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Splitting Multiple Access for Semantic-Aware Networks: an Age of  Incorrect Information Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dizdar%2C+O">Onur Dizdar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Stephen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07424" title="Abstract">arXiv:2312.07424</a> (replaced) [<a href="/pdf/2312.07424" title="Download PDF">pdf</a>, <a href="/format/2312.07424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary  Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhongyi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guanglin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Rundong He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tailin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yilong Yin</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added the investigation of Gemini. 66 pages, 41 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08140" title="Abstract">arXiv:2312.08140</a> (replaced) [<a href="/pdf/2312.08140" title="Download PDF">pdf</a>, <a href="/format/2312.08140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GVE-LPA: Fast Label Propagation Algorithm (LPA) for Community Detection  in Shared Memory Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+S">Subhajit Sahu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, 1 table. arXiv admin note: text overlap with <a href="/abs/2312.04876">arXiv:2312.04876</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08656" title="Abstract">arXiv:2312.08656</a> (replaced) [<a href="/pdf/2312.08656" title="Download PDF">pdf</a>, <a href="/format/2312.08656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural  Networks Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hongwu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Shivdikar%2C+K">Kaustubh Shivdikar</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+A">MD Amit Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiahui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+O">Omer Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kaeli%2C+D">David Kaeli</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Caiwen Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASPLOS 2024 accepted publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08846" title="Abstract">arXiv:2312.08846</a> (replaced) [<a href="/pdf/2312.08846" title="Download PDF">pdf</a>, <a href="/format/2312.08846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TiMix: Text-aware Image Mixing for Effective Vision-Language  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=ye%2C+W">Wei ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08953" title="Abstract">arXiv:2312.08953</a> (replaced) [<a href="/pdf/2312.08953" title="Download PDF">pdf</a>, <a href="/format/2312.08953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single and Multi-Objective Optimization Benchmark Problems Focusing on  Human-Powered Aircraft Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Namura%2C+N">Nobuo Namura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10584" title="Abstract">arXiv:2312.10584</a> (replaced) [<a href="/pdf/2312.10584" title="Download PDF">pdf</a>, <a href="/format/2312.10584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Optimization in RLHF: The Impact of Out-of-preference Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziniu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10615" title="Abstract">arXiv:2312.10615</a> (replaced) [<a href="/pdf/2312.10615" title="Download PDF">pdf</a>, <a href="/format/2312.10615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Symmetric Multigrid-Preconditioned Krylov Subspace Solver for Stokes  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tao%2C+Y">Yutian Tao</a>, 
<a href="/search/math?searchtype=author&query=Sifakis%2C+E">Eftychios Sifakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10730" title="Abstract">arXiv:2312.10730</a> (replaced) [<a href="/pdf/2312.10730" title="Download PDF">pdf</a>, <a href="/format/2312.10730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Distillation Helps Smaller Language Model Better Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianglong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Caiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yicheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress, 17 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11871" title="Abstract">arXiv:2312.11871</a> (replaced) [<a href="/pdf/2312.11871" title="Download PDF">pdf</a>, <a href="/format/2312.11871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meili: Enabling SmartNIC as a Service in the Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qiang Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shaofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhixiong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+R">Ran Shu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yongqiang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C+J">Chun Jason Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zaoxing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12173" title="Abstract">arXiv:2312.12173</a> (replaced) [<a href="/e-print/2312.12173" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Globally Convergent Policy Gradient Method for Linear Quadratic  Gaussian (LQG) Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sadamoto%2C+T">Tomonori Sadamoto</a>, 
<a href="/search/math?searchtype=author&query=Nakamata%2C+F">Fumiya Nakamata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Due to a technical issue discovered in the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12430" title="Abstract">arXiv:2312.12430</a> (replaced) [<a href="/pdf/2312.12430" title="Download PDF">pdf</a>, <a href="/format/2312.12430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jize Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+D">Daqian Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+H">Heyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuxiang Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12612" title="Abstract">arXiv:2312.12612</a> (replaced) [<a href="/pdf/2312.12612" title="Download PDF">pdf</a>, <a href="/format/2312.12612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Control Barrier Functions for Economics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=van+Wijk%2C+D">David van Wijk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15519" title="Abstract">arXiv:2312.15519</a> (replaced) [<a href="/pdf/2312.15519" title="Download PDF">pdf</a>, <a href="/ps/2312.15519" title="Download PostScript">ps</a>, <a href="/format/2312.15519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-kernels in split graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Langlois%2C+H">H&#xe9;l&#xe8;ne Langlois</a>, 
<a href="/search/math?searchtype=author&query=Meunier%2C+F">Fr&#xe9;d&#xe9;ric Meunier</a>, 
<a href="/search/math?searchtype=author&query=Rizzi%2C+R">Romeo Rizzi</a>, 
<a href="/search/math?searchtype=author&query=Vialette%2C+S">St&#xe9;phane Vialette</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yacong Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15591" title="Abstract">arXiv:2312.15591</a> (replaced) [<a href="/pdf/2312.15591" title="Download PDF">pdf</a>, <a href="/format/2312.15591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserved Neural Graph Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaxin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15698" title="Abstract">arXiv:2312.15698</a> (replaced) [<a href="/pdf/2312.15698" title="Download PDF">pdf</a>, <a href="/format/2312.15698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for  Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Andr&#xe9; Silva</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Sen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15927" title="Abstract">arXiv:2312.15927</a> (replaced) [<a href="/pdf/2312.15927" title="Download PDF">pdf</a>, <a href="/format/2312.15927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hansong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shikun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengju Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shiming Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15942" title="Abstract">arXiv:2312.15942</a> (replaced) [<a href="/pdf/2312.15942" title="Download PDF">pdf</a>, <a href="/format/2312.15942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry  from Sparse Low Dynamic Range Panoramic Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xudong Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16108" title="Abstract">arXiv:2312.16108</a> (replaced) [<a href="/pdf/2312.16108" title="Download PDF">pdf</a>, <a href="/format/2312.16108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaneSegNet: Map Learning with Lane Segment Perception for Autonomous  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peijin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bangjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongyang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16424" title="Abstract">arXiv:2312.16424</a> (replaced) [<a href="/pdf/2312.16424" title="Download PDF">pdf</a>, <a href="/format/2312.16424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Contrastive Learning for Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+T">Taeyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kibok Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS Workshop on Self-Supervised Learning: Theory and Practice, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16427" title="Abstract">arXiv:2312.16427</a> (replaced) [<a href="/pdf/2312.16427" title="Download PDF">pdf</a>, <a href="/format/2312.16427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Embed Time Series Patches Independently
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+T">Taeyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kibok Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS Workshop on Self-Supervised Learning: Theory and Practice, 2023. Oral presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16815" title="Abstract">arXiv:2312.16815</a> (replaced) [<a href="/pdf/2312.16815" title="Download PDF">pdf</a>, <a href="/format/2312.16815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence and Causality in Complex Systems: A Survey on Causal Emergence  and Related Quantitative Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yuan%2C+B">Bing Yuan</a>, 
<a href="/search/physics?searchtype=author&query=Jiang%2C+Z">Zhang Jiang</a>, 
<a href="/search/physics?searchtype=author&query=Lyu%2C+A">Aobo Lyu</a>, 
<a href="/search/physics?searchtype=author&query=Wu%2C+J">Jiayun Wu</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+M">Mingzhe Yang</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+K">Kaiwei Liu</a>, 
<a href="/search/physics?searchtype=author&query=Mou%2C+M">Muyun Mou</a>, 
<a href="/search/physics?searchtype=author&query=Cui%2C+P">Peng Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages, 17 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Artificial Intelligence (cs.AI); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17235" title="Abstract">arXiv:2312.17235</a> (replaced) [<a href="/pdf/2312.17235" title="Download PDF">pdf</a>, <a href="/format/2312.17235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple LLM Framework for Long-Range Video Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Taixi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Mohaiminul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shoubin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Bertasius%2C+G">Gedas Bertasius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00110" title="Abstract">arXiv:2401.00110</a> (replaced) [<a href="/pdf/2401.00110" title="Download PDF">pdf</a>, <a href="/format/2401.00110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model with Perceptual Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shanchuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00546" title="Abstract">arXiv:2401.00546</a> (replaced) [<a href="/pdf/2401.00546" title="Download PDF">pdf</a>, <a href="/format/2401.00546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with  Thirteen Modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Run Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiujun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">YanSheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dapeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shizhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haifeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 16 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01583" title="Abstract">arXiv:2401.01583</a> (replaced) [<a href="/pdf/2401.01583" title="Download PDF">pdf</a>, <a href="/format/2401.01583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Representation in Medical Vision-Language Foundation Models  via Multi-Scale Information Extraction Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weijian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiarun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hairong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shanshan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01642" title="Abstract">arXiv:2401.01642</a> (replaced) [<a href="/pdf/2401.01642" title="Download PDF">pdf</a>, <a href="/format/2401.01642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLADE: Box-Level Supervised Amodal Segmentation through Directed  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tingting Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01646" title="Abstract">arXiv:2401.01646</a> (replaced) [<a href="/pdf/2401.01646" title="Download PDF">pdf</a>, <a href="/format/2401.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototypical Information Bottlenecking and Disentangling for Multimodal  Cancer Survival Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yilan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yingxue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+F">Fengying Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02873" title="Abstract">arXiv:2401.02873</a> (replaced) [<a href="/pdf/2401.02873" title="Download PDF">pdf</a>, <a href="/format/2401.02873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Chaining of Vehicle Plans with Time Windows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fiedler%2C+D">David Fiedler</a>, 
<a href="/search/math?searchtype=author&query=Difonzo%2C+F+V">Fabio V. Difonzo</a>, 
<a href="/search/math?searchtype=author&query=Mrkos%2C+J">Jan Mrkos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 7 figures, submitted to "Transportation Research Part C: Emerging Technologies" journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03321" title="Abstract">arXiv:2401.03321</a> (replaced) [<a href="/pdf/2401.03321" title="Download PDF">pdf</a>, <a href="/format/2401.03321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIXAR: Auto-Regressive Language Modeling in Pixel Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Yintao Tai</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiyang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Suglia%2C+A">Alessandro Suglia</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04081" title="Abstract">arXiv:2401.04081</a> (replaced) [<a href="/pdf/2401.04081" title="Download PDF">pdf</a>, <a href="/format/2401.04081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoE-Mamba: Efficient Selective State Space Models with Mixture of  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pi%C3%B3ro%2C+M">Maciej Pi&#xf3;ro</a>, 
<a href="/search/cs?searchtype=author&query=Ciebiera%2C+K">Kamil Ciebiera</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%B3l%2C+K">Krystian Kr&#xf3;l</a>, 
<a href="/search/cs?searchtype=author&query=Ludziejewski%2C+J">Jan Ludziejewski</a>, 
<a href="/search/cs?searchtype=author&query=Krutul%2C+M">Micha&#x142; Krutul</a>, 
<a href="/search/cs?searchtype=author&query=Krajewski%2C+J">Jakub Krajewski</a>, 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+S">Szymon Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%82o%C5%9B%2C+P">Piotr Mi&#x142;o&#x15b;</a>, 
<a href="/search/cs?searchtype=author&query=Cygan%2C+M">Marek Cygan</a>, 
<a href="/search/cs?searchtype=author&query=Jaszczur%2C+S">Sebastian Jaszczur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04348" title="Abstract">arXiv:2401.04348</a> (replaced) [<a href="/pdf/2401.04348" title="Download PDF">pdf</a>, <a href="/format/2401.04348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using  Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+K+M">Khoi M.Le</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Trinh Pham</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+T">Tho Quan</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contribute equally. Accepted at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04653" title="Abstract">arXiv:2401.04653</a> (replaced) [<a href="/pdf/2401.04653" title="Download PDF">pdf</a>, <a href="/format/2401.04653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-certified Input-constrained NMPC via Koopman Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/math?searchtype=author&query=Ganko%2C+K">Krystian Ganko</a>, 
<a href="/search/math?searchtype=author&query=Braatz%2C+R+D">Richard D. Braatz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, submitted into 8th IFAC Conference on Nonlinear Model Predictive Control NMPC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05200" title="Abstract">arXiv:2401.05200</a> (replaced) [<a href="/pdf/2401.05200" title="Download PDF">pdf</a>, <a href="/format/2401.05200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Sharing in Manufacturing using Large Language Models: User  Evaluation and Model Benchmarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freire%2C+S+K">Samuel Kernan Freire</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaofan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Foosherian%2C+M">Mina Foosherian</a>, 
<a href="/search/cs?searchtype=author&query=Wellsandt%2C+S">Stefan Wellsandt</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Arenas%2C+S">Santiago Ruiz-Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Niforatos%2C+E">Evangelos Niforatos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, and 1 table. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05273" title="Abstract">arXiv:2401.05273</a> (replaced) [<a href="/pdf/2401.05273" title="Download PDF">pdf</a>, <a href="/format/2401.05273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INACIA: Integrating Large Language Models in Brazilian Audit Courts:  Opportunities and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pereira%2C+J">Jayr Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Assumpcao%2C+A">Andre Assumpcao</a>, 
<a href="/search/cs?searchtype=author&query=Trecenti%2C+J">Julio Trecenti</a>, 
<a href="/search/cs?searchtype=author&query=Airosa%2C+L">Luiz Airosa</a>, 
<a href="/search/cs?searchtype=author&query=Lente%2C+C">Caio Lente</a>, 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9to%2C+J">Jhonatan Cl&#xe9;to</a>, 
<a href="/search/cs?searchtype=author&query=Dobins%2C+G">Guilherme Dobins</a>, 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+R">Rodrigo Nogueira</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+L">Luis Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Lotufo%2C+R">Roberto Lotufo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05653" title="Abstract">arXiv:2401.05653</a> (replaced) [<a href="/pdf/2401.05653" title="Download PDF">pdf</a>, <a href="/ps/2401.05653" title="Download PostScript">ps</a>, <a href="/format/2401.05653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Marketing Performance at Channel-Partner Level by Using  Marketing Mix Modeling (MMM) and Shapley Value Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Sean Tang</a>, 
<a href="/search/cs?searchtype=author&query=Musunuru%2C+S">Sriya Musunuru</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+B">Baoshi Zong</a>, 
<a href="/search/cs?searchtype=author&query=Thornton%2C+B">Brooks Thornton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05657" title="Abstract">arXiv:2401.05657</a> (replaced) [<a href="/pdf/2401.05657" title="Download PDF">pdf</a>, <a href="/ps/2401.05657" title="Download PostScript">ps</a>, <a href="/format/2401.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An impossibility theorem concerning positive involvement in voting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Holliday%2C+W+H">Wesley H. Holliday</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Economics Letters, Vol. 236, March 2024, 111589
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06091" title="Abstract">arXiv:2401.06091</a> (replaced) [<a href="/pdf/2401.06091" title="Download PDF">pdf</a>, <a href="/format/2401.06091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look at AUROC and AUPRC under Class Imbalance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McDermott%2C+M+B+A">Matthew B. A. McDermott</a> (1), 
<a href="/search/cs?searchtype=author&query=Hansen%2C+L+H">Lasse Hyldig Hansen</a> (2), 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a> (3), 
<a href="/search/cs?searchtype=author&query=Angelotti%2C+G">Giovanni Angelotti</a> (4), 
<a href="/search/cs?searchtype=author&query=Gallifant%2C+J">Jack Gallifant</a> (3) ((1) Harvard Medical School, (2) Aarhus University, (3) Massachusetts Institute of Technology, (4) IRCCS Humanitas Research Hospital)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06125" title="Abstract">arXiv:2401.06125</a> (replaced) [<a href="/pdf/2401.06125" title="Download PDF">pdf</a>, <a href="/ps/2401.06125" title="Download PostScript">ps</a>, <a href="/format/2401.06125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Capacity Outer Bound for Private Quadratic Monomial Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%A6hli%2C+K+M">Karen M. D&#xe6;hli</a>, 
<a href="/search/cs?searchtype=author&query=Obead%2C+S+A">Sarah A Obead</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsuan-Yin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rosnes%2C+E">Eirik Rosnes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, and 1 table. An extended version of a paper accepted for presentation at the 2024 International Zurich Seminar on Information and Communication (IZS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06137" title="Abstract">arXiv:2401.06137</a> (replaced) [<a href="/pdf/2401.06137" title="Download PDF">pdf</a>, <a href="/format/2401.06137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuasiNet: a neural network with trainable product layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malinovsk%C3%A1%2C+K">Krist&#xed;na Malinovsk&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Holenda%2C+S">Slavom&#xed;r Holenda</a>, 
<a href="/search/cs?searchtype=author&query=Malinovsk%C3%BD%2C+%C4%BD">&#x13d;udov&#xed;t Malinovsk&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Presented at International Conference on Artificial Neural Networks (ICANN) 2023. Accepted:1.7.2023 Published:26.9.2023. Code: <a href="https://doi.org/10.5281/zenodo.10702248">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06857" title="Abstract">arXiv:2401.06857</a> (replaced) [<a href="/pdf/2401.06857" title="Download PDF">pdf</a>, <a href="/ps/2401.06857" title="Download PostScript">ps</a>, <a href="/format/2401.06857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Tensor Decomposition over Finite Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jason Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ISSAC 2024; 9 pages, 0 figures; improved hidden constants, better wording
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07314" title="Abstract">arXiv:2401.07314</a> (replaced) [<a href="/pdf/2401.07314" title="Download PDF">pdf</a>, <a href="/format/2401.07314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MapGPT: Map-Guided Prompting with Adaptive Path Planning for  Vision-and-Language Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bingqian Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Z">Zhenhua Chai</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K">Kwan-Yee K. Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07386" title="Abstract">arXiv:2401.07386</a> (replaced) [<a href="/pdf/2401.07386" title="Download PDF">pdf</a>, <a href="/ps/2401.07386" title="Download PostScript">ps</a>, <a href="/format/2401.07386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do machines learn? Evaluating the AIcon2abs method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Queiroz%2C+R+L">Rubens Lacerda Queiroz</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+C">Cabral Lima</a>, 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+F+F">Fabio Ferrentini Sampaio</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+P+M+V">Priscila Machado Vieira Lima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07529" title="Abstract">arXiv:2401.07529</a> (replaced) [<a href="/pdf/2401.07529" title="Download PDF">pdf</a>, <a href="/format/2401.07529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of  Multimodal Large Language Models in Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yusheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Heyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07958" title="Abstract">arXiv:2401.07958</a> (replaced) [<a href="/pdf/2401.07958" title="Download PDF">pdf</a>, <a href="/format/2401.07958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GD-CAF: Graph Dual-stream Convolutional Attention Fusion for  Precipitation Nowcasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatamany%2C+L">Lorand Vatamany</a>, 
<a href="/search/cs?searchtype=author&query=Mehrkanoon%2C+S">Siamak Mehrkanoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08174" title="Abstract">arXiv:2401.08174</a> (replaced) [<a href="/pdf/2401.08174" title="Download PDF">pdf</a>, <a href="/format/2401.08174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Instance Segmentation Framework for Completely Occluded  Objects and Dense Objects in Robot Vision Measurement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Junfeng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunkai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+F">Fengshui Jing</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Min Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08664" title="Abstract">arXiv:2401.08664</a> (replaced) [<a href="/pdf/2401.08664" title="Download PDF">pdf</a>, <a href="/format/2401.08664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Large Language Models for Education: Foundational Capabilities,  Potentials, and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Lingyue Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingwei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10065" title="Abstract">arXiv:2401.10065</a> (replaced) [<a href="/pdf/2401.10065" title="Download PDF">pdf</a>, <a href="/format/2401.10065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puerto%2C+H">Haritz Puerto</a>, 
<a href="/search/cs?searchtype=author&query=Tutek%2C+M">Martin Tutek</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaodan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, prompt templates, prompts, and outputs are publicly available at <a href="https://github.com/UKPLab/arxiv2024-conditional-reasoning-llms">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10357" title="Abstract">arXiv:2401.10357</a> (replaced) [<a href="/pdf/2401.10357" title="Download PDF">pdf</a>, <a href="/format/2401.10357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Color Blind User Interface Accessibility via Simulated  Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamil%2C+A">Amaan Jamil</a>, 
<a href="/search/cs?searchtype=author&query=Denes%2C+G">Gyorgy Denes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, published to MDPI Computers
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> MDPI Computers 13, 53, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10852" title="Abstract">arXiv:2401.10852</a> (replaced) [<a href="/pdf/2401.10852" title="Download PDF">pdf</a>, <a href="/format/2401.10852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Resource Disaggregation for HPC with Serverless Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Copik%2C+M">Marcin Copik</a>, 
<a href="/search/cs?searchtype=author&query=Chrapek%2C+M">Marcin Chrapek</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+L">Larissa Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Calotoiu%2C+A">Alexandru Calotoiu</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the 2024 International Parallel and Distributed Processing Symposium (IPDPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11314" title="Abstract">arXiv:2401.11314</a> (replaced) [<a href="/pdf/2401.11314" title="Download PDF">pdf</a>, <a href="/format/2401.11314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming  Assistant that Balances Student and Educator Needs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazemitabaar%2C+M">Majeed Kazemitabaar</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Runlong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Henley%2C+A+Z">Austin Z. Henley</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Craig%2C+M">Michelle Craig</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+T">Tovi Grossman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI 2024 Paper - The paper includes 17 pages, 8 figures, 2 tables, along with a 2-page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11626" title="Abstract">arXiv:2401.11626</a> (replaced) [<a href="/pdf/2401.11626" title="Download PDF">pdf</a>, <a href="/format/2401.11626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Freely Long-Thinking Transformer (FraiLT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tabak%2C+A">Akbay Tabak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11648" title="Abstract">arXiv:2401.11648</a> (replaced) [<a href="/pdf/2401.11648" title="Download PDF">pdf</a>, <a href="/format/2401.11648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal  Contrastive EHR Modelling with Hierarchical Regularisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koo%2C+H">Heejoon Koo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 (The 18th Conference of the European Chapter of the Association for Computational Linguistics)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13345" title="Abstract">arXiv:2401.13345</a> (replaced) [<a href="/pdf/2401.13345" title="Download PDF">pdf</a>, <a href="/ps/2401.13345" title="Download PostScript">ps</a>, <a href="/format/2401.13345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPGA Implementation of an Intelligent Traffic Light Controller (I-TLC)  in Verilog
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Banerjee%2C+A">Apoorva Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The nature of the changes in the updated version involves incorporating synthesis work. Additionally, hardware implementation results on the FPGA board have been added. Moderate changes have been made, as they introduce new aspects related to synthesis and provide valuable insights into the hardware implementation, but they do not alter or affect the existing simulation results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13488" title="Abstract">arXiv:2401.13488</a> (replaced) [<a href="/pdf/2401.13488" title="Download PDF">pdf</a>, <a href="/format/2401.13488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Inverse Model Transformation: Algebraic Framework for Fast Data  Plane Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shenshen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y+R">Yang Richard Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages pre-reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13505" title="Abstract">arXiv:2401.13505</a> (replaced) [<a href="/pdf/2401.13505" title="Download PDF">pdf</a>, <a href="/format/2401.13505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Human Motion Stylization in Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yuxuan Mu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xinxin Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+P">Peng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Youliang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Juwei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Li Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13560" title="Abstract">arXiv:2401.13560</a> (replaced) [<a href="/pdf/2401.13560" title="Download PDF">pdf</a>, <a href="/format/2401.13560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhaohu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Tian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yijun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code has released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14142" title="Abstract">arXiv:2401.14142</a> (replaced) [<a href="/pdf/2401.14142" title="Download PDF">pdf</a>, <a href="/format/2401.14142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept  Intervention, and Probabilistic Interpretations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yi Qin</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Lu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaomeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14194" title="Abstract">arXiv:2401.14194</a> (replaced) [<a href="/pdf/2401.14194" title="Download PDF">pdf</a>, <a href="/format/2401.14194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Conversational Recommender System as a Language  Processing Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravaut%2C+M">Mathieu Ravaut</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 8 tables, EACL 2024 conference, fixed typo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14211" title="Abstract">arXiv:2401.14211</a> (replaced) [<a href="/pdf/2401.14211" title="Download PDF">pdf</a>, <a href="/format/2401.14211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Federated Learning through Adaptive Weight  Clustering and Server-Side Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsouvalas%2C+V">Vasileios Tsouvalas</a>, 
<a href="/search/cs?searchtype=author&query=Saeed%2C+A">Aaqib Saeed</a>, 
<a href="/search/cs?searchtype=author&query=Ozcelebi%2C+T">Tanir Ozcelebi</a>, 
<a href="/search/cs?searchtype=author&query=Meratnia%2C+N">Nirvana Meratnia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, Accepted on ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14591" title="Abstract">arXiv:2401.14591</a> (replaced) [<a href="/pdf/2401.14591" title="Download PDF">pdf</a>, <a href="/format/2401.14591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ricci flow-guided autoencoders in learning time-dependent dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gracyk%2C+A">Andrew Gracyk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14669" title="Abstract">arXiv:2401.14669</a> (replaced) [<a href="/pdf/2401.14669" title="Download PDF">pdf</a>, <a href="/ps/2401.14669" title="Download PostScript">ps</a>, <a href="/format/2401.14669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Markov Models and the Bayes Filter in Categorical Probability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fritz%2C+T">Tobias Fritz</a>, 
<a href="/search/math?searchtype=author&query=Klingler%2C+A">Andreas Klingler</a>, 
<a href="/search/math?searchtype=author&query=McNeely%2C+D">Drew McNeely</a>, 
<a href="/search/math?searchtype=author&query=Shah-Mohammed%2C+A">Areeb Shah-Mohammed</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yuwen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 76 pages. v2: added Examples 3.8 and 3.23 on possibilistic filter and Section 1.5 on implementation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Systems and Control (eess.SY); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15006" title="Abstract">arXiv:2401.15006</a> (replaced) [<a href="/pdf/2401.15006" title="Download PDF">pdf</a>, <a href="/format/2401.15006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Airavata: Introducing Hindi Instruction-tuned LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gala%2C+J">Jay Gala</a>, 
<a href="/search/cs?searchtype=author&query=Jayakumar%2C+T">Thanmay Jayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Husain%2C+J+A">Jaavid Aktar Husain</a>, 
<a href="/search/cs?searchtype=author&query=M%2C+A+K">Aswanth Kumar M</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+S+U+R">Mohammed Safi Ur Rahman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kanojia%2C+D">Diptesh Kanojia</a>, 
<a href="/search/cs?searchtype=author&query=Puduppully%2C+R">Ratish Puduppully</a>, 
<a href="/search/cs?searchtype=author&query=Khapra%2C+M+M">Mitesh M. Khapra</a>, 
<a href="/search/cs?searchtype=author&query=Dabre%2C+R">Raj Dabre</a>, 
<a href="/search/cs?searchtype=author&query=Murthy%2C+R">Rudra Murthy</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15204" title="Abstract">arXiv:2401.15204</a> (replaced) [<a href="/pdf/2401.15204" title="Download PDF">pdf</a>, <a href="/format/2401.15204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brateanu%2C+A">A. Brateanu</a>, 
<a href="/search/cs?searchtype=author&query=Balmez%2C+R">R. Balmez</a>, 
<a href="/search/cs?searchtype=author&query=Avram%2C+A">A. Avram</a>, 
<a href="/search/cs?searchtype=author&query=Orhei%2C+C">C. Orhei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, submitted to ICIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15207" title="Abstract">arXiv:2401.15207</a> (replaced) [<a href="/pdf/2401.15207" title="Download PDF">pdf</a>, <a href="/format/2401.15207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongkang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15906" title="Abstract">arXiv:2401.15906</a> (replaced) [<a href="/pdf/2401.15906" title="Download PDF">pdf</a>, <a href="/format/2401.15906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rameshwar%2C+V+A">V. Arvind Rameshwar</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+A">Anshoo Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prajjwal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+N">Novoneel Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhay Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16321" title="Abstract">arXiv:2401.16321</a> (replaced) [<a href="/pdf/2401.16321" title="Download PDF">pdf</a>, <a href="/format/2401.16321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Control of Renewable Energy Communities subject to Network Peak  Fees with Model Predictive Control and Reinforcement Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Aittahar%2C+S">Samy Aittahar</a>, 
<a href="/search/eess?searchtype=author&query=Bolland%2C+A">Adrien Bolland</a>, 
<a href="/search/eess?searchtype=author&query=Derval%2C+G">Guillaume Derval</a>, 
<a href="/search/eess?searchtype=author&query=Ernst%2C+D">Damien Ernst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages (excl. appendices and references), 14 pages of appendix. 10 figures and 10 tables. To be reviewed as a journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16359" title="Abstract">arXiv:2401.16359</a> (replaced) [<a href="/pdf/2401.16359" title="Download PDF">pdf</a>, <a href="/format/2401.16359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reference Coverage Analysis of OpenAlex compared to Web of Science and  Scopus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Culbert%2C+J">Jack Culbert</a>, 
<a href="/search/cs?searchtype=author&query=Hobert%2C+A">Anne Hobert</a>, 
<a href="/search/cs?searchtype=author&query=Jahn%2C+N">Najko Jahn</a>, 
<a href="/search/cs?searchtype=author&query=Haupka%2C+N">Nick Haupka</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+M">Marion Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Donner%2C+P">Paul Donner</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+P">Philipp Mayr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16553" title="Abstract">arXiv:2401.16553</a> (replaced) [<a href="/pdf/2401.16553" title="Download PDF">pdf</a>, <a href="/format/2401.16553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelectLLM: Can LLMs Select Important Instructions to Annotate?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parkar%2C+R+S">Ritik Sachin Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaehyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+I">Jong Inn Park</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author: Jong Inn Park | PI: Dongyeop Kang
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00082" title="Abstract">arXiv:2402.00082</a> (replaced) [<a href="/pdf/2402.00082" title="Download PDF">pdf</a>, <a href="/ps/2402.00082" title="Download PostScript">ps</a>, <a href="/format/2402.00082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Grover&#x27;s Search Algorithm: A Modified Approach to Increase the  Probability of Good States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Abdulrahman%2C+I">Ismael Abdulrahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00263" title="Abstract">arXiv:2402.00263</a> (replaced) [<a href="/pdf/2402.00263" title="Download PDF">pdf</a>, <a href="/format/2402.00263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation  to Fine-tuned Contrastive Learning Detector would be Better
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zehua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengzhengxu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yu Lan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00357" title="Abstract">arXiv:2402.00357</a> (replaced) [<a href="/pdf/2402.00357" title="Download PDF">pdf</a>, <a href="/format/2402.00357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety of Multimodal Large Language Models on Images and Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00397" title="Abstract">arXiv:2402.00397</a> (replaced) [<a href="/pdf/2402.00397" title="Download PDF">pdf</a>, <a href="/format/2402.00397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhanyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guanjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yanwei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. Text overlap with <a href="/abs/2308.09727">arXiv:2308.09727</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01118" title="Abstract">arXiv:2402.01118</a> (replaced) [<a href="/pdf/2402.01118" title="Download PDF">pdf</a>, <a href="/format/2402.01118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiansheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01213" title="Abstract">arXiv:2402.01213</a> (replaced) [<a href="/pdf/2402.01213" title="Download PDF">pdf</a>, <a href="/ps/2402.01213" title="Download PostScript">ps</a>, <a href="/format/2402.01213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forcing with Language Fragments, Extending Namba Forcing, and Models of  Theories with Constraints in Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lau%2C+D">Desmond Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 90 pages. Corrected some typos and refined some of the questions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01231" title="Abstract">arXiv:2402.01231</a> (replaced) [<a href="/pdf/2402.01231" title="Download PDF">pdf</a>, <a href="/format/2402.01231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Delay Effects in Traffic Forecasting: A Perspective from  Spatial-Temporal Delay Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Q">Qingqing Long</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01383" title="Abstract">arXiv:2402.01383</a> (replaced) [<a href="/pdf/2402.01383" title="Download PDF">pdf</a>, <a href="/format/2402.01383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based NLG Evaluation: Current Status and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+X">Xiao Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01573" title="Abstract">arXiv:2402.01573</a> (replaced) [<a href="/pdf/2402.01573" title="Download PDF">pdf</a>, <a href="/format/2402.01573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Actionable Framework for Understanding and Improving Talent Retention  as a Competitive Advantage in IT Organizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costa%2C+L+A">Luiz Alexandre Costa</a>, 
<a href="/search/cs?searchtype=author&query=Dias%2C+E">Edson Dias</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+D+M">Danilo Monteiro Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Font%C3%A3o%2C+A">Awdren Font&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+G">Gustavo Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+R+P+d">Rodrigo Pereira dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Serebrenik%2C+A">Alexander Serebrenik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.06352">arXiv:2205.06352</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01729" title="Abstract">arXiv:2402.01729</a> (replaced) [<a href="/pdf/2402.01729" title="Download PDF">pdf</a>, <a href="/format/2402.01729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualization Distillation from Large Language Model for Knowledge  Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024 findings v3: add missing citations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02130" title="Abstract">arXiv:2402.02130</a> (replaced) [<a href="/pdf/2402.02130" title="Download PDF">pdf</a>, <a href="/format/2402.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shuai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02229" title="Abstract">arXiv:2402.02229</a> (replaced) [<a href="/pdf/2402.02229" title="Download PDF">pdf</a>, <a href="/format/2402.02229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vanilla Bayesian Optimization Performs Great in High Dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E+O">Erik Orm Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02320" title="Abstract">arXiv:2402.02320</a> (replaced) [<a href="/pdf/2402.02320" title="Download PDF">pdf</a>, <a href="/format/2402.02320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spin: An Efficient Secure Computation Framework with GPU Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wuxuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiangjun Song</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Shenbai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02344" title="Abstract">arXiv:2402.02344</a> (replaced) [<a href="/pdf/2402.02344" title="Download PDF">pdf</a>, <a href="/ps/2402.02344" title="Download PostScript">ps</a>, <a href="/format/2402.02344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Secure mmWave RSMA Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sha Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gaofeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,8 figures, accepted by IEEE Internet of Things Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03167" title="Abstract">arXiv:2402.03167</a> (replaced) [<a href="/pdf/2402.03167" title="Download PDF">pdf</a>, <a href="/format/2402.03167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic  Update and Transient Iteration Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kong%2C+B">Boao Kong</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">Shuchen Zhu</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+S">Songtao Lu</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xinmeng Huang</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03246" title="Abstract">arXiv:2402.03246</a> (replaced) [<a href="/pdf/2402.03246" title="Download PDF">pdf</a>, <a href="/format/2402.03246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guohao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Na Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03530" title="Abstract">arXiv:2402.03530</a> (replaced) [<a href="/pdf/2402.03530" title="Download PDF">pdf</a>, <a href="/format/2402.03530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Aaron Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y+S">Yun Seo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Dow%2C+S+P">Steven P. Dow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, accepted at the 29th ACM Conference on Intelligent User Interfaces (IUI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03762" title="Abstract">arXiv:2402.03762</a> (replaced) [<a href="/pdf/2402.03762" title="Download PDF">pdf</a>, <a href="/format/2402.03762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhetao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lechen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingrui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Compared to the previous version, we have fixed some problems in the encoding and the result has been improved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03780" title="Abstract">arXiv:2402.03780</a> (replaced) [<a href="/pdf/2402.03780" title="Download PDF">pdf</a>, <a href="/format/2402.03780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing propaganda: an analysis of stylistic cues comparing human  annotations and machine classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faye%2C+G">G&#xe9;raud Faye</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+B">Benjamin Icard</a>, 
<a href="/search/cs?searchtype=author&query=Casanova%2C+M">Morgane Casanova</a>, 
<a href="/search/cs?searchtype=author&query=Chanson%2C+J">Julien Chanson</a>, 
<a href="/search/cs?searchtype=author&query=Maine%2C+F">Fran&#xe7;ois Maine</a>, 
<a href="/search/cs?searchtype=author&query=Bancilhon%2C+F">Fran&#xe7;ois Bancilhon</a>, 
<a href="/search/cs?searchtype=author&query=Gadek%2C+G">Guillaume Gadek</a>, 
<a href="/search/cs?searchtype=author&query=Gravier%2C+G">Guillaume Gravier</a>, 
<a href="/search/cs?searchtype=author&query=%C3%89gr%C3%A9%2C+P">Paul &#xc9;gr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper to appear in the EACL 2024 Proceedings of the Third Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03822" title="Abstract">arXiv:2402.03822</a> (replaced) [<a href="/pdf/2402.03822" title="Download PDF">pdf</a>, <a href="/format/2402.03822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Si Shen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+P">Peijun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Danhao Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04284" title="Abstract">arXiv:2402.04284</a> (replaced) [<a href="/pdf/2402.04284" title="Download PDF">pdf</a>, <a href="/format/2402.04284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Difan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04830" title="Abstract">arXiv:2402.04830</a> (replaced) [<a href="/pdf/2402.04830" title="Download PDF">pdf</a>, <a href="/format/2402.04830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the Gap Between SGP4 and High-Precision Propagation via  Differentiable Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acciarini%2C+G">Giacomo Acciarini</a>, 
<a href="/search/cs?searchtype=author&query=Baydin%2C+A+G">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, 
<a href="/search/cs?searchtype=author&query=Izzo%2C+D">Dario Izzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Earth and Planetary Astrophysics (astro-ph.EP)

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04875" title="Abstract">arXiv:2402.04875</a> (replaced) [<a href="/pdf/2402.04875" title="Download PDF">pdf</a>, <a href="/format/2402.04875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Provable Length and Compositional Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04880" title="Abstract">arXiv:2402.04880</a> (replaced) [<a href="/pdf/2402.04880" title="Download PDF">pdf</a>, <a href="/format/2402.04880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Cloud and Mobile Computing for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruiqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianchi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ruiqi Xu and Tianchi Zhang contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05391" title="Abstract">arXiv:2402.05391</a> (replaced) [<a href="/pdf/2402.05391" title="Download PDF">pdf</a>, <a href="/format/2402.05391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yuxia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yushan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13 Figures, 619 citations; Paper list is available at <a href="https://github.com/zjukg/KG-MM-Survey">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05493" title="Abstract">arXiv:2402.05493</a> (replaced) [<a href="/pdf/2402.05493" title="Download PDF">pdf</a>, <a href="/format/2402.05493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating White-Box Attacks for On-Device Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hailong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in The International Conference on Software Engineering 2024 (ICSE'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05608" title="Abstract">arXiv:2402.05608</a> (replaced) [<a href="/pdf/2402.05608" title="Download PDF">pdf</a>, <a href="/format/2402.05608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Diffusion Models with State Space Backbone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhengcong Fei</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingyuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changqian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junshi Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05902" title="Abstract">arXiv:2402.05902</a> (replaced) [<a href="/pdf/2402.05902" title="Download PDF">pdf</a>, <a href="/ps/2402.05902" title="Download PostScript">ps</a>, <a href="/format/2402.05902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClickSAM: Fine-tuning Segment Anything Model using click prompts for  ultrasound image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Aimee Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+G">Grace Fei</a>, 
<a href="/search/cs?searchtype=author&query=Pasupuleti%2C+H">Hemanth Pasupuleti</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, SPIE Medical Imaging Conference 2024. Project page: <a href="https://sites.google.com/view/clicksam/home">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06136" title="Abstract">arXiv:2402.06136</a> (replaced) [<a href="/pdf/2402.06136" title="Download PDF">pdf</a>, <a href="/format/2402.06136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIR: Multi-view Inverse Rendering with Decomposable Shadow for Indoor  Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaokang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuoman Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luximon%2C+Y">Yan Luximon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07083" title="Abstract">arXiv:2402.07083</a> (replaced) [<a href="/pdf/2402.07083" title="Download PDF">pdf</a>, <a href="/format/2402.07083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Highlight Removal Method for Capsule Endoscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07140" title="Abstract">arXiv:2402.07140</a> (replaced) [<a href="/pdf/2402.07140" title="Download PDF">pdf</a>, <a href="/format/2402.07140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Descriptive Order Improves Reasoning with Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuyao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wenjie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07188" title="Abstract">arXiv:2402.07188</a> (replaced) [<a href="/pdf/2402.07188" title="Download PDF">pdf</a>, <a href="/ps/2402.07188" title="Download PostScript">ps</a>, <a href="/format/2402.07188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Placement Delivery Arrays from $t$-Designs with Application to  Hierarchical Coded Caching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T.%2C+R+U+N">Rashid Ummer N.T.</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+B+S">B. Sundar Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Title has been changed. Some changes have been incorporated in the results. 11 pages, 5 figures and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07281" title="Abstract">arXiv:2402.07281</a> (replaced) [<a href="/pdf/2402.07281" title="Download PDF">pdf</a>, <a href="/format/2402.07281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A  Benchmarking Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Santonu Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Shanay Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+N">Nicole Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+J">Jyotirmoy Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Snehanshu Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07852" title="Abstract">arXiv:2402.07852</a> (replaced) [<a href="/pdf/2402.07852" title="Download PDF">pdf</a>, <a href="/ps/2402.07852" title="Download PostScript">ps</a>, <a href="/format/2402.07852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Algebraic Algorithms for LWE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steiner%2C+M+J">Matthias Johann Steiner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07876" title="Abstract">arXiv:2402.07876</a> (replaced) [<a href="/pdf/2402.07876" title="Download PDF">pdf</a>, <a href="/format/2402.07876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Improvement using Language Feedback Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+V">Victor Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+D">Dipendra Misra</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre C&#xf4;t&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08046" title="Abstract">arXiv:2402.08046</a> (replaced) [<a href="/pdf/2402.08046" title="Download PDF">pdf</a>, <a href="/format/2402.08046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Algorithm for Connected Odd Cycle Transversal Parameterized by  Clique-width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bojikian%2C+N">Narek Bojikian</a>, 
<a href="/search/cs?searchtype=author&query=Kratsch%2C+S">Stefan Kratsch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08068" title="Abstract">arXiv:2402.08068</a> (replaced) [<a href="/pdf/2402.08068" title="Download PDF">pdf</a>, <a href="/ps/2402.08068" title="Download PostScript">ps</a>, <a href="/format/2402.08068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Blocklace: A Universal, Byzantine Fault-Tolerant, Conflict-free  Replicated Data Type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeida%2C+P+S">Paulo S&#xe9;rgio Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+E">Ehud Shapiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08277" title="Abstract">arXiv:2402.08277</a> (replaced) [<a href="/pdf/2402.08277" title="Download PDF">pdf</a>, <a href="/format/2402.08277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Faithful and Robust LLM Specialists for Evidence-Based  Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schimanski%2C+T">Tobias Schimanski</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Mathias Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08318" title="Abstract">arXiv:2402.08318</a> (replaced) [<a href="/pdf/2402.08318" title="Download PDF">pdf</a>, <a href="/format/2402.08318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Values That Are Explicitly Present in Fairy Tales: Comparing Samples  from German, Italian and Portuguese Traditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diaz-Faes%2C+A+M">Alba Morollon Diaz-Faes</a>, 
<a href="/search/cs?searchtype=author&query=Murteira%2C+C+S+R">Carla Sofia Ribeiro Murteira</a>, 
<a href="/search/cs?searchtype=author&query=Ruskov%2C+M">Martin Ruskov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the Joint 3rd International Conference on Natural Language Processing for Digital Humanities and 8th International Workshop on Computational Linguistics for Uralic Languages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08479" title="Abstract">arXiv:2402.08479</a> (replaced) [<a href="/pdf/2402.08479" title="Download PDF">pdf</a>, <a href="/format/2402.08479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plausible Extractive Rationalization through Semi-Supervised Entailment  Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeo%2C+W+J">Wei Jie Yeo</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08582" title="Abstract">arXiv:2402.08582</a> (replaced) [<a href="/pdf/2402.08582" title="Download PDF">pdf</a>, <a href="/format/2402.08582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing  Medical Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chodvadiya%2C+C">Charulkumar Chodvadiya</a>, 
<a href="/search/cs?searchtype=author&query=Mahla%2C+N">Navyansh Mahla</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K+G">Kinshuk Gaurav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Jadhav%2C+K+S">Kshitij Sharad Jadhav</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08595" title="Abstract">arXiv:2402.08595</a> (replaced) [<a href="/pdf/2402.08595" title="Download PDF">pdf</a>, <a href="/format/2402.08595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homomorphism Counts for Graph Neural Networks: All About That Basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+E">Emily Jin</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+I+I">Ismail Ilkan Ceylan</a>, 
<a href="/search/cs?searchtype=author&query=Lanzinger%2C+M">Matthias Lanzinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08893" title="Abstract">arXiv:2402.08893</a> (replaced) [<a href="/pdf/2402.08893" title="Download PDF">pdf</a>, <a href="/format/2402.08893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inconsistency of evaluation metrics in link prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Y">Yilin Bi</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+X">Xinshan Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yan-Li Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08983" title="Abstract">arXiv:2402.08983</a> (replaced) [<a href="/pdf/2402.08983" title="Download PDF">pdf</a>, <a href="/format/2402.08983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhangchen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fengqing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Luyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09456" title="Abstract">arXiv:2402.09456</a> (replaced) [<a href="/pdf/2402.09456" title="Download PDF">pdf</a>, <a href="/format/2402.09456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingru Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+W">Wenqiang Pu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09724" title="Abstract">arXiv:2402.09724</a> (replaced) [<a href="/pdf/2402.09724" title="Download PDF">pdf</a>, <a href="/format/2402.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Region Feature Descriptor Adapted to High Affine Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bin Nan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09824" title="Abstract">arXiv:2402.09824</a> (replaced) [<a href="/pdf/2402.09824" title="Download PDF">pdf</a>, <a href="/format/2402.09824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the discrete-time origins of the replicator dynamics: From  convergence to instability and chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Falniowski%2C+F">Fryderyk Falniowski</a>, 
<a href="/search/math?searchtype=author&query=Mertikopoulos%2C+P">Panayotis Mertikopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09963" title="Abstract">arXiv:2402.09963</a> (replaced) [<a href="/pdf/2402.09963" title="Download PDF">pdf</a>, <a href="/format/2402.09963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why are Sensitive Functions Hard for Transformers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+M">Michael Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Rofin%2C+M">Mark Rofin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10002" title="Abstract">arXiv:2402.10002</a> (replaced) [<a href="/pdf/2402.10002" title="Download PDF">pdf</a>, <a href="/format/2402.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D  Point Cloud Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai-Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mofei Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10009" title="Abstract">arXiv:2402.10009</a> (replaced) [<a href="/pdf/2402.10009" title="Download PDF">pdf</a>, <a href="/format/2402.10009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manor%2C+H">Hila Manor</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updating consistent notations; Examples and code available in <a href="https://hilamanor.github.io/AudioEditing/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10207" title="Abstract">arXiv:2402.10207</a> (replaced) [<a href="/pdf/2402.10207" title="Download PDF">pdf</a>, <a href="/format/2402.10207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shuang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10340" title="Abstract">arXiv:2402.10340</a> (replaced) [<a href="/pdf/2402.10340" title="Download PDF">pdf</a>, <a href="/format/2402.10340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting  the Risks and Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruiqi Xian</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B">Brian Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10441" title="Abstract">arXiv:2402.10441</a> (replaced) [<a href="/pdf/2402.10441" title="Download PDF">pdf</a>, <a href="/format/2402.10441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barrier-Enhanced Homotopic Parallel Trajectory Optimization for  Safety-Critical Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M+Y">Michael Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10588" title="Abstract">arXiv:2402.10588</a> (replaced) [<a href="/pdf/2402.10588" title="Download PDF">pdf</a>, <a href="/format/2402.10588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Llamas Work in English? On the Latent Language of Multilingual  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wendler%2C+C">Chris Wendler</a>, 
<a href="/search/cs?searchtype=author&query=Veselovsky%2C+V">Veniamin Veselovsky</a>, 
<a href="/search/cs?searchtype=author&query=Monea%2C+G">Giovanni Monea</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages. 28 with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10802" title="Abstract">arXiv:2402.10802</a> (replaced) [<a href="/pdf/2402.10802" title="Download PDF">pdf</a>, <a href="/format/2402.10802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly  Detection Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+H">Haotian Si</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+C">Changhua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yongqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingjing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jing Han</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+D">Dan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Gaogang Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10877" title="Abstract">arXiv:2402.10877</a> (replaced) [<a href="/pdf/2402.10877" title="Download PDF">pdf</a>, <a href="/format/2402.10877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust agents learn causal world models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richens%2C+J">Jonathan Richens</a>, 
<a href="/search/cs?searchtype=author&query=Everitt%2C+T">Tom Everitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (oral). Updated discussion of emergent capabilities + references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10965" title="Abstract">arXiv:2402.10965</a> (replaced) [<a href="/pdf/2402.10965" title="Download PDF">pdf</a>, <a href="/format/2402.10965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization in Healthcare AI: Evaluation of a Clinical Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Salman Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L+Y">Lavender Yao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gabriel%2C+S">Saadia Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Aphinyanaphongs%2C+Y">Yindalon Aphinyanaphongs</a>, 
<a href="/search/cs?searchtype=author&query=Oermann%2C+E+K">Eric Karl Oermann</a>, 
<a href="/search/cs?searchtype=author&query=Chunara%2C+R">Rumi Chunara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11101" title="Abstract">arXiv:2402.11101</a> (replaced) [<a href="/pdf/2402.11101" title="Download PDF">pdf</a>, <a href="/ps/2402.11101" title="Download PostScript">ps</a>, <a href="/format/2402.11101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-based material parameters extraction from perovskite experiments  via Gaussian process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Zhan%2C+H">Hualin Zhan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ahmad%2C+V">Viqar Ahmad</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mayon%2C+A">Azul Mayon</a>, 
<a href="/search/cond-mat?searchtype=author&query=Tabi%2C+G">Grace Tabi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bui%2C+A+D">Anh Dinh Bui</a>, 
<a href="/search/cond-mat?searchtype=author&query=Li%2C+Z">Zhuofeng Li</a>, 
<a href="/search/cond-mat?searchtype=author&query=Walter%2C+D">Daniel Walter</a>, 
<a href="/search/cond-mat?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>, 
<a href="/search/cond-mat?searchtype=author&query=Weber%2C+K">Klaus Weber</a>, 
<a href="/search/cond-mat?searchtype=author&query=White%2C+T">Thomas White</a>, 
<a href="/search/cond-mat?searchtype=author&query=Catchpole%2C+K">Kylie Catchpole</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The work is supported by the Australian Centre for Advanced Photovoltaics (ACAP) and received funding from the Australian Renewable Energy Agency (ARENA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11361" title="Abstract">arXiv:2402.11361</a> (replaced) [<a href="/pdf/2402.11361" title="Download PDF">pdf</a>, <a href="/format/2402.11361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The matrix-free macro-element hybridized Discontinuous Galerkin method  for steady and unsteady compressible flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Badrkhani%2C+V">Vahid Badrkhani</a>, 
<a href="/search/cs?searchtype=author&query=Eikelder%2C+M+F+P+t">Marco F.P. ten Eikelder</a>, 
<a href="/search/cs?searchtype=author&query=Hiemstra%2C+R+R">Rene R. Hiemstra</a>, 
<a href="/search/cs?searchtype=author&query=Schillinger%2C+D">Dominik Schillinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11404" title="Abstract">arXiv:2402.11404</a> (replaced) [<a href="/pdf/2402.11404" title="Download PDF">pdf</a>, <a href="/ps/2402.11404" title="Download PostScript">ps</a>, <a href="/format/2402.11404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Stability of Deep Learning Latent Feature Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mabadeje%2C+A+O">Ademide O. Mabadeje</a>, 
<a href="/search/cs?searchtype=author&query=Pyrcz%2C+M+J">Michael J. Pyrcz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 11 figures, submitted to Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11406" title="Abstract">arXiv:2402.11406</a> (replaced) [<a href="/pdf/2402.11406" title="Download PDF">pdf</a>, <a href="/format/2402.11406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Go To Extremes: Revealing the Excessive Sensitivity and  Calibration Limitations of LLMs in Implicit Hate Speech Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jianfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Taoran Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang-Tien Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11431" title="Abstract">arXiv:2402.11431</a> (replaced) [<a href="/pdf/2402.11431" title="Download PDF">pdf</a>, <a href="/format/2402.11431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Error-Resistant View Selection Method for 3D Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bin Nan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11588" title="Abstract">arXiv:2402.11588</a> (replaced) [<a href="/pdf/2402.11588" title="Download PDF">pdf</a>, <a href="/format/2402.11588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDiT: Spiking Diffusion Model with Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hanzhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengting Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Aili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+E">Er-Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11592" title="Abstract">arXiv:2402.11592</a> (replaced) [<a href="/pdf/2402.11592" title="Download PDF">pdf</a>, <a href="/format/2402.11592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Zeroth-Order Optimization for Memory-Efficient LLM  Fine-Tuning: A Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Junyuan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wotao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11748" title="Abstract">arXiv:2402.11748</a> (replaced) [<a href="/pdf/2402.11748" title="Download PDF">pdf</a>, <a href="/format/2402.11748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-power SNN-based audio source localisation using a Hilbert Transform  spike encoding scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghighatshoar%2C+S">Saeid Haghighatshoar</a>, 
<a href="/search/cs?searchtype=author&query=Muir%2C+D+R">Dylan R Muir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11791" title="Abstract">arXiv:2402.11791</a> (replaced) [<a href="/pdf/2402.11791" title="Download PDF">pdf</a>, <a href="/format/2402.11791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jialei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11793" title="Abstract">arXiv:2402.11793</a> (replaced) [<a href="/pdf/2402.11793" title="Download PDF">pdf</a>, <a href="/format/2402.11793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Kaleidoscopic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+H">Harsh Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11846" title="Abstract">arXiv:2402.11846</a> (replaced) [<a href="/pdf/2402.11846" title="Download PDF">pdf</a>, <a href="/format/2402.11846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning  for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuguang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11863" title="Abstract">arXiv:2402.11863</a> (replaced) [<a href="/pdf/2402.11863" title="Download PDF">pdf</a>, <a href="/format/2402.11863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Interpretable are Reasoning Explanations from Prompting Large  Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeo%2C+W+J">Wei Jie Yeo</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Mong%2C+G+S">Goh Siow Mong</a>, 
<a href="/search/cs?searchtype=author&query=Rick">Rick</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11871" title="Abstract">arXiv:2402.11871</a> (replaced) [<a href="/pdf/2402.11871" title="Download PDF">pdf</a>, <a href="/format/2402.11871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions,  and Models for Planning from Raw Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Naman Shah</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+J">Jayesh Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Pulkit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11942" title="Abstract">arXiv:2402.11942</a> (replaced) [<a href="/pdf/2402.11942" title="Download PDF">pdf</a>, <a href="/format/2402.11942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effect of Leaky ReLUs on the training and generalization of  overparameterized networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yinglong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+G">Gilad Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12036" title="Abstract">arXiv:2402.12036</a> (replaced) [<a href="/pdf/2402.12036" title="Download PDF">pdf</a>, <a href="/format/2402.12036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Adaptation to Specialized Domains through Selective  Masking based on Genre and Topical Characteristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belfathi%2C+A">Anas Belfathi</a>, 
<a href="/search/cs?searchtype=author&query=Gallina%2C+Y">Ygor Gallina</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+N">Nicolas Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a>, 
<a href="/search/cs?searchtype=author&query=Monceaux%2C+L">Laura Monceaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12168" title="Abstract">arXiv:2402.12168</a> (replaced) [<a href="/pdf/2402.12168" title="Download PDF">pdf</a>, <a href="/format/2402.12168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Leilei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Meihuizi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12177" title="Abstract">arXiv:2402.12177</a> (replaced) [<a href="/pdf/2402.12177" title="Download PDF">pdf</a>, <a href="/ps/2402.12177" title="Download PostScript">ps</a>, <a href="/format/2402.12177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingtian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+S">Shawn Lan</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+P">Peter Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+D">David Barber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12226" title="Abstract">arXiv:2402.12226</a> (replaced) [<a href="/pdf/2402.12226" title="Download PDF">pdf</a>, <a href="/format/2402.12226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+J">Jun Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Junqi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiasheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhigeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruibin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yugang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 16 figures, under review, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12289" title="Abstract">arXiv:2402.12289</a> (replaced) [<a href="/pdf/2402.12289" title="Download PDF">pdf</a>, <a href="/format/2402.12289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DriveVLM: The Convergence of Autonomous Driving and Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Junru Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bailin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chenxu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+K">Kun Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+X">Xianpeng Lang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://tsinghua-mars-lab.github.io/DriveVLM/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12388" title="Abstract">arXiv:2402.12388</a> (replaced) [<a href="/pdf/2402.12388" title="Download PDF">pdf</a>, <a href="/format/2402.12388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sakashita%2C+M">Mose Sakashita</a>, 
<a href="/search/cs?searchtype=author&query=Guimbreti%C3%A8re%2C+F">Fran&#xe7;ois Guimbreti&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 21 figures, 6 tables, To appear in the Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12537" title="Abstract">arXiv:2402.12537</a> (replaced) [<a href="/pdf/2402.12537" title="Download PDF">pdf</a>, <a href="/format/2402.12537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Bayes Approach to Personalized Federated Unsupervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozkara%2C+K">Kaan Ozkara</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bruce Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruida Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Diggavi%2C+S">Suhas Diggavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12554" title="Abstract">arXiv:2402.12554</a> (replaced) [<a href="/pdf/2402.12554" title="Download PDF">pdf</a>, <a href="/format/2402.12554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense  and Hypothetical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Danna Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12656" title="Abstract">arXiv:2402.12656</a> (replaced) [<a href="/pdf/2402.12656" title="Download PDF">pdf</a>, <a href="/ps/2402.12656" title="Download PostScript">ps</a>, <a href="/format/2402.12656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperMoE: Paying Attention to Unselected Experts in Mixture of Experts  via Dynamic Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zihan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12702" title="Abstract">arXiv:2402.12702</a> (replaced) [<a href="/pdf/2402.12702" title="Download PDF">pdf</a>, <a href="/format/2402.12702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Cloud to Edge: Rethinking Generative AI for Low-Resource Design  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vuruma%2C+S+K+R">Sai Krishna Revanth Vuruma</a>, 
<a href="/search/cs?searchtype=author&query=Margetts%2C+A">Ashley Margetts</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jianhai Su</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+B">Biplav Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the Artificial Intelligence for Design Problems bridge program at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12716" title="Abstract">arXiv:2402.12716</a> (replaced) [<a href="/pdf/2402.12716" title="Download PDF">pdf</a>, <a href="/format/2402.12716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Path TCP Hijacking in Wi-Fi Networks: A Packet-Size Side Channel  Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xuewei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianping Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12735" title="Abstract">arXiv:2402.12735</a> (replaced) [<a href="/pdf/2402.12735" title="Download PDF">pdf</a>, <a href="/format/2402.12735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising OCT Images Using Steered Mixture of Experts with Multi-Model  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=%C3%96zkan%2C+A">Ayta&#xe7; &#xd6;zkan</a>, 
<a href="/search/eess?searchtype=author&query=Stoykova%2C+E">Elena Stoykova</a>, 
<a href="/search/eess?searchtype=author&query=Sikora%2C+T">Thomas Sikora</a>, 
<a href="/search/eess?searchtype=author&query=Madjarova%2C+V">Violeta Madjarova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission contains 10 pages and 4 figures. It was presented at the 2024 SPIE Photonics West, held in San Francisco. The paper details advancements in photonics applications related to healthcare and includes supplementary material with additional datasets for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12832" title="Abstract">arXiv:2402.12832</a> (replaced) [<a href="/pdf/2402.12832" title="Download PDF">pdf</a>, <a href="/ps/2402.12832" title="Download PostScript">ps</a>, <a href="/format/2402.12832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Optimal Fault Tolerant Distance Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+D">Dipan Dey</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Manoj Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in STOC, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12948" title="Abstract">arXiv:2402.12948</a> (replaced) [<a href="/pdf/2402.12948" title="Download PDF">pdf</a>, <a href="/format/2402.12948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GumbelSoft: Diversified Language Model Watermarking via the  GumbelMax-trick
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jiayi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13172" title="Abstract">arXiv:2402.13172</a> (replaced) [<a href="/pdf/2402.13172" title="Download PDF">pdf</a>, <a href="/format/2402.13172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Kinematics Estimation from Video with a Biomechanical Model and  Synthetic Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhi-Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+B">Bofan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+J+C">Judith Cueto Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Kruk%2C+E">Eline van der Kruk</a>, 
<a href="/search/cs?searchtype=author&query=Seth%2C+A">Ajay Seth</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xucong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13185" title="Abstract">arXiv:2402.13185</a> (replaced) [<a href="/pdf/2402.13185" title="Download PDF">pdf</a>, <a href="/format/2402.13185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jianhongbai.github.io/UniEdit/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13488" title="Abstract">arXiv:2402.13488</a> (replaced) [<a href="/pdf/2402.13488" title="Download PDF">pdf</a>, <a href="/format/2402.13488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Feature Matching Method Based on Multi-Level Refinement Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaxing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13609" title="Abstract">arXiv:2402.13609</a> (replaced) [<a href="/pdf/2402.13609" title="Download PDF">pdf</a>, <a href="/format/2402.13609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VOOM: Robust Visual Object Odometry and Mapping using Hierarchical  Landmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 4 tables, conference icra 2024 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13718" title="Abstract">arXiv:2402.13718</a> (replaced) [<a href="/pdf/2402.13718" title="Download PDF">pdf</a>, <a href="/format/2402.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingfa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+M+K">Moo Khai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+Z+L">Zhen Leng Thai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023.12.15ARR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13777" title="Abstract">arXiv:2402.13777</a> (replaced) [<a href="/pdf/2402.13777" title="Download PDF">pdf</a>, <a href="/format/2402.13777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+B">Bhargav Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yongsheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added more insights on future directions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13866" title="Abstract">arXiv:2402.13866</a> (replaced) [<a href="/pdf/2402.13866" title="Download PDF">pdf</a>, <a href="/format/2402.13866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kuaiji: the First Chinese Accounting Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiayuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songhua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xiaoling Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Panyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nai%2C+Y">Yufei Nai</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenxuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinke Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> version 2.0
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13918" title="Abstract">arXiv:2402.13918</a> (replaced) [<a href="/pdf/2402.13918" title="Download PDF">pdf</a>, <a href="/format/2402.13918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for  Cloud Detection and Segmentation in Remote Sensing Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabio%2C+L">Loddo Fabio</a>, 
<a href="/search/cs?searchtype=author&query=Piga%2C+D">Dario Piga</a>, 
<a href="/search/cs?searchtype=author&query=Umberto%2C+M">Michelucci Umberto</a>, 
<a href="/search/cs?searchtype=author&query=Safouane%2C+E+G">El Ghazouali Safouane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Expert Systems and Applications. Under license CC-BY-NC-ND
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13929" title="Abstract">arXiv:2402.13929</a> (replaced) [<a href="/pdf/2402.13929" title="Download PDF">pdf</a>, <a href="/format/2402.13929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDXL-Lightning: Progressive Adversarial Diffusion Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shanchuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Anran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13963" title="Abstract">arXiv:2402.13963</a> (replaced) [<a href="/pdf/2402.13963" title="Download PDF">pdf</a>, <a href="/format/2402.13963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Building Multilingual Language Model for Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+P">Pengcheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chaoyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoman Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weixiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14105" title="Abstract">arXiv:2402.14105</a> (replaced) [<a href="/pdf/2402.14105" title="Download PDF">pdf</a>, <a href="/format/2402.14105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Definitions and Performance Comparison of Consistency Models for  Parallel File Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mohror%2C+K">Kathryn Mohror</a>, 
<a href="/search/cs?searchtype=author&query=Snir%2C+M">Marc Snir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. Submitted to IEEE TPDS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Operating Systems (cs.OS)

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14169" title="Abstract">arXiv:2402.14169</a> (replaced) [<a href="/pdf/2402.14169" title="Download PDF">pdf</a>, <a href="/format/2402.14169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Temporal Bias Correction using a Machine Learning Attention model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nivron%2C+O">Omer Nivron</a>, 
<a href="/search/cs?searchtype=author&query=Wischik%2C+D+J">Damon J. Wischik</a>, 
<a href="/search/cs?searchtype=author&query=Vrac%2C+M">Mathieu Vrac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14174" title="Abstract">arXiv:2402.14174</a> (replaced) [<a href="/pdf/2402.14174" title="Download PDF">pdf</a>, <a href="/format/2402.14174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blending Data-Driven Priors in Dynamic Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lidard%2C+J">Justin Lidard</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haimin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hancock%2C+A">Asher Hancock</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Contreras%2C+A+G">Albert Gim&#xf3; Contreras</a>, 
<a href="/search/cs?searchtype=author&query=Modi%2C+V">Vikash Modi</a>, 
<a href="/search/cs?searchtype=author&query=DeCastro%2C+J">Jonathan DeCastro</a>, 
<a href="/search/cs?searchtype=author&query=Gopinath%2C+D">Deepak Gopinath</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+G">Guy Rosman</a>, 
<a href="/search/cs?searchtype=author&query=Leonard%2C+N">Naomi Leonard</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M">Mar&#xed;a Santos</a>, 
<a href="/search/cs?searchtype=author&query=Fisac%2C+J+F">Jaime Fern&#xe1;ndez Fisac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14332" title="Abstract">arXiv:2402.14332</a> (replaced) [<a href="/pdf/2402.14332" title="Download PDF">pdf</a>, <a href="/format/2402.14332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Large to Small Datasets: Size Generalization for Clustering  Algorithm Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatziafratis%2C+V">Vaggos Chatziafratis</a>, 
<a href="/search/cs?searchtype=author&query=Karmarkar%2C+I">Ishani Karmarkar</a>, 
<a href="/search/cs?searchtype=author&query=Vitercik%2C+E">Ellen Vitercik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14345" title="Abstract">arXiv:2402.14345</a> (replaced) [<a href="/pdf/2402.14345" title="Download PDF">pdf</a>, <a href="/format/2402.14345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Error-Matching Exclusion Method for Accelerating Visual SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaxing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Atadjanov%2C+I+R">Ibragim R. Atadjanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14404" title="Abstract">arXiv:2402.14404</a> (replaced) [<a href="/pdf/2402.14404" title="Download PDF">pdf</a>, <a href="/format/2402.14404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Menghan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+P">Peng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14418" title="Abstract">arXiv:2402.14418</a> (replaced) [<a href="/pdf/2402.14418" title="Download PDF">pdf</a>, <a href="/format/2402.14418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Evaluation for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kostumov%2C+V">Vasily Kostumov</a>, 
<a href="/search/cs?searchtype=author&query=Nutfullin%2C+B">Bulat Nutfullin</a>, 
<a href="/search/cs?searchtype=author&query=Pilipenko%2C+O">Oleg Pilipenko</a>, 
<a href="/search/cs?searchtype=author&query=Ilyushin%2C+E">Eugene Ilyushin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14494" title="Abstract">arXiv:2402.14494</a> (replaced) [<a href="/pdf/2402.14494" title="Download PDF">pdf</a>, <a href="/format/2402.14494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yueyan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tingfeng Hui</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daichi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14503" title="Abstract">arXiv:2402.14503</a> (replaced) [<a href="/pdf/2402.14503" title="Download PDF">pdf</a>, <a href="/format/2402.14503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Human-AI Collaboration in Music Therapy Through Co-Design  with Therapists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingjing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yucheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jiangtao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14539" title="Abstract">arXiv:2402.14539</a> (replaced) [<a href="/pdf/2402.14539" title="Download PDF">pdf</a>, <a href="/format/2402.14539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transforming Norm-based To Graph-based Spatial Representation for  Spatio-Temporal Epidemiological Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazebnik%2C+T">Teddy Lazebnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14586" title="Abstract">arXiv:2402.14586</a> (replaced) [<a href="/pdf/2402.14586" title="Download PDF">pdf</a>, <a href="/format/2402.14586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yan Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ligang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daolun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14609" title="Abstract">arXiv:2402.14609</a> (replaced) [<a href="/pdf/2402.14609" title="Download PDF">pdf</a>, <a href="/format/2402.14609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weifeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaxin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lixin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14634" title="Abstract">arXiv:2402.14634</a> (replaced) [<a href="/pdf/2402.14634" title="Download PDF">pdf</a>, <a href="/format/2402.14634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Sicheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+S">Saif Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qikang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Guimbreti%C3%A8re%2C+F">Fran&#xe7;ois Guimbreti&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 7 tables, The 30th Annual International Conference on Mobile Computing and Networking (ACM MobiCom 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14693" title="Abstract">arXiv:2402.14693</a> (replaced) [<a href="/pdf/2402.14693" title="Download PDF">pdf</a>, <a href="/ps/2402.14693" title="Download PostScript">ps</a>, <a href="/format/2402.14693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint AP-UE Association and Power Factor Optimization for Distributed  Massive MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+S+A">Mohd Saif Ali Khan</a>, 
<a href="/search/cs?searchtype=author&query=Agnihotri%2C+S">Samar Agnihotri</a>, 
<a href="/search/cs?searchtype=author&query=M%2C+K+R">Karthik R.M</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14707" title="Abstract">arXiv:2402.14707</a> (replaced) [<a href="/pdf/2402.14707" title="Download PDF">pdf</a>, <a href="/format/2402.14707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Cytopathological Image Synthesis for Augmenting Cervical  Abnormality Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhenrong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+M">Manman Fei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jiangdong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14711" title="Abstract">arXiv:2402.14711</a> (replaced) [<a href="/pdf/2402.14711" title="Download PDF">pdf</a>, <a href="/format/2402.14711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Observability for Nonlinear Systems: Connecting Variational Dynamics,  Lyapunov Exponents, and Empirical Gramians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kazma%2C+M+H">Mohamad H. Kazma</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+A+F">Ahmad F. Taha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14740" title="Abstract">arXiv:2402.14740</a> (replaced) [<a href="/pdf/2402.14740" title="Download PDF">pdf</a>, <a href="/format/2402.14740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to Basics: Revisiting REINFORCE Style Optimization for Learning  from Human Feedback in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadian%2C+A">Arash Ahmadian</a>, 
<a href="/search/cs?searchtype=author&query=Cremer%2C+C">Chris Cremer</a>, 
<a href="/search/cs?searchtype=author&query=Gall%C3%A9%2C+M">Matthias Gall&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Fadaee%2C+M">Marzieh Fadaee</a>, 
<a href="/search/cs?searchtype=author&query=Kreutzer%2C+J">Julia Kreutzer</a>, 
<a href="/search/cs?searchtype=author&query=Pietquin%2C+O">Olivier Pietquin</a>, 
<a href="/search/cs?searchtype=author&query=%C3%9Cst%C3%BCn%2C+A">Ahmet &#xdc;st&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14742" title="Abstract">arXiv:2402.14742</a> (replaced) [<a href="/pdf/2402.14742" title="Download PDF">pdf</a>, <a href="/ps/2402.14742" title="Download PostScript">ps</a>, <a href="/format/2402.14742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New scattered linearized quadrinomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Smaldore%2C+V">Valentino Smaldore</a>, 
<a href="/search/math?searchtype=author&query=Zanella%2C+C">Corrado Zanella</a>, 
<a href="/search/math?searchtype=author&query=Zullo%2C+F">Ferdinando Zullo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14807" title="Abstract">arXiv:2402.14807</a> (replaced) [<a href="/pdf/2402.14807" title="Download PDF">pdf</a>, <a href="/format/2402.14807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit  Tasks in Public Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behari%2C+N">Nikhil Behari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Edwin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aparna Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14873" title="Abstract">arXiv:2402.14873</a> (replaced) [<a href="/pdf/2402.14873" title="Download PDF">pdf</a>, <a href="/format/2402.14873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report on the Checkfor.ai AI-Generated Text Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emi%2C+B">Bradley Emi</a>, 
<a href="/search/cs?searchtype=author&query=Spero%2C+M">Max Spero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14890" title="Abstract">arXiv:2402.14890</a> (replaced) [<a href="/pdf/2402.14890" title="Download PDF">pdf</a>, <a href="/format/2402.14890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vygotsky Distance: Measure for Benchmark Task Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Surkov%2C+M+K">Maxim K. Surkov</a>, 
<a href="/search/cs?searchtype=author&query=Yamshchikov%2C+I+P">Ivan P. Yamshchikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14891" title="Abstract">arXiv:2402.14891</a> (replaced) [<a href="/pdf/2402.14891" title="Download PDF">pdf</a>, <a href="/format/2402.14891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMBind: A Unified Modality-Task Integration Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinfa Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qi Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhenyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+M">Mingjun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14948" title="Abstract">arXiv:2402.14948</a> (replaced) [<a href="/pdf/2402.14948" title="Download PDF">pdf</a>, <a href="/format/2402.14948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Examine Distantly Supervised NER: A New Benchmark and a Simple  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuepei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Q">Qiao Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15119" title="Abstract">arXiv:2402.15119</a> (replaced) [<a href="/pdf/2402.15119" title="Download PDF">pdf</a>, <a href="/ps/2402.15119" title="Download PostScript">ps</a>, <a href="/format/2402.15119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multidisciplinary framework for deconstructing bots&#x27; pluripotency in  dualistic antagonism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wentao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sasahara%2C+K">Kazutoshi Sasahara</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jianxun Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenlu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiwen Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15175" title="Abstract">arXiv:2402.15175</a> (replaced) [<a href="/pdf/2402.15175" title="Download PDF">pdf</a>, <a href="/format/2402.15175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified View of Grokking, Double Descent and Emergent Abilities: A  Perspective from Circuits Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15202" title="Abstract">arXiv:2402.15202</a> (replaced) [<a href="/pdf/2402.15202" title="Download PDF">pdf</a>, <a href="/format/2402.15202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Detoxification via Instance-Level Prefixes for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xin Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Linlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoling Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15281" title="Abstract">arXiv:2402.15281</a> (replaced) [<a href="/pdf/2402.15281" title="Download PDF">pdf</a>, <a href="/format/2402.15281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Implicit Swept Volume Models for Fast Collision Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joho%2C+D">Dominik Joho</a>, 
<a href="/search/cs?searchtype=author&query=Schwinn%2C+J">Jonas Schwinn</a>, 
<a href="/search/cs?searchtype=author&query=Safronov%2C+K">Kirill Safronov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15313" title="Abstract">arXiv:2402.15313</a> (replaced) [<a href="/pdf/2402.15313" title="Download PDF">pdf</a>, <a href="/format/2402.15313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArabianGPT: Native Arabic GPT-based Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koubaa%2C+A">Anis Koubaa</a>, 
<a href="/search/cs?searchtype=author&query=Ammar%2C+A">Adel Ammar</a>, 
<a href="/search/cs?searchtype=author&query=Ghouti%2C+L">Lahouari Ghouti</a>, 
<a href="/search/cs?searchtype=author&query=Najar%2C+O">Omar Najar</a>, 
<a href="/search/cs?searchtype=author&query=Sibaee%2C+S">Serry Sibaee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15361" title="Abstract">arXiv:2402.15361</a> (replaced) [<a href="/pdf/2402.15361" title="Download PDF">pdf</a>, <a href="/ps/2402.15361" title="Download PostScript">ps</a>, <a href="/format/2402.15361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A priori error estimates of Runge-Kutta discontinuous Galerkin schemes  to smooth solutions of fractional conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leotta%2C+F">Fabio Leotta</a>, 
<a href="/search/math?searchtype=author&query=Giesselmann%2C+J">Jan Giesselmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15367" title="Abstract">arXiv:2402.15367</a> (replaced) [<a href="/pdf/2402.15367" title="Download PDF">pdf</a>, <a href="/format/2402.15367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A CWENO large time-step scheme for Hamilton--Jacobi equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carlini%2C+E">E. Carlini</a>, 
<a href="/search/math?searchtype=author&query=Ferretti%2C+R">R. Ferretti</a>, 
<a href="/search/math?searchtype=author&query=Preda%2C+S">S. Preda</a>, 
<a href="/search/math?searchtype=author&query=Semplice%2C+M">M. Semplice</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15434" title="Abstract">arXiv:2402.15434</a> (replaced) [<a href="/pdf/2402.15434" title="Download PDF">pdf</a>, <a href="/ps/2402.15434" title="Download PostScript">ps</a>, <a href="/format/2402.15434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Pulse of Community during Disasters: Resilience Analysis  Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15481" title="Abstract">arXiv:2402.15481</a> (replaced) [<a href="/pdf/2402.15481" title="Download PDF">pdf</a>, <a href="/format/2402.15481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prejudice and Caprice: A Statistical Framework for Measuring Social  Discrimination in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiran Liu</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Ke Yang</a> (1 and 3), 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zehan Qi</a> (2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a> (2), 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a> (2), 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">Chengxiang Zhai</a> (3) ((1) Equal contributions, (2) Tsinghua University, (3) University of Illinois Urbana-Champaign)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15506" title="Abstract">arXiv:2402.15506</a> (replaced) [<a href="/pdf/2402.15506" title="Download PDF">pdf</a>, <a href="/format/2402.15506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentOhana: Design Unified Data and Training Pipeline for Effective  Agent Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Murthy%2C+R">Rithesh Murthy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weiran Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Juntao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T">Thai Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Awalgaonkar%2C+T">Tulika Awalgaonkar</a>, 
<a href="/search/cs?searchtype=author&query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+S">Shelby Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add testing results on ToolBench and correct typographical errors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item612">Cross-lists</a></li>
<li><a href="#item688">Replacements</a></li>
</ul>
<small>[ total of 1206 entries:  <b>1-1206</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
