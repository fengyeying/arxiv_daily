<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon 19 Feb 24  to  Tue 20 Feb 24, announced Wed, 21 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item405">Cross-lists</a></li>
<li><a href="#item456">Replacements</a></li>
</ul>
<small>[ total of 748 entries:  <b>1-748</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed, 21 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12381" title="Abstract">arXiv:2402.12381</a> [<a href="/pdf/2402.12381" title="Download PDF">pdf</a>, <a href="/format/2402.12381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Multi-objective Optimization with Deep Reinforcement  Learning Assisted Operator Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ming%2C+F">Fei Ming</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+W">Wenyin Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Ling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Solving constrained multi-objective optimization problems with evolutionary
algorithms has attracted considerable attention. Various constrained
multi-objective optimization evolutionary algorithms (CMOEAs) have been
developed with the use of different algorithmic strategies, evolutionary
operators, and constraint-handling techniques. The performance of CMOEAs may be
heavily dependent on the operators used, however, it is usually difficult to
select suitable operators for the problem at hand. Hence, improving operator
selection is promising and necessary for CMOEAs. This work proposes an online
operator selection framework assisted by Deep Reinforcement Learning. The
dynamics of the population, including convergence, diversity, and feasibility,
are regarded as the state; the candidate operators are considered as actions;
and the improvement of the population state is treated as the reward. By using
a Q-Network to learn a policy to estimate the Q-values of all actions, the
proposed approach can adaptively select an operator that maximizes the
improvement of the population according to the current state and thereby
improve the algorithmic performance. The framework is embedded into four
popular CMOEAs and assessed on 42 benchmark problems. The experimental results
reveal that the proposed Deep Reinforcement Learning-assisted operator
selection significantly improves the performance of these CMOEAs and the
resulting algorithm obtains better versatility compared to nine
state-of-the-art CMOEAs.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12388" title="Abstract">arXiv:2402.12388</a> [<a href="/pdf/2402.12388" title="Download PDF">pdf</a>, <a href="/format/2402.12388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sakashita%2C+M">Mose Sakashita</a>, 
<a href="/search/cs?searchtype=author&query=Guimbreti%C3%A8re%2C+F">Fran&#xe7;ois Guimbreti&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 20 figures, 6 tables, To appear in the Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In this paper, we introduce EyeEcho, a minimally-obtrusive acoustic sensing
system designed to enable glasses to continuously monitor facial expressions.
It utilizes two pairs of speakers and microphones mounted on glasses, to emit
encoded inaudible acoustic signals directed towards the face, capturing subtle
skin deformations associated with facial expressions. The reflected signals are
processed through a customized machine-learning pipeline to estimate full
facial movements. EyeEcho samples at 83.3 Hz with a relatively low power
consumption of 167 mW. Our user study involving 12 participants demonstrates
that, with just four minutes of training data, EyeEcho achieves highly accurate
tracking performance across different real-world scenarios, including sitting,
walking, and after remounting the devices. Additionally, a semi-in-the-wild
study involving 10 participants further validates EyeEcho's performance in
naturalistic scenarios while participants engage in various daily activities.
Finally, we showcase EyeEcho's potential to be deployed on a
commercial-off-the-shelf (COTS) smartphone, offering real-time facial
expression tracking.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12390" title="Abstract">arXiv:2402.12390</a> [<a href="/pdf/2402.12390" title="Download PDF">pdf</a>, <a href="/ps/2402.12390" title="Download PostScript">ps</a>, <a href="/format/2402.12390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Semantic Social Network Analysis Tool for Sensitivity Analysis and  What-If Scenario Testing in Alcohol Consumption Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-Gonz%C3%A1lez%2C+A">Alejandro Rodr&#xed;guez-Gonz&#xe1;lez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Valde%C3%B3n%2C+L">Leticia S&#xe1;nchez-Valde&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+I">Isa&#xed;as Garc&#xed;a</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Int. J. Environ. Res. Public Health 2018, 15(11), 2420;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Social Network Analysis (SNA) is a set of techniques developed in the field
of social and behavioral sciences research, in order to characterize and study
the social relationships that are established among a set of individuals. When
building a social network for performing an SNA analysis, an initial process of
data gathering is achieved in order to extract the characteristics of the
individuals and their relationships. This is usually done by completing a
questionnaire containing different types of questions that will be later used
to obtain the SNA measures needed to perform the study. There are, then, a
great number of different possible network generating questions and also many
possibilities for mapping the responses to the corresponding characteristics
and relationships. Many variations may be introduced into these questions (the
way they are posed, the weights given to each of the responses, etc.) that may
have an effect on the resulting networks. All these different variations are
difficult to achieve manually, because the process is time-consuming and error
prone. The tool described in this paper uses semantic knowledge representation
techniques in order to facilitate this kind of sensitivity studies. The base of
the tool is a conceptual structure, called "ontology" that is able to represent
the different concepts and their definitions. The tool is compared to other
similar ones, and the advantages of the approach are highlighted, giving some
particular examples from an ongoing SNA study about alcohol consumption habits
in adolescents.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12393" title="Abstract">arXiv:2402.12393</a> [<a href="/pdf/2402.12393" title="Download PDF">pdf</a>, <a href="/format/2402.12393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Automating Video Game Testing by Planning and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balyo%2C+T">Tom&#xe1;&#x161; Balyo</a>, 
<a href="/search/cs?searchtype=author&query=Youngblood%2C+G+M">G. Michael Youngblood</a>, 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+F">Filip Dvo&#x159;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Bart%C3%A1k%2C+R">Roman Bart&#xe1;k</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a method and workflow for automating the testing of
certain video game aspects using automated planning and planning action model
learning techniques. The basic idea is to generate detailed gameplay logs and
apply action model learning to obtain a formal model in the planning domain
description language (PDDL). The workflow enables efficient cooperation of game
developers without any experience with PDDL or other formal systems and a
person experienced with PDDL modeling but no game development skills. We
describe the method and workflow in general and then demonstrate it on a
concrete proof-of-concept example -- a simple role-playing game provided as one
of the tutorial projects in the popular game development engine Unity. This
paper presents the first step towards minimizing or even eliminating the need
for a modeling expert in the workflow, thus making automated planning
accessible to a broader audience.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12394" title="Abstract">arXiv:2402.12394</a> [<a href="/pdf/2402.12394" title="Download PDF">pdf</a>, <a href="/format/2402.12394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Model&#x27;s Interpretability and Reliability using Biomarkers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gare%2C+G+R">Gautam Rajendrakumar Gare</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+T">Tom Fox</a>, 
<a href="/search/cs?searchtype=author&query=Chansangavej%2C+B">Beam Chansangavej</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+A">Amita Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+R+L">Ricardo Luis Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=deBoisblanc%2C+B+P">Bennett P deBoisblanc</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D+K">Deva Kannan Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Galeotti%2C+J+M">John Michael Galeotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at BIAS 2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Accurate and interpretable diagnostic models are crucial in the
safety-critical field of medicine. We investigate the interpretability of our
proposed biomarker-based lung ultrasound diagnostic pipeline to enhance
clinicians' diagnostic capabilities. The objective of this study is to assess
whether explanations from a decision tree classifier, utilizing biomarkers, can
improve users' ability to identify inaccurate model predictions compared to
conventional saliency maps. Our findings demonstrate that decision tree
explanations, based on clinically established biomarkers, can assist clinicians
in detecting false positives, thus improving the reliability of diagnostic
models in medicine.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12398" title="Abstract">arXiv:2402.12398</a> [<a href="/pdf/2402.12398" title="Download PDF">pdf</a>, <a href="/format/2402.12398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primary and Secondary Factor Consistency as Domain Knowledge to Guide  Happiness Computing in Online Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaohua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+F">Frank Xing</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jingling Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Happiness computing based on large-scale online web data and machine learning
methods is an emerging research topic that underpins a range of issues, from
personal growth to social stability. Many advanced Machine Learning (ML) models
with explanations are used to compute the happiness online assessment while
maintaining high accuracy of results. However, domain knowledge constraints,
such as the primary and secondary relations of happiness factors, are absent
from these models, which limits the association between computing results and
the right reasons for why they occurred. This article attempts to provide new
insights into the explanation consistency from an empirical study perspective.
Then we study how to represent and introduce domain knowledge constraints to
make ML models more trustworthy. We achieve this through: (1) proving that
multiple prediction models with additive factor attributions will have the
desirable property of primary and secondary relations consistency, and (2)
showing that factor relations with quantity can be represented as an importance
distribution for encoding domain knowledge. Factor explanation difference is
penalized by the Kullback-Leibler divergence-based loss among computing models.
Experimental results using two online web datasets show that domain knowledge
of stable factor relations exists. Using this knowledge not only improves
happiness computing accuracy but also reveals more significative happiness
factors for assisting decisions well.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12399" title="Abstract">arXiv:2402.12399</a> [<a href="/pdf/2402.12399" title="Download PDF">pdf</a>, <a href="/format/2402.12399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiyuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhaoye Fei</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Sparse Mixture of Experts (MoE) models are popular for training large
language models due to their computational efficiency. However, the commonly
used top-$k$ routing mechanism suffers from redundancy computation and memory
costs due to the unbalanced routing. Some experts are overflow, where the
exceeding tokens are dropped. While some experts are vacant, which are padded
with zeros, negatively impacting model performance. To address the dropped
tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU
Rectification and the Fill-in Rectification. The Intra-GPU Rectification
handles dropped tokens, efficiently routing them to experts within the GPU
where they are located to avoid inter-GPU communication. The Fill-in
Rectification addresses padding by replacing padding tokens with the tokens
that have high routing scores. Our experimental results demonstrate that the
Intra-GPU Rectification and the Fill-in Rectification effectively handle
dropped tokens and padding, respectively. Furthermore, the combination of them
achieves superior performance, surpassing the accuracy of the vanilla top-1
router by 4.7%.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12406" title="Abstract">arXiv:2402.12406</a> [<a href="/pdf/2402.12406" title="Download PDF">pdf</a>, <a href="/format/2402.12406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Hyunjune Shin</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+D">Dong-Wan Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge
to a student model with the help of a generator without using original data. In
such data-free scenarios, achieving stable performance of DFKD is essential due
to the unavailability of validation data. Unfortunately, this paper has
discovered that existing DFKD methods are quite sensitive to different teacher
models, occasionally showing catastrophic failures of distillation, even when
using well-trained teacher models. Our observation is that the generator in
DFKD is not always guaranteed to produce precise yet diverse samples using the
existing representative strategy of minimizing both class-prior and adversarial
losses. Through our empirical study, we focus on the fact that class-prior not
only decreases the diversity of generated samples, but also cannot completely
address the problem of generating unexpectedly low-quality samples depending on
teacher models. In this paper, we propose the teacher-agnostic data-free
knowledge distillation (TA-DFKD) method, with the goal of more robust and
stable performance regardless of teacher models. Our basic idea is to assign
the teacher model a lenient expert role for evaluating samples, rather than a
strict supervisor that enforces its class-prior on the generator. Specifically,
we design a sample selection approach that takes only clean samples verified by
the teacher model without imposing restrictions on the power of generating
diverse samples. Through extensive experiments, we show that our method
successfully achieves both robustness and training stability across various
teacher models, while outperforming the existing DFKD methods.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12408" title="Abstract">arXiv:2402.12408</a> [<a href="/pdf/2402.12408" title="Download PDF">pdf</a>, <a href="/format/2402.12408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ModelGPT: Unleashing LLM&#x27;s Capabilities for Tailored Model Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zihao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zheqi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The rapid advancement of Large Language Models (LLMs) has revolutionized
various sectors by automating routine tasks, marking a step toward the
realization of Artificial General Intelligence (AGI). However, they still
struggle to accommodate the diverse and specific needs of users and simplify
the utilization of AI models for the average user. In response, we propose
ModelGPT, a novel framework designed to determine and generate AI models
specifically tailored to the data or task descriptions provided by the user,
leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able
to provide tailored models at most 270x faster than the previous paradigms
(e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV,
and Tabular datasets attest to the effectiveness of our framework in making AI
models more accessible and user-friendly. Our code is available at
https://github.com/IshiKura-a/ModelGPT.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12411" title="Abstract">arXiv:2402.12411</a> [<a href="/pdf/2402.12411" title="Download PDF">pdf</a>, <a href="/format/2402.12411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Structural Knowledge Exploitation and Synergy for Estimating Node  Importance Value on Heterogeneous Information Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yankai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yixiang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiongyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xin Cao</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Node importance estimation problem has been studied conventionally with
homogeneous network topology analysis. To deal with network heterogeneity, a
few recent methods employ graph neural models to automatically learn diverse
sources of information. However, the major concern revolves around that their
full adaptive learning process may lead to insufficient information
exploration, thereby formulating the problem as the isolated node value
prediction with underperformance and less interpretability. In this work, we
propose a novel learning framework: SKES. Different from previous automatic
learning designs, SKES exploits heterogeneous structural knowledge to enrich
the informativeness of node representations. Based on a sufficiently
uninformative reference, SKES estimates the importance value for any input
node, by quantifying its disparity against the reference. This establishes an
interpretable node importance computation paradigm. Furthermore, SKES dives
deep into the understanding that "nodes with similar characteristics are prone
to have similar importance values" whilst guaranteeing that such
informativeness disparity between any different nodes is orderly reflected by
the embedding distance of their associated latent features. Extensive
experiments on three widely-evaluated benchmarks demonstrate the performance
superiority of SKES over several recent competing methods.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12412" title="Abstract">arXiv:2402.12412</a> [<a href="/pdf/2402.12412" title="Download PDF">pdf</a>, <a href="/format/2402.12412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI:  Unpredictable Plays Never Repeating The Same
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Yim%2C+H">Hyun-Jeong Yim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sung-Ik Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper introduces a media service model that exploits artificial
intelligence (AI) video generators at the receive end. This proposal deviates
from the traditional multimedia ecosystem, completely relying on in-house
production, by shifting part of the content creation onto the receiver. We
bring a semantic process into the framework, allowing the distribution network
to provide service elements that prompt the content generator, rather than
distributing encoded data of fully finished programs. The service elements
include fine-tailored text descriptions, lightweight image data of some
objects, or application programming interfaces, comprehensively referred to as
semantic sources, and the user terminal translates the received semantic data
into video frames. Empowered by the random nature of generative AI, the users
could then experience super-personalized services accordingly. The proposed
idea incorporates the situations in which the user receives different service
providers' element packages; a sequence of packages over time, or multiple
packages at the same time. Given promised in-context coherence and content
integrity, the combinatory dynamics will amplify the service diversity,
allowing the users to always chance upon new experiences. This work
particularly aims at short-form videos and advertisements, which the users
would easily feel fatigued by seeing the same frame sequence every time. In
those use cases, the content provider's role will be recast as scripting
semantic sources, transformed from a thorough producer. Overall, this work
explores a new form of media ecosystem facilitated by receiver-embedded
generative models, featuring both random content dynamics and enhanced delivery
efficiency simultaneously.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12415" title="Abstract">arXiv:2402.12415</a> [<a href="/pdf/2402.12415" title="Download PDF">pdf</a>, <a href="/ps/2402.12415" title="Download PostScript">ps</a>, <a href="/format/2402.12415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vehicle-group-based Crash Risk Formation and Propagation Analysis for  Expressways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Ling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yiheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wanjing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Abdel-Aty%2C+M">Mohamed Abdel-Aty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Previous studies in predicting crash risk primarily associated the number or
likelihood of crashes on a road segment with traffic parameters or geometric
characteristics of the segment, usually neglecting the impact of vehicles'
continuous movement and interactions with nearby vehicles. Advancements in
communication technologies have empowered driving information collected from
surrounding vehicles, enabling the study of group-based crash risks. Based on
high-resolution vehicle trajectory data, this research focused on vehicle
groups as the subject of analysis and explored risk formation and propagation
mechanisms considering features of vehicle groups and road segments. Several
key factors contributing to crash risks were identified, including past
high-risk vehicle-group states, complex vehicle behaviors, high percentage of
large vehicles, frequent lane changes within a vehicle group, and specific road
geometries. A multinomial logistic regression model was developed to analyze
the spatial risk propagation patterns, which were classified based on the trend
of high-risk occurrences within vehicle groups. The results indicated that
extended periods of high-risk states, increase in vehicle-group size, and
frequent lane changes are associated with adverse risk propagation patterns.
Conversely, smoother traffic flow and high initial crash risk values are linked
to risk dissipation. Furthermore, the study conducted sensitivity analysis on
different types of classifiers, prediction time intervalsss and adaptive TTC
thresholds. The highest AUC value for vehicle-group risk prediction surpassed
0.93. The findings provide valuable insights to researchers and practitioners
in understanding and prediction of vehicle-group safety, ultimately improving
active traffic safety management and operations of Connected and Autonomous
Vehicles.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12416" title="Abstract">arXiv:2402.12416</a> [<a href="/pdf/2402.12416" title="Download PDF">pdf</a>, <a href="/format/2402.12416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Individual and Collective Objectives in Multi-Agent Cooperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of multi-agent learning, the challenge of mixed-motive
cooperation is pronounced, given the inherent contradictions between individual
and collective goals. Current research in this domain primarily focuses on
incorporating domain knowledge into rewards or introducing additional
mechanisms to foster cooperation. However, many of these methods suffer from
the drawbacks of manual design costs and the lack of a theoretical grounding
convergence procedure to the solution. To address this gap, we approach the
mixed-motive game by modeling it as a differentiable game to study learning
dynamics. We introduce a novel optimization method named Altruistic Gradient
Adjustment (AgA) that employs gradient adjustments to novelly align individual
and collective objectives. Furthermore, we provide theoretical proof that the
selection of an appropriate alignment weight in AgA can accelerate convergence
towards the desired solutions while effectively avoiding the undesired ones.
The visualization of learning dynamics effectively demonstrates that AgA
successfully achieves alignment between individual and collective objectives.
Additionally, through evaluations conducted on established mixed-motive
benchmarks such as the public good game, Cleanup, Harvest, and our modified
mixed-motive SMAC environment, we validate AgA's capability to facilitate
altruistic and fair collaboration.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12417" title="Abstract">arXiv:2402.12417</a> [<a href="/pdf/2402.12417" title="Download PDF">pdf</a>, <a href="/ps/2402.12417" title="Download PostScript">ps</a>, <a href="/format/2402.12417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting trucking accidents with truck drivers &#x27;safety climate  perception across companies: A transfer learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kailai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tianxiang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Kam%2C+S+H">Say Hong Kam</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+Y+M">Yang Miang Goh</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yueng-Hsiang Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to journal: accident analysis and prevention
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There is a rising interest in using artificial intelligence (AI)-powered
safety analytics to predict accidents in the trucking industry. Companies may
face the practical challenge, however, of not having enough data to develop
good safety analytics models. Although pretrained models may offer a solution
for such companies, existing safety research using transfer learning has mostly
focused on computer vision and natural language processing, rather than
accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune
transfer learning approach to help any company leverage other companies' data
to develop AI models for a more accurate prediction of accident risk. We also
develop SafeNet, a deep neural network algorithm for classification tasks
suitable for accident prediction. Using the safety climate survey data from
seven trucking companies with different data sizes, we show that our proposed
approach results in better model performance compared to training the model
from scratch using only the target company's data. We also show that for the
transfer learning model to be effective, the pretrained model should be
developed with larger datasets from diverse sources. The trucking industry may,
thus, consider pooling safety analytics data from a wide range of companies to
develop pretrained models and share them within the industry for better
knowledge and resource transfer. The above contributions point to the promise
of advanced safety analytics to make the industry safer and more sustainable.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12418" title="Abstract">arXiv:2402.12418</a> [<a href="/pdf/2402.12418" title="Download PDF">pdf</a>, <a href="/format/2402.12418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%2C+A+G+R">Akash Guna R.T</a>, 
<a href="/search/cs?searchtype=author&query=Chavan%2C+A">Arnav Chavan</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Deepak Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted At ICLR 2024 (Tiny Paper Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Conventional scaling of neural networks typically involves designing a base
network and growing different dimensions like width, depth, etc. of the same by
some predefined scaling factors. We introduce an automated scaling approach
leveraging second-order loss landscape information. Our method is flexible
towards skip connections a mainstay in modern vision transformers. Our
training-aware method jointly scales and trains transformers without additional
training iterations. Motivated by the hypothesis that not all neurons need
uniform depth complexity, our approach embraces depth heterogeneity. Extensive
evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10%
parameter efficiency improvement over conventional scaling. Scaled networks
demonstrate superior performance upon training small scale datasets from
scratch. We introduce the first intact scaling mechanism for vision
transformers, a step towards efficient model scaling.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12419" title="Abstract">arXiv:2402.12419</a> [<a href="/pdf/2402.12419" title="Download PDF">pdf</a>, <a href="/format/2402.12419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiawu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+F">Fei Chao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Existing methods for fine-tuning sparse LLMs often suffer from
resource-intensive requirements and high retraining costs. Additionally, many
fine-tuning methods often rely on approximations or heuristic optimization
strategies, which may lead to suboptimal solutions. To address these issues, we
propose an efficient and fast framework for fine-tuning sparse LLMs based on
minimizing reconstruction error. Our approach involves sampling a small dataset
for calibration and utilizing backpropagation to iteratively optimize
block-wise reconstruction error, on a block-by-block basis, aiming for optimal
solutions. Extensive experiments on various benchmarks consistently demonstrate
the superiority of our method over other baselines. For instance, on the
Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a
perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of
75.14. Moreover, with a structured sparsity ratio of 26\%, EBFT achieves a
perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the
fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,
and the entire framework can be executed on a single 16GB GPU. The source code
is available at https://github.com/sunggo/EBFT.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12422" title="Abstract">arXiv:2402.12422</a> [<a href="/pdf/2402.12422" title="Download PDF">pdf</a>, <a href="/ps/2402.12422" title="Download PostScript">ps</a>, <a href="/format/2402.12422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulacra as Conscious Exotica
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shanahan%2C+M">Murray Shanahan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The advent of conversational agents with increasingly human-like behaviour
throws old philosophical questions into new light. Does it, or could it, ever
make sense to speak of AI agents built out of generative language models in
terms of consciousness, given that they are "mere" simulacra of human
behaviour, and that what they do can be seen as "merely" role play? Drawing on
the later writings of Wittgenstein, this paper attempts to tackle this question
while avoiding the pitfalls of dualistic thinking.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12423" title="Abstract">arXiv:2402.12423</a> [<a href="/pdf/2402.12423" title="Download PDF">pdf</a>, <a href="/format/2402.12423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassid%2C+M+V">Miri Varshavsky Hassid</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+R">Roy Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+R">Regev Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Golany%2C+T">Tomer Golany</a>, 
<a href="/search/cs?searchtype=author&query=Freedman%2C+D">Daniel Freedman</a>, 
<a href="/search/cs?searchtype=author&query=Rivlin%2C+E">Ehud Rivlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech
(TTS) domain is rising, providing great value in synthesizing high quality
speech. Although they exhibit impressive audio quality, the extent of their
semantic capabilities is unknown, and controlling their synthesized speech's
vocal properties remains a challenge. Inspired by recent advances in image
synthesis, we explore the latent space of frozen TTS models, which is composed
of the latent bottleneck activations of the DDM's denoiser. We identify that
this space contains rich semantic information, and outline several novel
methods for finding semantic directions within it, both supervised and
unsupervised. We then demonstrate how these enable off-the-shelf audio editing,
without any further training, architectural changes or data requirements. We
present evidence of the semantic and acoustic qualities of the edited audio,
and provide supplemental samples:
https://latent-analysis-grad-tts.github.io/speech-samples/.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12424" title="Abstract">arXiv:2402.12424</a> [<a href="/pdf/2402.12424" title="Download PDF">pdf</a>, <a href="/format/2402.12424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tables as Images? Exploring the Strengths and Limitations of LLMs on  Multimodal Representations of Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhenjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruiqi He</a>, 
<a href="/search/cs?searchtype=author&query=Sikka%2C+A">Aman Sikka</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we investigate the effectiveness of various LLMs in
interpreting tabular data through different prompting strategies and data
formats. Our analysis extends across six benchmarks for table-related tasks
such as question-answering and fact-checking. We introduce for the first time
the assessment of LLMs' performance on image-based table representations.
Specifically, we compare five text-based and three image-based table
representations, demonstrating the influence of representation and prompting on
LLM performance. Our study provides insights into the effective use of LLMs on
table-related tasks.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12426" title="Abstract">arXiv:2402.12426</a> [<a href="/pdf/2402.12426" title="Download PDF">pdf</a>, <a href="/ps/2402.12426" title="Download PostScript">ps</a>, <a href="/format/2402.12426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacks on Node Attributes in Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lanier%2C+M">Michael Lanier</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anindya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024 AICS workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Graphs are commonly used to model complex networks prevalent in modern social
media and literacy applications. Our research investigates the vulnerability of
these graphs through the application of feature based adversarial attacks,
focusing on both decision-time attacks and poisoning attacks. In contrast to
state-of-the-art models like Net Attack and Meta Attack, which target node
attributes and graph structure, our study specifically targets node attributes.
For our analysis, we utilized the text dataset Hellaswag and graph datasets
Cora and CiteSeer, providing a diverse basis for evaluation. Our findings
indicate that decision-time attacks using Projected Gradient Descent (PGD) are
more potent compared to poisoning attacks that employ Mean Node Embeddings and
Graph Contrastive Learning strategies. This provides insights for graph data
security, pinpointing where graph-based models are most vulnerable and thereby
informing the development of stronger defense mechanisms against such attacks.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12431" title="Abstract">arXiv:2402.12431</a> [<a href="/pdf/2402.12431" title="Download PDF">pdf</a>, <a href="/format/2402.12431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Fine-grained Distortions in Reports of Scientific Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=W%C3%BChrl%2C+A">Amelie W&#xfc;hrl</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+D">Dustin Wright</a>, 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Distorted science communication harms individuals and society as it can lead
to unhealthy behavior change and decrease trust in scientific institutions.
Given the rapidly increasing volume of science communication in recent years, a
fine-grained understanding of how findings from scientific publications are
reported to the general public, and methods to detect distortions from the
original work automatically, are crucial. Prior work focused on individual
aspects of distortions or worked with unpaired data. In this work, we make
three foundational contributions towards addressing this problem: (1)
annotating 1,600 instances of scientific findings from academic papers paired
with corresponding findings as reported in news articles and tweets wrt. four
characteristics: causality, certainty, generality and sensationalism; (2)
establishing baselines for automatically detecting these characteristics; and
(3) analyzing the prevalence of changes in these characteristics in both
human-annotated and large-scale unlabeled data. Our results show that
scientific findings frequently undergo subtle distortions when reported. Tweets
distort findings more often than science news reports. Detecting fine-grained
distortions automatically poses a challenging task. In our experiments,
fine-tuned task-specific models consistently outperform few-shot LLM prompting.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12451" title="Abstract">arXiv:2402.12451</a> [<a href="/pdf/2402.12451" title="Download PDF">pdf</a>, <a href="/format/2402.12451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The (R)Evolution of Multimodal Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caffagni%2C+D">Davide Caffagni</a>, 
<a href="/search/cs?searchtype=author&query=Cocchi%2C+F">Federico Cocchi</a>, 
<a href="/search/cs?searchtype=author&query=Barsellotti%2C+L">Luca Barsellotti</a>, 
<a href="/search/cs?searchtype=author&query=Moratelli%2C+N">Nicholas Moratelli</a>, 
<a href="/search/cs?searchtype=author&query=Sarto%2C+S">Sara Sarto</a>, 
<a href="/search/cs?searchtype=author&query=Baraldi%2C+L">Lorenzo Baraldi</a>, 
<a href="/search/cs?searchtype=author&query=Baraldi%2C+L">Lorenzo Baraldi</a>, 
<a href="/search/cs?searchtype=author&query=Cornia%2C+M">Marcella Cornia</a>, 
<a href="/search/cs?searchtype=author&query=Cucchiara%2C+R">Rita Cucchiara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
<p class="mathjax">Connecting text and visual modalities plays an essential role in generative
intelligence. For this reason, inspired by the success of large language
models, significant research efforts are being devoted to the development of
Multimodal Large Language Models (MLLMs). These models can seamlessly integrate
visual and textual modalities, both as input and output, while providing a
dialogue-based interface and instruction-following capabilities. In this paper,
we provide a comprehensive review of recent visual-based MLLMs, analyzing their
architectural choices, multimodal alignment strategies, and training
techniques. We also conduct a detailed analysis of these models across a wide
range of tasks, including visual grounding, image generation and editing,
visual understanding, and domain-specific applications. Additionally, we
compile and describe training datasets and evaluation benchmarks, conducting
comparisons among existing models in terms of performance and computational
requirements. Overall, this survey offers a comprehensive overview of the
current state of the art, laying the groundwork for future MLLMs.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12465" title="Abstract">arXiv:2402.12465</a> [<a href="/pdf/2402.12465" title="Download PDF">pdf</a>, <a href="/format/2402.12465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuro-mimetic Task-free Unsupervised Online Learning with Continual  Self-Organizing Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+H">Hitesh Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Desell%2C+T">Travis Desell</a>, 
<a href="/search/cs?searchtype=author&query=Mali%2C+A">Ankur Mali</a>, 
<a href="/search/cs?searchtype=author&query=Ororbia%2C+A">Alexander Ororbia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">An intelligent system capable of continual learning is one that can process
and extract knowledge from potentially infinitely long streams of pattern
vectors. The major challenge that makes crafting such a system difficult is
known as catastrophic forgetting - an agent, such as one based on artificial
neural networks (ANNs), struggles to retain previously acquired knowledge when
learning from new samples. Furthermore, ensuring that knowledge is preserved
for previous tasks becomes more challenging when input is not supplemented with
task boundary information. Although forgetting in the context of ANNs has been
studied extensively, there still exists far less work investigating it in terms
of unsupervised architectures such as the venerable self-organizing map (SOM),
a neural model often used in clustering and dimensionality reduction. While the
internal mechanisms of SOMs could, in principle, yield sparse representations
that improve memory retention, we observe that, when a fixed-size SOM processes
continuous data streams, it experiences concept drift. In light of this, we
propose a generalization of the SOM, the continual SOM (CSOM), which is capable
of online unsupervised learning under a low memory budget. Our results, on
benchmarks including MNIST, Kuzushiji-MNIST, and Fashion-MNIST, show almost a
two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art
result when tested on (online) unsupervised class incremental learning setting.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12467" title="Abstract">arXiv:2402.12467</a> [<a href="/pdf/2402.12467" title="Download PDF">pdf</a>, <a href="/ps/2402.12467" title="Download PostScript">ps</a>, <a href="/format/2402.12467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Discussion about Computational Challenges of Programmable Money in  Blockchain-based CBDCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Concei%C3%A7%C3%A3o%2C+A+F">Arlindo F. da Concei&#xe7;&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Vitenberg%2C+R">Roman Vitenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">This article discusses the implementation of programmable money on DLT-based
CBDCs. After briefly introducing what programmable money is, we enumerate some
initiatives worldwide and discuss the critical steps for implementation. We
look at the challenges from the Computer Science perspective. Four aspects were
analyzed: architectural design, security, scalability, and energy consumption.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12475" title="Abstract">arXiv:2402.12475</a> [<a href="/pdf/2402.12475" title="Download PDF">pdf</a>, <a href="/ps/2402.12475" title="Download PostScript">ps</a>, <a href="/format/2402.12475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffeomorphism Neural Operator for various domains and parameters of  partial differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+Z">Zhiwei Zhao</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Changqing Liu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yingguang Li</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Z">Zhibin Chen</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+X">Xu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages; 5 figures;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many science and engineering applications demand partial differential
equations (PDE) evaluations that are traditionally computed with
resource-intensive numerical solvers. Neural operator models provide an
efficient alternative by learning the governing physical laws directly from
data in a class of PDEs with different parameters, but constrained in a fixed
boundary (domain). Many applications, such as design and manufacturing, would
benefit from neural operators with flexible domains when studied at scale. Here
we present a diffeomorphism neural operator learning framework towards
developing domain-flexible models for physical systems with various and complex
domains. Specifically, a neural operator trained in a shared domain mapped from
various domains of fields by diffeomorphism is proposed, which transformed the
problem of learning function mappings in varying domains (spaces) into the
problem of learning operators on a shared diffeomorphic domain. Meanwhile, an
index is provided to evaluate the generalization of diffeomorphism neural
operators in different domains by the domain diffeomorphism similarity.
Experiments on statics scenarios (Darcy flow, mechanics) and dynamic scenarios
(pipe flow, airfoil flow) demonstrate the advantages of our approach for neural
operator learning under various domains, where harmonic and volume
parameterization are used as the diffeomorphism for 2D and 3D domains. Our
diffeomorphism neural operator approach enables strong learning capability and
robust generalization across varying domains and parameters.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12479" title="Abstract">arXiv:2402.12479</a> [<a href="/pdf/2402.12479" title="Download PDF">pdf</a>, <a href="/format/2402.12479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In deep reinforcement learning, a pruned network is a good network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+P+S">Pablo Samuel Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent work has shown that deep reinforcement learning agents have difficulty
in effectively using their network parameters. We leverage prior insights into
the advantages of sparse training techniques and demonstrate that gradual
magnitude pruning enables agents to maximize parameter effectiveness. This
results in networks that yield dramatic performance improvements over
traditional networks and exhibit a type of "scaling law", using only a small
fraction of the full network parameters.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12482" title="Abstract">arXiv:2402.12482</a> [<a href="/pdf/2402.12482" title="Download PDF">pdf</a>, <a href="/format/2402.12482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SECP: A Speech Enhancement-Based Curation Pipeline For Scalable  Acquisition Of Clean Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabra%2C+A">Adam Sabra</a>, 
<a href="/search/cs?searchtype=author&query=Wronka%2C+C">Cyprian Wronka</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+M">Michelle Mao</a>, 
<a href="/search/cs?searchtype=author&query=Hijazi%2C+S">Samer Hijazi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">As more speech technologies rely on a supervised deep learning approach with
clean speech as the ground truth, a methodology to onboard said speech at scale
is needed. However, this approach needs to minimize the dependency on human
listening and annotation, only requiring a human-in-the-loop when needed. In
this paper, we address this issue by outlining Speech Enhancement-based
Curation Pipeline (SECP) which serves as a framework to onboard clean speech.
This clean speech can then train a speech enhancement model, which can further
refine the original dataset and thus close the iterative loop. By running two
iterative rounds, we observe that enhanced output used as ground truth does not
degrade model performance according to $\Delta_{PESQ}$, a metric used in this
paper. We also show through comparative mean opinion score (CMOS) based
subjective tests that the highest and lowest bound of refined data is
perceptually better than the original data.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12483" title="Abstract">arXiv:2402.12483</a> [<a href="/pdf/2402.12483" title="Download PDF">pdf</a>, <a href="/format/2402.12483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions  Without the Question?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balepur%2C+N">Nishant Balepur</a>, 
<a href="/search/cs?searchtype=author&query=Ravichander%2C+A">Abhilasha Ravichander</a>, 
<a href="/search/cs?searchtype=author&query=Rudinger%2C+R">Rachel Rudinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In-progress preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multiple-choice question answering (MCQA) is often used to evaluate large
language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if
LLMs can perform MCQA with choices-only prompts, where models must select the
correct answer only from the choices. In three MCQA datasets and four LLMs,
this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy
gain. To help explain this behavior, we conduct an in-depth, black-box analysis
on memorization, choice dynamics, and question inference. Our key findings are
threefold. First, we find no evidence that the choices-only accuracy stems from
memorization alone. Second, priors over individual choices do not fully explain
choices-only accuracy, hinting that LLMs use the group dynamics of choices.
Third, LLMs have some ability to infer a relevant question from choices, and
surprisingly can sometimes even match the original question. We hope to
motivate the use of stronger baselines in MCQA benchmarks, the design of robust
MCQA datasets, and further efforts to explain LLM decision-making.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12484" title="Abstract">arXiv:2402.12484</a> [<a href="/pdf/2402.12484" title="Download PDF">pdf</a>, <a href="/format/2402.12484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Bit Complexity of Iterated Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toyos-Marfurt%2C+G">Guillermo Toyos-Marfurt</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+P">Petr Kuznetsov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures. To be published in 31st International Colloquium On Structural Information and Communication Complexity (SIROCCO 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Computability, in the presence of asynchrony and failures, is one of the
central questions in distributed computing. The celebrated asynchronous
computability theorem (ACT) charaterizes the computing power of the read-write
shared-memory model through the geometric properties of its protocol complex: a
combinatorial structure describing the states the model can reach via its
finite executions. This characterization assumes that the memory is of
unbounded capacity, in particular, it is able to store the exponentially
growing states of the full-information protocol.
<br />In this paper, we tackle an orthogonal question: what is the minimal memory
capacity that allows us to simulate a given number of rounds of the
full-information protocol? In the iterated immediate snapshot model (IIS), we
determine necessary and sufficient conditions on the number of bits an IIS
element should be able to store so that the resulting protocol is equivalent,
up to isomorphism, to the full-information protocol. Our characterization
implies that $n\geq 3$ processes can simulate $r$ rounds of the
full-information IIS protocol as long as the bit complexity per process is
within $\Omega(r n)$ and $O(r n \log n)$. Two processes, however, can simulate
any number of rounds of the full-information protocol using only $2$ bits per
process, which implies, in particular, that just $2$ bits per process are
sufficient to solve $\varepsilon$-agreement for arbitrarily small
$\varepsilon$.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12486" title="Abstract">arXiv:2402.12486</a> [<a href="/pdf/2402.12486" title="Download PDF">pdf</a>, <a href="/format/2402.12486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Pre-Trained Language Models Detect and Understand Semantic  Underspecification? Ask the DUST!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wildenburg%2C+F">Frank Wildenburg</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+M">Michael Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In everyday language use, speakers frequently utter and interpret sentences
that are semantically underspecified, namely, whose content is insufficient to
fully convey their message or interpret them univocally. For example, to
interpret the underspecified sentence "Don't spend too much", which leaves
implicit what (not) to spend, additional linguistic context or outside
knowledge is needed. In this work, we propose a novel Dataset of semantically
Underspecified Sentences grouped by Type (DUST) and use it to study whether
pre-trained language models (LMs) correctly identify and interpret
underspecified sentences. We find that newer LMs are reasonably able to
identify underspecified sentences when explicitly prompted. However,
interpreting them correctly is much harder for any LMs. Our experiments show
that when interpreting underspecified sentences, LMs exhibit little
uncertainty, contrary to what theoretical accounts of underspecification would
predict. Overall, our study reveals limitations in current models' processing
of sentence semantics and highlights the importance of using naturalistic data
and communicative scenarios when evaluating LMs' language capabilities.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12488" title="Abstract">arXiv:2402.12488</a> [<a href="/pdf/2402.12488" title="Download PDF">pdf</a>, <a href="/format/2402.12488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Control Design for Unlocking the Energy Flexibility of  Heat Pump and Thermal Energy Storage Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+W">Weihong Tang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/eess?searchtype=author&query=Walker%2C+S">Shalika Walker</a>, 
<a href="/search/eess?searchtype=author&query=Keviczky%2C+T">Tamas Keviczky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to The 8th IEEE Conference on Control Technology and Applications (CCTA) 2024, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Heat pump and thermal energy storage (HPTES) systems, which are widely
utilized in modern buildings for providing domestic hot water, contribute to a
large share of household electricity consumption. With the increasing
integration of renewable energy sources (RES) into modern power grids,
demand-side management (DSM) becomes crucial for balancing power generation and
consumption by adjusting end users' power consumption. This paper explores an
energy flexible Model Predictive Control (MPC) design for a class of HPTES
systems to facilitate demand-side management. The proposed DSM strategy
comprises two key components: i) flexibility assessment, and ii) flexibility
exploitation. Firstly, for flexibility assessment, a tailored MPC formulation,
supplemented by a set of auxiliary linear constraints, is developed to
quantitatively assess the flexibility potential inherent in HPTES systems.
Subsequently, in flexibility exploitation, the energy flexibility is
effectively harnessed in response to feasible demand response (DR) requests,
which can be formulated as a standard mixed-integer MPC problem. Numerical
experiments, based on a real-world HPTES installation, are conducted to
demonstrate the efficacy of the proposed design.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12490" title="Abstract">arXiv:2402.12490</a> [<a href="/pdf/2402.12490" title="Download PDF">pdf</a>, <a href="/format/2402.12490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Cross-Domain Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Carvalho%2C+M">Marcus de Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Pratama%2C+M">Mahardhika Pratama</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Haoyan%2C+C">Chua Haoyan</a>, 
<a href="/search/cs?searchtype=author&query=Yapp%2C+E">Edward Yapp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 Figures, 4 Tables. To be published at the IEEE International Conference on Data Engineering (ICDE) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Continual learning is a process that involves training learning agents to
sequentially master a stream of tasks or classes without revisiting past data.
The challenge lies in leveraging previously acquired knowledge to learn new
tasks efficiently, while avoiding catastrophic forgetting. Existing methods
primarily focus on single domains, restricting their applicability to specific
problems.
<br />In this work, we introduce a novel approach called Cross-Domain Continual
Learning (CDCL) that addresses the limitations of being limited to single
supervised domains. Our method combines inter- and intra-task cross-attention
mechanisms within a compact convolutional network. This integration enables the
model to maintain alignment with features from previous tasks, thereby delaying
the data drift that may occur between tasks, while performing unsupervised
cross-domain (UDA) between related domains. By leveraging an
intra-task-specific pseudo-labeling method, we ensure accurate input pairs for
both labeled and unlabeled samples, enhancing the learning process. To validate
our approach, we conduct extensive experiments on public UDA datasets,
showcasing its positive performance on cross-domain continual learning
challenges. Additionally, our work introduces incremental ideas that contribute
to the advancement of this field.
<br />We make our code and models available to encourage further exploration and
reproduction of our results: \url{https://github.com/Ivsucram/CDCL}
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12498" title="Abstract">arXiv:2402.12498</a> [<a href="/pdf/2402.12498" title="Download PDF">pdf</a>, <a href="/format/2402.12498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feudal Networks for Visual Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+F">Faith Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+B+B">Bryan Bo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dana%2C+K">Kristin Dana</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Ashwin Ashok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Visual navigation follows the intuition that humans can navigate without
detailed maps. A common approach is interactive exploration while building a
topological graph with images at nodes that can be used for planning. Recent
variations learn from passive videos and can navigate using complex social and
semantic cues. However, a significant number of training videos are needed,
large graphs are utilized, and scenes are not unseen since odometry is
utilized. We introduce a new approach to visual navigation using feudal
learning, which employs a hierarchical structure consisting of a worker agent,
a mid-level manager, and a high-level manager. Key to the feudal learning
paradigm, agents at each level see a different aspect of the task and operate
at different spatial and temporal scales. Two unique modules are developed in
this framework. For the high- level manager, we learn a memory proxy map in a
self supervised manner to record prior observations in a learned latent space
and avoid the use of graphs and odometry. For the mid-level manager, we develop
a waypoint network that outputs intermediate subgoals imitating human waypoint
selection during local navigation. This waypoint network is pre-trained using a
new, small set of teleoperation videos that we make publicly available, with
training environments different from testing environments. The resulting feudal
navigation network achieves near SOTA performance, while providing a novel
no-RL, no-graph, no-odometry, no-metric map approach to the image goal
navigation task.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12499" title="Abstract">arXiv:2402.12499</a> [<a href="/pdf/2402.12499" title="Download PDF">pdf</a>, <a href="/format/2402.12499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Security Response through Online Learning with Adaptive  Conjectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammar%2C+K">Kim Hammar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Stadler%2C+R">Rolf Stadler</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">We study automated security response for an IT infrastructure and formulate
the interaction between an attacker and a defender as a partially observed,
non-stationary game. We relax the standard assumption that the game model is
correctly specified and consider that each player has a probabilistic
conjecture about the model, which may be misspecified in the sense that the
true model has probability 0. This formulation allows us to capture uncertainty
about the infrastructure and the intents of the players. To learn effective
game strategies online, we design a novel method where a player iteratively
adapts its conjecture using Bayesian learning and updates its strategy through
rollout. We prove that the conjectures converge to best fits, and we provide a
bound on the performance improvement that rollout enables with a conjectured
model. To characterize the steady state of the game, we propose a variant of
the Berk-Nash equilibrium. We present our method through an advanced persistent
threat use case. Simulation studies based on testbed measurements show that our
method produces effective security strategies that adapt to a changing
environment. We also find that our method enables faster convergence than
current reinforcement learning techniques.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12500" title="Abstract">arXiv:2402.12500</a> [<a href="/pdf/2402.12500" title="Download PDF">pdf</a>, <a href="/format/2402.12500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating kNN with Foundation Models for Adaptable and Privacy-Aware  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doerrich%2C+S">Sebastian Doerrich</a>, 
<a href="/search/cs?searchtype=author&query=Archut%2C+T">Tobias Archut</a>, 
<a href="/search/cs?searchtype=author&query=Di+Salvo%2C+F">Francesco Di Salvo</a>, 
<a href="/search/cs?searchtype=author&query=Ledig%2C+C">Christian Ledig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 21st IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Traditional deep learning models implicity encode knowledge limiting their
transparency and ability to adapt to data changes. Yet, this adaptability is
vital for addressing user data privacy concerns. We address this limitation by
storing embeddings of the underlying training data independently of the model
weights, enabling dynamic data modifications without retraining. Specifically,
our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a
vision-based foundation model, pre-trained self-supervised on natural images,
enhancing interpretability and adaptability. We share open-source
implementations of a previously unpublished baseline method as well as our
performance-improving contributions. Quantitative experiments confirm improved
classification across established benchmark datasets and the method's
applicability to distinct medical image classification tasks. Additionally, we
assess the method's robustness in continual learning and data removal
scenarios. The approach exhibits great promise for bridging the gap between
foundation models' performance and challenges tied to data privacy. The source
code is available at
https://github.com/TobArc/privacy-aware-image-classification-with-kNN.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12501" title="Abstract">arXiv:2402.12501</a> [<a href="/pdf/2402.12501" title="Download PDF">pdf</a>, <a href="/format/2402.12501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Your Vision-Language Model Itself Is a Strong Filter: Towards  High-Quality Instruction Tuning with Data Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruibo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qi He</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+T">Tianyi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junfeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Data selection in instruction tuning emerges as a pivotal process for
acquiring high-quality data and training instruction-following large language
models (LLMs), but it is still a new and unexplored research area for
vision-language models (VLMs). Existing data selection approaches on LLMs
either rely on single unreliable scores, or use downstream tasks for selection,
which is time-consuming and can lead to potential over-fitting on the chosen
evaluation datasets. To address this challenge, we introduce a novel dataset
selection method, Self-Filter, that utilizes the VLM itself as a filter. This
approach is inspired by the observation that VLMs benefit from training with
the most challenging instructions. Self-Filter operates in two stages. In the
first stage, we devise a scoring network to evaluate the difficulty of training
instructions, which is co-trained with the VLM. In the second stage, we use the
trained score net to measure the difficulty of each instruction, select the
most challenging samples, and penalize similar samples to encourage diversity.
Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can
reach better results compared to full data settings with merely about 15%
samples, and can achieve superior performance against competitive baselines.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12503" title="Abstract">arXiv:2402.12503</a> [<a href="/pdf/2402.12503" title="Download PDF">pdf</a>, <a href="/format/2402.12503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PARCv2: Physics-aware Recurrent Convolutional Neural Networks for  Spatiotemporal Dynamics Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P+C+H">Phong C.H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinlun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Arfaza%2C+S">Shahab Arfaza</a>, 
<a href="/search/cs?searchtype=author&query=Seshadri%2C+P">Pradeep Seshadri</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Y+T">Yen T. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Munho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sanghun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Udaykumar%2C+H+S">H.S. Udaykumar</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S">Stephen Baek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modeling unsteady, fast transient, and advection-dominated physics problems
is a pressing challenge for physics-aware deep learning (PADL). The physics of
complex systems is governed by large systems of partial differential equations
(PDEs) and ancillary constitutive models with nonlinear structures, as well as
evolving state fields exhibiting sharp gradients and rapidly deforming material
interfaces. Here, we investigate an inductive bias approach that is versatile
and generalizable to model generic nonlinear field evolution problems. Our
study focuses on the recent physics-aware recurrent convolutions (PARC), which
incorporates a differentiator-integrator architecture that inductively models
the spatiotemporal dynamics of generic physical systems. We extend the
capabilities of PARC to simulate unsteady, transient, and advection-dominant
systems. The extended model, referred to as PARCv2, is equipped with
differential operators to model advection-reaction-diffusion equations, as well
as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested
on both standard benchmark problems in fluid dynamics, namely Burgers and
Navier-Stokes equations, and then applied to more complex shock-induced
reaction problems in energetic materials. We evaluate the behavior of PARCv2 in
comparison to other physics-informed and learning bias models and demonstrate
its potential to model unsteady and advection-dominant dynamics regimes.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12505" title="Abstract">arXiv:2402.12505</a> [<a href="/pdf/2402.12505" title="Download PDF">pdf</a>, <a href="/format/2402.12505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Situating Data Sets: Making Public Data Actionable for Housing Justice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+A">Anh-Ton Tran</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+G">Grace Guo</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">Jordan Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+K">Katsuki Chan</a>, 
<a href="/search/cs?searchtype=author&query=Raymond%2C+E">Elora Raymond</a>, 
<a href="/search/cs?searchtype=author&query=DiSalvo%2C+C">Carl DiSalvo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages including references, 4 figures, 1 table, ACM CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Activists, governmentsm and academics regularly advocate for more open data.
But how is data made open, and for whom is it made useful and usable? In this
paper, we investigate and describe the work of making eviction data open to
tenant organizers. We do this through an ethnographic description of ongoing
work with a local housing activist organization. This work combines
observation, direct participation in data work, and creating media artifacts,
specifically digital maps. Our interpretation is grounded in D'Ignazio and
Klein's Data Feminism, emphasizing standpoint theory. Through our analysis and
discussion, we highlight how shifting positionalities from data intermediaries
to data accomplices affects the design of data sets and maps. We provide HCI
scholars with three design implications when situating data for grassroots
organizers: becoming a domain beginner, striving for data actionability, and
evaluating our design artifacts by the social relations they sustain rather
than just their technical efficacy.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12508" title="Abstract">arXiv:2402.12508</a> [<a href="/pdf/2402.12508" title="Download PDF">pdf</a>, <a href="/format/2402.12508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDEs for Minimax Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Compagnoni%2C+E+M">Enea Monzio Compagnoni</a>, 
<a href="/search/cs?searchtype=author&query=Orvieto%2C+A">Antonio Orvieto</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+H">Hans Kersting</a>, 
<a href="/search/cs?searchtype=author&query=Proske%2C+F+N">Frank Norbert Proske</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AISTATS 2024 (Poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Minimax optimization problems have attracted a lot of attention over the past
few years, with applications ranging from economics to machine learning. While
advanced optimization methods exist for such problems, characterizing their
dynamics in stochastic scenarios remains notably challenging. In this paper, we
pioneer the use of stochastic differential equations (SDEs) to analyze and
compare Minimax optimizers. Our SDE models for Stochastic Gradient
Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient
Descent are provable approximations of their algorithmic counterparts, clearly
showcasing the interplay between hyperparameters, implicit regularization, and
implicit curvature-induced noise. This perspective also allows for a unified
and simplified analysis strategy based on the principles of It\^o calculus.
Finally, our approach facilitates the derivation of convergence conditions and
closed-form solutions for the dynamics in simplified settings, unveiling
further insights into the behavior of different optimizers.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12509" title="Abstract">arXiv:2402.12509</a> [<a href="/pdf/2402.12509" title="Download PDF">pdf</a>, <a href="/format/2402.12509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk Through It: End User Directed Manipulation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winge%2C+C">Carl Winge</a>, 
<a href="/search/cs?searchtype=author&query=Imdieke%2C+A">Adam Imdieke</a>, 
<a href="/search/cs?searchtype=author&query=Aldeeb%2C+B">Bahaa Aldeeb</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Desingh%2C+K">Karthik Desingh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Training generalist robot agents is an immensely difficult feat due to the
requirement to perform a huge range of tasks in many different environments. We
propose selectively training robots based on end-user preferences instead.
Given a factory model that lets an end user instruct a robot to perform
lower-level actions (e.g. 'Move left'), we show that end users can collect
demonstrations using language to train their home model for higher-level tasks
specific to their needs (e.g. 'Open the top drawer and put the block inside').
We demonstrate this hierarchical robot learning framework on robot manipulation
tasks using RLBench environments. Our method results in a 16% improvement in
skill success rates compared to a baseline method. In further experiments, we
explore the use of the large vision-language model (VLM), Bard, to
automatically break down tasks into sequences of lower-level instructions,
aiming to bypass end-user involvement. The VLM is unable to break tasks down to
our lowest level, but does achieve good results breaking high-level tasks into
mid-level skills. We have a supplemental video and additional results at
talk-through-it.github.io.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12510" title="Abstract">arXiv:2402.12510</a> [<a href="/pdf/2402.12510" title="Download PDF">pdf</a>, <a href="/ps/2402.12510" title="Download PostScript">ps</a>, <a href="/format/2402.12510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Function Class Learning with Genetic Programming: Towards Explainable  Meta Learning for Tumor Growth Functionals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sijben%2C+E+M+C">E.M.C. Sijben</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+J+C">J.C. Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Bosman%2C+P+A+N">P.A.N. Bosman</a>, 
<a href="/search/cs?searchtype=author&query=Alderliesten%2C+T">T. Alderliesten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Paragangliomas are rare, primarily slow-growing tumors for which the
underlying growth pattern is unknown. Therefore, determining the best care for
a patient is hard. Currently, if no significant tumor growth is observed,
treatment is often delayed, as treatment itself is not without risk. However,
by doing so, the risk of (irreversible) adverse effects due to tumor growth may
increase. Being able to predict the growth accurately could assist in
determining whether a patient will need treatment during their lifetime and, if
so, the timing of this treatment. The aim of this work is to learn the general
underlying growth pattern of paragangliomas from multiple tumor growth data
sets, in which each data set contains a tumor's volume over time. To do so, we
propose a novel approach based on genetic programming to learn a function
class, i.e., a parameterized function that can be fit anew for each tumor. We
do so in a unique, multi-modal, multi-objective fashion to find multiple
potentially in- teresting function classes in a single run. We evaluate our
approach on a synthetic and a real-world data set. By analyzing the resulting
function classes, we can effectively explain the general patterns in the data.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12512" title="Abstract">arXiv:2402.12512</a> [<a href="/pdf/2402.12512" title="Download PDF">pdf</a>, <a href="/format/2402.12512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Input Constrained Control Barrier Functions for Guaranteed  Safety of Car-Like Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%BCggemann%2C+S">Sven Br&#xfc;ggemann</a>, 
<a href="/search/cs?searchtype=author&query=Nightingale%2C+D">Dominic Nightingale</a>, 
<a href="/search/cs?searchtype=author&query=Silberman%2C+J">Jack Silberman</a>, 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+M">Maur&#xed;cio de Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We propose a design method for a robust safety filter based on Input
Constrained Control Barrier Functions (ICCBF) for car-like robots moving in
complex environments. A robust ICCBF that can be efficiently implemented is
obtained by learning a smooth function of the environment using Support Vector
Machine regression. The method takes into account steering constraints and is
validated in simulation and a real experiment.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12513" title="Abstract">arXiv:2402.12513</a> [<a href="/pdf/2402.12513" title="Download PDF">pdf</a>, <a href="/format/2402.12513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Induced Model Matching: How Restricted Models Can Help Larger Ones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muneeb%2C+U">Usama Muneeb</a>, 
<a href="/search/cs?searchtype=author&query=Ohannessian%2C+M+I">Mesrob I. Ohannessian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We consider scenarios where a very accurate predictive model using restricted
features is available at the time of training of a larger, full-featured,
model. This restricted model may be thought of as "side-information", derived
either from an auxiliary exhaustive dataset or on the same dataset, by forcing
the restriction. How can the restricted model be useful to the full model? We
propose an approach for transferring the knowledge of the restricted model to
the full model, by aligning the full model's context-restricted performance
with that of the restricted model's. We call this methodology Induced Model
Matching (IMM) and first illustrate its general applicability by using logistic
regression as a toy example. We then explore IMM's use in language modeling,
the application that initially inspired it, and where it offers an explicit
foundation in contrast to the implicit use of restricted models in techniques
such as noising. We demonstrate the methodology on both LSTM and transformer
full models, using $N$-grams as restricted models. To further illustrate the
potential of the principle whenever it is much cheaper to collect restricted
rather than full information, we conclude with a simple RL example where POMDP
policies can improve learned MDP policies via IMM.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12514" title="Abstract">arXiv:2402.12514</a> [<a href="/pdf/2402.12514" title="Download PDF">pdf</a>, <a href="/ps/2402.12514" title="Download PostScript">ps</a>, <a href="/format/2402.12514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assume-guarantee contract algebras are bounded Sugihara monoids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+J+L">Jose Luis Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Ertola-Biraben%2C+R">Rodolfo Ertola-Biraben</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, no figures. Submitted article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Rings and Algebras (math.RA)

</div>
<p class="mathjax">In [Incer Romeo, I. X., \textit{The Algebra of Contracts}. Ph.D. Thesis, UC
Berkeley (2022)] an algebraic perspective on assume-guarantee contracts is
proposed. This proposal relies heavily on a construction involving Boolean
algebras. However, the structures thus proposed lack a clearly prescribed set
of basic operations, necessary if we want to see them as a class of algebras
(in the sense of Universal Algebra). In this article, by prescribing a suitable
set of basic operations on contracts, we manage to describe these algebras as
(a generating set of members of) well-known varieties.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12517" title="Abstract">arXiv:2402.12517</a> [<a href="/pdf/2402.12517" title="Download PDF">pdf</a>, <a href="/format/2402.12517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified-Field Monolithic Fictitious Domain-Finite Element Method for  Fluid-Structure-Contact Interactions and Applications to Deterministic  Lateral Displacement Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+P">Pengtao Sun</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yumiao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jinchao Xu</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yan Chen</a>, 
<a href="/search/math?searchtype=author&query=Han%2C+J">Jiarui Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 42 figures, 5 tables, 66 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Based upon two overlapped, body-unfitted meshes, a type of unified-field
monolithic fictitious domain-finite element method (UFMFD-FEM) is developed in
this paper for moving interface problems of dynamic fluid-structure
interactions (FSI) accompanying with high-contrast physical coefficients across
the interface and contacting collisions between the structure and fluidic
channel wall when the structure is immersed in the fluid. In particular, the
proposed novel numerical method consists of a monolithic, stabilized mixed
finite element method within the frame of fictitious domain/immersed boundary
method (IBM) for generic fluid-structure-contact interaction (FSCI) problems in
the Eulerian--updated Lagrangian description, while involving the no-slip type
of interface conditions on the fluid-structure interface, and the repulsive
contact force on the structural surface when the immersed structure contacts
the fluidic channel wall. The developed UFMFD-FEM for FSI or FSCI problems can
deal with the structural motion with large rotational and translational
displacements and/or large deformation in an accurate and efficient fashion,
which are first validated by two benchmark FSI problems and one FSCI model
problem, then by experimental results of a realistic FSCI scenario -- the
microfluidic deterministic lateral displacement (DLD) problem that is applied
to isolate circulating tumor cells (CTCs) from blood cells in the blood fluid
through a cascaded filter DLD microchip in practice, where a particulate fluid
with the pillar obstacles effect in the fluidic channel, i.e., the effects of
fluid-structure interaction and structure collision, play significant roles to
sort particles (cells) of different sizes with tilted pillar arrays.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12518" title="Abstract">arXiv:2402.12518</a> [<a href="/pdf/2402.12518" title="Download PDF">pdf</a>, <a href="/format/2402.12518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Process Neural Additive Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Barr%2C+B">Brian Barr</a>, 
<a href="/search/cs?searchtype=author&query=Paisley%2C+J">John Paisley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks have revolutionized many fields, but their black-box
nature also occasionally prevents their wider adoption in fields such as
healthcare and finance, where interpretable and explainable models are
required. The recent development of Neural Additive Models (NAMs) is a
significant step in the direction of interpretable deep learning for tabular
datasets. In this paper, we propose a new subclass of NAMs that use a
single-layer neural network construction of the Gaussian process via random
Fourier features, which we call Gaussian Process Neural Additive Models
(GP-NAM). GP-NAMs have the advantage of a convex objective function and number
of trainable parameters that grows linearly with feature dimensionality. It
suffers no loss in performance compared to deeper NAM approaches because GPs
are well-suited for learning complex non-parametric univariate functions. We
demonstrate the performance of GP-NAM on several tabular datasets, showing that
it achieves comparable or better performance in both classification and
regression tasks with a large reduction in the number of parameters.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12519" title="Abstract">arXiv:2402.12519</a> [<a href="/pdf/2402.12519" title="Download PDF">pdf</a>, <a href="/format/2402.12519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System Identification of Neural Systems: Going Beyond Images to  Modelling Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gamal%2C+M">Mai Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Rashad%2C+M">Mohamed Rashad</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+E">Eman Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Eldawlatly%2C+S">Seif Eldawlatly</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vast literature has compared the recordings of biological neurons in the
brain to deep neural networks. The ultimate goal is to interpret deep networks
or to better understand and encode biological neural systems. Recently, there
has been a debate on whether system identification is possible and how much it
can tell us about the brain computation. System identification recognizes
whether one model is more valid to represent the brain computation over
another. Nonetheless, previous work did not consider the time aspect and how
video and dynamics (e.g., motion) modelling in deep networks relate to these
biological neural systems within a large-scale comparison. Towards this end, we
propose a system identification study focused on comparing single image vs.
video understanding models with respect to the visual cortex recordings. Our
study encompasses two sets of experiments; a real environment setup and a
simulated environment setup. The study also encompasses more than 30 models
and, unlike prior works, we focus on convolutional vs. transformer-based,
single vs. two-stream, and fully vs. self-supervised video understanding
models. The goal is to capture a greater variety of architectures that model
dynamics. As such, this signifies the first large-scale study of video
understanding models from a neuroscience perspective. Our results in the
simulated experiments, show that system identification can be attained to a
certain level in differentiating image vs. video understanding models.
Moreover, we provide key insights on how video understanding models predict
visual cortex responses; showing video understanding better than image
understanding models, convolutional models are better in the early-mid regions
than transformer based except for multiscale transformers that are still good
in predicting these regions, and that two-stream models are better than single
stream.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12522" title="Abstract">arXiv:2402.12522</a> [<a href="/pdf/2402.12522" title="Download PDF">pdf</a>, <a href="/format/2402.12522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An evaluation of Deep Learning based stereo dense matching dataset shift  from aerial images and a large scale stereo dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Teng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vallet%2C+B">Bruno Vallet</a>, 
<a href="/search/cs?searchtype=author&query=Pierrot-Deseilligny%2C+M">Marc Pierrot-Deseilligny</a>, 
<a href="/search/cs?searchtype=author&query=Rupnik%2C+E">Ewelina Rupnik</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Applied Earth Observation and
  Geoinformation, 128(2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dense matching is crucial for 3D scene reconstruction since it enables the
recovery of scene 3D geometry from image acquisition. Deep Learning (DL)-based
methods have shown effectiveness in the special case of epipolar stereo
disparity estimation in the computer vision community. DL-based methods depend
heavily on the quality and quantity of training datasets. However, generating
ground-truth disparity maps for real scenes remains a challenging task in the
photogrammetry community. To address this challenge, we propose a method for
generating ground-truth disparity maps directly from Light Detection and
Ranging (LiDAR) and images to produce a large and diverse dataset for six
aerial datasets across four different areas and two areas with different
resolution images. We also introduce a LiDAR-to-image co-registration
refinement to the framework that takes special precautions regarding occlusions
and refrains from disparity interpolation to avoid precision loss. Evaluating
11 dense matching methods across datasets with diverse scene types, image
resolutions, and geometric configurations, which are deeply investigated in
dataset shift, GANet performs best with identical training and testing data,
and PSMNet shows robustness across different datasets, and we proposed the best
strategy for training with a limit dataset. We will also provide the dataset
and training models; more information can be found at
https://github.com/whuwuteng/Aerial_Stereo_Dataset.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12525" title="Abstract">arXiv:2402.12525</a> [<a href="/pdf/2402.12525" title="Download PDF">pdf</a>, <a href="/format/2402.12525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LangXAI: Integrating Large Vision Models for Generating Textual  Explanations to Enhance Explainability in Visual Perception Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T+H">Truong Thanh Hung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Clement%2C+T">Tobias Clement</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P+T+L">Phuc Truong Loc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kemmerzell%2C+N">Nils Kemmerzell</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+V+B">Van Binh Truong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+T+K">Vo Thanh Khang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Abdelaal%2C+M">Mohamed Abdelaal</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hung Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">LangXAI is a framework that integrates Explainable Artificial Intelligence
(XAI) with advanced vision models to generate textual explanations for visual
recognition tasks. Despite XAI advancements, an understanding gap persists for
end-users with limited domain knowledge in artificial intelligence and computer
vision. LangXAI addresses this by furnishing text-based explanations for
classification, object detection, and semantic segmentation model outputs to
end-users. Preliminary results demonstrate LangXAI's enhanced plausibility,
with high BERTScore across tasks, fostering a more transparent and reliable AI
framework on vision tasks for end-users.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12526" title="Abstract">arXiv:2402.12526</a> [<a href="/pdf/2402.12526" title="Download PDF">pdf</a>, <a href="/ps/2402.12526" title="Download PostScript">ps</a>, <a href="/format/2402.12526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimize Energy Consumption of Wireless Sensor Networks by using  modified Ant Colony Optimization ACO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razooqi%2C+Y+S">Yasameen Sajid Razooqi</a>, 
<a href="/search/cs?searchtype=author&query=Al-Asfoor%2C+M">Muntasir Al-Asfoor</a>, 
<a href="/search/cs?searchtype=author&query=Abed%2C+M+H">Mohammed Hamzah Abed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Routing represents a pivotal concern in the context of Wireless Sensor
Networks (WSN) owing to its divergence from traditional network routing
paradigms. The inherent dynamism of the WSN environment, coupled with the
scarcity of available resources, engenders considerable challenges for industry
and academia alike in devising efficient routing strategies. Addressing these
challenges, a viable recourse lies in applying heuristic search methodologies
to ascertain the most optimal path in WSNs. Ant Colony Optimization (ACO) is a
well-established heuristic algorithm that has demonstrated notable advancements
in routing contexts. This paper introduces a modify routing protocols based on
Ant colony optimization. In these protocols, we incorporate the inverse of the
distance between nodes and their neighbours in the probability equations of ACO
along with considering pheromone levels and residual energy. These formulation
modifications facilitate the selection of the most suitable candidate for the
subsequent hop, effectively minimizing the average energy consumption across
all nodes in each iteration. Furthermore, in this protocol, we iteratively
fine-tune ACO's parameter values based on the outcomes of several experimental
trials. The experimental analysis is conducted through a diverse set of network
topologies, and the results are subjected to comparison against
well-established ACO algorithm and routing protocols. The efficacy of the
proposed protocol is assessed based on various performance metrics,
encompassing throughput, energy consumption, network lifetime, energy
consumption, the extent of data transferred over the network, and the length of
paths traversed by packets. These metrics collectively provide a comprehensive
evaluation of the performance attainments of the routing protocols.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12527" title="Abstract">arXiv:2402.12527</a> [<a href="/pdf/2402.12527" title="Download PDF">pdf</a>, <a href="/format/2402.12527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sims%2C+A">Anya Sims</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Teh%2C+Y+W">Yee Whye Teh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code open-sourced at: <a href="https://github.com/anyasims/edge-of-reach">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning aims to enable agents to be trained from
pre-collected datasets, however, this comes with the added challenge of
estimating the value of behavior not covered in the dataset. Model-based
methods offer a solution by allowing agents to collect additional synthetic
data via rollouts in a learned dynamics model. The prevailing theoretical
understanding is that this can then be viewed as online reinforcement learning
in an approximate dynamics model, and any remaining gap is therefore assumed to
be due to the imperfect dynamics model. Surprisingly, however, we find that if
the learned dynamics model is replaced by the true error-free dynamics,
existing model-based methods completely fail. This reveals a major
misconception. Our subsequent investigation finds that the general procedure
used in model-based algorithms results in the existence of a set of
edge-of-reach states which trigger pathological value overestimation and
collapse in Bellman-based algorithms. We term this the edge-of-reach problem.
Based on this, we fill some gaps in existing theory and also explain how prior
model-based methods are inadvertently addressing the true underlying
edge-of-reach problem. Finally, we propose Reach-Aware Value Learning (RAVL), a
simple and robust method that directly addresses the edge-of-reach problem and
achieves strong performance across both proprioceptive and pixel-based
benchmarks. Code open-sourced at: https://github.com/anyasims/edge-of-reach.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12530" title="Abstract">arXiv:2402.12530</a> [<a href="/pdf/2402.12530" title="Download PDF">pdf</a>, <a href="/format/2402.12530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Structures in Pre-training Data Yield In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+K">Kathleen McKeown</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pre-trained language models (LMs) are capable of in-context learning (ICL):
they can adapt to a task with only a few examples given in the prompt without
any parameter update. However, it is unclear where this capability comes from
as there is a stark distribution shift between pre-training text and ICL
prompts. In this work, we study what patterns of the pre-training data
contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel
structures}$ in the pre-training data -- pairs of phrases following similar
templates in the same context window. Specifically, we detect parallel
structures by checking whether training on one phrase improves prediction of
the other, and conduct ablation experiments to study their effect on ICL. We
show that removing parallel structures in the pre-training data reduces LMs'
ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when
excluding common patterns such as n-gram repetitions and long-range dependency,
showing the diversity and generality of parallel structures. A closer look at
the detected parallel structures indicates that they cover diverse linguistic
tasks and span long distances in the data.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12531" title="Abstract">arXiv:2402.12531</a> [<a href="/pdf/2402.12531" title="Download PDF">pdf</a>, <a href="/format/2402.12531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Deep Generative Models on Many-To-One Image-to-Image  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saxena%2C+S">Sagar Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Teli%2C+M+N">Mohammad Nayeem Teli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep generative models have been applied to multiple applications in image-
to-image translation. Generative Adversarial Networks and Diffusion Models have
presented impressive results, setting new state-of-the-art results on these
tasks. Most methods have symmetric setups across the different domains in a
dataset. These methods assume that all domains have either multiple modalities
or only one modality. However, there are many datasets that have a many-to-one
relationship between two domains. In this work, we first introduce a Colorized
MNIST dataset and a Color-Recall score that can provide a simple benchmark for
evaluating models on many-to-one translation. We then introduce a new
asymmetric framework to improve existing deep generative models on many-to-one
image-to- image translation. We apply this framework to StarGAN V2 and show
that in both unsupervised and semi-supervised settings, the performance of this
new model improves on many-to-one image-to-image translation.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12532" title="Abstract">arXiv:2402.12532</a> [<a href="/pdf/2402.12532" title="Download PDF">pdf</a>, <a href="/format/2402.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Human-Machine Point Cloud Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ulhaq%2C+M">Mateen Ulhaq</a>, 
<a href="/search/cs?searchtype=author&query=Baji%C4%87%2C+I+V">Ivan V. Baji&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 2024 Picture Coding Symposium (PCS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Due to the limited computational capabilities of edge devices, deep learning
inference can be quite expensive. One remedy is to compress and transmit point
cloud data over the network for server-side processing. Unfortunately, this
approach can be sensitive to network factors, including available bitrate.
Luckily, the bitrate requirements can be reduced without sacrificing inference
accuracy by using a machine task-specialized codec. In this paper, we present a
scalable codec for point-cloud data that is specialized for the machine task of
classification, while also providing a mechanism for human viewing. In the
proposed scalable codec, the "base" bitstream supports the machine task, and an
"enhancement" bitstream may be used for better input reconstruction performance
for human viewing. We base our architecture on PointNet++, and test its
efficacy on the ModelNet40 dataset. We show significant improvements over prior
non-specialized codecs.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12535" title="Abstract">arXiv:2402.12535</a> [<a href="/pdf/2402.12535" title="Download PDF">pdf</a>, <a href="/format/2402.12535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality-Sensitive Hashing-Based Efficient Point Transformer with  Applications in High-Energy Physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+S">Siqi Miao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Duarte%2C+J">Javier Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">This study introduces a novel transformer model optimized for large-scale
point cloud processing in scientific domains such as high-energy physics (HEP)
and astrophysics. Addressing the limitations of graph neural networks and
standard transformers, our model integrates local inductive bias and achieves
near-linear complexity with hardware-friendly regular operations. One
contribution of this work is the quantitative analysis of the error-complexity
tradeoff of various sparsification techniques for building efficient
transformers. Our findings highlight the superiority of using
locality-sensitive hashing (LSH), especially OR \&amp; AND-construction LSH, in
kernel approximation for large-scale point cloud data with local inductive
bias. Based on this finding, we propose LSH-based Efficient Point Transformer
(\textbf{HEPT}), which combines E$^2$LSH with OR \&amp; AND constructions and is
built upon regular computations. HEPT demonstrates remarkable performance in
two critical yet time-consuming HEP tasks, significantly outperforming existing
GNNs and transformers in accuracy and computational speed, marking a
significant advancement in geometric deep learning and large-scale scientific
data processing. Our code is available at
\url{https://github.com/Graph-COM/HEPT}.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12536" title="Abstract">arXiv:2402.12536</a> [<a href="/pdf/2402.12536" title="Download PDF">pdf</a>, <a href="/format/2402.12536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing High-Performing Networks for Multi-Scale Computer Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Picron%2C+C">C&#xe9;dric Picron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Since the emergence of deep learning, the computer vision field has
flourished with models improving at a rapid pace on more and more complex
tasks. We distinguish three main ways to improve a computer vision model: (1)
improving the data aspect by for example training on a large, more diverse
dataset, (2) improving the training aspect by for example designing a better
optimizer, and (3) improving the network architecture (or network for short).
In this thesis, we chose to improve the latter, i.e. improving the network
designs of computer vision models. More specifically, we investigate new
network designs for multi-scale computer vision tasks, which are tasks
requiring to make predictions about concepts at different scales. The goal of
these new network designs is to outperform existing baseline designs from the
literature. Specific care is taken to make sure the comparisons are fair, by
guaranteeing that the different network designs were trained and evaluated with
the same settings. Code is publicly available at
https://github.com/CedricPicron/DetSeg.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12537" title="Abstract">arXiv:2402.12537</a> [<a href="/pdf/2402.12537" title="Download PDF">pdf</a>, <a href="/format/2402.12537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Bayes Approach to Personalized Federated Unsupervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozkara%2C+K">Kaan Ozkara</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bruce Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruida Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Diggavi%2C+S">Suhas Diggavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Statistical heterogeneity of clients' local data is an important
characteristic in federated learning, motivating personalized algorithms
tailored to the local data statistics. Though there has been a plethora of
algorithms proposed for personalized supervised learning, discovering the
structure of local data through personalized unsupervised learning is less
explored. We initiate a systematic study of such personalized unsupervised
learning by developing algorithms based on optimization criteria inspired by a
hierarchical Bayesian statistical framework. We develop adaptive algorithms
that discover the balance between using limited local data and collaborative
information. We do this in the context of two unsupervised learning tasks:
personalized dimensionality reduction and personalized diffusion models. We
develop convergence analyses for our adaptive algorithms which illustrate the
dependence on problem parameters (e.g., heterogeneity, local sample size). We
also develop a theoretical framework for personalized diffusion models, which
shows the benefits of collaboration even under heterogeneity. We finally
evaluate our proposed algorithms using synthetic and real data, demonstrating
the effective sample amplification for personalized tasks, induced through
collaboration, despite data heterogeneity.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12538" title="Abstract">arXiv:2402.12538</a> [<a href="/pdf/2402.12538" title="Download PDF">pdf</a>, <a href="/ps/2402.12538" title="Download PostScript">ps</a>, <a href="/format/2402.12538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Ensemble Model for the Detection of Cyberbullying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alqahtani%2C+A+F">Abulkarim Faraj Alqahtani</a>, 
<a href="/search/cs?searchtype=author&query=Ilyas%2C+M">Mohammad Ilyas</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Artificial Intelligence and Applications
  (IJAIA), Vol.15, No.1, January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The pervasive use of social media platforms, such as Facebook, Instagram, and
X, has significantly amplified our electronic interconnectedness. Moreover,
these platforms are now easily accessible from any location at any given time.
However, the increased popularity of social media has also led to
cyberbullying.It is imperative to address the need for finding, monitoring, and
mitigating cyberbullying posts on social media platforms. Motivated by this
necessity, we present this paper to contribute to developing an automated
system for detecting binary labels of aggressive tweets.Our study has
demonstrated remarkable performance compared to previous experiments on the
same dataset. We employed the stacking ensemble machine learning method,
utilizing four various feature extraction techniques to optimize performance
within the stacking ensemble learning framework. Combining five machine
learning algorithms,Decision Trees, Random Forest, Linear Support Vector
Classification, Logistic Regression, and K-Nearest Neighbors into an ensemble
method, we achieved superior results compared to traditional machine learning
classifier models. The stacking classifier achieved a high accuracy rate of
94.00%, outperforming traditional machine learning models and surpassing the
results of prior experiments that utilized the same dataset. The outcomes of
our experiments showcased an accuracy rate of 0.94% in detection tweets as
aggressive or non-aggressive.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12539" title="Abstract">arXiv:2402.12539</a> [<a href="/pdf/2402.12539" title="Download PDF">pdf</a>, <a href="/format/2402.12539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of data usage for forecasting on performance of model predictive  control in buildings with smart energy storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Langtry%2C+M">Max Langtry</a>, 
<a href="/search/eess?searchtype=author&query=Wichitwechkarn%2C+V">Vijja Wichitwechkarn</a>, 
<a href="/search/eess?searchtype=author&query=Ward%2C+R">Rebecca Ward</a>, 
<a href="/search/eess?searchtype=author&query=Zhuang%2C+C">Chaoqun Zhuang</a>, 
<a href="/search/eess?searchtype=author&query=Kreitmair%2C+M+J">Monika J. Kreitmair</a>, 
<a href="/search/eess?searchtype=author&query=Makasis%2C+N">Nikolas Makasis</a>, 
<a href="/search/eess?searchtype=author&query=Conti%2C+Z+X">Zack Xuereb Conti</a>, 
<a href="/search/eess?searchtype=author&query=Choudhary%2C+R">Ruchi Choudhary</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data is required to develop forecasting models for use in Model Predictive
Control (MPC) schemes in building energy systems. However, data usage incurs
costs from both its collection and exploitation. Determining cost optimal data
usage requires understanding of the forecast accuracy and resulting MPC
operational performance it enables. This study investigates the performance of
both simple and state-of-the-art machine learning prediction models for MPC in
a multi-building energy system simulation using historic building energy data.
The impact of data usage on forecast accuracy is quantified for the following
data efficiency measures: reuse of prediction models, reduction of training
data volumes, reduction of model data features, and online model training. A
simple linear multi-layer perceptron model is shown to provide equivalent
forecast accuracy to state-of-the-art models, with greater data efficiency and
generalisability. The use of more than 2 years of training data for load
prediction models provided no significant improvement in forecast accuracy.
Forecast accuracy and data efficiency were improved simultaneously by using
change-point analysis to screen training data. Reused models and those trained
with 3 months of data had on average 10% higher error than baseline, indicating
that deploying MPC systems without prior data collection may be economic.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12541" title="Abstract">arXiv:2402.12541</a> [<a href="/pdf/2402.12541" title="Download PDF">pdf</a>, <a href="/format/2402.12541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness  in Online Dating Recommendations Based on User Sexual Orientation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wisniewski%2C+P">Pamela Wisniewski</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+C">Charu Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Online dating platforms have gained widespread popularity as a means for
individuals to seek potential romantic relationships. While recommender systems
have been designed to improve the user experience in dating platforms by
providing personalized recommendations, increasing concerns about fairness have
encouraged the development of fairness-aware recommender systems from various
perspectives (e.g., gender and race). However, sexual orientation, which plays
a significant role in finding a satisfying relationship, is under-investigated.
To fill this crucial gap, we propose a novel metric, Opposite Gender
Interaction Ratio (OGIR), as a way to investigate potential unfairness for
users with varying preferences towards the opposite gender. We empirically
analyze a real online dating dataset and observe existing recommender
algorithms could suffer from group unfairness according to OGIR. We further
investigate the potential causes for such gaps in recommendation quality, which
lead to the challenges of group quantity imbalance and group calibration
imbalance. Ultimately, we propose a fair recommender system based on
re-weighting and re-ranking strategies to respectively mitigate these
associated imbalance challenges. Experimental results demonstrate both
strategies improve fairness while their combination achieves the best
performance towards maintaining model utility while improving fairness.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12545" title="Abstract">arXiv:2402.12545</a> [<a href="/pdf/2402.12545" title="Download PDF">pdf</a>, <a href="/format/2402.12545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Danna Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Danyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated impressive capabilities across
various domains, prompting a surge in their practical applications. However,
concerns have arisen regarding the trustworthiness of LLMs outputs,
particularly in closed-book question-answering tasks, where non-experts may
struggle to identify inaccuracies due to the absence of contextual or ground
truth information. This paper introduces TrustScore, a framework based on the
concept of Behavioral Consistency, which evaluates whether an LLMs response
aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly
integrate with fact-checking methods, which assesses alignment with external
knowledge sources. The experimental results show that TrustScore achieves
strong correlations with human judgments, surpassing existing reference-free
metrics, and achieving results on par with reference-based metrics.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12550" title="Abstract">arXiv:2402.12550</a> [<a href="/pdf/2402.12550" title="Download PDF">pdf</a>, <a href="/format/2402.12550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilinear Mixture of Experts: Scalable Expert Specialization through  Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oldfield%2C+J">James Oldfield</a>, 
<a href="/search/cs?searchtype=author&query=Georgopoulos%2C+M">Markos Georgopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Chrysos%2C+G+G">Grigorios G. Chrysos</a>, 
<a href="/search/cs?searchtype=author&query=Tzelepis%2C+C">Christos Tzelepis</a>, 
<a href="/search/cs?searchtype=author&query=Panagakis%2C+Y">Yannis Panagakis</a>, 
<a href="/search/cs?searchtype=author&query=Nicolaou%2C+M+A">Mihalis A. Nicolaou</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiankang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github: <a href="https://github.com/james-oldfield/MMoE.">this https URL</a> Project page: <a href="https://eecs.qmul.ac.uk/~jo001/MMoE/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Mixture of Experts (MoE) paradigm provides a powerful way to decompose
inscrutable dense layers into smaller, modular computations often more amenable
to human interpretation, debugging, and editability. A major problem however
lies in the computational cost of scaling the number of experts to achieve
sufficiently fine-grained specialization. In this paper, we propose the
Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision
models. MMoE layers perform an implicit computation on prohibitively large
weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid
the issues incurred through the discrete expert routing in the popular 'sparse'
MoE models, yet (2) do not incur the restrictively high inference-time costs of
'soft' MoE alternatives. We present both qualitative and quantitative evidence
(through visualization and counterfactual interventions respectively) that
scaling MMoE layers when fine-tuning foundation models for vision tasks leads
to more specialized experts at the class-level whilst remaining competitive
with the performance of parameter-matched linear layer counterparts. Finally,
we show that learned expert specialism further facilitates manual correction of
demographic bias in CelebA attribute classification. Our MMoE model code is
available at https://github.com/james-oldfield/MMoE.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12551" title="Abstract">arXiv:2402.12551</a> [<a href="/pdf/2402.12551" title="Download PDF">pdf</a>, <a href="/format/2402.12551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landmark-based Localization using Stereo Vision and Deep Learning in  GPS-Denied Battlefield Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+G">Ganesh Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Madria%2C+S">Sanjay Madria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.12320">arXiv:2402.12320</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Localization in a battlefield environment is increasingly challenging as GPS
connectivity is often denied or unreliable, and physical deployment of anchor
nodes across wireless networks for localization can be difficult in hostile
battlefield terrain. Existing range-free localization methods rely on
radio-based anchors and their average hop distance which suffers from accuracy
and stability in dynamic and sparse wireless network topology. Vision-based
methods like SLAM and Visual Odometry use expensive sensor fusion techniques
for map generation and pose estimation. This paper proposes a novel framework
for localization in non-GPS battlefield environments using only the passive
camera sensors and considering naturally existing or artificial landmarks as
anchors. The proposed method utilizes a customcalibrated stereo vision camera
for distance estimation and the YOLOv8s model, which is trained and fine-tuned
with our real-world dataset for landmark recognition. The depth images are
generated using an efficient stereomatching algorithm, and distances to
landmarks are determined by extracting the landmark depth feature utilizing a
bounding box predicted by the landmark recognition model. The position of the
unknown node is then obtained using the efficient least square algorithm and
then optimized using the L-BFGS-B (limited-memory quasi-Newton code for
bound-constrained optimization) method. Experimental results demonstrate that
our proposed framework performs better than existing anchorbased DV-Hop
algorithms and competes with the most efficient vision-based algorithms in
terms of localization error (RMSE).
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12554" title="Abstract">arXiv:2402.12554</a> [<a href="/pdf/2402.12554" title="Download PDF">pdf</a>, <a href="/format/2402.12554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense  and Hypothetical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Danna Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present Archer, a challenging bilingual text-to-SQL dataset specific to
complex reasoning, including arithmetic, commonsense and hypothetical
reasoning. It contains 1,042 English questions and 1,042 Chinese questions,
along with 521 unique SQL queries, covering 20 English databases across 20
domains. Notably, this dataset demonstrates a significantly higher level of
complexity compared to existing publicly available datasets. Our evaluation
shows that Archer challenges the capabilities of current state-of-the-art
models, with a high-ranked model on the Spider leaderboard achieving only 6.73%
execution accuracy on Archer test set. Thus, Archer presents a significant
challenge for future research in this field.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12556" title="Abstract">arXiv:2402.12556</a> [<a href="/pdf/2402.12556" title="Download PDF">pdf</a>, <a href="/format/2402.12556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMBUE: Improving Interpersonal Effectiveness through Simulation and  Just-in-time Feedback with Human-Language Model Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+I+W">Inna Wanyin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Ashish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Rytting%2C+C+M">Christopher Michael Rytting</a>, 
<a href="/search/cs?searchtype=author&query=Miner%2C+A+S">Adam S. Miner</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+J">Jina Suh</a>, 
<a href="/search/cs?searchtype=author&query=Althoff%2C+T">Tim Althoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Navigating certain communication situations can be challenging due to
individuals' lack of skills and the interference of strong emotions. However,
effective learning opportunities are rarely accessible. In this work, we
conduct a human-centered study that uses language models to simulate bespoke
communication training and provide just-in-time feedback to support the
practice and learning of interpersonal effectiveness skills. We apply the
interpersonal effectiveness framework from Dialectical Behavioral Therapy
(DBT), DEAR MAN, which focuses on both conversational and emotional skills. We
present IMBUE, an interactive training system that provides feedback 25% more
similar to experts' feedback, compared to that generated by GPT-4. IMBUE is the
first to focus on communication skills and emotion management simultaneously,
incorporate experts' domain knowledge in providing feedback, and be grounded in
psychology theory. Through a randomized trial of 86 participants, we find that
IMBUE's simulation-only variant significantly improves participants'
self-efficacy (up to 17%) and reduces negative emotions (up to 25%). With
IMBUE's additional just-in-time feedback, participants demonstrate 17%
improvement in skill mastery, along with greater enhancements in self-efficacy
(27% more) and reduction of negative emotions (16% more) compared to
simulation-only. The improvement in skill mastery is the only measure that is
transferred to new and more difficult situations; situation specific training
is necessary for improving self-efficacy and emotion reduction.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12557" title="Abstract">arXiv:2402.12557</a> [<a href="/pdf/2402.12557" title="Download PDF">pdf</a>, <a href="/format/2402.12557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating a Fine Grained Entity Type Taxonomy Using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gunn%2C+M">Michael Gunn</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+D">Dohyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+N">Nidhish Kamath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we investigate the potential of GPT-4 and its advanced
iteration, GPT-4 Turbo, in autonomously developing a detailed entity type
taxonomy. Our objective is to construct a comprehensive taxonomy, starting from
a broad classification of entity types - including objects, time, locations,
organizations, events, actions, and subjects - similar to existing manually
curated taxonomies. This classification is then progressively refined through
iterative prompting techniques, leveraging GPT-4's internal knowledge base. The
result is an extensive taxonomy comprising over 5000 nuanced entity types,
which demonstrates remarkable quality upon subjective evaluation.
<br />We employed a straightforward yet effective prompting strategy, enabling the
taxonomy to be dynamically expanded. The practical applications of this
detailed taxonomy are diverse and significant. It facilitates the creation of
new, more intricate branches through pattern-based combinations and notably
enhances information extraction tasks, such as relation extraction and event
argument extraction. Our methodology not only introduces an innovative approach
to taxonomy creation but also opens new avenues for applying such taxonomies in
various computational linguistics and AI-related fields.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12558" title="Abstract">arXiv:2402.12558</a> [<a href="/pdf/2402.12558" title="Download PDF">pdf</a>, <a href="/format/2402.12558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of Country Dietary Habits Using Machine Learning Techniques  in Relation to Deaths from COVID-19
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Arias%2C+N">Natalia Arias</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Olalla%2C+O">Oscar Garc&#xed;a-Olalla</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Healthcare 2020, 8(4), 371
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">COVID-19 disease has affected almost every country in the world. The large
number of infected people and the different mortality rates between countries
has given rise to many hypotheses about the key points that make the virus so
lethal in some places. In this study, the eating habits of 170 countries were
evaluated in order to find correlations between these habits and mortality
rates caused by COVID-19 using machine learning techniques that group the
countries together according to the different distribution of fat, energy, and
protein across 23 different types of food, as well as the amount ingested in
kilograms. Results shown how obesity and the high consumption of fats appear in
countries with the highest death rates, whereas countries with a lower rate
have a higher level of cereal consumption accompanied by a lower total average
intake of kilocalories.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12560" title="Abstract">arXiv:2402.12560</a> [<a href="/pdf/2402.12560" title="Download PDF">pdf</a>, <a href="/format/2402.12560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CausalGym: Benchmarking causal interpretability methods on linguistic  tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aryaman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages main text, 26 pages total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs) have proven to be powerful tools for psycholinguistic
research, but most prior work has focused on purely behavioural measures (e.g.,
surprisal comparisons). At the same time, research in model interpretability
has begun to illuminate the abstract causal mechanisms shaping LM behavior. To
help bring these strands of research closer together, we introduce CausalGym.
We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of
interpretability methods to causally affect model behaviour. To illustrate how
CausalGym can be used, we study the pythia models (14M--6.9B) and assess the
causal efficacy of a wide range of interpretability methods, including linear
probing and distributed alignment search (DAS). We find that DAS outperforms
the other methods, and so we use it to study the learning trajectory of two
difficult linguistic phenomena in pythia-1b: negative polarity item licensing
and filler--gap dependencies. Our analysis shows that the mechanism
implementing both of these tasks is learned in discrete stages, not gradually.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12562" title="Abstract">arXiv:2402.12562</a> [<a href="/pdf/2402.12562" title="Download PDF">pdf</a>, <a href="/ps/2402.12562" title="Download PostScript">ps</a>, <a href="/format/2402.12562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Pricing and Learning with Long-term Reference Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shipra Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wei Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, two figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We consider a dynamic pricing problem where customer response to the current
price is impacted by the customer price expectation, aka reference price. We
study a simple and novel reference price mechanism where reference price is the
average of the past prices offered by the seller. As opposed to the more
commonly studied exponential smoothing mechanism, in our reference price
mechanism the prices offered by seller have a longer term effect on the future
customer expectations.
<br />We show that under this mechanism, a markdown policy is near-optimal
irrespective of the parameters of the model. This matches the common intuition
that a seller may be better off by starting with a higher price and then
decreasing it, as the customers feel like they are getting bargains on items
that are ordinarily more expensive. For linear demand models, we also provide a
detailed characterization of the near-optimal markdown policy along with an
efficient way of computing it.
<br />We then consider a more challenging dynamic pricing and learning problem,
where the demand model parameters are apriori unknown, and the seller needs to
learn them online from the customers' responses to the offered prices while
simultaneously optimizing revenue. The objective is to minimize regret, i.e.,
the $T$-round revenue loss compared to a clairvoyant optimal policy. This task
essentially amounts to learning a non-stationary optimal policy in a
time-variant Markov Decision Process (MDP). For linear demand models, we
provide an efficient learning algorithm with an optimal $\tilde{O}(\sqrt{T})$
regret upper bound.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12563" title="Abstract">arXiv:2402.12563</a> [<a href="/pdf/2402.12563" title="Download PDF">pdf</a>, <a href="/format/2402.12563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Loka Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yusheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent success of Large Language Models (LLMs) has catalyzed an
increasing interest in their self-correction capabilities. This paper presents
a comprehensive investigation into the intrinsic self-correction of LLMs,
attempting to address the ongoing debate about its feasibility. Our research
has identified an important latent factor - the ``confidence'' of LLMs - during
the self-correction process. Overlooking this factor may cause the models to
over-criticize themselves, resulting in unreliable conclusions regarding the
efficacy of self-correction. We have experimentally observed that LLMs possess
the capability to understand the ``confidence'' in their own responses. It
motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed
to guide LLMs in assessing their own ``confidence'', facilitating intrinsic
self-corrections. We conduct extensive experiments and demonstrate that our
IoE-based Prompt can achieve a consistent improvement regarding the accuracy of
self-corrected responses over the initial answers. Our study not only sheds
light on the underlying factors affecting self-correction in LLMs, but also
introduces a practical framework that utilizes the IoE prompting principle to
efficiently improve self-correction capabilities with ``confidence''. The code
is available at \url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12566" title="Abstract">arXiv:2402.12566</a> [<a href="/pdf/2402.12566" title="Download PDF">pdf</a>, <a href="/format/2402.12566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K">Kundan Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Ramprasad%2C+S">Sanjana Ramprasad</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prakhar Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Bigham%2C+J+P">Jeffrey P. Bigham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">LLMs can generate factually incorrect statements even when provided access to
reference documents. Such errors can be dangerous in high-stakes applications
(e.g., document-grounded QA for healthcare or finance). We present GenAudit --
a tool intended to assist fact-checking LLM responses for document-grounded
tasks. GenAudit suggests edits to the LLM response by revising or removing
claims that are not supported by the reference document, and also presents
evidence from the reference for facts that do appear to have support. We train
models to execute these tasks, and design an interactive interface to present
suggested edits and evidence to users. Comprehensive evaluation by human raters
shows that GenAudit can detect errors in 8 different LLM outputs when
summarizing documents from diverse domains. To ensure that most errors are
flagged by the system, we propose a method that can increase the error recall
while minimizing impact on precision. We will release our tool (GenAudit) and
fact-checking model for public use.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12570" title="Abstract">arXiv:2402.12570</a> [<a href="/pdf/2402.12570" title="Download PDF">pdf</a>, <a href="/format/2402.12570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Multi-task Transfer RL with Representational Penalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bose%2C+A">Avinandan Bose</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon Shaolei Du</a>, 
<a href="/search/cs?searchtype=author&query=Fazel%2C+M">Maryam Fazel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the problem of representation transfer in offline Reinforcement
Learning (RL), where a learner has access to episodic data from a number of
source tasks collected a priori, and aims to learn a shared representation to
be used in finding a good policy for a target task. Unlike in online RL where
the agent interacts with the environment while learning a policy, in the
offline setting there cannot be such interactions in either the source tasks or
the target task; thus multi-task offline RL can suffer from incomplete
coverage.
<br />We propose an algorithm to compute pointwise uncertainty measures for the
learnt representation, and establish a data-dependent upper bound for the
suboptimality of the learnt policy for the target task. Our algorithm leverages
the collective exploration done by source tasks to mitigate poor coverage at
some points by a few tasks, thus overcoming the limitation of needing uniformly
good coverage for a meaningful transfer by existing offline algorithms. We
complement our theoretical results with empirical evaluation on a
rich-observation MDP which requires many samples for complete coverage. Our
findings illustrate the benefits of penalizing and quantifying the uncertainty
in the learnt representation.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12571" title="Abstract">arXiv:2402.12571</a> [<a href="/pdf/2402.12571" title="Download PDF">pdf</a>, <a href="/format/2402.12571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving fluid flow problems in space-time with multiscale stabilization:  formulation and examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khara%2C+B">Biswajit Khara</a>, 
<a href="/search/math?searchtype=author&query=Dyja%2C+R">Robert Dyja</a>, 
<a href="/search/math?searchtype=author&query=Saurabh%2C+K">Kumar Saurabh</a>, 
<a href="/search/math?searchtype=author&query=Sharma%2C+A">Anupam Sharma</a>, 
<a href="/search/math?searchtype=author&query=Ganapathysubramanian%2C+B">Baskar Ganapathysubramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We solve fluid flow problems through a space-time finite element method. The
weak form of the Navier-Stokes equations is stabilized using the variational
multi-scale formulation. The finite element problem is posed on the "full"
space-time domain, considering time as another dimension. We apply this method
on two benchmark problems in computational fluid dynamics, namely, lid-driven
cavity flow and flow past a circular cylinder. We validate the current method
with existing results from literature and show that very large space-time
blocks can be solved using our approach.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12572" title="Abstract">arXiv:2402.12572</a> [<a href="/pdf/2402.12572" title="Download PDF">pdf</a>, <a href="/format/2402.12572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairProof : Confidential and Certifiable Fairness for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+C">Chhavi Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+R">Amrita Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Boneh%2C+D">Dan Boneh</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Machine learning models are increasingly used in societal applications, yet
legal and privacy concerns demand that they very often be kept confidential.
Consequently, there is a growing distrust about the fairness properties of
these models in the minds of consumers, who are often at the receiving end of
model predictions. To this end, we propose FairProof - a system that uses
Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the
fairness of a model, while maintaining confidentiality. We also propose a
fairness certification algorithm for fully-connected neural networks which is
befitting to ZKPs and is used in this system. We implement FairProof in Gnark
and demonstrate empirically that our system is practically feasible.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12577" title="Abstract">arXiv:2402.12577</a> [<a href="/pdf/2402.12577" title="Download PDF">pdf</a>, <a href="/format/2402.12577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proximal Byzantine Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shadmon%2C+R">Roy Shadmon</a>, 
<a href="/search/cs?searchtype=author&query=Spencer%2C+D">Daniel Spencer</a>, 
<a href="/search/cs?searchtype=author&query=Arden%2C+O">Owen Arden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Distributed control systems require high reliability and availability
guarantees despite often being deployed at the edge of network infrastructure.
Edge computing resources are less secure and less reliable than centralized
resources in data centers. Replication and consensus protocols improve
robustness to network faults and crashed or corrupted nodes, but these volatile
environments can cause non-faulty nodes to temporarily diverge, increasing the
time needed for replicas to converge on a consensus value, and give Byzantine
attackers too much influence over the convergence process.
<br />This paper proposes proximal Byzantine consensus, a new approximate consensus
protocol where clients use statistical models of streaming computations to
decide a consensus value. In addition, it provides an interval around the
decision value and the probability that the true (non-faulty, noise-free) value
falls within this interval. Proximal consensus (PC) tolerates unreliable
network conditions, Byzantine behavior, and other sources of noise that cause
honest replica states to diverge. We evaluate our approach for scalar values,
and compare PC simulations against a vector consensus (VC) protocol simulation.
Our simulations demonstrate that consensus values selected by PC have lower
error and are more robust against Byzantine attacks. We formally characterize
the security guarantees against Byzantine attacks and demonstrate attacker
influence is bound with high probability. Additionally, an informal complexity
analysis suggests PC scales better to higher dimensions than convex hull-based
protocols such as VC.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12584" title="Abstract">arXiv:2402.12584</a> [<a href="/pdf/2402.12584" title="Download PDF">pdf</a>, <a href="/ps/2402.12584" title="Download PostScript">ps</a>, <a href="/format/2402.12584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Moments on Redundancies in Noisy Parallel Computing Setup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarmasarkar%2C+S">Sahasrajit Sarmasarkar</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+H">Harish Pillai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We consider the problem of job assignment where a master server aims to
compute some tasks and is provided a few child servers to compute under a
uniform straggling pattern where each server is equally likely to straggle. We
distribute tasks to the servers so that the master is able to receive most of
the tasks even if a significant number of child servers fail to communicate. We
first show that all \textit{balanced} assignment schemes have the same
expectation on the number of distinct tasks received and then study the
variance. The variance or the second moment is a useful metric to study as
there could be a high \textit{variation} in the number of distinct tasks
received. We show constructions using a generalization of ``Balanced Incomplete
Block Design'' [11,40] minimizes the variance, and constructions based on
repetition coding schemes attain the largest variance. Both minimum variance
and maximum variance attaining designs have their own use cases depending on
whether the master aims for a heavy-tailed or light-tailed distribution on the
number of distinct jobs. We further show the equivalence between job and
server-based assignment schemes when the number of jobs and child servers are
equal.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12587" title="Abstract">arXiv:2402.12587</a> [<a href="/pdf/2402.12587" title="Download PDF">pdf</a>, <a href="/format/2402.12587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Disentanglement of Tube Inequalities in Concentric Tube Continuum  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grassmann%2C+R+M">Reinhard M. Grassmann</a>, 
<a href="/search/cs?searchtype=author&query=Senyk%2C+A">Anastasiia Senyk</a>, 
<a href="/search/cs?searchtype=author&query=Burgner-Kahrs%2C+J">Jessica Burgner-Kahrs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in International Conference on Robotics and Automation (ICRA 2024). 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Concentric tube continuum robots utilize nested tubes, which are subject to a
set of inequalities. Current approaches to account for inequalities rely on
branching methods such as if-else statements. It can introduce discontinuities,
may result in a complicated decision tree, has a high wall-clock time, and
cannot be vectorized. This affects the behavior and result of downstream
methods in control, learning, workspace estimation, and path planning, among
others.
<br />In this paper, we investigate a mapping to mitigate branching methods. We
derive a lower triangular transformation matrix to disentangle the inequalities
and provide proof for the unique existence. It transforms the interdependent
inequalities into independent box constraints. Further investigations are made
for sampling, control, and workspace estimation. Approaches utilizing the
proposed mapping are at least 14 times faster (up to 176 times faster),
generate always valid joint configurations, are more interpretable, and are
easier to extend.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12590" title="Abstract">arXiv:2402.12590</a> [<a href="/pdf/2402.12590" title="Download PDF">pdf</a>, <a href="/format/2402.12590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving AI Collectives to Enhance Human Diversity and Enable  Self-Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Shiyang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Potter%2C+Y">Yujin Potter</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junsol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+R">Richard Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+J">James Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Large language models steer their behaviors based on texts generated by
others. This capacity and their increasing prevalence in online settings
portend that they will intentionally or unintentionally "program" one another
and form emergent AI subjectivities, relationships, and collectives. Here, we
call upon the research community to investigate these "society-like" properties
of interacting artificial intelligences to increase their rewards and reduce
their risks for human society and the health of online environments. We use a
simple model and its outputs to illustrate how such emergent, decentralized AI
collectives can expand the bounds of human diversity and reduce the risk of
toxic, anti-social behavior online. Finally, we discuss opportunities for AI
self-moderation and address ethical issues and design challenges associated
with creating and maintaining decentralized AI collectives.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12593" title="Abstract">arXiv:2402.12593</a> [<a href="/pdf/2402.12593" title="Download PDF">pdf</a>, <a href="/format/2402.12593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Standardize: Aligning Language Models with Expert-Defined Standards for  Content Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imperial%2C+J+M">Joseph Marvin Imperial</a>, 
<a href="/search/cs?searchtype=author&query=Forey%2C+G">Gail Forey</a>, 
<a href="/search/cs?searchtype=author&query=Madabushi%2C+H+T">Harish Tayyar Madabushi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Domain experts across engineering, healthcare, and education follow strict
standards for producing quality content such as technical manuals, medication
instructions, and children's reading materials. However, current works in
controllable text generation have yet to explore using these standards as
references for control. Towards this end, we introduce Standardize, a
retrieval-style in-context learning-based framework to guide large language
models to align with expert-defined standards. Focusing on English language
standards in the education domain as a use case, we consider the Common
European Framework of Reference for Languages (CEFR) and Common Core Standards
(CCS) for the task of open-ended content generation. Our findings show that
models can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4,
respectively, demonstrating that the use of knowledge artifacts extracted from
standards and integrating them in the generation process can effectively guide
models to produce better standard-aligned content.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12598" title="Abstract">arXiv:2402.12598</a> [<a href="/pdf/2402.12598" title="Download PDF">pdf</a>, <a href="/format/2402.12598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Virtual Sensing from Sparse and Partial Multivariate  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Felice%2C+G">Giovanni De Felice</a>, 
<a href="/search/cs?searchtype=author&query=Cini%2C+A">Andrea Cini</a>, 
<a href="/search/cs?searchtype=author&query=Zambon%2C+D">Daniele Zambon</a>, 
<a href="/search/cs?searchtype=author&query=Gusev%2C+V+V">Vladimir V. Gusev</a>, 
<a href="/search/cs?searchtype=author&query=Alippi%2C+C">Cesare Alippi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Virtual sensing techniques allow for inferring signals at new unmonitored
locations by exploiting spatio-temporal measurements coming from physical
sensors at different locations. However, as the sensor coverage becomes sparse
due to costs or other constraints, physical proximity cannot be used to support
interpolation. In this paper, we overcome this challenge by leveraging
dependencies between the target variable and a set of correlated variables
(covariates) that can frequently be associated with each location of interest.
From this viewpoint, covariates provide partial observability, and the problem
consists of inferring values for unobserved channels by exploiting observations
at other locations to learn how such variables can correlate. We introduce a
novel graph-based methodology to exploit such relationships and design a graph
deep learning architecture, named GgNet, implementing the framework. The
proposed approach relies on propagating information over a nested graph
structure that is used to learn dependencies between variables as well as
locations. GgNet is extensively evaluated under different virtual sensing
scenarios, demonstrating higher reconstruction accuracy compared to the
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12602" title="Abstract">arXiv:2402.12602</a> [<a href="/pdf/2402.12602" title="Download PDF">pdf</a>, <a href="/format/2402.12602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physically Consistent Modeling of Stacked Intelligent Metasurfaces  Implemented with Beyond Diagonal RIS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nerini%2C+M">Matteo Nerini</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Stacked intelligent metasurface (SIM) has emerged as a technology enabling
wave domain beamforming through multiple stacked reconfigurable intelligent
surfaces (RISs). SIM has been implemented so far with diagonal RIS (D-RIS),
while SIM implemented with beyond diagonal RIS (BD-RIS) remains unexplored.
Furthermore, a model of SIM accounting for mutual coupling is not yet
available. To fill these gaps, we derive a physically consistent channel model
for SIM-aided systems and clarify the assumptions needed to obtain the
simplified model used in related works. Using this model, we show that 1-layer
SIM implemented with BD-RIS achieves the performance upper bound with limited
complexity.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12605" title="Abstract">arXiv:2402.12605</a> [<a href="/pdf/2402.12605" title="Download PDF">pdf</a>, <a href="/ps/2402.12605" title="Download PostScript">ps</a>, <a href="/format/2402.12605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is a word?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murphy%2C+E">Elliot Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In order to design strong paradigms for isolating lexical access and
semantics, we need to know what a word is. Surprisingly few linguists and
philosophers have a clear model of what a word is, even though words impact
basically every aspect of human life. Researchers that regularly publish
academic papers about language often rely on outdated, or inaccurate,
assumptions about wordhood. This short pedagogical document outlines what the
lexicon is most certainly not (though is often mistakenly taken to be), what it
might be (based on current good theories), and what some implications for
experimental design are.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12608" title="Abstract">arXiv:2402.12608</a> [<a href="/pdf/2402.12608" title="Download PDF">pdf</a>, <a href="/format/2402.12608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patient-Centric Knowledge Graphs: A Survey of Current Methods,  Challenges, and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatib%2C+H+S+A">Hassan S. Al Khatib</a>, 
<a href="/search/cs?searchtype=author&query=Neupane%2C+S">Subash Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Manchukonda%2C+H+K">Harish Kumar Manchukonda</a>, 
<a href="/search/cs?searchtype=author&query=Golilarz%2C+N+A">Noorbakhsh Amiri Golilarz</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sudip Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Amirlatifi%2C+A">Amin Amirlatifi</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+S">Shahram Rahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Patient-Centric Knowledge Graphs (PCKGs) represent an important shift in
healthcare that focuses on individualized patient care by mapping the patient's
health information in a holistic and multi-dimensional way. PCKGs integrate
various types of health data to provide healthcare professionals with a
comprehensive understanding of a patient's health, enabling more personalized
and effective care. This literature review explores the methodologies,
challenges, and opportunities associated with PCKGs, focusing on their role in
integrating disparate healthcare data and enhancing patient care through a
unified health perspective. In addition, this review also discusses the
complexities of PCKG development, including ontology design, data integration
techniques, knowledge extraction, and structured representation of knowledge.
It highlights advanced techniques such as reasoning, semantic search, and
inference mechanisms essential in constructing and evaluating PCKGs for
actionable healthcare insights. We further explore the practical applications
of PCKGs in personalized medicine, emphasizing their significance in improving
disease prediction and formulating effective treatment plans. Overall, this
review provides a foundational perspective on the current state-of-the-art and
best practices of PCKGs, guiding future research and applications in this
dynamic field.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12612" title="Abstract">arXiv:2402.12612</a> [<a href="/pdf/2402.12612" title="Download PDF">pdf</a>, <a href="/format/2402.12612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A System Development Kit for Big Data Applications on FPGA-based  Clusters: The EVEREST Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilato%2C+C">Christian Pilato</a>, 
<a href="/search/cs?searchtype=author&query=Banik%2C+S">Subhadeep Banik</a>, 
<a href="/search/cs?searchtype=author&query=Beranek%2C+J">Jakub Beranek</a>, 
<a href="/search/cs?searchtype=author&query=Brocheton%2C+F">Fabien Brocheton</a>, 
<a href="/search/cs?searchtype=author&query=Castrillon%2C+J">Jeronimo Castrillon</a>, 
<a href="/search/cs?searchtype=author&query=Cevasco%2C+R">Riccardo Cevasco</a>, 
<a href="/search/cs?searchtype=author&query=Cmar%2C+R">Radim Cmar</a>, 
<a href="/search/cs?searchtype=author&query=Curzel%2C+S">Serena Curzel</a>, 
<a href="/search/cs?searchtype=author&query=Ferrandi%2C+F">Fabrizio Ferrandi</a>, 
<a href="/search/cs?searchtype=author&query=Friebel%2C+K+F+A">Karl F. A. Friebel</a>, 
<a href="/search/cs?searchtype=author&query=Galizia%2C+A">Antonella Galizia</a>, 
<a href="/search/cs?searchtype=author&query=Grasso%2C+M">Matteo Grasso</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+P">Paulo Silva</a>, 
<a href="/search/cs?searchtype=author&query=Martinovic%2C+J">Jan Martinovic</a>, 
<a href="/search/cs?searchtype=author&query=Palermo%2C+G">Gianluca Palermo</a>, 
<a href="/search/cs?searchtype=author&query=Paolino%2C+M">Michele Paolino</a>, 
<a href="/search/cs?searchtype=author&query=Parodi%2C+A">Andrea Parodi</a>, 
<a href="/search/cs?searchtype=author&query=Parodi%2C+A">Antonio Parodi</a>, 
<a href="/search/cs?searchtype=author&query=Pintus%2C+F">Fabio Pintus</a>, 
<a href="/search/cs?searchtype=author&query=Polig%2C+R">Raphael Polig</a>, 
<a href="/search/cs?searchtype=author&query=Poulet%2C+D">David Poulet</a>, 
<a href="/search/cs?searchtype=author&query=Regazzoni%2C+F">Francesco Regazzoni</a>, 
<a href="/search/cs?searchtype=author&query=Ringlein%2C+B">Burkhard Ringlein</a>, 
<a href="/search/cs?searchtype=author&query=Rocco%2C+R">Roberto Rocco</a>, 
<a href="/search/cs?searchtype=author&query=Slaninova%2C+K">Katerina Slaninova</a>, 
<a href="/search/cs?searchtype=author&query=Slooff%2C+T">Tom Slooff</a>, 
<a href="/search/cs?searchtype=author&query=Soldavini%2C+S">Stephanie Soldavini</a>, 
<a href="/search/cs?searchtype=author&query=Suchert%2C+F">Felix Suchert</a>, 
<a href="/search/cs?searchtype=author&query=Tibaldi%2C+M">Mattia Tibaldi</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+B">Beat Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Hagleitner%2C+C">Christoph Hagleitner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at DATE 2024 (multi-partner project session)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Modern big data workflows are characterized by computationally intensive
kernels. The simulated results are often combined with knowledge extracted from
AI models to ultimately support decision-making. These energy-hungry workflows
are increasingly executed in data centers with energy-efficient hardware
accelerators since FPGAs are well-suited for this task due to their inherent
parallelism. We present the H2020 project EVEREST, which has developed a system
development kit (SDK) to simplify the creation of FPGA-accelerated kernels and
manage the execution at runtime through a virtualization environment. This
paper describes the main components of the EVEREST SDK and the benefits that
can be achieved in our use cases.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12613" title="Abstract">arXiv:2402.12613</a> [<a href="/pdf/2402.12613" title="Download PDF">pdf</a>, <a href="/format/2402.12613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Using Sigmoid Loss for Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chungpa Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Joonhwan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+J">Jy-yong Sohn</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024, Valencia, Spain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Contrastive learning has emerged as a prominent branch of self-supervised
learning for several years. Especially, CLIP, which applies contrastive
learning to large sets of captioned images, has garnered significant attention.
Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid
loss instead of the standard InfoNCE loss. SigLIP achieves the performance
comparable to CLIP in a more efficient manner by eliminating the need for a
global view. However, theoretical understanding of using the sigmoid loss in
contrastive learning is underexplored. In this paper, we provide a theoretical
analysis of using the sigmoid loss in contrastive learning, in the perspective
of the geometric structure of learned embeddings. First, we propose the
double-Constant Embedding Model (CCEM), a framework for parameterizing various
well-known embedding structures by a single variable. Interestingly, the
proposed CCEM is proven to contain the optimal embedding with respect to the
sigmoid loss. Second, we mathematically analyze the optimal embedding
minimizing the sigmoid loss for contrastive learning. The optimal embedding
ranges from simplex equiangular-tight-frame to antipodal structure, depending
on the temperature parameter used in the sigmoid loss. Third, our experimental
results on synthetic datasets coincide with the theoretical results on the
optimal embedding structures.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12616" title="Abstract">arXiv:2402.12616</a> [<a href="/pdf/2402.12616" title="Download PDF">pdf</a>, <a href="/format/2402.12616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-objective Binary Coordinate Search for Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyandoab%2C+S+Z">Sevil Zanjani Miyandoab</a>, 
<a href="/search/cs?searchtype=author&query=Rahnamayan%2C+S">Shahryar Rahnamayan</a>, 
<a href="/search/cs?searchtype=author&query=Bidgoli%2C+A+A">Azam Asilian Bidgoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC), pp. 4176-4183. IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">A supervised feature selection method selects an appropriate but concise set
of features to differentiate classes, which is highly expensive for large-scale
datasets. Therefore, feature selection should aim at both minimizing the number
of selected features and maximizing the accuracy of classification, or any
other task. However, this crucial task is computationally highly demanding on
many real-world datasets and requires a very efficient algorithm to reach a set
of optimal features with a limited number of fitness evaluations. For this
purpose, we have proposed the binary multi-objective coordinate search (MOCS)
algorithm to solve large-scale feature selection problems. To the best of our
knowledge, the proposed algorithm in this paper is the first multi-objective
coordinate search algorithm. In this method, we generate new individuals by
flipping a variable of the candidate solutions on the Pareto front. This
enables us to investigate the effectiveness of each feature in the
corresponding subset. In fact, this strategy can play the role of crossover and
mutation operators to generate distinct subsets of features. The reported
results indicate the significant superiority of our method over NSGA-II, on
five real-world large-scale datasets, particularly when the computing budget is
limited. Moreover, this simple hyper-parameter-free algorithm can solve feature
selection much faster and more efficiently than NSGA-II.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12617" title="Abstract">arXiv:2402.12617</a> [<a href="/pdf/2402.12617" title="Download PDF">pdf</a>, <a href="/format/2402.12617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI Security: Challenges and Countermeasures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+N">Norman Mu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+D">David Wagner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative AI's expanding footprint across numerous industries has led to
both excitement and increased scrutiny. This paper delves into the unique
security challenges posed by Generative AI, and outlines potential research
directions for managing these risks.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12620" title="Abstract">arXiv:2402.12620</a> [<a href="/pdf/2402.12620" title="Download PDF">pdf</a>, <a href="/format/2402.12620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models (LLMs) Good Social Predictors?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaiqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+T">Tai-Quan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The prediction has served as a crucial scientific method in modern social
studies. With the recent advancement of Large Language Models (LLMs), efforts
have been made to leverage LLMs to predict the human features in social life,
such as presidential voting. These works suggest that LLMs are capable of
generating human-like responses. However, we find that the promising
performance achieved by previous studies is because of the existence of input
shortcut features to the response. In fact, by removing these shortcuts, the
performance is reduced dramatically. To further revisit the ability of LLMs, we
introduce a novel social prediction task, Soc-PRF Prediction, which utilizes
general features as input and simulates real-world social study settings. With
the comprehensive investigations on various LLMs, we reveal that LLMs cannot
work as expected on social prediction when given general input features without
shortcuts. We further investigate possible reasons for this phenomenon that
suggest potential ways to enhance LLMs for social prediction.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12621" title="Abstract">arXiv:2402.12621</a> [<a href="/pdf/2402.12621" title="Download PDF">pdf</a>, <a href="/format/2402.12621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Runlong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Beibin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">As language models (LMs) demonstrate their capabilities in various fields,
their application to tasks requiring multi-round interactions has become
increasingly popular. These tasks usually have complex dynamics, so supervised
fine-tuning (SFT) on a limited offline dataset does not yield good performance.
However, only a few works attempted to directly train the LMs within
interactive decision-making environments. We aim to create an effective
mechanism to fine-tune LMs with online reinforcement learning (RL) in these
environments. We propose Reflect-RL, a two-player system to fine-tune an LM
using online RL, where a frozen reflection model assists the policy model. To
generate data for the warm-up SFT stage, we use negative example generation to
enhance the error-correction ability of the reflection model. Furthermore, we
designed single-prompt action enumeration and applied curriculum learning to
allow the policy model to learn more efficiently. Empirically, we verify that
Reflect-RL outperforms SFT and online RL without reflection. Testing results
indicate GPT-2-xl after Reflect-RL also outperforms those of untuned
pre-trained LMs, such as Mistral 7B.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12623" title="Abstract">arXiv:2402.12623</a> [<a href="/pdf/2402.12623" title="Download PDF">pdf</a>, <a href="/format/2402.12623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Edge Ranking via Random Walk with Restart
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Renchi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Given a network G, edge centrality is a metric used to evaluate the
importance of edges in G, which is a key concept in analyzing networks and
finds vast applications involving edge ranking. In spite of a wealth of
research on devising edge centrality measures, they incur either prohibitively
high computation costs or varied deficiencies that lead to sub-optimal ranking
quality.
<br />To overcome their limitations, this paper proposes EdgeRAKE, a new centrality
measure for edge ranking that leverages the novel notion of the edgewise random
walk with restart. Based thereon, we present a linear-complexity algorithm for
EdgeRAKE approximation, followed by an in-depth theoretical analysis in terms
of various aspects. Extensive experiments comparing EdgeRAKE against six edge
centrality metrics in graph analytics tasks on real networks showcase that
EdgeRAKE offers superior practical effectiveness without significantly reducing
computation efficiency
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12624" title="Abstract">arXiv:2402.12624</a> [<a href="/pdf/2402.12624" title="Download PDF">pdf</a>, <a href="/format/2402.12624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Parameter Mining and Freezing for Continual Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menezes%2C+A+G">Angelo G. Menezes</a>, 
<a href="/search/cs?searchtype=author&query=Peterlevitz%2C+A+J">Augusto J. Peterlevitz</a>, 
<a href="/search/cs?searchtype=author&query=Chinelatto%2C+M+A">Mateus A. Chinelatto</a>, 
<a href="/search/cs?searchtype=author&query=de+Carvalho%2C+A+C+P+L+F">Andr&#xe9; C. P. L. F. de Carvalho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 2: VISAPP, ISBN 978-989-758-679-8, ISSN 2184-4321, pages 466-474
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Continual Object Detection is essential for enabling intelligent agents to
interact proactively with humans in real-world settings. While
parameter-isolation strategies have been extensively explored in the context of
continual learning for classification, they have yet to be fully harnessed for
incremental object detection scenarios. Drawing inspiration from prior research
that focused on mining individual neuron responses and integrating insights
from recent developments in neural pruning, we proposed efficient ways to
identify which layers are the most important for a network to maintain the
performance of a detector across sequential updates. The presented findings
highlight the substantial advantages of layer-level parameter isolation in
facilitating incremental learning within object detection models, offering
promising avenues for future research and application in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12625" title="Abstract">arXiv:2402.12625</a> [<a href="/pdf/2402.12625" title="Download PDF">pdf</a>, <a href="/format/2402.12625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compact NSGA-II for Multi-objective Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyandoab%2C+S+Z">Sevil Zanjani Miyandoab</a>, 
<a href="/search/cs?searchtype=author&query=Rahnamayan%2C+S">Shahryar Rahnamayan</a>, 
<a href="/search/cs?searchtype=author&query=Bidgoli%2C+A+A">Azam Asilian Bidgoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC), pp. 3868-3875. IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Feature selection is an expensive challenging task in machine learning and
data mining aimed at removing irrelevant and redundant features. This
contributes to an improvement in classification accuracy, as well as the budget
and memory requirements for classification, or any other post-processing task
conducted after feature selection. In this regard, we define feature selection
as a multi-objective binary optimization task with the objectives of maximizing
classification accuracy and minimizing the number of selected features. In
order to select optimal features, we have proposed a binary Compact NSGA-II
(CNSGA-II) algorithm. Compactness represents the population as a probability
distribution to enhance evolutionary algorithms not only to be more
memory-efficient but also to reduce the number of fitness evaluations. Instead
of holding two populations during the optimization process, our proposed method
uses several Probability Vectors (PVs) to generate new individuals. Each PV
efficiently explores a region of the search space to find non-dominated
solutions instead of generating candidate solutions from a small population as
is the common approach in most evolutionary algorithms. To the best of our
knowledge, this is the first compact multi-objective algorithm proposed for
feature selection. The reported results for expensive optimization cases with a
limited budget on five datasets show that the CNSGA-II performs more
efficiently than the well-known NSGA-II method in terms of the hypervolume (HV)
performance metric requiring less memory. The proposed method and experimental
results are explained and analyzed in detail.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12626" title="Abstract">arXiv:2402.12626</a> [<a href="/pdf/2402.12626" title="Download PDF">pdf</a>, <a href="/format/2402.12626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yiwei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M+Y+R">Matthew Y.R. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaoliang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SaTML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Machine learning models have achieved great success in supervised learning
tasks for end-to-end training, which requires a large amount of labeled data
that is not always feasible. Recently, many practitioners have shifted to
self-supervised learning methods that utilize cheap unlabeled data to learn a
general feature extractor via pre-training, which can be further applied to
personalized downstream tasks by simply training an additional linear layer
with limited labeled data. However, such a process may also raise concerns
regarding data poisoning attacks. For instance, indiscriminate data poisoning
attacks, which aim to decrease model utility by injecting a small number of
poisoned data into the training set, pose a security risk to machine learning
models, but have only been studied for end-to-end supervised learning. In this
paper, we extend the exploration of the threat of indiscriminate attacks on
downstream tasks that apply pre-trained feature extractors. Specifically, we
propose two types of attacks: (1) the input space attacks, where we modify
existing attacks to directly craft poisoned data in the input space. However,
due to the difficulty of optimization under constraints, we further propose (2)
the feature targeted attacks, where we mitigate the challenge with three
stages, firstly acquiring target parameters for the linear head; secondly
finding poisoned features by treating the learned feature representations as a
dataset; and thirdly inverting the poisoned features back to the input space.
Our experiments examine such attacks in popular downstream tasks of fine-tuning
on the same dataset and transfer learning that considers domain adaptation.
Empirical results reveal that transfer learning is more vulnerable to our
attacks. Additionally, input space attacks are a strong threat if no
countermeasures are posed, but are otherwise weaker than feature targeted
attacks.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12627" title="Abstract">arXiv:2402.12627</a> [<a href="/pdf/2402.12627" title="Download PDF">pdf</a>, <a href="/format/2402.12627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review of Machine Learning Advances on Data Change: A  Cross-Field Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jeng-Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chih-Fan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Ming-Ching Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Chao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent artificial intelligence (AI) technologies show remarkable evolution in
various academic fields and industries. However, in the real world, dynamic
data lead to principal challenges for deploying AI models. An unexpected data
change brings about severe performance degradation in AI models. We identify
two major related research fields, domain shift and concept drift according to
the setting of the data change. Although these two popular research fields aim
to solve distribution shift and non-stationary data stream problems, the
underlying properties remain similar which also encourages similar technical
approaches. In this review, we regroup domain shift and concept drift into a
single research problem, namely the data change problem, with a systematic
overview of state-of-the-art methods in the two research fields. We propose a
three-phase problem categorization scheme to link the key ideas in the two
technical fields. We thus provide a novel scope for researchers to explore
contemporary technical strategies, learn industrial applications, and identify
future directions for addressing data change challenges.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12629" title="Abstract">arXiv:2402.12629</a> [<a href="/pdf/2402.12629" title="Download PDF">pdf</a>, <a href="/format/2402.12629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Television Discourse Decoded: Comprehensive Multimodal Analytics at  Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Anmol Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Priyadarshi%2C+P">Pratyush Priyadarshi</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Shiven Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shrey Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jangra%2C+H">Hitkul Jangra</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+K">Kiran Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In this paper, we tackle the complex task of analyzing televised debates,
with a focus on a prime time news debate show from India. Previous methods,
which often relied solely on text, fall short in capturing the multimedia
essence of these debates. To address this gap, we introduce a comprehensive
automated toolkit that employs advanced computer vision and speech-to-text
techniques for large-scale multimedia analysis. Utilizing state-of-the-art
computer vision algorithms and speech-to-text methods, we transcribe, diarize,
and analyze thousands of YouTube videos of prime-time television debates in
India. These debates are a central part of Indian media but have been
criticized for compromised journalistic integrity and excessive dramatization.
Our toolkit provides concrete metrics to assess bias and incivility, capturing
a comprehensive multimedia perspective that includes text, audio utterances,
and video frames. Our findings reveal significant biases in topic selection and
panelist representation, along with alarming levels of incivility. This work
offers a scalable, automated approach for future research in multimedia
analysis, with profound implications for the quality of public discourse and
democratic debate. We will make our data analysis pipeline and collected data
publicly available to catalyze further research in this domain.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12634" title="Abstract">arXiv:2402.12634</a> [<a href="/pdf/2402.12634" title="Download PDF">pdf</a>, <a href="/format/2402.12634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Storytelling in Data Visualisation: Does it Enhance the Efficiency  and Effectiveness of Information Retrieval and Insights Comprehension?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Honbo Shao</a>, 
<a href="/search/cs?searchtype=author&query=Martinez-Maldonado%2C+R">Roberto Martinez-Maldonado</a>, 
<a href="/search/cs?searchtype=author&query=Echeverria%2C+V">Vanessa Echeverria</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gasevic%2C+D">Dragan Gasevic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CHI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data storytelling (DS) is rapidly gaining attention as an approach that
integrates data, visuals, and narratives to create data stories that can help a
particular audience to comprehend the key messages underscored by the data with
enhanced efficiency and effectiveness. It is been posited that DS can be
especially advantageous for audiences with limited visualisation literacy, by
presenting the data clearly and concisely. However, empirical studies
confirming whether data stories indeed provide these benefits over conventional
data visualisations are scarce. To bridge this gap, we conducted a study with
103 participants to determine whether DS indeed improve both efficiency and
effectiveness in tasks related to information retrieval and insights
comprehension. Our findings suggest that data stories do improve the efficiency
of comprehension tasks, as well as the effectiveness of comprehension tasks
that involve a single insight compared with conventional visualisations.
Interestingly, these benefits were not associated with participants'
visualisation literacy.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12635" title="Abstract">arXiv:2402.12635</a> [<a href="/pdf/2402.12635" title="Download PDF">pdf</a>, <a href="/format/2402.12635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Feedback-Informed Interface Design for Flow Management Data and  Services (FMDS)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdulhak%2C+S">Sinan Abdulhak</a>, 
<a href="/search/cs?searchtype=author&query=Carvette%2C+A">Anthony Carvette</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+K">Kate Shen</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+R">Robert Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Tuck%2C+B">Bill Tuck</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+Z">Max Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The transition to a microservices-based Flow Management Data and Services
(FMDS) architecture from the existing Traffic Flow Management System (TFMS) is
a critical enabler of the vision for an Information-Centric National Airspace
System (NAS). The need to design a user-centric interface for FMDS is a key
technical gap, as this interface connects NAS data and services to the traffic
management specialists within all stakeholder groups (e.g., FAA, airlines). We
provide a research-driven approach towards designing such a graphical user
interface (GUI) for FMDS. Major goals include unifying the more than 50
disparate traffic management services currently hosted on TFMS, as well as
streamlining the process of evaluating, modeling, and monitoring Traffic
Management Initiatives (TMIs). Motivated by this, we iteratively designed a GUI
leveraging human factors engineering and user experience design principles, as
well as user interviews. Through user testing and interviews, we identify
workflow benefits of our GUI (e.g., reduction in task completion time), along
with next steps for developing a live prototype.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12636" title="Abstract">arXiv:2402.12636</a> [<a href="/pdf/2402.12636" title="Download PDF">pdf</a>, <a href="/format/2402.12636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gaoxiang Cong</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuankai Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Beheshti%2C+A">Amin Beheshti</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhedong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Hengel%2C+A">Anton van den Hengel</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenggang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is
to generate speech that aligns well with the video in both time and emotion,
based on the tone of a reference audio track. Existing state-of-the-art V2C
models break the phonemes in the script according to the divisions between
video frames, which solves the temporal alignment problem but leads to
incomplete phoneme pronunciation and poor identity stability. To address this
problem, we propose StyleDubber, which switches dubbing learning from the frame
level to phoneme level. It contains three main components: (1) A multimodal
style adaptor operating at the phoneme level to learn pronunciation style from
the reference audio, and generate intermediate representations informed by the
facial emotion presented in the video; (2) An utterance-level style learning
module, which guides both the mel-spectrogram decoding and the refining
processes from the intermediate embeddings to improve the overall style
expression; And (3) a phoneme-guided lip aligner to maintain lip sync.
Extensive experiments on two of the primary benchmarks, V2C and Grid,
demonstrate the favorable performance of the proposed method as compared to the
current state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12637" title="Abstract">arXiv:2402.12637</a> [<a href="/pdf/2402.12637" title="Download PDF">pdf</a>, <a href="/format/2402.12637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting into the Flow: Towards Better Type Error Messages for  Constraint-Based Type Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhanuka%2C+I">Ishan Bhanuka</a>, 
<a href="/search/cs?searchtype=author&query=Parreaux%2C+L">Lionel Parreaux</a>, 
<a href="/search/cs?searchtype=author&query=Binder%2C+D">David Binder</a>, 
<a href="/search/cs?searchtype=author&query=Brachth%C3%A4user%2C+J+I">Jonathan Immanuel Brachth&#xe4;user</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Program. Lang. 7, OOPSLA2, Article 237 (October 2023),
  29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Creating good type error messages for constraint-based type inference systems
is difficult. Typical type error messages reflect implementation details of the
underlying constraint-solving algorithms rather than the specific factors
leading to type mismatches. We propose using subtyping constraints that capture
data flow to classify and explain type errors. Our algorithm explains type
errors as faulty data flows, which programmers are already used to reasoning
about, and illustrates these data flows as sequences of relevant program
locations. We show that our ideas and algorithm are not limited to languages
with subtyping, as they can be readily integrated with Hindley-Milner type
inference. In addition to these core contributions, we present the results of a
user study to evaluate the quality of our messages compared to other
implementations. While the quantitative evaluation does not show that
flow-based messages improve the localization or understanding of the causes of
type errors, the qualitative evaluation suggests a real need and demand for
flow-based messages.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12641" title="Abstract">arXiv:2402.12641</a> [<a href="/pdf/2402.12641" title="Download PDF">pdf</a>, <a href="/format/2402.12641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional  and Large Kernel Design for Antenna Interference Source Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jintao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Rui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zebo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the era of 5G communication, removing interference sources that affect
communication is a resource-intensive task. The rapid development of computer
vision has enabled unmanned aerial vehicles to perform various high-altitude
detection tasks. Because the field of object detection for antenna interference
sources has not been fully explored, this industry lacks dedicated learning
samples and detection models for this specific task. In this article, an
antenna dataset is created to address important antenna interference source
detection issues and serves as the basis for subsequent research. We introduce
YOLO-Ant, a lightweight CNN and transformer hybrid detector specifically
designed for antenna interference source detection. Specifically, we initially
formulated a lightweight design for the network depth and width, ensuring that
subsequent investigations were conducted within a lightweight framework. Then,
we propose a DSLK-Block module based on depthwise separable convolution and
large convolution kernels to enhance the network's feature extraction ability,
effectively improving small object detection. To address challenges such as
complex backgrounds and large interclass differences in antenna detection, we
construct DSLKVit-Block, a powerful feature extraction module that combines
DSLK-Block and transformer structures. Considering both its lightweight design
and accuracy, our method not only achieves optimal performance on the antenna
dataset but also yields competitive results on public datasets.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12642" title="Abstract">arXiv:2402.12642</a> [<a href="/pdf/2402.12642" title="Download PDF">pdf</a>, <a href="/format/2402.12642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rampo: A CEGAR-based Integration of Binary Code Analysis and System  Falsification for Cyber-Kinetic Vulnerability Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsujio%2C+K">Kohei Tsujio</a>, 
<a href="/search/cs?searchtype=author&query=Faruque%2C+M+A+A">Mohammad Abdullah Al Faruque</a>, 
<a href="/search/cs?searchtype=author&query=Shoukry%2C+Y">Yasser Shoukry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper presents a novel tool, named Rampo, that can perform binary code
analysis to identify cyber kinetic vulnerabilities in CPS. The tool takes as
input a Signal Temporal Logic (STL) formula that describes the kinetic effect,
i.e., the behavior of the physical system, that one wants to avoid. The tool
then searches the possible cyber trajectories in the binary code that may lead
to such physical behavior. This search integrates binary code analysis tools
and hybrid systems falsification tools using a Counter-Example Guided
Abstraction Refinement (CEGAR) approach. Rampo starts by analyzing the binary
code to extract symbolic constraints that represent the different paths in the
code. These symbolic constraints are then passed to a Satisfiability Modulo
Theories (SMT) solver to extract the range of control signals that can be
produced by each path in the code. The next step is to search over possible
physical trajectories using a hybrid systems falsification tool that adheres to
the behavior of the cyber paths and yet leads to violations of the STL formula.
Since the number of cyber paths that need to be explored increases
exponentially with the length of physical trajectories, we iteratively perform
refinement of the cyber path constraints based on the previous falsification
result and traverse the abstract path tree obtained from the control program to
explore the search space of the system. To illustrate the practical utility of
binary code analysis in identifying cyber kinetic vulnerabilities, we present
case studies from diverse CPS domains, showcasing how they can be discovered in
their control programs. Our tool could compute the same number of
vulnerabilities while leading to a speedup that ranges from 3x to 98x.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12644" title="Abstract">arXiv:2402.12644</a> [<a href="/pdf/2402.12644" title="Download PDF">pdf</a>, <a href="/format/2402.12644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromorphic Synergy for Video Binarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shijie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiaowei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Bimodal objects, such as the checkerboard pattern used in camera calibration,
markers for object tracking, and text on road signs, to name a few, are
prevalent in our daily lives and serve as a visual form to embed information
that can be easily recognized by vision systems. While binarization from
intensity images is crucial for extracting the embedded information in the
bimodal objects, few previous works consider the task of binarization of blurry
images due to the relative motion between the vision sensor and the
environment. The blurry images can result in a loss in the binarization quality
and thus degrade the downstream applications where the vision system is in
motion. Recently, neuromorphic cameras offer new capabilities for alleviating
motion blur, but it is non-trivial to first deblur and then binarize the images
in a real-time manner. In this work, we propose an event-based binary
reconstruction method that leverages the prior knowledge of the bimodal
target's properties to perform inference independently in both event space and
image space and merge the results from both domains to generate a sharp binary
image. We also develop an efficient integration method to propagate this binary
image to high frame rate binary video. Finally, we develop a novel method to
naturally fuse events and images for unsupervised threshold identification. The
proposed method is evaluated in publicly available and our collected data
sequence, and shows the proposed method can outperform the SOTA methods to
generate high frame rate binary video in real-time on CPU-only devices.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12645" title="Abstract">arXiv:2402.12645</a> [<a href="/pdf/2402.12645" title="Download PDF">pdf</a>, <a href="/format/2402.12645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirahara%2C+S">Shuichi Hirahara</a>, 
<a href="/search/cs?searchtype=author&query=Ohsaka%2C+N">Naoto Ohsaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In the Minmax Set Cover Reconfiguration problem, given a set system
$\mathcal{F}$ over a universe and its two covers $\mathcal{C}^\mathsf{start}$
and $\mathcal{C}^\mathsf{goal}$ of size $k$, we wish to transform
$\mathcal{C}^\mathsf{start}$ into $\mathcal{C}^\mathsf{goal}$ by repeatedly
adding or removing a single set of $\mathcal{F}$ while covering the universe in
any intermediate state. Then, the objective is to minimize the maximize size of
any intermediate cover during transformation. We prove that Minmax Set Cover
Reconfiguration and Minmax Dominating Set Reconfiguration are
$\mathsf{PSPACE}$-hard to approximate within a factor of
$2-\frac{1}{\operatorname{polyloglog} N}$, where $N$ is the size of the
universe and the number of vertices in a graph, respectively, improving upon
Ohsaka (SODA 2024) and Karthik C. S. and Manurangsi (2023). This is the first
result that exhibits a sharp threshold for the approximation factor of any
reconfiguration problem because both problems admit a $2$-factor approximation
algorithm as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno
(Theor. Comput. Sci., 2011). Our proof is based on a reconfiguration analogue
of the FGLSS reduction from Probabilistically Checkable Reconfiguration Proofs
of Hirahara and Ohsaka (2024). We also prove that for any constant $\varepsilon
\in (0,1)$, Minmax Hypergraph Vertex Cover Reconfiguration on
$\operatorname{poly}(\varepsilon^{-1})$-uniform hypergraphs is
$\mathsf{PSPACE}$-hard to approximate within a factor of $2-\varepsilon$.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12646" title="Abstract">arXiv:2402.12646</a> [<a href="/pdf/2402.12646" title="Download PDF">pdf</a>, <a href="/format/2402.12646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Artificial Neural Networks by Coordinate Search Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rokhsatyazdi%2C+E">Ehsan Rokhsatyazdi</a>, 
<a href="/search/cs?searchtype=author&query=Rahnamayan%2C+S">Shahryar Rahnamayan</a>, 
<a href="/search/cs?searchtype=author&query=Miyandoab%2C+S+Z">Sevil Zanjani Miyandoab</a>, 
<a href="/search/cs?searchtype=author&query=Bidgoli%2C+A+A">Azam Asilian Bidgoli</a>, 
<a href="/search/cs?searchtype=author&query=Tizhoosh%2C+H+R">H.R. Tizhoosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE Symposium Series on Computational Intelligence (SSCI),
  pp. 1540-1546. IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Training Artificial Neural Networks poses a challenging and critical problem
in machine learning. Despite the effectiveness of gradient-based learning
methods, such as Stochastic Gradient Descent (SGD), in training neural
networks, they do have several limitations. For instance, they require
differentiable activation functions, and cannot optimize a model based on
several independent non-differentiable loss functions simultaneously; for
example, the F1-score, which is used during testing, can be used during
training when a gradient-free optimization algorithm is utilized. Furthermore,
the training in any DNN can be possible with a small size of the training
dataset. To address these concerns, we propose an efficient version of the
gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern
Search methods, for training neural networks. The proposed algorithm can be
used with non-differentiable activation functions and tailored to
multi-objective/multi-loss problems. Finding the optimal values for weights of
ANNs is a large-scale optimization problem. Therefore instead of finding the
optimal value for each variable, which is the common technique in classical CS,
we accelerate optimization and convergence by bundling the weights. In fact,
this strategy is a form of dimension reduction for optimization problems. Based
on the experimental results, the proposed method, in some cases, outperforms
the gradient-based approach, particularly, in situations with insufficient
labeled training data. The performance plots demonstrate a high convergence
rate, highlighting the capability of our suggested method to find a reasonable
solution with fewer function calls. As of now, the only practical and efficient
way of training ANNs with hundreds of thousands of weights is gradient-based
algorithms such as SGD or Adam. In this paper we introduce an alternative
method for training ANN.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12647" title="Abstract">arXiv:2402.12647</a> [<a href="/pdf/2402.12647" title="Download PDF">pdf</a>, <a href="/format/2402.12647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal  Category-level Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+T">Takuya Ikeda</a>, 
<a href="/search/cs?searchtype=author&query=Zakharov%2C+S">Sergey Zakharov</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+T">Tianyi Ko</a>, 
<a href="/search/cs?searchtype=author&query=Irshad%2C+M+Z">Muhammad Zubair Irshad</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R">Robert Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Katherine Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ambrus%2C+R">Rares Ambrus</a>, 
<a href="/search/cs?searchtype=author&query=Nishiwaki%2C+K">Koichi Nishiwaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper addresses the challenging problem of category-level pose
estimation. Current state-of-the-art methods for this task face challenges when
dealing with symmetric objects and when attempting to generalize to new
environments solely through synthetic data training. In this work, we address
these challenges by proposing a probabilistic model that relies on diffusion to
estimate dense canonical maps crucial for recovering partial object shapes as
well as establishing correspondences essential for pose estimation.
Furthermore, we introduce critical components to enhance performance by
leveraging the strength of the diffusion models with multi-modal input
representations. We demonstrate the effectiveness of our method by testing it
on a range of real datasets. Despite being trained solely on our generated
synthetic data, our approach achieves state-of-the-art performance and
unprecedented generalization qualities, outperforming baselines, even those
specifically trained on the target domain.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12649" title="Abstract">arXiv:2402.12649</a> [<a href="/pdf/2402.12649" title="Download PDF">pdf</a>, <a href="/format/2402.12649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lum%2C+K">Kristian Lum</a>, 
<a href="/search/cs?searchtype=author&query=Anthis%2C+J+R">Jacy Reese Anthis</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+C">Chirag Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=D%27Amour%2C+A">Alexander D&#x27;Amour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Bias benchmarks are a popular method for studying the negative impacts of
bias in LLMs, yet there has been little empirical investigation of whether
these benchmarks are actually indicative of how real world harm may manifest in
the real world. In this work, we study the correspondence between such
decontextualized "trick tests" and evaluations that are more grounded in
Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this
correlation in the context of gender-occupation bias--a popular genre of bias
evaluation. We compare three de-contextualized evaluations adapted from the
current literature to three analogous RUTEd evaluations applied to long-form
content generation. We conduct each evaluation for seven instruction-tuned
LLMs. For the RUTEd evaluations, we conduct repeated trials of three text
generation tasks: children's bedtime stories, user personas, and English
language learning exercises. We found no correspondence between trick tests and
RUTEd evaluations. Specifically, selecting the least biased model based on the
de-contextualized results coincides with selecting the model with the best
performance on RUTEd evaluations only as often as random chance. We conclude
that evaluations that are not based in realistic use are likely insufficient to
mitigate and assess bias and real-world harms.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12652" title="Abstract">arXiv:2402.12652</a> [<a href="/pdf/2402.12652" title="Download PDF">pdf</a>, <a href="/format/2402.12652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDEformer: Towards a Foundation Model for One-Dimensional Partial  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ye%2C+Z">Zhanhong Ye</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xiang Huang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Leheng Chen</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hongsheng Liu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zidong Wang</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+B">Bin Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper introduces PDEformer, a neural solver for partial differential
equations (PDEs) capable of simultaneously addressing various types of PDEs. We
advocate representing the PDE in the form of a computational graph,
facilitating the seamless integration of both symbolic and numerical
information inherent in a PDE. A graph Transformer and an implicit neural
representation (INR) are employed to generate mesh-free predicted solutions.
Following pretraining on data exhibiting a certain level of diversity, our
model achieves zero-shot accuracies on benchmark datasets that surpass those of
adequately trained expert models. Additionally, PDEformer demonstrates
promising results in the inverse problem of PDE coefficient recovery.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12653" title="Abstract">arXiv:2402.12653</a> [<a href="/pdf/2402.12653" title="Download PDF">pdf</a>, <a href="/format/2402.12653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased Estimation for Total Treatment Effect Under Interference Using  Aggregated Dyadic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Lu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yilin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">JingJing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Applications (stat.AP)

</div>
<p class="mathjax">In social media platforms, user behavior is often influenced by interactions
with other users, complicating the accurate estimation of causal effects in
traditional A/B experiments. This study investigates situations where an
individual's outcome can be broken down into the sum of multiple pairwise
outcomes, a reflection of user interactions. These outcomes, referred to as
dyadic data, are prevalent in many social network contexts. Utilizing a
Bernoulli randomized design, we introduce a novel unbiased estimator for the
total treatment effect (TTE), which quantifies the difference in population
mean when all individuals are assigned to treatment versus control groups. We
further explore the bias of our estimator in scenarios where it is impractical
to include all individuals in the experiment, a common constraint in online
control experiments. Our numerical results reveal that our proposed estimator
consistently outperforms some commonly used estimators, underscoring its
potential for more precise causal effect estimation in social media
environments.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12654" title="Abstract">arXiv:2402.12654</a> [<a href="/pdf/2402.12654" title="Download PDF">pdf</a>, <a href="/format/2402.12654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech  Recognition, Translation, and Language Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sudo%2C+Y">Yui Sudo</a>, 
<a href="/search/cs?searchtype=author&query=Shakeel%2C+M">Muhammad Shakeel</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">There has been an increasing interest in large speech models that can perform
multiple speech processing tasks in a single model. Such models usually adopt
the encoder-decoder or decoder-only architecture due to their popularity and
good performance in many domains. However, autoregressive models can be slower
during inference compared to non-autoregressive models and also have potential
risks of hallucination. Though prior studies observed promising results of
non-autoregressive models for certain tasks at small scales, it remains unclear
if they can be scaled to speech-to-text generation in diverse languages and
tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we
propose OWSM-CTC, a novel encoder-only speech foundation model based on
Connectionist Temporal Classification (CTC). It is trained on 180k hours of
public audio data for multilingual automatic speech recognition (ASR), speech
translation (ST), and language identification (LID). Compared to
encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up
to 25% relative improvement on ST, while it is more robust and 3 to 4 times
faster for inference. OWSM-CTC also improves the long-form ASR result with 20x
speed-up. We will publicly release our codebase, pre-trained model, and
training logs to promote open science in speech foundation models.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12655" title="Abstract">arXiv:2402.12655</a> [<a href="/pdf/2402.12655" title="Download PDF">pdf</a>, <a href="/format/2402.12655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ego Group Partition: A Novel Framework for Improving Ego Experiments in  Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Lu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">JingJing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Estimating the average treatment effect in social networks is challenging due
to individuals influencing each other. One approach to address interference is
ego cluster experiments, where each cluster consists of a central individual
(ego) and its peers (alters). Clusters are randomized, and only the effects on
egos are measured. In this work, we propose an improved framework for ego
cluster experiments called ego group partition (EGP), which directly generates
two groups and an ego sub-population instead of ego clusters. Under specific
model assumptions, we propose two ego group partition algorithms. Compared to
the original ego clustering algorithm, our algorithms produce more egos, yield
smaller biases, and support parallel computation. The performance of our
algorithms is validated through simulation and real-world case studies.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12656" title="Abstract">arXiv:2402.12656</a> [<a href="/pdf/2402.12656" title="Download PDF">pdf</a>, <a href="/ps/2402.12656" title="Download PostScript">ps</a>, <a href="/format/2402.12656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperMoE: Towards Better Mixture of Experts via Transferring Among  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zihan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Mixture of Experts (MoE) for language models has been proven effective in
augmenting the capacity of models by dynamically routing each input token to a
specific subset of experts for processing. Despite the success, most existing
methods face a challenge for balance between sparsity and the availability of
expert knowledge: enhancing performance through increased use of expert
knowledge often results in diminishing sparsity during expert selection. To
mitigate this contradiction, we propose HyperMoE, a novel MoE framework built
upon Hypernetworks. This framework integrates the computational processes of
MoE with the concept of knowledge transferring in multi-task learning. Specific
modules generated based on the information of unselected experts serve as
supplementary information, which allows the knowledge of experts not selected
to be used while maintaining selection sparsity. Our comprehensive empirical
evaluations across multiple datasets and backbones establish that HyperMoE
significantly outperforms existing MoE methods under identical conditions
concerning the number of experts.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12658" title="Abstract">arXiv:2402.12658</a> [<a href="/pdf/2402.12658" title="Download PDF">pdf</a>, <a href="/format/2402.12658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding the underwater acoustic target recognition with interpretable  contrastive learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Ji Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> OCEANS 2023-Limerick. IEEE, 2023: 1-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recognizing underwater targets from acoustic signals is a challenging task
owing to the intricate ocean environments and variable underwater channels.
While deep learning-based systems have become the mainstream approach for
underwater acoustic target recognition, they have faced criticism for their
lack of interpretability and weak generalization performance in practical
applications. In this work, we apply the class activation mapping (CAM) to
generate visual explanations for the predictions of a spectrogram-based
recognition system. CAM can help to understand the behavior of recognition
models by highlighting the regions of the input features that contribute the
most to the prediction. Our explorations reveal that recognition models tend to
focus on the low-frequency line spectrum and high-frequency periodic modulation
information of underwater signals. Based on the observation, we propose an
interpretable contrastive learning (ICL) strategy that employs two encoders to
learn from acoustic features with different emphases (line spectrum and
modulation information). By imposing constraints between encoders, the proposed
strategy can enhance the generalization performance of the recognition system.
Our experiments demonstrate that the proposed contrastive learning approach can
improve the recognition accuracy and bring significant improvements across
various underwater databases.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12659" title="Abstract">arXiv:2402.12659</a> [<a href="/pdf/2402.12659" title="Download PDF">pdf</a>, <a href="/format/2402.12659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The FinBen: An Holistic Financial Benchmark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+R">Ruoyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yueru He</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mengxi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yijing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Haoqiang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Ziyan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenhan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zheheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guojun Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuechen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhiyuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haohang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yangyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Gang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiajia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Lira%2C+A">Alejandro Lopez-Lira</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yanzhao Lai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+M">Min Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">LLMs have transformed NLP and shown promise in various fields, yet their
potential in finance is underexplored due to a lack of thorough evaluations and
the complexity of financial tasks. This along with the rapid development of
LLMs, highlights the urgent need for a systematic financial evaluation
benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive
open-sourced evaluation benchmark, specifically designed to thoroughly assess
the capabilities of LLMs in the financial domain. FinBen encompasses 35
datasets across 23 financial tasks, organized into three spectrums of
difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs'
cognitive abilities in inductive reasoning, associative memory, quantitative
reasoning, crystallized intelligence, and more. Our evaluation of 15
representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals
insights into their strengths and limitations within the financial domain. The
findings indicate that GPT-4 leads in quantification, extraction, numerical
reasoning, and stock trading, while Gemini shines in generation and
forecasting; however, both struggle with complex extraction and forecasting,
showing a clear need for targeted enhancements. Instruction tuning boosts
simple task performance but falls short in improving complex reasoning and
forecasting abilities. FinBen seeks to continuously evaluate LLMs in finance,
fostering AI development with regular updates of tasks and models.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12660" title="Abstract">arXiv:2402.12660</a> [<a href="/pdf/2402.12660" title="Download PDF">pdf</a>, <a href="/format/2402.12660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SingVisio: Visual Analytics of Diffusion Model for Singing Voice  Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Human-Computer Interaction (cs.HC); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this study, we present SingVisio, an interactive visual analysis system
that aims to explain the diffusion model used in singing voice conversion.
SingVisio provides a visual display of the generation process in diffusion
models, showcasing the step-by-step denoising of the noisy spectrum and its
transformation into a clean spectrum that captures the desired singer's timbre.
The system also facilitates side-by-side comparisons of different conditions,
such as source content, melody, and target timbre, highlighting the impact of
these conditions on the diffusion generation process and resulting conversions.
Through comprehensive evaluations, SingVisio demonstrates its effectiveness in
terms of system design, functionality, explainability, and user-friendliness.
It offers users of various backgrounds valuable learning experiences and
insights into the diffusion model for singing voice conversion.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12663" title="Abstract">arXiv:2402.12663</a> [<a href="/pdf/2402.12663" title="Download PDF">pdf</a>, <a href="/format/2402.12663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoftQE: Learned Representations of Queries Expanded by LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pimpalkhute%2C+V">Varad Pimpalkhute</a>, 
<a href="/search/cs?searchtype=author&query=Heyer%2C+J">John Heyer</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xusen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sameer Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ECIR 2024 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the integration of Large Language Models (LLMs) into query
encoders to improve dense retrieval without increasing latency and cost, by
circumventing the dependency on LLMs at inference time. SoftQE incorporates
knowledge from LLMs by mapping embeddings of input queries to those of the
LLM-expanded queries. While improvements over various strong baselines on
in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83
absolute percentage points on average on five out-of-domain BEIR tasks.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12664" title="Abstract">arXiv:2402.12664</a> [<a href="/pdf/2402.12664" title="Download PDF">pdf</a>, <a href="/format/2402.12664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminant Distance-Aware Representation on Deterministic Uncertainty  Quantification Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamalika Das</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sricharan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Uncertainty estimation is a crucial aspect of deploying dependable deep
learning models in safety-critical systems. In this study, we introduce a novel
and efficient method for deterministic uncertainty estimation called
Discriminant Distance-Awareness Representation (DDAR). Our approach involves
constructing a DNN model that incorporates a set of prototypes in its latent
representations, enabling us to analyze valuable feature information from the
input data. By leveraging a distinction maximization layer over optimal
trainable prototypes, DDAR can learn a discriminant distance-awareness
representation. We demonstrate that DDAR overcomes feature collapse by relaxing
the Lipschitz constraint that hinders the practicality of deterministic
uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a
flexible and architecture-agnostic method that can be easily integrated as a
pluggable layer with distance-sensitive metrics, outperforming state-of-the-art
uncertainty estimation methods on multiple benchmark problems.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12665" title="Abstract">arXiv:2402.12665</a> [<a href="/pdf/2402.12665" title="Download PDF">pdf</a>, <a href="/format/2402.12665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Antifragile Perimeter Control: Anticipating and Gaining from Disruptions  with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+L">Linghang Sun</a>, 
<a href="/search/eess?searchtype=author&query=Makridis%2C+M+A">Michail A. Makridis</a>, 
<a href="/search/eess?searchtype=author&query=Genser%2C+A">Alexander Genser</a>, 
<a href="/search/eess?searchtype=author&query=Axenie%2C+C">Cristian Axenie</a>, 
<a href="/search/eess?searchtype=author&query=Grossi%2C+M">Margherita Grossi</a>, 
<a href="/search/eess?searchtype=author&query=Kouvelas%2C+A">Anastasios Kouvelas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The optimal operation of transportation networks is often susceptible to
unexpected disruptions, such as traffic incidents and social events. Many
established control strategies rely on mathematical models that struggle to
cope with real-world uncertainties, leading to a significant decline in
effectiveness when faced with substantial disruptions. While previous research
works have dedicated efforts to improving the robustness or resilience of
transportation systems against disruptions, this paper applies the cutting-edge
concept of antifragility to better design a traffic control strategy for urban
road networks. Antifragility sets itself apart from robustness and resilience
as it represents a system's ability to not only withstand stressors, shocks,
and volatility but also thrive and enhance performance in the presence of such
adversarial events. Hence, modern transportation systems call for solutions
that are antifragile. In this work, we propose a model-free deep Reinforcement
Learning (RL) scheme to control a two-region urban traffic perimeter network.
The system exploits the learning capability of RL under disruptions to achieve
antifragility. By monitoring the change rate and curvature of the traffic state
with the RL framework, the proposed algorithm anticipates imminent disruptions.
An additional term is also integrated into the RL algorithm as redundancy to
improve the performance under disruption scenarios. When compared to a
state-of-the-art model predictive control approach and a state-of-the-art RL
algorithm, our proposed method demonstrates two antifragility-related
properties: (a) gradual performance improvement under disruptions of constant
magnitude; and (b) increasingly superior performance under growing disruptions.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12666" title="Abstract">arXiv:2402.12666</a> [<a href="/pdf/2402.12666" title="Download PDF">pdf</a>, <a href="/format/2402.12666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning  for End-to-end Navigation of Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongbo Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous driving (AD) technology, leveraging artificial intelligence,
strives for vehicle automation. End-toend strategies, emerging to simplify
traditional driving systems by integrating perception, decision-making, and
control, offer new avenues for advanced driving functionalities. Despite their
potential, current challenges include data efficiency, training complexities,
and poor generalization. This study addresses these issues with a novel
end-to-end AD training model, enhancing system adaptability and intelligence.
The model incorporates a Transformer module into the policy network, undergoing
initial behavior cloning (BC) pre-training for update gradients. Subsequently,
fine-tuning through reinforcement learning with human guidance (RLHG) adapts
the model to specific driving environments, aiming to surpass the performance
limits of imitation learning (IL). The fine-tuning process involves human
interactions, guiding the model to acquire more efficient and safer driving
behaviors through supervision, intervention, demonstration, and reward
feedback. Simulation results demonstrate that this framework accelerates
learning, achieving precise control and significantly enhancing safety and
reliability. Compared to other advanced baseline methods, the proposed approach
excels in challenging AD tasks. The introduction of the Transformer module and
human-guided fine-tuning provides valuable insights and methods for research
and applications in the AD field.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12667" title="Abstract">arXiv:2402.12667</a> [<a href="/pdf/2402.12667" title="Download PDF">pdf</a>, <a href="/format/2402.12667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remote Possibilities: Where there is a WIL, is there a Way? AI Education  for Remote Learners in a New Era of Work-Integrated-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacoby%2C+D">Derek Jacoby</a>, 
<a href="/search/cs?searchtype=author&query=Savage%2C+S">Saiph Savage</a>, 
<a href="/search/cs?searchtype=author&query=Coady%2C+Y">Yvonne Coady</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Increasing diversity in educational settings is challenging in part due to
the lack of access to resources for non-traditional learners in remote
communities. Post-pandemic platforms designed specifically for remote and
hybrid learning -- supporting team-based collaboration online -- are positioned
to bridge this gap. Our work combines the use of these new platforms with
co-creation and collaboration tools for AI assisted remote
Work-Integrated-Learning (WIL) opportunities, including efforts in community
and with the public library system. This paper outlines some of our experiences
to date, and proposes methods to further integrate AI education into
community-driven applications for remote WIL.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12669" title="Abstract">arXiv:2402.12669</a> [<a href="/pdf/2402.12669" title="Download PDF">pdf</a>, <a href="/format/2402.12669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lax-Wendroff Flux Reconstruction for advection-diffusion equations with  error-based time stepping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Babbar%2C+A">Arpit Babbar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekar%2C+P">Praveen Chandrashekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 Pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work introduces an extension of the high order, single stage
Lax-Wendroff Flux Reconstruction (LWFR) of Babbar et al., JCP (2022) to solve
second order time-dependent partial differential equations in conservative form
on curvilinear meshes. The method uses BR1 scheme to reduce the system to first
order so that the earlier LWFR scheme can be applied. The work makes use of the
embedded error-based time stepping introduced in Babbar, Chandrashekar (2024)
which becomes particularly relevant in the absence of CFL stability limit for
parabolic equations. The scheme is verified to show optimal order convergence
and validated with transonic flow over airfoil and unsteady flow over cylinder.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12670" title="Abstract">arXiv:2402.12670</a> [<a href="/pdf/2402.12670" title="Download PDF">pdf</a>, <a href="/format/2402.12670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Validation of Autonomous Vehicles Across Scales using an  Integrated Digital Twin Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samak%2C+T+V">Tanmay Vilas Samak</a>, 
<a href="/search/cs?searchtype=author&query=Samak%2C+C+V">Chinmay Vilas Samak</a>, 
<a href="/search/cs?searchtype=author&query=Krovi%2C+V+N">Venkat Narayan Krovi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous vehicle platforms of varying spatial scales are employed within
the research and development spectrum based on space, safety and monetary
constraints. However, deploying and validating autonomy algorithms across
varying operational scales presents challenges due to scale-specific dynamics,
sensor integration complexities, computational constraints, regulatory
considerations, environmental variability, interaction with other traffic
participants and scalability concerns. In such a milieu, this work focuses on
developing a unified framework for modeling and simulating digital twins of
autonomous vehicle platforms across different scales and operational design
domains (ODDs) to help support the streamlined development and validation of
autonomy software stacks. Particularly, this work discusses the development of
digital twin representations of 4 autonomous ground vehicles, which span across
3 different scales and target 3 distinct ODDs. We study the adoption of these
autonomy-oriented digital twins to deploy a common autonomy software stack with
an aim of end-to-end map-based navigation to achieve the ODD-specific
objective(s) for each vehicle. Finally, we also discuss the flexibility of the
proposed framework to support virtual, hybrid as well as physical testing with
seamless sim2real transfer.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12673" title="Abstract">arXiv:2402.12673</a> [<a href="/pdf/2402.12673" title="Download PDF">pdf</a>, <a href="/format/2402.12673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Worst-case Attacks: Robust RL with Adaptive Defense via  Non-dominated Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chenghao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR) 2024, spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In light of the burgeoning success of reinforcement learning (RL) in diverse
real-world applications, considerable focus has been directed towards ensuring
RL policies are robust to adversarial attacks during test time. Current
approaches largely revolve around solving a minimax problem to prepare for
potential worst-case scenarios. While effective against strong attacks, these
methods often compromise performance in the absence of attacks or the presence
of only weak attacks. To address this, we study policy robustness under the
well-accepted state-adversarial attack model, extending our focus beyond only
worst-case attacks. We first formalize this task at test time as a regret
minimization problem and establish its intrinsic hardness in achieving
sublinear regret when the baseline policy is from a general continuous policy
class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy
class $\Pi$ prior to test time, aiming for efficient adaptation within a finite
policy class $\Tilde{\Pi}$, which can resort to an adversarial bandit
subroutine. In light of the importance of a small, finite $\Tilde{\Pi}$, we
propose a novel training-time algorithm to iteratively discover
\textit{non-dominated policies}, forming a near-optimal and minimal
$\Tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency.
Empirical validation on the Mujoco corroborates the superiority of our approach
in terms of natural and robust performance, as well as adaptability to various
attack scenarios.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12675" title="Abstract">arXiv:2402.12675</a> [<a href="/pdf/2402.12675" title="Download PDF">pdf</a>, <a href="/format/2402.12675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative  Cognition Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puebla%2C+G">Guillermo Puebla</a>, 
<a href="/search/cs?searchtype=author&query=Bowers%2C+J+S">Jeffrey S. Bowers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Achieving visual reasoning is a long-term goal of artificial intelligence. In
the last decade, several studies have applied deep neural networks (DNNs) to
the task of learning visual relations from images, with modest results in terms
of generalization of the relations learned. However, in recent years,
object-centric representation learning has been put forward as a way to achieve
visual reasoning within the deep learning framework. Object-centric models
attempt to model input scenes as compositions of objects and relations between
them. To this end, these models use several kinds of attention mechanisms to
segregate the individual objects in a scene from the background and from other
objects. In this work we tested relation learning and generalization in several
object-centric models, as well as a ResNet-50 baseline. In contrast to previous
research, which has focused heavily in the same-different task in order to
asses relational reasoning in DNNs, we use a set of tasks -- with varying
degrees of difficulty -- derived from the comparative cognition literature. Our
results show that object-centric models are able to segregate the different
objects in a scene, even in many out-of-distribution cases. In our simpler
tasks, this improves their capacity to learn and generalize visual relations in
comparison to the ResNet-50 baseline. However, object-centric models still
struggle in our more difficult tasks and conditions. We conclude that abstract
visual reasoning remains an open challenge for DNNs, including object-centric
models.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12676" title="Abstract">arXiv:2402.12676</a> [<a href="/pdf/2402.12676" title="Download PDF">pdf</a>, <a href="/format/2402.12676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Monocular Video-Based Gait Analysis Using Motion Imitation  with Physics-Based Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smyrnakis%2C+N">Nikolaos Smyrnakis</a>, 
<a href="/search/cs?searchtype=author&query=Karakostas%2C+T">Tasos Karakostas</a>, 
<a href="/search/cs?searchtype=author&query=Cotton%2C+R+J">R. James Cotton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Gait analysis from videos obtained from a smartphone would open up many
clinical opportunities for detecting and quantifying gait impairments. However,
existing approaches for estimating gait parameters from videos can produce
physically implausible results. To overcome this, we train a policy using
reinforcement learning to control a physics simulation of human movement to
replicate the movement seen in video. This forces the inferred movements to be
physically plausible, while improving the accuracy of the inferred step length
and walking velocity.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12677" title="Abstract">arXiv:2402.12677</a> [<a href="/pdf/2402.12677" title="Download PDF">pdf</a>, <a href="/format/2402.12677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-level Geometric Structure Preserving for Natural Image Stitching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Wenxiao Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wankou Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The topic of stitching images with globally natural structures holds
paramount significance. Current methodologies exhibit the ability to preserve
local geometric structures, yet fall short in maintaining relationships between
these geometric structures. In this paper, we endeavor to safeguard the
overall, OBJect-level structures within images based on Global Similarity
Prior, while concurrently mitigating distortion and ghosting artifacts with
OBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric
structures with semantic information, enhancing the algorithm's ability to
preserve objects in a manner that aligns more intuitively with human
perception. We seek to identify spatial constraints that govern the
relationships between various geometric boundaries. Recognizing that multiple
geometric boundaries collectively define complete objects, we employ triangular
meshes to safeguard not only individual geometric structures but also the
overall shapes of objects within the images. Empirical evaluations across
multiple image stitching datasets demonstrate that our method establishes a new
state-of-the-art benchmark in image stitching. Our implementation and dataset
is publicly available at https://github.com/RussRobin/OBJ-GSP .
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12682" title="Abstract">arXiv:2402.12682</a> [<a href="/pdf/2402.12682" title="Download PDF">pdf</a>, <a href="/format/2402.12682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Mobility Digital Twin Based Automated Vehicle Navigation System: A  Proof of Concept
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kui Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zongdian Li</a>, 
<a href="/search/eess?searchtype=author&query=Nonomura%2C+K">Kazuma Nonomura</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/eess?searchtype=author&query=Sakaguchi%2C+K">Kei Sakaguchi</a>, 
<a href="/search/eess?searchtype=author&query=Hashash%2C+O">Omar Hashash</a>, 
<a href="/search/eess?searchtype=author&query=Saad%2C+W">Walid Saad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Digital twins (DTs) have driven major advancements across various industrial
domains over the past two decades. With the rapid advancements in autonomous
driving and vehicle-to-everything (V2X) technologies, integrating DTs into
vehicular platforms is anticipated to further revolutionize smart mobility
systems. In this paper, a new smart mobility DT (SMDT) platform is proposed for
the control of connected and automated vehicles (CAVs) over next-generation
wireless networks. In particular, the proposed platform enables cloud services
to leverage the abilities of DTs to promote the autonomous driving experience.
To enhance traffic efficiency and road safety measures, a novel navigation
system that exploits available DT information is designed. The SMDT platform
and navigation system are implemented with state-of-the-art products, e.g.,
CAVs and roadside units (RSUs), and emerging technologies, e.g., cloud and
cellular V2X (C-V2X). In addition, proof-of-concept (PoC) experiments are
conducted to validate system performance. The performance of SMDT is evaluated
from two standpoints: (i) the rewards of the proposed navigation system on
traffic efficiency and safety and, (ii) the latency and reliability of the SMDT
platform. Our experimental results using SUMO-based large-scale traffic
simulations show that the proposed SMDT can reduce the average travel time and
the blocking probability due to unexpected traffic incidents. Furthermore, the
results record a peak overall latency for DT modeling and route planning
services to be 155.15 ms and 810.59 ms, respectively, which validates that our
proposed design aligns with the 3GPP requirements for emerging V2X use cases
and fulfills the targets of the proposed design. Our demonstration video can be
found at https://youtu.be/3waQwlaHQkk.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12683" title="Abstract">arXiv:2402.12683</a> [<a href="/pdf/2402.12683" title="Download PDF">pdf</a>, <a href="/format/2402.12683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchCP: A Library for Conformal Prediction based on PyTorch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianguo Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Statistics Theory (math.ST)

</div>
<p class="mathjax">TorchCP is a Python toolbox for conformal prediction research on deep
learning models. It contains various implementations for posthoc and training
methods for classification and regression tasks (including multi-dimension
output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the
advantages of matrix computation to provide concise and efficient inference
implementations. The code is licensed under the LGPL license and is
open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this
https URL}}$.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12684" title="Abstract">arXiv:2402.12684</a> [<a href="/pdf/2402.12684" title="Download PDF">pdf</a>, <a href="/ps/2402.12684" title="Download PostScript">ps</a>, <a href="/format/2402.12684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolation by the Exact Inversion of the Gram Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Spitzer%2C+J">John Spitzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Created on Cocalc
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Using a lemma of Davis on Gram matrices applied to the classical Orthogonal
Polynomials to generate reproducing kernel interpolation over the classical
domains for polynomials. These kernels have terms which are exact over the
rational ring. The Condition Numbers are readily shown to get very large with
the size of the Gram matrices as expected. The calculation of the error
variances for trigonometric functions and the exponential show a significant
improvement over the equivalent Taylor expansion variances.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12685" title="Abstract">arXiv:2402.12685</a> [<a href="/pdf/2402.12685" title="Download PDF">pdf</a>, <a href="/format/2402.12685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XRL-Bench: A Benchmark for Evaluating and Comparing Explainable  Reinforcement Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhipeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Ye Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Runze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+K">Kai Guan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xingchen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Ji Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+T">Tangjie Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Changjie Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Reinforcement Learning (RL) has demonstrated substantial potential across
diverse fields, yet understanding its decision-making process, especially in
real-world scenarios where rationality and safety are paramount, is an ongoing
challenge. This paper delves in to Explainable RL (XRL), a subfield of
Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our
focus rests on state-explaining techniques, a crucial subset within XRL
methods, as they reveal the underlying factors influencing an agent's actions
at any given time. Despite their significant role, the lack of a unified
evaluation framework hinders assessment of their accuracy and effectiveness. To
address this, we introduce XRL-Bench, a unified standardized benchmark tailored
for the evaluation and comparison of XRL methods, encompassing three main
modules: standard RL environments, explainers based on state importance, and
standard evaluators. XRL-Bench supports both tabular and image data for state
explanation. We also propose TabularSHAP, an innovative and competitive XRL
method. We demonstrate the practical utility of TabularSHAP in real-world
online gaming services and offer an open-source benchmark platform for the
straightforward implementation and evaluation of XRL methods. Our contributions
facilitate the continued progression of XRL technology.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12686" title="Abstract">arXiv:2402.12686</a> [<a href="/pdf/2402.12686" title="Download PDF">pdf</a>, <a href="/format/2402.12686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Platform-Driven Collaboration Patterns: Structural Evolution Over Time  and Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maddah%2C+N">Negin Maddah</a>, 
<a href="/search/cs?searchtype=author&query=Heydari%2C+B">Babak Heydari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Within an increasingly digitalized organizational landscape, this research
delves into the dynamics of decentralized collaboration, contrasting it with
traditional collaboration models. An effective capturing of high-level
collaborations (beyond direct massages) is introduced as the network
construction methodology including both temporal and content dimensions of user
collaborations - an Alternating Timed Interaction (ATI) metric as the first
aspect, and a quantitative strategy of thematic similarity as the second
aspect. This study validates three hypotheses that collectively underscore the
complexities of digital team dynamics within sociotechnical systems: Firstly,
it establishes the significant influence of problem context on team structures
in work environments, emphasizing the need to consider the specific nature of
tasks in analyzing collaborative dynamics. Secondly, the study reveals specific
evolving patterns of team structures on digital platforms concerning team size
and artifact maturity. Lastly, it identifies substantial differences in team
structure patterns between digital platforms and traditional organizational
settings, underscoring the unexplored nature of digital collaboration dynamics.
The findings of this study are instrumental for organizations navigating the
digital era, offering insights into effective knowledge sharing in the
decentralized leadership of digital teams. By mapping out network structures
and collaborative patterns, this study, with a focus on Wikipedia as a
representative digital platform, paves the way for strategic interventions to
optimize digital team dynamics and align them with broader organizational
goals.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12687" title="Abstract">arXiv:2402.12687</a> [<a href="/pdf/2402.12687" title="Download PDF">pdf</a>, <a href="/format/2402.12687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning on manifolds without manifold learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mhaskar%2C+H+N">H. N. Mhaskar</a>, 
<a href="/search/cs?searchtype=author&query=O%27Dowd%2C+R">Ryan O&#x27;Dowd</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Function approximation based on data drawn randomly from an unknown
distribution is an important problem in machine learning. In contrast to the
prevalent paradigm of solving this problem by minimizing a loss functional, we
have given a direct one-shot construction together with optimal error bounds
under the manifold assumption; i.e., one assumes that the data is sampled from
an unknown sub-manifold of a high dimensional Euclidean space. A great deal of
research deals with obtaining information about this manifold, such as the
eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and
using this information for function approximation. This two step approach
implies some extra errors in the approximation stemming from basic quantities
of the data in addition to the errors inherent in function approximation. In
Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to
achieve function approximation without requiring the extraction of any
information about the manifold other than its dimension. However, one cannot
pin down the class of approximants used in that paper.
<br />In this paper, we view the unknown manifold as a sub-manifold of an ambient
hypersphere and study the question of constructing a one-shot approximation
using the spherical polynomials based on the hypersphere. Our approach does not
require preprocessing of the data to obtain information about the manifold
other than its dimension. We give optimal rates of approximation for relatively
"rough" functions.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12688" title="Abstract">arXiv:2402.12688</a> [<a href="/pdf/2402.12688" title="Download PDF">pdf</a>, <a href="/format/2402.12688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust-Wide: Robust Watermarking against Instruction-driven Image  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Runyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Instruction-driven image editing allows users to quickly edit an image
according to text instructions in a forward pass. Nevertheless, malicious users
can easily exploit this technique to create fake images, which could cause a
crisis of trust and harm the rights of the original image owners. Watermarking
is a common solution to trace such malicious behavior. Unfortunately,
instruction-driven image editing can significantly change the watermarked image
at the semantic level, making it less robust and effective. We propose
Robust-Wide, the first robust watermarking methodology against
instruction-driven image editing. Specifically, we adopt the widely-used
encoder-decoder framework for watermark embedding and extraction. To achieve
robustness against semantic distortions, we introduce a novel Partial
Instruction-driven Denoising Sampling Guidance (PIDSG) module, which consists
of a large variety of instruction injections and substantial modifications of
images at different semantic levels. With PIDSG, the encoder tends to embed the
watermark into more robust and semantic-aware areas, which remains in existence
even after severe image editing. Experiments demonstrate that Robust-Wide can
effectively extract the watermark from the edited image with a low bit error
rate of nearly 2.6% for 64-bit watermark messages. Meanwhile, it only induces a
neglectable influence on the visual quality and editability of the original
images. Moreover, Robust-Wide holds general robustness against different
sampling configurations and other image editing methods such as
ControlNet-InstructPix2Pix, MagicBrush, Inpainting and DDIM Inversion.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12690" title="Abstract">arXiv:2402.12690</a> [<a href="/pdf/2402.12690" title="Download PDF">pdf</a>, <a href="/format/2402.12690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simpson&#x27;s Paradox and the Accuracy-Fluency Tradeoff in Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+Z+W">Zheng Wei Lim</a>, 
<a href="/search/cs?searchtype=author&query=Vylomova%2C+E">Ekaterina Vylomova</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+T">Trevor Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Kemp%2C+C">Charles Kemp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A good translation should be faithful to the source and should respect the
norms of the target language. We address a theoretical puzzle about the
relationship between these objectives. On one hand, intuition and some prior
work suggest that accuracy and fluency should trade off against each other, and
that capturing every detail of the source can only be achieved at the cost of
fluency. On the other hand, quality assessment researchers often suggest that
accuracy and fluency are highly correlated and difficult for human raters to
distinguish (Callison-Burch et al. 2007). We show that the tension between
these views is an instance of Simpson's paradox, and that accuracy and fluency
are positively correlated at the level of the corpus but trade off at the level
of individual source segments. We further suggest that the relationship between
accuracy and fluency is best evaluated at the segment (or sentence) level, and
that the trade off between these dimensions has implications both for assessing
translation quality and developing improved MT systems.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12691" title="Abstract">arXiv:2402.12691</a> [<a href="/pdf/2402.12691" title="Download PDF">pdf</a>, <a href="/format/2402.12691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Planted Transformers: Large Language Models with Implicit Syntactic  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+R">Ryo Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Someya%2C+T">Taiga Someya</a>, 
<a href="/search/cs?searchtype=author&query=Oseki%2C+Y">Yohei Oseki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable success thanks to
scalability on large text corpora, but have some drawback in training
efficiency. In contrast, Syntactic Language Models (SLMs) can be trained
efficiently to reach relatively high performance thanks to syntactic
supervision, but have trouble with scalability. Thus, given these complementary
advantages of LLMs and SLMs, it is necessary to develop an architecture that
integrates the scalability of LLMs with the training efficiency of SLMs, namely
Syntactic Large Language Models (SLLM). In this paper, we propose a novel
method dubbed tree-planting: implicitly "plant" trees into attention weights of
Transformer LMs to reflect syntactic structures of natural language.
Specifically, Transformer LMs trained with tree-planting will be called
Tree-Planted Transformers (TPT), which learn syntax on small treebanks via
tree-planting and then scale on large text corpora via continual learning with
syntactic scaffolding. Targeted syntactic evaluations on the SyntaxGym
benchmark demonstrated that TPTs, despite the lack of explicit syntactic
supervision, significantly outperformed various SLMs with explicit syntactic
supervision that generate hundreds of syntactic structures in parallel,
suggesting that tree-planting and TPTs are the promising foundation for SLLMs.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12692" title="Abstract">arXiv:2402.12692</a> [<a href="/pdf/2402.12692" title="Download PDF">pdf</a>, <a href="/format/2402.12692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormulaQA: A Question Answering Dataset for Formula-Based Numerical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sichen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bolin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=liu%2C+Y">Yiwei liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Gong Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The application of formulas is a fundamental ability of humans when
addressing numerical reasoning problems. However, existing numerical reasoning
datasets seldom explicitly indicate the formulas employed during the reasoning
steps. To bridge this gap, we propose a question answering dataset for
formula-based numerical reasoning called FormulaQA, from junior high school
physics examinations. We further conduct evaluations on LLMs with size ranging
from 7B to over 100B parameters utilizing zero-shot and few-shot
chain-of-thoughts methods and we explored the approach of using
retrieval-augmented LLMs when providing an external formula database. We also
fine-tune on smaller models with size not exceeding 2B. Our empirical findings
underscore the significant potential for improvement in existing models when
applied to our complex, formula-driven FormulaQA.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12694" title="Abstract">arXiv:2402.12694</a> [<a href="/pdf/2402.12694" title="Download PDF">pdf</a>, <a href="/format/2402.12694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revitalizing Multivariate Time Series Forecasting: Learnable  Decomposition with Inter-Series Dependencies and Intra-Series Variations  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guoqi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jing Zou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaowei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A+I">Angelica I. Aviles-Rivero</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shujun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Predicting multivariate time series is crucial, demanding precise modeling of
intricate patterns, including inter-series dependencies and intra-series
variations. Distinctive trend characteristics in each time series pose
challenges, and existing methods, relying on basic moving average kernels, may
struggle with the non-linear structure and complex trends in real-world data.
Given that, we introduce a learnable decomposition strategy to capture dynamic
trend information more reasonably. Additionally, we propose a dual attention
module tailored to capture inter-series dependencies and intra-series
variations simultaneously for better time series forecasting, which is
implemented by channel-wise self-attention and autoregressive self-attention.
To evaluate the effectiveness of our method, we conducted experiments across
eight open-source datasets and compared it with the state-of-the-art methods.
Through the comparison results, our Leddam (LEarnable Decomposition and Dual
Attention Module) not only demonstrates significant advancements in predictive
performance, but also the proposed decomposition strategy can be plugged into
other methods with a large performance-boosting, from 11.87% to 48.56% MSE
error degradation.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12702" title="Abstract">arXiv:2402.12702</a> [<a href="/pdf/2402.12702" title="Download PDF">pdf</a>, <a href="/format/2402.12702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Cloud to Edge: Rethinking Generative AI for Low-Resource Design  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vuruma%2C+S+K+R">Sai Krishna Revanth Vuruma</a>, 
<a href="/search/cs?searchtype=author&query=Margetts%2C+A">Ashley Margetts</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jianhai Su</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+B">Biplav Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Generative Artificial Intelligence (AI) has shown tremendous prospects in all
aspects of technology, including design. However, due to its heavy demand on
resources, it is usually trained on large computing infrastructure and often
made available as a cloud-based service. In this position paper, we consider
the potential, challenges, and promising approaches for generative AI for
design on the edge, i.e., in resource-constrained settings where memory,
compute, energy (battery) and network connectivity may be limited. Adapting
generative AI for such settings involves overcoming significant hurdles,
primarily in how to streamline complex models to function efficiently in
low-resource environments. This necessitates innovative approaches in model
compression, efficient algorithmic design, and perhaps even leveraging edge
computing. The objective is to harness the power of generative AI in creating
bespoke solutions for design problems, such as medical interventions, farm
equipment maintenance, and educational material design, tailored to the unique
constraints and needs of remote areas. These efforts could democratize access
to advanced technology and foster sustainable development, ensuring universal
accessibility and environmental consideration of AI-driven design benefits.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12705" title="Abstract">arXiv:2402.12705</a> [<a href="/pdf/2402.12705" title="Download PDF">pdf</a>, <a href="/ps/2402.12705" title="Download PostScript">ps</a>, <a href="/format/2402.12705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distance Recoloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+N">Niranka Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Engels%2C+C">Christian Engels</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+A">Duc A. Hoang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Coloring a graph is a well known problem and used in many different contexts.
Here we want to assign $k \geq 1$ colors to each vertex of a graph $G$ such
that each edge has two different colors at each endpoint. Such a
vertex-coloring, if exists, is called a feasible coloring of $G$.
\textsc{Distance Coloring} is an extension to the standard \textsc{Coloring}
problem. Here we want to enforce that every pair of distinct vertices of
distance less than or equal to $d$ have different colors, for integers $d \geq
1$ and $k \geq d+1$.
<br />Reconfiguration problems ask if two given configurations can be transformed
into each other with certain rules. For example, the well-known
\textsc{Coloring Reconfiguration} asks if there is a way to change one vertex's
color at a time, starting from a feasible given coloring $\alpha$ of a graph
$G$ to reach another feasible given coloring $\beta$ of $G$, such that all
intermediate colorings are also feasible. In this paper, we study the
reconfiguration of distance colorings on certain graph classes.
<br />We show that even for planar, bipartite, and $2$-degenerate graphs,
reconfiguring distance colorings is $\mathsf{PSPACE}$-complete for $d \geq 2$
and $k = \Omega(d^2)$ via a reduction from the well-known \textsc{Sliding
Tokens} problem. Additionally, we show that the problem on split graphs remains
$\mathsf{PSPACE}$-complete when $d = 2$ and large $k$ but can be solved in
polynomial time when $d \geq 3$ and $k \geq d+1$, and design a quadratic-time
algorithm to solve the problem on paths for any $d \geq 2$ and $k \geq d+1$.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12706" title="Abstract">arXiv:2402.12706</a> [<a href="/pdf/2402.12706" title="Download PDF">pdf</a>, <a href="/format/2402.12706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Domain-Invariant Temporal Dynamics for Few-Shot Action  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuke Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Abramowitz%2C+B">Ben Abramowitz</a>, 
<a href="/search/cs?searchtype=author&query=Anzellott%2C+S">Stefano Anzellott</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot action recognition aims at quickly adapting a pre-trained model to
the novel data with a distribution shift using only a limited number of
samples. Key challenges include how to identify and leverage the transferable
knowledge learned by the pre-trained model. Our central hypothesis is that
temporal invariance in the dynamic system between latent variables lends itself
to transferability (domain-invariance). We therefore propose DITeD, or
Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the
temporal invariance part, we propose a generative framework with a two-stage
training strategy during pre-training. Specifically, we explicitly model
invariant dynamics including temporal dynamic generation and transitions, and
the variant visual and domain encoders. Then we pre-train the model with the
self-supervised signals to learn the representation. After that, we fix the
whole representation model and tune the classifier. During adaptation, we fix
the transferable temporal dynamics and update the image encoder. The efficacy
of our approach is revealed by the superior accuracy of DITeD over leading
alternatives across standard few-shot action recognition datasets. Moreover, we
validate that the learned temporal dynamic transition and temporal dynamic
generation modules possess transferable qualities.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12707" title="Abstract">arXiv:2402.12707</a> [<a href="/pdf/2402.12707" title="Download PDF">pdf</a>, <a href="/ps/2402.12707" title="Download PostScript">ps</a>, <a href="/format/2402.12707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weight Structure of Low/High-Rate Polar Codes and Its Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rowshan%2C+M">Mohammad Rowshan</a>, 
<a href="/search/cs?searchtype=author&query=Dr%C4%83goi%2C+V">Vlad-Florin Dr&#x103;goi</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jinhong Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The structure of a linear block code is pivotal in defining fundamental
properties, particularly weight distribution, and code design. In this study,
we characterize the Type II structure of polar codewords with weights less than
twice the minimum weight $w_{min}$, utilizing the lower triangular affine (LTA)
transform. We present a closed-form formula for their enumeration. Leveraging
this structure and additionally characterizing the structure of weight
$2w_{min}$, we ascertain the complete weight distribution of low-rate and,
through the utilization of dual codes properties, high-rate polar codes,
subcodes of Reed--Muller (RM) codes, and RMxPolar codes. Furthermore, we
introduce a partial order based on the weight distribution and briefly explore
its properties and applications in code construction and analysis.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12711" title="Abstract">arXiv:2402.12711</a> [<a href="/pdf/2402.12711" title="Download PDF">pdf</a>, <a href="/ps/2402.12711" title="Download PostScript">ps</a>, <a href="/format/2402.12711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Near-Optimal Regret for Bandit Algorithms with Uniform  Last-Iterate Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lin Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Existing performance measures for bandit algorithms such as regret, PAC
bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative
performance, while allowing the play of an arbitrarily bad arm at any finite
time t. Such a behavior can be highly detrimental in high-stakes applications.
This paper introduces a stronger performance measure, the uniform last-iterate
(ULI) guarantee, capturing both cumulative and instantaneous performance of
bandit algorithms. Specifically, ULI characterizes the instantaneous
performance since it ensures that the per-round regret of the played arm is
bounded by a function, monotonically decreasing w.r.t. (large) round t,
preventing revisits to bad arms when sufficient samples are available. We
demonstrate that a near-optimal ULI guarantee directly implies near-optimal
cumulative performance across aforementioned performance measures. To examine
the achievability of ULI in the finite arm setting, we first provide two
positive results that some elimination-based algorithms and high-probability
adversarial algorithms with stronger analysis or additional designs, can attain
near-optimal ULI guarantees. Then, we also provide a negative result,
indicating that optimistic algorithms cannot achieve a near-optimal ULI
guarantee. Finally, we propose an efficient algorithm for linear bandits with
infinitely many arms, which achieves the ULI guarantee, given access to an
optimization oracle.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12712" title="Abstract">arXiv:2402.12712</a> [<a href="/pdf/2402.12712" title="Download PDF">pdf</a>, <a href="/format/2402.12712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for  Single or Sparse-view 3D Object Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shitao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chengzhou Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuchen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+V">Vikas Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Furukawa%2C+Y">Yasutaka Furukawa</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Rakesh Ranjan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3D generation, project page: <a href="https://mvdiffusion-plusplus.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a neural architecture MVDiffusion++ for 3D object
reconstruction that synthesizes dense and high-resolution views of an object
given one or a few images without camera poses. MVDiffusion++ achieves superior
flexibility and scalability with two surprisingly simple ideas: 1) A
``pose-free architecture'' where standard self-attention among 2D latent
features learns 3D consistency across an arbitrary number of conditional and
generation views without explicitly using camera pose information; and 2) A
``view dropout strategy'' that discards a substantial number of output views
during training, which reduces the training-time memory footprint and enables
dense and high-resolution view synthesis at test time. We use the Objaverse for
training and the Google Scanned Objects for evaluation with standard novel view
synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly
outperforms the current state of the arts. We also demonstrate a text-to-3D
application example by combining MVDiffusion++ with a text-to-image generative
model.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12713" title="Abstract">arXiv:2402.12713</a> [<a href="/pdf/2402.12713" title="Download PDF">pdf</a>, <a href="/format/2402.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Rational Investors?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yuchen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+G">Guangnan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+H">Hongfeng Chai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are progressively being adopted in financial
analysis to harness their extensive knowledge base for interpreting complex
market data and trends. However, their application in the financial domain is
challenged by intrinsic biases (i.e., risk-preference bias) and a superficial
grasp of market intricacies, underscoring the need for a thorough assessment of
their financial insight. This study introduces a novel framework, Financial
Bias Indicators (FBI), to critically evaluate the financial rationality of
LLMs, focusing on their ability to discern and navigate the subtleties of
financial information and to identify any irrational biases that might skew
market analysis.
<br />Our research adopts an innovative methodology to measure financial
rationality, integrating principles of behavioral finance to scrutinize the
biases and decision-making patterns of LLMs. We conduct a comprehensive
evaluation of 19 leading LLMs, considering factors such as model scale,
training datasets, input strategies, etc. The findings reveal varying degrees
of financial irrationality among the models, influenced by their design and
training. Models trained specifically on financial datasets might exhibit
greater irrationality, and it's possible that even larger financial language
models (FinLLMs) could display more biases than smaller, more generalized
models. This outcomes provide profound insights into how these elements affect
the financial rationality of LLMs, indicating that targeted training and
structured input methods could improve model performance. This work enriches
our understanding of LLMs' strengths and weaknesses in financial applications,
laying the groundwork for the development of more dependable and rational
financial analysis tools.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12714" title="Abstract">arXiv:2402.12714</a> [<a href="/pdf/2402.12714" title="Download PDF">pdf</a>, <a href="/format/2402.12714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Pretrained Transformer for Unified Geometric Learning on  Multi-Domain 3D Molecules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+R">Rui Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xiangzhe Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Ziyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenbing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Pretraining on a large number of unlabeled 3D molecules has showcased
superiority in various scientific applications. However, prior efforts
typically focus on pretraining models on a specific domain, either proteins or
small molecules, missing the opportunity to leverage the cross-domain
knowledge. To mitigate this gap, we introduce Equivariant Pretrained
Transformer (EPT), a novel pretraining framework designed to harmonize the
geometric learning of small molecules and proteins. To be specific, EPT unifies
the geometric modeling of multi-domain molecules via the block-enhanced
representation that can attend a broader context of each atom. Upon transformer
framework, EPT is further enhanced with E(3) equivariance to facilitate the
accurate representation of 3D structures. Another key innovation of EPT is its
block-level pretraining task, which allows for joint pretraining on datasets
comprising both small molecules and proteins. Experimental evaluations on a
diverse group of benchmarks, including ligand binding affinity prediction,
molecular property prediction, and protein property prediction, show that EPT
significantly outperforms previous SOTA methods for affinity prediction, and
achieves the best or comparable performance with existing domain-specific
pretraining models for other tasks.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12715" title="Abstract">arXiv:2402.12715</a> [<a href="/pdf/2402.12715" title="Download PDF">pdf</a>, <a href="/format/2402.12715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spurious Correlations in Machine Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wenqian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guangtao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning systems are known to be sensitive to spurious correlations
between biased features of the inputs (e.g., background, texture, and secondary
objects) and the corresponding labels. These features and their correlations
with the labels are known as "spurious" because they tend to change with shifts
in real-world data distributions, which can negatively impact the model's
generalization and robustness. In this survey, we provide a comprehensive
review of this issue, along with a taxonomy of current state-of-the-art methods
for addressing spurious correlations in machine learning models. Additionally,
we summarize existing datasets, benchmarks, and metrics to aid future research.
The paper concludes with a discussion of the recent advancements and future
research challenges in this field, aiming to provide valuable insights for
researchers in the related domains.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12716" title="Abstract">arXiv:2402.12716</a> [<a href="/pdf/2402.12716" title="Download PDF">pdf</a>, <a href="/format/2402.12716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Path TCP Hijacking in Wi-Fi Networks: A Packet-Size Side Channel  Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xuewei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this paper, we unveil a fundamental side channel in Wi-Fi networks,
specifically the observable frame size, which can be exploited by attackers to
conduct TCP hijacking attacks. Despite the various security mechanisms (e.g.,
WEP and WPA2/WPA3) implemented to safeguard Wi-Fi networks, our study reveals
that an off path attacker can still extract sufficient information from the
frame size side channel to hijack the victim's TCP connection. Our side channel
attack is based on two significant findings: (i) response packets (e.g., ACK
and RST) generated by TCP receivers vary in size, and (ii) the encrypted frames
containing these response packets have consistent and distinguishable sizes. By
observing the size of the victim's encrypted frames, the attacker can detect
and hijack the victim's TCP connections. We validate the effectiveness of this
side channel attack through two case studies, i.e., SSH DoS and web traffic
manipulation. Furthermore, we conduct extensive measurements to evaluate the
impact of our attack on real-world Wi-Fi networks. We test 30 popular wireless
routers from 9 well-known vendors, and none of these routers can protect
victims from our attack. Also, we implement our attack in 80 real-world Wi-Fi
networks and successfully hijack the victim's TCP connections in 69 (86%)
evaluated Wi-Fi networks. We have responsibly disclosed the vulnerability to
the Wi-Fi Alliance and proposed several mitigation strategies to address this
issue.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12718" title="Abstract">arXiv:2402.12718</a> [<a href="/pdf/2402.12718" title="Download PDF">pdf</a>, <a href="/ps/2402.12718" title="Download PostScript">ps</a>, <a href="/format/2402.12718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fostering Joint Innovation: A Global Online Platform for Ideas Sharing  and Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamali%2C+H">Hossein Jamali</a>, 
<a href="/search/cs?searchtype=author&query=Dascalu%2C+S+M">Sergiu M. Dascalu</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+F+C">Frederick C. Harris Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, ITNG 2024 21st International Conference on Information Technology. Las Vegas, Nevada, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In today's world, where moving forward hinges on innovation and working
together, this article introduces a new global online platform that is all
about sparking teamwork to come up with new ideas. This platform goes beyond
borders and barriers between different fields, creating an exciting space where
people from all over the world can swap ideas, get helpful feedback, and team
up on exciting projects. What sets our platform apart is its ability to tap
into the combined brainpower of a diverse bunch of users, giving people the
power to come up with game-changing ideas that tackle big global problems. By
making it easy for people to share ideas and promoting a culture of working
together, our platform is like a buddy for innovation, boosting creativity and
problem-solving on a global level. This article spills the details on what the
platform aims to do, how it works, and what makes it special, emphasizing how
it can kickstart creativity, ramp up problem-solving skills, and get different
fields collaborating. It is not just a tool it is a whole new way of teaming up
to make daily life better and build a global community of problem-solving pals.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12720" title="Abstract">arXiv:2402.12720</a> [<a href="/pdf/2402.12720" title="Download PDF">pdf</a>, <a href="/format/2402.12720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Information Capacity of Neural Network Watermarks: Upper  Bound Estimation and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fangqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haodong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shilin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To trace the copyright of deep neural networks, an owner can embed its
identity information into its model as a watermark. The capacity of the
watermark quantify the maximal volume of information that can be verified from
the watermarked model. Current studies on capacity focus on the ownership
verification accuracy under ordinary removal attacks and fail to capture the
relationship between robustness and fidelity. This paper studies the capacity
of deep neural network watermarks from an information theoretical perspective.
We propose a new definition of deep neural network watermark capacity analogous
to channel capacity, analyze its properties, and design an algorithm that
yields a tight estimation of its upper bound under adversarial overwriting. We
also propose a universal non-invasive method to secure the transmission of the
identity message beyond capacity by multiple rounds of ownership verification.
Our observations provide evidence for neural network owners and defenders that
are curious about the tradeoff between the integrity of their ownership and the
performance degradation of their products.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12721" title="Abstract">arXiv:2402.12721</a> [<a href="/pdf/2402.12721" title="Download PDF">pdf</a>, <a href="/format/2402.12721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for  Recognizing Low-Quality Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jinsung Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hyundong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jonghyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Sanghyun Hong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kookjin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Noseong Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A standard practice in developing image recognition models is to train a
model on a specific image resolution and then deploy it. However, in real-world
inference, models often encounter images different from the training sets in
resolution and/or subject to natural variations such as weather changes, noise
types and compression artifacts. While traditional solutions involve training
multiple models for different resolutions or input variations, these methods
are computationally expensive and thus do not scale in practice. To this end,
we propose a novel neural network model, parallel-structured and all-component
Fourier neural operator (PAC-FNO), that addresses the problem. Unlike
conventional feed-forward neural networks, PAC-FNO operates in the frequency
domain, allowing it to handle images of varying resolutions within a single
model. We also propose a two-stage algorithm for training PAC-FNO with a
minimal modification to the original, downstream model. Moreover, the proposed
PAC-FNO is ready to work with existing image recognition models. Extensively
evaluating methods with seven image recognition benchmarks, we show that the
proposed PAC-FNO improves the performance of existing baseline models on images
with various resolutions by up to 77.1% and various types of natural variations
in the images at inference.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12722" title="Abstract">arXiv:2402.12722</a> [<a href="/pdf/2402.12722" title="Download PDF">pdf</a>, <a href="/format/2402.12722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Knowledge Informed Continual Multivariate Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zijie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yushan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dongjin Song</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sahil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Rasul%2C+K">Kashif Rasul</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+A">Anderson Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Nevmyvaka%2C+Y">Yuriy Nevmyvaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies in multivariate time series (MTS) forecasting reveal that
explicitly modeling the hidden dependencies among different time series can
yield promising forecasting performance and reliable explanations. However,
modeling variable dependencies remains underexplored when MTS is continuously
accumulated under different regimes (stages). Due to the potential distribution
and dependency disparities, the underlying model may encounter the catastrophic
forgetting problem, i.e., it is challenging to memorize and infer different
types of variable dependencies across different regimes while maintaining
forecasting performance. To address this issue, we propose a novel Structural
Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS
forecasting within a continual learning paradigm, which leverages structural
knowledge to steer the forecasting model toward identifying and adapting to
different regimes, and selects representative MTS samples from each regime for
memory replay. Specifically, we develop a forecasting model based on graph
structure learning, where a consistency regularization scheme is imposed
between the learned variable dependencies and the structural knowledge while
optimizing the forecasting objective over the MTS data. As such, MTS
representations learned in each regime are associated with distinct structural
knowledge, which helps the model memorize a variety of conceivable scenarios
and results in accurate forecasts in the continual learning context. Meanwhile,
we develop a representation-matching memory replay scheme that maximizes the
temporal coverage of MTS data to efficiently preserve the underlying temporal
dynamics and dependency structures of each regime. Thorough empirical studies
on synthetic and real-world benchmarks validate SKI-CL's efficacy and
advantages over the state-of-the-art for continual MTS forecasting tasks.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12727" title="Abstract">arXiv:2402.12727</a> [<a href="/pdf/2402.12727" title="Download PDF">pdf</a>, <a href="/format/2402.12727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Posterior Sampling is Computationally Intractable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivam Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jalal%2C+A">Ajil Jalal</a>, 
<a href="/search/cs?searchtype=author&query=Parulekar%2C+A">Aditya Parulekar</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+E">Eric Price</a>, 
<a href="/search/cs?searchtype=author&query=Xun%2C+Z">Zhiyang Xun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models are a remarkably effective way of learning and sampling from
a distribution $p(x)$. In posterior sampling, one is also given a measurement
model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x
\mid y)$. Posterior sampling is useful for tasks such as inpainting,
super-resolution, and MRI reconstruction, so a number of recent works have
given algorithms to heuristically approximate it; but none are known to
converge to the correct distribution in polynomial time.
<br />In this paper we show that posterior sampling is \emph{computationally
intractable}: under the most basic assumption in cryptography -- that one-way
functions exist -- there are instances for which \emph{every} algorithm takes
superpolynomial time, even though \emph{unconditional} sampling is provably
fast. We also show that the exponential-time rejection sampling algorithm is
essentially optimal under the stronger plausible assumption that there are
one-way functions that take exponential time to invert.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12728" title="Abstract">arXiv:2402.12728</a> [<a href="/pdf/2402.12728" title="Download PDF">pdf</a>, <a href="/format/2402.12728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality-Aware Integration with Large Language Models for  Knowledge-based Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junnan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huachi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+P">Pai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,3 figures and 1 page appendix; The processed graphs and codes will be avalibale
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge-based visual question answering (KVQA) has been extensively studied
to answer visual questions with external knowledge, e.g., knowledge graphs
(KGs). While several attempts have been proposed to leverage large language
models (LLMs) as an implicit knowledge source, it remains challenging since
LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,
images, KGs and LLMs, cannot be readily aligned for complex scenarios. To
tackle these, we present a novel modality-aware integration with LLMs for KVQA
(MAIL). It carefully leverages multimodal knowledge for both image
understanding and knowledge reasoning. Specifically, (i) we propose a two-stage
prompting strategy with LLMs to densely embody the image into a scene graph
with detailed visual features; (ii) We construct a coupled concept graph by
linking the mentioned entities with external facts. (iii) A tailored
pseudo-siamese graph medium fusion is designed for sufficient multimodal
fusion. We utilize the shared mentioned entities in two graphs as mediums to
bridge a tight inter-modal exchange, while maximally preserving insightful
intra-modal learning by constraining the fusion within mediums. Extensive
experiments on two benchmark datasets show the superiority of MAIL with 24x
less resources.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12729" title="Abstract">arXiv:2402.12729</a> [<a href="/pdf/2402.12729" title="Download PDF">pdf</a>, <a href="/format/2402.12729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable and reliable deep transfer learning for intelligent fault  detection via multi-scale neural processes embedded with knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+J">Jingqi Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+J">Jianliang Ai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yiqun Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep transfer learning (DTL) is a fundamental method in the field of
Intelligent Fault Detection (IFD). It aims to mitigate the degradation of
method performance that arises from the discrepancies in data distribution
between training set (source domain) and testing set (target domain).
Considering the fact that fault data collection is challenging and certain
faults are scarce, DTL-based methods face the limitation of available
observable data, which reduces the detection performance of the methods in the
target domain. Furthermore, DTL-based methods lack comprehensive uncertainty
analysis that is essential for building reliable IFD systems. To address the
aforementioned problems, this paper proposes a novel DTL-based method known as
Neural Processes-based deep transfer learning with graph convolution network
(GTNP). Feature-based transfer strategy of GTNP bridges the data distribution
discrepancies of source domain and target domain in high-dimensional space.
Both the joint modeling based on global and local latent variables and sparse
sampling strategy reduce the demand of observable data in the target domain.
The multi-scale uncertainty analysis is obtained by using the distribution
characteristics of global and local latent variables. Global analysis of
uncertainty enables GTNP to provide quantitative values that reflect the
complexity of methods and the difficulty of tasks. Local analysis of
uncertainty allows GTNP to model uncertainty (confidence of the fault detection
result) at each sample affected by noise and bias. The validation of the
proposed method is conducted across 3 IFD tasks, consistently showing the
superior detection performance of GTNP compared to the other DTL-based methods.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12730" title="Abstract">arXiv:2402.12730</a> [<a href="/pdf/2402.12730" title="Download PDF">pdf</a>, <a href="/format/2402.12730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with  and without machine translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dipta%2C+S+R">Shubhashis Roy Dipta</a>, 
<a href="/search/cs?searchtype=author&query=Vallurupalli%2C+S">Sai Vallurupalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to SemEval 2024 (Colocated with NAACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper describes the system we developed for SemEval-2024 Task 1,
"Semantic Textual Relatedness for African and Asian Languages." The aim of the
task is to build a model that can identify semantic textual relatedness (STR)
between two sentences of a target language belonging to a collection of African
and Asian languages. We participated in Subtasks A and C and explored
supervised and cross-lingual training leveraging large language models (LLMs).
Pre-trained large language models have been extensively used for machine
translation and semantic similarity. Using a combination of machine translation
and sentence embedding LLMs, we developed a unified STR model, TranSem, for
subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for
use in subtask C. Our model results for 7 languages in subtask A were better
than the official baseline for 3 languages and on par with the baseline for the
remaining 4 languages. Our model results for the 12 languages in subtask C
resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place
for English with low performance for the remaining 9 languages.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12733" title="Abstract">arXiv:2402.12733</a> [<a href="/pdf/2402.12733" title="Download PDF">pdf</a>, <a href="/format/2402.12733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weixin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Weike Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ming%2C+Z">Zhong Ming</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In real recommendation scenarios, users often have different types of
behaviors, such as clicking and buying. Existing research methods show that it
is possible to capture the heterogeneous interests of users through different
types of behaviors. However, most multi-behavior approaches have limitations in
learning the relationship between different behaviors. In this paper, we
propose a novel multilayer perceptron (MLP)-based heterogeneous sequential
recommendation method, namely behavior-aware multilayer perceptron (BMLP).
Specifically, it has two main modules, including a heterogeneous interest
perception (HIP) module, which models behaviors at multiple granularities
through behavior types and transition relationships, and a purchase intent
perception (PIP) module, which adaptively fuses subsequences of auxiliary
behaviors to capture users' purchase intent. Compared with mainstream sequence
models, MLP is competitive in terms of accuracy and has unique advantages in
simplicity and efficiency. Extensive experiments show that BMLP achieves
significant improvement over state-of-the-art algorithms on four public
datasets. In addition, its pure MLP architecture leads to a linear time
complexity.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12734" title="Abstract">arXiv:2402.12734</a> [<a href="/pdf/2402.12734" title="Download PDF">pdf</a>, <a href="/ps/2402.12734" title="Download PostScript">ps</a>, <a href="/format/2402.12734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Permutation Algorithm for Online Facility Assignment on a Line
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Tsubasa Harada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In the online facility assignment on a line (OFAL) with a set $S$ of $k$
servers and a capacity $c:S\to\mathbb{N}$, each server $s\in S$ with a capacity
$c(s)$ is placed on a line and a request arrives on a line one-by-one. The task
of an online algorithm is to irrevocably assign a current request to one of the
servers with vacancies before the next request arrives. An algorithm can assign
up to $c(s)$ requests to each server $s\in S$.
<br />In this paper, we show that the competitive ratio of the permutation
algorithm is at least $k+1$ for OFAL where the servers are evenly placed on a
line. This result resolves the contradiction between Ahmed et al.'s result that
it is $k$-competitive and Itoh et al.'s result that the competitive ratio of
any algorithm for OFAL is at least 3 when $k\geq 2$.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12736" title="Abstract">arXiv:2402.12736</a> [<a href="/pdf/2402.12736" title="Download PDF">pdf</a>, <a href="/format/2402.12736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Achieving a universally high accuracy in object detection is quite
challenging, and the mainstream focus in the industry currently lies on
detecting specific classes of objects. However, deploying one or multiple
object detection networks requires a certain amount of GPU memory for training
and storage capacity for inference. This presents challenges in terms of how to
effectively coordinate multiple object detection tasks under
resource-constrained conditions. This paper introduces a lightweight
fine-tuning strategy called Calibration side tuning, which integrates aspects
of adapter tuning and side tuning to adapt the successful techniques employed
in transformers for use with ResNet. The Calibration side tuning architecture
that incorporates maximal transition calibration, utilizing a small number of
additional parameters to enhance network performance while maintaining a smooth
training process. Furthermore, this paper has conducted an analysis on multiple
fine-tuning strategies and have implemented their application within ResNet,
thereby expanding the research on fine-tuning strategies for object detection
networks. Besides, this paper carried out extensive experiments using five
benchmark datasets. The experimental results demonstrated that this method
outperforms other compared state-of-the-art techniques, and a better balance
between the complexity and performance of the finetune schemes is achieved.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12737" title="Abstract">arXiv:2402.12737</a> [<a href="/pdf/2402.12737" title="Download PDF">pdf</a>, <a href="/format/2402.12737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantee Regions for Local Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havasi%2C+M">Marton Havasi</a>, 
<a href="/search/cs?searchtype=author&query=Parbhoo%2C+S">Sonali Parbhoo</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Interpretability methods that utilise local surrogate models (e.g. LIME) are
very good at describing the behaviour of the predictive model at a point of
interest, but they are not guaranteed to extrapolate to the local region
surrounding the point. However, overfitting to the local curvature of the
predictive model and malicious tampering can significantly limit extrapolation.
We propose an anchor-based algorithm for identifying regions in which local
explanations are guaranteed to be correct by explicitly describing those
intervals along which the input features can be trusted. Our method produces an
interpretable feature-aligned box where the prediction of the local surrogate
model is guaranteed to match the predictive model. We demonstrate that our
algorithm can be used to find explanations with larger guarantee regions that
better cover the data manifold compared to existing baselines. We also show how
our method can identify misleading local explanations with significantly poorer
guarantee regions.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12738" title="Abstract">arXiv:2402.12738</a> [<a href="/pdf/2402.12738" title="Download PDF">pdf</a>, <a href="/ps/2402.12738" title="Download PostScript">ps</a>, <a href="/format/2402.12738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models be Used to Provide Psychological Counselling?  An Analysis of GPT-4-Generated Responses Using Role-play Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Inaba%2C+M">Michimasa Inaba</a>, 
<a href="/search/cs?searchtype=author&query=Ukiyo%2C+M">Mariko Ukiyo</a>, 
<a href="/search/cs?searchtype=author&query=Takamizo%2C+K">Keiko Takamizo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a conference paper at IWSDS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Mental health care poses an increasingly serious challenge to modern
societies. In this context, there has been a surge in research that utilizes
information technologies to address mental health problems, including those
aiming to develop counseling dialogue systems. However, there is a need for
more evaluations of the performance of counseling dialogue systems that use
large language models. For this study, we collected counseling dialogue data
via role-playing scenarios involving expert counselors, and the utterances were
annotated with the intentions of the counselors. To determine the feasibility
of a dialogue system in real-world counseling scenarios, third-party counselors
evaluated the appropriateness of responses from human counselors and those
generated by GPT-4 in identical contexts in role-play dialogue data. Analysis
of the evaluation results showed that the responses generated by GPT-4 were
competitive with those of human counselors.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12741" title="Abstract">arXiv:2402.12741</a> [<a href="/pdf/2402.12741" title="Download PDF">pdf</a>, <a href="/format/2402.12741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://measure-infinity.github.io/mulan">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing text-to-image models still struggle to generate images of multiple
objects, especially in handling their spatial positions, relative sizes,
overlapping, and attribute bindings. In this paper, we develop a training-free
Multimodal-LLM agent (MuLan) to address these challenges by progressive
multi-object generation with planning and feedback control, like a human
painter. MuLan harnesses a large language model (LLM) to decompose a prompt to
a sequence of sub-tasks, each generating only one object conditioned on
previously generated objects by stable diffusion. Unlike existing LLM-grounded
methods, MuLan only produces a high-level plan at the beginning while the exact
size and location of each object are determined by an LLM and attention
guidance upon each sub-task. Moreover, MuLan adopts a vision-language model
(VLM) to provide feedback to the image generated in each sub-task and control
the diffusion model to re-generate the image if it violates the original
prompt. Hence, each model in every step of MuLan only needs to address an easy
sub-task it is specialized for. We collect 200 prompts containing multi-objects
with spatial relationships and attribute bindings from different benchmarks to
evaluate MuLan. The results demonstrate the superiority of MuLan in generating
multiple objects over baselines. The code is available on
https://github.com/measure-infinity/mulan-code.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12742" title="Abstract">arXiv:2402.12742</a> [<a href="/pdf/2402.12742" title="Download PDF">pdf</a>, <a href="/format/2402.12742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDC: A Vision for a Disaggregated Datacenter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ewais%2C+M">Mohammad Ewais</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+P">Paul Chow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Datacenters of today have maintained the same architecture for decades using
the server as the primary building block. However, this traditional approach
suffers from under-utilization of its resources, often caused by
over-allocating these resources when deploying applications to accommodate
worst-case scenarios. Specifically, servers can quickly drain their
over-allocated memory resources while their CPUs are not fully utilized.
<br />This problem gives rise to a different school of thought, where resources are
disaggregated instead of tightly bound to servers. This can address the
utilization problem by allowing each type of resource to be allocated, utilized
and freed separately as required. New high performance communication protocols,
like CXL, could pave the way for practical implementations of resource
disaggregation. In this article, we argue it is time to reconsider the
datacenter architecture as a whole. We present our vision for a disaggregated
datacenter aided by well-established computer architecture design
methodologies.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12743" title="Abstract">arXiv:2402.12743</a> [<a href="/pdf/2402.12743" title="Download PDF">pdf</a>, <a href="/ps/2402.12743" title="Download PostScript">ps</a>, <a href="/format/2402.12743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APT-MMF: An advanced persistent threat actor attribution method based on  multimodal and multilevel feature fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+N">Nan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+B">Bo Lang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yikai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Threat actor attribution is a crucial defense strategy for combating advanced
persistent threats (APTs). Cyber threat intelligence (CTI), which involves
analyzing multisource heterogeneous data from APTs, plays an important role in
APT actor attribution. The current attribution methods extract features from
different CTI perspectives and employ machine learning models to classify CTI
reports according to their threat actors. However, these methods usually
extract only one kind of feature and ignore heterogeneous information,
especially the attributes and relations of indicators of compromise (IOCs),
which form the core of CTI. To address these problems, we propose an APT actor
attribution method based on multimodal and multilevel feature fusion (APT-MMF).
First, we leverage a heterogeneous attributed graph to characterize APT reports
and their IOC information. Then, we extract and fuse multimodal features,
including attribute type features, natural language text features and
topological relationship features, to construct comprehensive node
representations. Furthermore, we design multilevel heterogeneous graph
attention networks to learn the deep hidden features of APT report nodes; these
networks integrate IOC type-level, metapath-based neighbor node-level, and
metapath semantic-level attention. Utilizing multisource threat intelligence,
we construct a heterogeneous attributed graph dataset for verification
purposes. The experimental results show that our method not only outperforms
the existing methods but also demonstrates its good interpretability for
attribution analysis tasks.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12747" title="Abstract">arXiv:2402.12747</a> [<a href="/pdf/2402.12747" title="Download PDF">pdf</a>, <a href="/format/2402.12747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN  Generation and Forward Noise Suppression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zheng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fengye Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsiao-Hwa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hamalainen%2C+T">Timo Hamalainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Due to the constraints on power supply and limited encryption capability,
data security based on physical layer security (PLS) techniques in backscatter
communications has attracted a lot of attention. In this work, we propose to
enhance PLS in a full-duplex symbiotic radio (FDSR) system with a proactive
eavesdropper, which may overhear the information and interfere legitimate
communications simultaneously by emitting attack signals. To deal with the
eavesdroppers, we propose a security strategy based on pseudo-decoding and
artificial noise (AN) injection to ensure the performance of legitimate
communications through forward noise suppression. A novel AN signal generation
scheme is proposed using a pseudo-decoding method, where AN signal is
superimposed on data signal to safeguard the legitimate channel. The phase
control in the forward noise suppression scheme and the power allocation
between AN and data signals are optimized to maximize security throughput. The
formulated problem can be solved via problem decomposition and alternate
optimization algorithms. Simulation results demonstrate the superiority of the
proposed scheme in terms of security throughput and attack mitigation
performance.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12749" title="Abstract">arXiv:2402.12749</a> [<a href="/pdf/2402.12749" title="Download PDF">pdf</a>, <a href="/ps/2402.12749" title="Download PostScript">ps</a>, <a href="/format/2402.12749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Me LLaMA: Foundation Large Language Models for Medical Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Cheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fongci Lin</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xueqing Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jeffrey Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Keloth%2C+V">Vipina Keloth</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Huan He</a>, 
<a href="/search/cs?searchtype=author&query=Ohno-Machido%2C+L">Lucila Ohno-Machido</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent large language models (LLMs) like ChatGPT and LLaMA have shown great
promise in many AI applications. However, their performance on medical tasks is
suboptimal and can be further improved by training on large domain-specific
datasets. This study introduces Me LLaMA, a medical LLM family including
foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA
13/70B-chat, developed through the continual pre-training and instruction
tuning of LLaMA2 using large medical data. Our domain-specific data suite for
training and evaluation, includes a large-scale continual pre-training dataset
with 129B tokens, an instruction tuning dataset with 214k samples, and a
medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our
extensive evaluation using MIBE shows that Me LLaMA models surpass existing
open-source medical LLMs in zero-shot and few-shot learning and outperform
commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8
datasets. In addition, we empirically investigated the catastrophic forgetting
problem, and our results show that Me LLaMA models outperform other medical
LLMs. Me LLaMA is one of the first and largest open-source foundational LLMs
designed for the medical domain, using both biomedical and clinical data. It
exhibits superior performance across both general and medical tasks compared to
other medical LLMs, rendering it an attractive choice for medical AI
applications. All resources are available at:
https://github.com/BIDS-Xu-Lab/Me-LLaMA.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12750" title="Abstract">arXiv:2402.12750</a> [<a href="/pdf/2402.12750" title="Download PDF">pdf</a>, <a href="/format/2402.12750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Composition for Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yiyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code will be available at <a href="https://github.com/THUNLP-MT/ModelCompose">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent developments in Multimodal Large Language Models (MLLMs) have shown
rapid progress, moving towards the goal of creating versatile MLLMs that
understand inputs from various modalities. However, existing methods typically
rely on joint training with paired multimodal instruction data, which is
resource-intensive and challenging to extend to new modalities. In this paper,
we propose a new paradigm through the model composition of existing MLLMs to
create a new model that retains the modal understanding capabilities of each
original model. Our basic implementation, NaiveMC, demonstrates the
effectiveness of this paradigm by reusing modality encoders and merging LLM
parameters. Furthermore, we introduce DAMC to address parameter interference
and mismatch issues during the merging process, thereby enhancing the model
performance. To facilitate research in this area, we propose MCUB, a benchmark
for assessing ability of MLLMs to understand inputs from diverse modalities.
Experiments on this benchmark and four other multimodal understanding tasks
show significant improvements over baselines, proving that model composition
can create a versatile model capable of processing inputs from multiple
modalities.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12754" title="Abstract">arXiv:2402.12754</a> [<a href="/pdf/2402.12754" title="Download PDF">pdf</a>, <a href="/format/2402.12754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fingerprint Presentation Attack Detector Using Global-Local Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haozhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoqian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linlin Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted by IEEE Transactions on Cybernetics. Current version is updated with minor revisions on introduction and related works
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE TRANSACTIONS ON CYBERNETICS, VOL. 52, NO. 11, 12315-12328,
  November 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The vulnerability of automated fingerprint recognition systems (AFRSs) to
presentation attacks (PAs) promotes the vigorous development of PA detection
(PAD) technology. However, PAD methods have been limited by information loss
and poor generalization ability, resulting in new PA materials and fingerprint
sensors. This paper thus proposes a global-local model-based PAD (RTK-PAD)
method to overcome those limitations to some extent. The proposed method
consists of three modules, called: 1) the global module; 2) the local module;
and 3) the rethinking module. By adopting the cut-out-based global module, a
global spoofness score predicted from nonlocal features of the entire
fingerprint images can be achieved. While by using the texture
in-painting-based local module, a local spoofness score predicted from
fingerprint patches is obtained. The two modules are not independent but
connected through our proposed rethinking module by localizing two
discriminative patches for the local module based on the global spoofness
score. Finally, the fusion spoofness score by averaging the global and local
spoofness scores is used for PAD. Our experimental results evaluated on LivDet
2017 show that the proposed RTK-PAD can achieve an average classification error
(ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false
detection rate (FDR) equals 1.0%, which significantly outperformed the
state-of-the-art methods by $\sim$10% in terms of TDR (91.19% versus 80.74%).
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12756" title="Abstract">arXiv:2402.12756</a> [<a href="/pdf/2402.12756" title="Download PDF">pdf</a>, <a href="/format/2402.12756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi  Fingerprinting: A Discussion from a Data Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhe Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+R">Ruocheng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+S">Kyeong Soo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+S">Jeremy S. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, Invited paper with Excellent Paper Award to be presented at ICAIIC 2024, Osaka, Japan, Feb. 19--22, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Wi-Fi fingerprinting has emerged as the most popular approach to indoor
localization. The use of ML algorithms has greatly improved the localization
performance of Wi-Fi fingerprinting, but its success depends on the
availability of fingerprint databases composed of a large number of RSSIs, the
MAC addresses of access points, and the other measurement information. However,
most fingerprint databases do not reflect well the time varying nature of
electromagnetic interferences in complicated modern indoor environment. This
could result in significant changes in statistical characteristics of
training/validation and testing datasets, which are often constructed at
different times, and even the characteristics of the testing datasets could be
different from those of the data submitted by users during the operation of
localization systems after their deployment. In this paper, we consider the
implications of time-varying Wi-Fi fingerprints on indoor localization from a
data-centric point of view and discuss the differences between static and
dynamic databases. As a case study, we have constructed a dynamic database
covering three floors of the IR building of XJTLU based on RSSI measurements,
over 44 days, and investigated the differences between static and dynamic
databases in terms of statistical characteristics and localization performance.
The analyses based on variance calculations and Isolation Forest show the
temporal shifts in RSSIs, which result in a noticeable trend of the increase in
the localization error of a Gaussian process regression model with the maximum
error of 6.65 m after 14 days of training without model adjustments. The
results of the case study with the XJTLU dynamic database clearly demonstrate
the limitations of static databases and the importance of the creation and
adoption of dynamic databases for future indoor localization research and
real-world deployment.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12759" title="Abstract">arXiv:2402.12759</a> [<a href="/pdf/2402.12759" title="Download PDF">pdf</a>, <a href="/format/2402.12759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fair Allocation in Social Commerce Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anjali Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Nagori%2C+S+J">Shreyans J. Nagori</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Abhijnan Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Vaish%2C+R">Rohit Vaish</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>, 
<a href="/search/cs?searchtype=author&query=Nadkarni%2C+P+P">Prajit Prashant Nadkarni</a>, 
<a href="/search/cs?searchtype=author&query=Dasararaju%2C+N+V">Narendra Varma Dasararaju</a>, 
<a href="/search/cs?searchtype=author&query=Chelliah%2C+M">Muthusamy Chelliah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Social commerce platforms are emerging businesses where producers sell
products through re-sellers who advertise the products to other customers in
their social network. Due to the increasing popularity of this business model,
thousands of small producers and re-sellers are starting to depend on these
platforms for their livelihood; thus, it is important to provide fair earning
opportunities to them. The enormous product space in such platforms prohibits
manual search, and motivates the need for recommendation algorithms to
effectively allocate product exposure and, consequently, earning opportunities.
In this work, we focus on the fairness of such allocations in social commerce
platforms and formulate the problem of assigning products to re-sellers as a
fair division problem with indivisible items under two-sided cardinality
constraints, wherein each product must be given to at least a certain number of
re-sellers and each re-seller must get a certain number of products.
<br />Our work systematically explores various well-studied benchmarks of fairness
-- including Nash social welfare, envy-freeness up to one item (EF1), and
equitability up to one item (EQ1) -- from both theoretical and experimental
perspectives. We find that the existential and computational guarantees of
these concepts known from the unconstrained setting do not extend to our
constrained model. To address this limitation, we develop a mixed-integer
linear program and other scalable heuristics that provide near-optimal
approximation of Nash social welfare in simulated and real social commerce
datasets. Overall, our work takes the first step towards achieving provable
fairness alongside reasonable revenue guarantees on social commerce platforms.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12760" title="Abstract">arXiv:2402.12760</a> [<a href="/pdf/2402.12760" title="Download PDF">pdf</a>, <a href="/format/2402.12760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A User-Friendly Framework for Generating Model-Preferred Prompts in  Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hei%2C+N">Nailei Hei</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haofen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Well-designed prompts have demonstrated the potential to guide text-to-image
models in generating amazing images. Although existing prompt engineering
methods can provide high-level guidance, it is challenging for novice users to
achieve the desired results by manually entering prompts due to a discrepancy
between novice-user-input prompts and the model-preferred prompts. To bridge
the distribution gap between user input behavior and model training datasets,
we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and
propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG)
for automated prompt optimization. For CFP, we construct a novel dataset for
text-to-image tasks that combines coarse and fine-grained prompts to facilitate
the development of automated prompt generation methods. For UF-FGTG, we propose
a novel framework that automatically translates user-input prompts into
model-preferred prompts. Specifically, we propose a prompt refiner that
continually rewrites prompts to empower users to select results that align with
their unique needs. Meanwhile, we integrate image-related loss functions from
the text-to-image model into the training process of text generation to
generate model-preferred prompts. Additionally, we propose an adaptive feature
extraction module to ensure diversity in the generated results. Experiments
demonstrate that our approach is capable of generating more visually appealing
and diverse images than previous state-of-the-art methods, achieving an average
improvement of 5% across six quality and aesthetic metrics.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12761" title="Abstract">arXiv:2402.12761</a> [<a href="/pdf/2402.12761" title="Download PDF">pdf</a>, <a href="/format/2402.12761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGAD: Self-boosted Knowledge Distillation for An Effective Federated  Graph Anomaly Detection Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhoumin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenzhong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-kiong Ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Graph anomaly detection (GAD) aims to identify anomalous graphs that
significantly deviate from other ones, which has raised growing attention due
to the broad existence and complexity of graph-structured data in many
real-world scenarios. However, existing GAD methods usually execute with
centralized training, which may lead to privacy leakage risk in some sensitive
cases, thereby impeding collaboration among organizations seeking to
collectively develop robust GAD models. Although federated learning offers a
promising solution, the prevalent non-IID problems and high communication costs
present significant challenges, particularly pronounced in collaborations with
graph data distributed among different participants. To tackle these
challenges, we propose an effective federated graph anomaly detection framework
(FGAD). We first introduce an anomaly generator to perturb the normal graphs to
be anomalous, and train a powerful anomaly detector by distinguishing generated
anomalous graphs from normal ones. Then, we leverage a student model to distill
knowledge from the trained anomaly detector (teacher model), which aims to
maintain the personality of local models and alleviate the adverse impact of
non-IID problems. Moreover, we design an effective collaborative learning
mechanism that facilitates the personalization preservation of local models and
significantly reduces communication costs among clients. Empirical results of
the GAD tasks on non-IID graphs compared with state-of-the-art baselines
demonstrate the superiority and efficiency of the proposed FGAD method.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12763" title="Abstract">arXiv:2402.12763</a> [<a href="/pdf/2402.12763" title="Download PDF">pdf</a>, <a href="/format/2402.12763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qingyao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Huai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bingyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lujie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Localizing the bronchoscope in real time is essential for ensuring
intervention quality. However, most existing methods struggle to balance
between speed and generalization. To address these challenges, we present
BronchoTrack, an innovative real-time framework for accurate branch-level
localization, encompassing lumen detection, tracking, and airway association.To
achieve real-time performance, we employ a benchmark lightweight detector for
efficient lumen detection. We are the first to introduce multi-object tracking
to bronchoscopic localization, mitigating temporal confusion in lumen
identification caused by rapid bronchoscope movement and complex airway
structures. To ensure generalization across patient cases, we propose a
training-free detection-airway association method based on a semantic airway
graph that encodes the hierarchy of bronchial tree structures.Experiments on
nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64
\%, while accessing up to the 4th generation of airways.Furthermore, we tested
BronchoTrack in an in-vivo animal study using a porcine model, where it
successfully localized the bronchoscope into the 8th generation
airway.Experimental evaluation underscores BronchoTrack's real-time performance
in both satisfying accuracy and generalization, demonstrating its potential for
clinical applications.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12765" title="Abstract">arXiv:2402.12765</a> [<a href="/pdf/2402.12765" title="Download PDF">pdf</a>, <a href="/format/2402.12765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GOOD: Towards Domain Generalized Orientated Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Q">Qi Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Beichen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingjun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gui-Song Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Oriented object detection has been rapidly developed in the past few years,
but most of these methods assume the training and testing images are under the
same statistical distribution, which is far from reality. In this paper, we
propose the task of domain generalized oriented object detection, which intends
to explore the generalization of oriented object detectors on arbitrary unseen
target domains. Learning domain generalized oriented object detectors is
particularly challenging, as the cross-domain style variation not only
negatively impacts the content representation, but also leads to unreliable
orientation predictions. To address these challenges, we propose a generalized
oriented object detector (GOOD). After style hallucination by the emerging
contrastive language-image pre-training (CLIP), it consists of two key
components, namely, rotation-aware content consistency learning (RAC) and style
consistency learning (SEC). The proposed RAC allows the oriented object
detector to learn stable orientation representation from style-diversified
samples. The proposed SEC further stabilizes the generalization ability of
content representation from different image styles. Extensive experiments on
multiple cross-domain settings show the state-of-the-art performance of GOOD.
Source code will be publicly available.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12767" title="Abstract">arXiv:2402.12767</a> [<a href="/pdf/2402.12767" title="Download PDF">pdf</a>, <a href="/format/2402.12767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When and How: Learning Identifiable Latent States for Nonstationary Time  Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenhui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haiqin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiangchen Song</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Temporal distribution shifts are ubiquitous in time series data. One of the
most popular methods assumes that the temporal distribution shift occurs
uniformly to disentangle the stationary and nonstationary dependencies. But
this assumption is difficult to meet, as we do not know when the distribution
shifts occur. To solve this problem, we propose to learn IDentifiable latEnt
stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we
further disentangle the stationary and nonstationary latent states via
sufficient observation assumption to learn how the latent states change.
Specifically, we formalize the causal process with environment-irrelated
station- ary and environment-related nonstationary variables. Under mild
conditions, we show that latent environments and stationary/nonstationary
variables are identifiable. Based on these theories, we devise the IDEA model,
which incorporates an autoregressive hidden Markov model to estimate latent
environments and modular prior networks to identify latent states. The IDEA
model outperforms several latest nonstationary forecasting methods on various
benchmark datasets, highlighting its advantages in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12770" title="Abstract">arXiv:2402.12770</a> [<a href="/pdf/2402.12770" title="Download PDF">pdf</a>, <a href="/format/2402.12770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acknowledgment of Emotional States: Generating Validating Responses for  Empathetic Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z+H">Zi Haur Pang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yahui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lala%2C+D">Divesh Lala</a>, 
<a href="/search/cs?searchtype=author&query=Ochi%2C+K">Keiko Ochi</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+K">Koji Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+T">Tatsuya Kawahara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for presentation at International Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the realm of human-AI dialogue, the facilitation of empathetic responses
is important. Validation is one of the key communication techniques in
psychology, which entails recognizing, understanding, and acknowledging others'
emotional states, thoughts, and actions. This study introduces the first
framework designed to engender empathetic dialogue with validating responses.
Our approach incorporates a tripartite module system: 1) validation timing
detection, 2) users' emotional state identification, and 3) validating response
generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based
dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of
emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms
both random baseline and the ChatGPT performance, in term of F1-score, in all
modules. Further validation of our model's efficacy is confirmed in its
application to the TUT Emotional Storytelling Corpus (TESC), a speech-based
dialogue dataset, by surpassing both random baseline and the ChatGPT. This
consistent performance across both textual and speech-based dialogues
underscores the effectiveness of our framework in fostering empathetic human-AI
communication.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12772" title="Abstract">arXiv:2402.12772</a> [<a href="/pdf/2402.12772" title="Download PDF">pdf</a>, <a href="/format/2402.12772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GazePrompt: Enhancing Low Vision People&#x27;s Reading Experience with  Gaze-Aware Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Potter%2C+Z">Zach Potter</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+Y">Yun Ho</a>, 
<a href="/search/cs?searchtype=author&query=Killough%2C+D">Daniel Killough</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+L">Linxiu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Sanbrita Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Reading is a challenging task for low vision people. While conventional low
vision aids (e.g., magnification) offer certain support, they cannot fully
address the difficulties faced by low vision users, such as locating the next
line and distinguishing similar words. To fill this gap, we present GazePrompt,
a gaze-aware reading aid that provides timely and targeted visual and audio
augmentations based on users' gaze behaviors. GazePrompt includes two key
features: (1) a Line-Switching support that highlights the line a reader
intends to read; and (2) a Difficult-Word support that magnifies or reads aloud
a word that the reader hesitates with. Through a study with 13 low vision
participants who performed well-controlled reading-aloud tasks with and without
GazePrompt, we found that GazePrompt significantly reduced participants' line
switching time, reduced word recognition errors, and improved their subjective
reading experiences. A follow-up silent-reading study showed that GazePrompt
can enhance users' concentration and perceived comprehension of the reading
contents. We further derive design considerations for future gaze-based low
vision aids.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12774" title="Abstract">arXiv:2402.12774</a> [<a href="/pdf/2402.12774" title="Download PDF">pdf</a>, <a href="/format/2402.12774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Conversational Dense Retrieval by Rewriting-Enhanced  Inversion of Session Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yiruo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Conversational dense retrieval has shown to be effective in conversational
search. However, a major limitation of conversational dense retrieval is their
lack of interpretability, hindering intuitive understanding of model behaviors
for targeted improvements. This paper presents CONVINV, a simple yet effective
approach to shed light on interpretable conversational dense retrieval models.
CONVINV transforms opaque conversational session embeddings into explicitly
interpretable text while faithfully maintaining their original retrieval
performance as much as possible. Such transformation is achieved by training a
recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging
the fact that the session and query embeddings share the same space in existing
conversational dense retrieval. To further enhance interpretability, we propose
to incorporate external interpretable query rewrites into the transformation
process. Extensive evaluations on three conversational search benchmarks
demonstrate that CONVINV can yield more interpretable text and faithfully
preserve original retrieval performance than baselines. Our work connects
opaque session embeddings with transparent query rewriting, paving the way
toward trustworthy conversational search.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12777" title="Abstract">arXiv:2402.12777</a> [<a href="/pdf/2402.12777" title="Download PDF">pdf</a>, <a href="/format/2402.12777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of Quantum Extreme Learning Machines for QoS Prediction of  Elevators&#x27; Software in an Industrial Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Shaukat Ali</a>, 
<a href="/search/cs?searchtype=author&query=Arrieta%2C+A">Aitor Arrieta</a>, 
<a href="/search/cs?searchtype=author&query=Arcaini%2C+P">Paolo Arcaini</a>, 
<a href="/search/cs?searchtype=author&query=Arratibel%2C+M">Maite Arratibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Quantum Extreme Learning Machine (QELM) is an emerging technique that
utilizes quantum dynamics and an easy-training strategy to solve problems such
as classification and regression efficiently. Although QELM has many potential
benefits, its real-world applications remain limited. To this end, we present
QELM's industrial application in the context of elevators, by proposing an
approach called QUELL. In QUELL, we use QELM for the waiting time prediction
related to the scheduling software of elevators, with applications for software
regression testing, elevator digital twins, and real-time performance
prediction. The scheduling software has been implemented by our industrial
partner Orona, a globally recognized leader in elevator technology. We
demonstrate that QUELL can efficiently predict waiting times, with prediction
quality significantly better than that of classical ML models employed in a
state-of-the-practice approach. Moreover, we show that the prediction quality
of QUELL does not degrade when using fewer features. Based on our industrial
application, we further provide insights into using QELM in other applications
in Orona, and discuss how QELM could be applied to other industrial
applications.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12779" title="Abstract">arXiv:2402.12779</a> [<a href="/pdf/2402.12779" title="Download PDF">pdf</a>, <a href="/format/2402.12779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Rainfall-Forecasting Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">XuDong Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">ChaoRong Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+F">FengQing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">LiHong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuanyuan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks have made great achievements in rainfall
prediction.However, the current forecasting methods have certain limitations,
such as with blurry generated images and incorrect spatial positions. To
overcome these challenges, we propose a Two-stage Rainfall-Forecasting
Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall
forecasts and addressing the imbalance in performance between temporal and
spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The
task of the first stage is to capture robust temporal information while
preserving spatial information under low-resolution conditions. The task of the
second stage is to reconstruct the low-resolution images generated in the first
stage into high-resolution images. We demonstrate state-of-the-art results on
the MRMS and Swedish radar datasets. Our project is open source and available
on GitHub at:
\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12780" title="Abstract">arXiv:2402.12780</a> [<a href="/pdf/2402.12780" title="Download PDF">pdf</a>, <a href="/format/2402.12780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Byzantine Clients in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allouah%2C+Y">Youssef Allouah</a>, 
<a href="/search/cs?searchtype=author&query=Farhadkhani%2C+S">Sadegh Farhadkhani</a>, 
<a href="/search/cs?searchtype=author&query=GuerraouI%2C+R">Rachid GuerraouI</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Nirupam Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pinot%2C+R">Rafael Pinot</a>, 
<a href="/search/cs?searchtype=author&query=Rizk%2C+G">Geovani Rizk</a>, 
<a href="/search/cs?searchtype=author&query=Voitovych%2C+S">Sasha Voitovych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes
federated learning (FL) prone to arbitrary manipulation. The natural approach
to robustify FL against adversarial clients is to replace the simple averaging
operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a
\emph{robust averaging rule}. While a significant amount of work has been
devoted to studying the convergence of federated {\em robust averaging} (which
we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of
{\em client subsampling} and {\em local steps}, two fundamental FL
characteristics. While client subsampling increases the effective fraction of
Byzantine clients, local steps increase the drift between the local updates
computed by honest (i.e., non-Byzantine) clients. Consequently, a careless
deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this
observation by presenting an in-depth analysis of $\mathsf{FedRo}$ tightly
analyzing the impact of client subsampling and local steps. Specifically, we
present a sufficient condition on client subsampling for nearly-optimal
convergence of $\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show
that the rate of improvement in learning accuracy {\em diminishes} with respect
to the number of clients subsampled, as soon as the sample size exceeds a
threshold value. Interestingly, we also observe that under a careful choice of
step-sizes, the learning error due to Byzantine clients decreases with the
number of local steps. We validate our theory by experiments on the FEMNIST and
CIFAR-$10$ image classification tasks.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12782" title="Abstract">arXiv:2402.12782</a> [<a href="/pdf/2402.12782" title="Download PDF">pdf</a>, <a href="/format/2402.12782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing GenAI Assisted Programming--A Comparative Study on Prompt  Efficiency and Code Quality Between GPT-4 and GLM-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">Angus Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zehan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study aims to explore the best practices for utilizing GenAI as a
programming tool, through a comparative analysis between GPT-4 and GLM-4. By
evaluating prompting strategies at different levels of complexity, we identify
that simplest and straightforward prompting strategy yields best code
generation results. Additionally, adding a CoT-like preliminary confirmation
step would further increase the success rate. Our results reveal that while
GPT-4 marginally outperforms GLM-4, the difference is minimal for average
users. In our simplified evaluation model, we see a remarkable 30 to 100-fold
increase in code generation efficiency over traditional coding norms. Our GenAI
Coding Workshop highlights the effectiveness and accessibility of the prompting
methodology developed in this study. We observe that GenAI-assisted coding
would trigger a paradigm shift in programming landscape, which necessitates
developers to take on new roles revolving around supervising and guiding GenAI,
and to focus more on setting high-level objectives and engaging more towards
innovation.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12784" title="Abstract">arXiv:2402.12784</a> [<a href="/pdf/2402.12784" title="Download PDF">pdf</a>, <a href="/format/2402.12784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Koopman%2C+B">Bevan Koopman</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaoran Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The introduction of Vec2Text, a technique for inverting text embeddings, has
raised serious privacy concerns within dense retrieval systems utilizing text
embeddings, including those provided by OpenAI and Cohere. This threat comes
from the ability for a malicious attacker with access to text embeddings to
reconstruct the original text.
<br />In this paper, we investigate various aspects of embedding models that could
influence the recoverability of text using Vec2Text. Our exploration involves
factors such as distance metrics, pooling functions, bottleneck pre-training,
training with noise addition, embedding quantization, and embedding dimensions
-- aspects not previously addressed in the original Vec2Text paper. Through a
thorough analysis of these factors, our aim is to gain a deeper understanding
of the critical elements impacting the trade-offs between text recoverability
and retrieval effectiveness in dense retrieval systems. This analysis provides
valuable insights for practitioners involved in designing privacy-aware dense
retrieval systems. Additionally, we propose a straightforward fix for embedding
transformation that ensures equal ranking effectiveness while mitigating the
risk of text recoverability.
<br />Furthermore, we extend the application of Vec2Text to the separate task of
corpus poisoning, where, theoretically, Vec2Text presents a more potent threat
compared to previous attack methods. Notably, Vec2Text does not require access
to the dense retriever's model parameters and can efficiently generate numerous
adversarial passages.
<br />In summary, this study highlights the potential threat posed by Vec2Text to
existing dense retrieval systems, while also presenting effective methods to
patch and strengthen such systems against such risks.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12786" title="Abstract">arXiv:2402.12786</a> [<a href="/pdf/2402.12786" title="Download PDF">pdf</a>, <a href="/format/2402.12786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Large Language Models to Capture Varied Speaking Styles and  Respond Properly in Spoken Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guan-Ting Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Cheng-Han Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In spoken dialogue, even if two current turns are the same sentence, their
responses might still differ when they are spoken in different styles. The
spoken styles, containing paralinguistic and prosodic information, mark the
most significant difference between text and speech modality. When using
text-only LLMs to model spoken dialogue, text-only LLMs cannot give different
responses based on the speaking style of the current turn. In this paper, we
focus on enabling LLMs to listen to the speaking styles and respond properly.
Our goal is to teach the LLM that "even if the sentences are identical if they
are spoken in different styles, their corresponding responses might be
different". Since there is no suitable dataset for achieving this goal, we
collect a speech-to-speech dataset, StyleTalk, with the following desired
characteristics: when two current speeches have the same content but are spoken
in different styles, their responses will be different. To teach LLMs to
understand and respond properly to the speaking styles, we propose the
Spoken-LLM framework that can model the linguistic content and the speaking
styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage
training pipeline to help the Spoken-LLM better learn the speaking styles.
Based on extensive experiments, we show that Spoken-LLM outperforms text-only
baselines and prior speech LLMs methods.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12788" title="Abstract">arXiv:2402.12788</a> [<a href="/pdf/2402.12788" title="Download PDF">pdf</a>, <a href="/format/2402.12788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal  Periodic Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Bochao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zizheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiansheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huimin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Remote photoplethysmography (rPPG) is a non-contact method for detecting
physiological signals based on facial videos, holding high potential in various
applications such as healthcare, affective computing, anti-spoofing, etc. Due
to the periodicity nature of rPPG, the long-range dependency capturing capacity
of the Transformer was assumed to be advantageous for such signals. However,
existing approaches have not conclusively demonstrated the superior performance
of Transformer over traditional convolutional neural network methods, this gap
may stem from a lack of thorough exploration of rPPG periodicity. In this
paper, we propose RhythmFormer, a fully end-to-end transformer-based method for
extracting rPPG signals by explicitly leveraging the quasi-periodic nature of
rPPG. The core module, Hierarchical Temporal Periodic Transformer,
hierarchically extracts periodic features from multiple temporal scales. It
utilizes dynamic sparse attention based on periodicity in the temporal domain,
allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem
is proposed to guide self-attention to rPPG features effectively, and it can be
easily transferred to existing methods to enhance their performance
significantly. RhythmFormer achieves state-of-the-art performance with fewer
parameters and reduced computational complexity in comprehensive experiments
compared to previous approaches. The codes are available at
https://github.com/zizheng-guo/RhythmFormer.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12789" title="Abstract">arXiv:2402.12789</a> [<a href="/pdf/2402.12789" title="Download PDF">pdf</a>, <a href="/format/2402.12789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Classifiers Without Fair Training: An Influence-Guided Data  Sampling Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jinlong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jialu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaowei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A fair classifier should ensure the benefit of people from different groups,
while the group information is often sensitive and unsuitable for model
training. Therefore, learning a fair classifier but excluding sensitive
attributes in the training dataset is important. In this paper, we study
learning fair classifiers without implementing fair training algorithms to
avoid possible leakage of sensitive information. Our theoretical analyses
validate the possibility of this approach, that traditional training on a
dataset with an appropriate distribution shift can reduce both the upper bound
for fairness disparity and model generalization error, indicating that fairness
and accuracy can be improved simultaneously with simply traditional training.
We then propose a tractable solution to progressively shift the original
training data during training by sampling influential data, where the sensitive
attribute of new data is not accessed in sampling or used in training.
Extensive experiments on real-world data demonstrate the effectiveness of our
proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12790" title="Abstract">arXiv:2402.12790</a> [<a href="/pdf/2402.12790" title="Download PDF">pdf</a>, <a href="/format/2402.12790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Movements to Metrics: Evaluating Explainable AI Methods in  Skeleton-Based Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pellano%2C+K+N">Kimji N. Pellano</a>, 
<a href="/search/cs?searchtype=author&query=Str%C3%BCmke%2C+I">Inga Str&#xfc;mke</a>, 
<a href="/search/cs?searchtype=author&query=Ihlen%2C+E+A+F">Espen Alexander F. Ihlen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The advancement of deep learning in human activity recognition (HAR) using 3D
skeleton data is critical for applications in healthcare, security, sports, and
human-computer interaction. This paper tackles a well-known gap in the field,
which is the lack of testing in the applicability and reliability of XAI
evaluation metrics in the skeleton-based HAR domain. We have tested established
XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM)
and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this
problem. The study also introduces a perturbation method that respects human
biomechanical constraints to ensure realistic variations in human movement. Our
findings indicate that \textit{faithfulness} may not be a reliable metric in
certain contexts, such as with the EfficientGCN model. Conversely, stability
emerges as a more dependable metric when there is slight input data
perturbations. CAM and Grad-CAM are also found to produce almost identical
explanations, leading to very similar XAI metric performance. This calls for
the need for more diversified metrics and new XAI methods applied in
skeleton-based HAR.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12792" title="Abstract">arXiv:2402.12792</a> [<a href="/pdf/2402.12792" title="Download PDF">pdf</a>, <a href="/format/2402.12792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boeder%2C+S">Simon Boeder</a>, 
<a href="/search/cs?searchtype=author&query=Gigengack%2C+F">Fabian Gigengack</a>, 
<a href="/search/cs?searchtype=author&query=Risse%2C+B">Benjamin Risse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic occupancy has recently gained significant traction as a prominent 3D
scene representation. However, most existing methods rely on large and costly
datasets with fine-grained 3D voxel labels for training, which limits their
practicality and scalability, increasing the need for self-monitored learning
in this domain. In this work, we present a novel approach to occupancy
estimation inspired by neural radiance field (NeRF) using only 2D labels, which
are considerably easier to acquire. In particular, we employ differentiable
volumetric rendering to predict depth and semantic maps and train a 3D network
based on 2D supervision only. To enhance geometric accuracy and increase the
supervisory signal, we introduce temporal rendering of adjacent time steps.
Additionally, we introduce occupancy flow as a mechanism to handle dynamic
objects in the scene and ensure their temporal consistency. Through extensive
experimentation we demonstrate that 2D supervision only is sufficient to
achieve state-of-the-art performance compared to methods using 3D labels, while
outperforming concurrent 2D approaches. When combining 2D supervision with 3D
labels, temporal rendering and occupancy flow we outperform all previous
occupancy estimation models significantly. We conclude that the proposed
rendering supervision and occupancy flow advances occupancy estimation and
further bridges the gap towards self-supervised learning in this domain.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12794" title="Abstract">arXiv:2402.12794</a> [<a href="/pdf/2402.12794" title="Download PDF">pdf</a>, <a href="/ps/2402.12794" title="Download PostScript">ps</a>, <a href="/format/2402.12794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Reality Modelling for Cultural Heritage Sites employing  cooperative quadrupedal robots and unmanned aerial vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giakoumidis%2C+N">Nikolaos Giakoumidis</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostopoulos%2C+C">Christos-Nikolaos Anagnostopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners,
mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has
become the prevalent practice for 3D Reality Modeling and digitization of
large-scale monuments of Cultural Heritage (CH). In practice, this process is
heavily related to the expertise of the surveying team, handling the laborious
planning and time-consuming execution of the 3D mapping process that is
tailored to the specific requirements and constraints of each site. To minimize
human intervention, this paper introduces a novel methodology for autonomous 3D
Reality Modeling for CH monuments by employing au-tonomous biomimetic
quadrupedal robotic agents and UAVs equipped with the appropriate sensors.
These autonomous robotic agents carry out the 3D RM process in a systematic and
repeatable ap-proach. The outcomes of this automated process may find
applications in digital twin platforms, facilitating secure monitoring and
management of cultural heritage sites and spaces, in both indoor and outdoor
environments.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12797" title="Abstract">arXiv:2402.12797</a> [<a href="/pdf/2402.12797" title="Download PDF">pdf</a>, <a href="/format/2402.12797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cat%2C+S">Songzi Cat</a>, 
<a href="/search/cs?searchtype=author&query=Cat%2C+J">Juzi Cat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages (without reference), 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We introduce a novel approach for the reconstruction of tubular shapes from
skeletal representations. Our method processes all skeletal points as a whole,
eliminating the need for splitting input structure into multiple segments. We
represent the tubular shape as a truncated signed distance function (TSDF) in a
voxel hashing manner, in which the signed distance between a voxel center and
the object is computed through a simple geometric algorithm. Our method does
not involve any surface sampling scheme or solving large matrix equations, and
therefore is a faster and more elegant solution for tubular shape
reconstruction compared to other approaches. Experiments demonstrate the
efficiency and effectiveness of the proposed method. Code is avaliable at
https://github.com/wlsdzyzl/Dragon.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12800" title="Abstract">arXiv:2402.12800</a> [<a href="/pdf/2402.12800" title="Download PDF">pdf</a>, <a href="/ps/2402.12800" title="Download PostScript">ps</a>, <a href="/format/2402.12800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radar-Based Recognition of Static Hand Gestures in American Sign  Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schuessler%2C+C">Christian Schuessler</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A4unig%2C+J">Johanna Br&#xe4;unig</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+M">Marcel Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Stelzig%2C+M">Michael Stelzig</a>, 
<a href="/search/cs?searchtype=author&query=Vossiek%2C+M">Martin Vossiek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures. Accepted to IEEE Radarconf2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the fast-paced field of human-computer interaction (HCI) and virtual
reality (VR), automatic gesture recognition has become increasingly essential.
This is particularly true for the recognition of hand signs, providing an
intuitive way to effortlessly navigate and control VR and HCI applications.
Considering increased privacy requirements, radar sensors emerge as a
compelling alternative to cameras. They operate effectively in low-light
conditions without capturing identifiable human details, thanks to their lower
resolution and distinct wavelength compared to visible light.
<br />While previous works predominantly deploy radar sensors for dynamic hand
gesture recognition based on Doppler information, our approach prioritizes
classification using an imaging radar that operates on spatial information,
e.g. image-like data. However, generating large training datasets required for
neural networks (NN) is a time-consuming and challenging process, often falling
short of covering all potential scenarios. Acknowledging these challenges, this
study explores the efficacy of synthetic data generated by an advanced radar
ray-tracing simulator. This simulator employs an intuitive material model that
can be adjusted to introduce data diversity.
<br />Despite exclusively training the NN on synthetic data, it demonstrates
promising performance when put to the test with real measurement data. This
emphasizes the practicality of our methodology in overcoming data scarcity
challenges and advancing the field of automatic gesture recognition in VR and
HCI applications.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12801" title="Abstract">arXiv:2402.12801</a> [<a href="/pdf/2402.12801" title="Download PDF">pdf</a>, <a href="/format/2402.12801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few shot clinical entity recognition in three languages: Masked language  models outperform LLM prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naguib%2C+M">Marco Naguib</a>, 
<a href="/search/cs?searchtype=author&query=Tannier%2C+X">Xavier Tannier</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%A9v%C3%A9ol%2C+A">Aur&#xe9;lie N&#xe9;v&#xe9;ol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Journal of Artificial Intelligence in Medicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models are becoming the go-to solution for many natural
language processing tasks, including in specialized domains where their
few-shot capacities are expected to yield high performance in low-resource
settings. Herein, we aim to assess the performance of Large Language Models for
few shot clinical entity recognition in multiple languages. We evaluate named
entity recognition in English, French and Spanish using 8 in-domain (clinical)
and 6 out-domain gold standard corpora. We assess the performance of 10
auto-regressive language models using prompting and 16 masked language models
used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot
set-up by limiting the amount of annotated data available to 100 sentences. Our
experiments show that although larger prompt-based models tend to achieve
competitive F-measure for named entity recognition outside the clinical domain,
this level of performance does not carry over to the clinical domain where
lighter supervised taggers relying on masked language models perform better,
even with the performance drop incurred from the few-shot set-up. In all
experiments, the CO2 impact of masked language models is inferior to that of
auto-regressive models. Results are consistent over the three languages and
suggest that few-shot learning using Large language models is not production
ready for named entity recognition in the clinical domain. Instead, models
could be used for speeding-up the production of gold standard annotated data.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12804" title="Abstract">arXiv:2402.12804</a> [<a href="/pdf/2402.12804" title="Download PDF">pdf</a>, <a href="/ps/2402.12804" title="Download PostScript">ps</a>, <a href="/format/2402.12804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modular Assurance of Complex Systems Using Contract-Based Design  Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McGeorge%2C+D">Dag McGeorge</a>, 
<a href="/search/cs?searchtype=author&query=Glomsrud%2C+J+A">Jon Arne Glomsrud</a> (Group Research and Development, DNV, H&#xf8;vik, Norway)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, manuscript submitted to the 43rd International Conference in Computer Safety, Reliability, and Security (SAFECOMP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">A growing number of safety-critical industries agree that building confidence
in complex systems can be achieved through evidence and structured
argumentation framed in assurance cases. Nevertheless, assurance cases can
easily become too rigorous and difficult to develop and maintain when applied
to complex systems. Therefore, we propose to use contract-based development
(CBD), a method to manage complexity originally developed in computer science,
to simplify assurance cases by modularizing them. This paper will not only
summarize relevant previous work such as constructing consistent modular
assurance cases using CBD, but more importantly also propose a novel approach
to integrate CBD with the argumentation in assurance case modules. This
approach will allow interdisciplinary subject-matter and domain experts to
build assurance cases together without even knowing about CBD. This helps
subject matter experts outside of computer science to reap benefits from CBD
and helps with interdisciplinary co-development of assurance cases that cover
all the required fields. This paper motivates four rules of thumb aimed to help
practitioners developing high-quality modular assurance cases. It also explains
how modularization of assurance is an enabler for multi-concern assurance that
accounts for the inter-dependency of different concerns such as safety,
security and performance.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12806" title="Abstract">arXiv:2402.12806</a> [<a href="/pdf/2402.12806" title="Download PDF">pdf</a>, <a href="/format/2402.12806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SymBa: Symbolic Backward Chaining for Multi-step Natural Language  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jinu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+W">Wonseok Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages (8 pages for main text),9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have recently demonstrated remarkable reasoning
ability as in Chain-of-thought prompting, but faithful multi-step reasoning
remains a challenge. We specifically focus on backward chaining, where the
query is recursively decomposed using logical rules until proven. To address
the limitations of current backward chaining implementations, we propose SymBa
(Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls
the entire proof process and the LLM is called to generate a single reasoning
step only when the solver encounters a dead end. By this novel solver-LLM
integration, while being able to produce an interpretable, structured proof,
SymBa achieves significant improvement in performance, proof faithfulness, and
efficiency in diverse multi-step reasoning benchmarks (ProofWriter,
Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward
chaining baselines.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12808" title="Abstract">arXiv:2402.12808</a> [<a href="/pdf/2402.12808" title="Download PDF">pdf</a>, <a href="/format/2402.12808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Generalization and Regularization of Nonhomogeneous Temporal  Poisson Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van%2C+S+N">Son Nguyen Van</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+H+N">Hoai Nguyen Xuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is
an essentially important counting process with numerous real-world
applications. Up to date, almost all works in the literature have been on the
estimation of NHPPs with infinite data using non-data driven binning methods.
In this paper, we formulate the problem of estimation of NHPPs from finite and
limited data as a learning generalization problem. We mathematically show that
while binning methods are essential for the estimation of NHPPs, they pose a
threat of overfitting when the amount of data is limited. We propose a
framework for regularized learning of NHPPs with two new adaptive and
data-driven binning methods that help to remove the ad-hoc tuning of binning
parameters. Our methods are experimentally tested on synthetic and real-world
datasets and the results show their effectiveness.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12810" title="Abstract">arXiv:2402.12810</a> [<a href="/pdf/2402.12810" title="Download PDF">pdf</a>, <a href="/format/2402.12810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIP-Net: Pedestrian Intention Prediction in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azarmi%2C+M">Mohsen Azarmi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+M">Mahdi Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Glaser%2C+S">Sebastien Glaser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Image and Video Processing (eess.IV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs)
is one of the current research challenges in this field. In this article, we
introduce PIP-Net, a novel framework designed to predict pedestrian crossing
intentions by AVs in real-world urban scenarios. We offer two variants of
PIP-Net designed for different camera mounts and setups. Leveraging both
kinematic data and spatial features from the driving scene, the proposed model
employs a recurrent and temporal attention-based solution, outperforming
state-of-the-art performance. To enhance the visual representation of road
users and their proximity to the ego vehicle, we introduce a categorical depth
feature map, combined with a local motion flow feature, providing rich insights
into the scene dynamics. Additionally, we explore the impact of expanding the
camera's field of view, from one to three cameras surrounding the ego vehicle,
leading to enhancement in the model's contextual perception. Depending on the
traffic scenario and road environment, the model excels in predicting
pedestrian crossing intentions up to 4 seconds in advance which is a
breakthrough in current research studies in pedestrian intention prediction.
Finally, for the first time, we present the Urban-PIP dataset, a customised
pedestrian intention prediction dataset, with multi-camera annotations in
real-world automated driving scenarios.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12812" title="Abstract">arXiv:2402.12812</a> [<a href="/pdf/2402.12812" title="Download PDF">pdf</a>, <a href="/format/2402.12812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Decentralized Algorithms for Online Personalized Mean  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galante%2C+F">Franco Galante</a>, 
<a href="/search/cs?searchtype=author&query=Neglia%2C+G">Giovanni Neglia</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+E">Emilio Leonardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In numerous settings, agents lack sufficient data to directly learn a model.
Collaborating with other agents may help, but it introduces a bias-variance
trade-off, when local data distributions differ. A key challenge is for each
agent to identify clients with similar distributions while learning the model,
a problem that remains largely unresolved. This study focuses on a simplified
version of the overarching problem, where each agent collects samples from a
real-valued distribution over time to estimate its mean. Existing algorithms
face impractical space and time complexities (quadratic in the number of agents
A). To address scalability challenges, we propose a framework where agents
self-organize into a graph, allowing each agent to communicate with only a
selected number of peers r. We introduce two collaborative mean estimation
algorithms: one draws inspiration from belief propagation, while the other
employs a consensus-based approach, with complexity of O( r |A| log |A|) and
O(r |A|), respectively. We establish conditions under which both algorithms
yield asymptotically optimal estimates and offer a theoretical characterization
of their performance.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12813" title="Abstract">arXiv:2402.12813</a> [<a href="/pdf/2402.12813" title="Download PDF">pdf</a>, <a href="/format/2402.12813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws Behind Code Understanding Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiayi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hande Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yutao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The scaling law is becoming a fundamental law in many machine learning areas.
That is, test error falls off with the power law when increasing training data,
model size, and computing resource. However, whether this law is suitable for
the task of code understanding is not well studied, and most current language
models for code understanding are about 100M parameters, which are relatively
"small" compared to large language models. In this paper, we conduct extensive
experiments to investigate the scaling law for the code understanding task by
varying training data, model size, and computing resource. We validate that the
test error of code understanding models falls off with the power law when using
larger models, indicating that the scaling law is suitable for the code
understanding task. Besides, we apply different scales of models to two
downstream code understanding tasks, and find that the performance increases
with larger scale of models. Finally, we train a large-scale code understanding
model named CoLSBERT with 1.5B parameters on a large dataset using more
computing resource, which outperforms previous work by a large margin. We will
release our code and the CoLSBERT model when our paper is published.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12814" title="Abstract">arXiv:2402.12814</a> [<a href="/pdf/2402.12814" title="Download PDF">pdf</a>, <a href="/format/2402.12814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Interaction of Creative Writers with AI-Powered Writing  Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Alicia Guo</a>, 
<a href="/search/cs?searchtype=author&query=Pataranutaporn%2C+P">Pat Pataranutaporn</a>, 
<a href="/search/cs?searchtype=author&query=Maes%2C+P">Pattie Maes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">AI-based virtual assistants are increasingly used to support daily ideation
tasks. The values or bias present in these agents can influence output in
hidden ways. They may also affect how people perceive the ideas produced with
these AI agents and lead to implications for the design of AI-based tools. We
explored the effects of AI agents with different values on the ideation process
and user perception of idea quality, ownership, agent competence, and values
present in the output. Our study tasked 180 participants with brainstorming
practical solutions to a set of problems with AI agents of different values.
Results show no significant difference in self-evaluation of idea quality and
perception of the agent based on value alignment; however, ideas generated
reflected the AI's values and feeling of ownership is affected. This highlights
an intricate interplay between AI values and human ideation, suggesting careful
design considerations for future AI-supported brainstorming tools.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12817" title="Abstract">arXiv:2402.12817</a> [<a href="/pdf/2402.12817" title="Download PDF">pdf</a>, <a href="/format/2402.12817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Sensitivity of Learning with Limited Labelled Data to the Effects of  Randomness: Impact of Interactions and Systematic Choices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pecher%2C+B">Branislav Pecher</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12819" title="Abstract">arXiv:2402.12819</a> [<a href="/pdf/2402.12819" title="Download PDF">pdf</a>, <a href="/format/2402.12819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How  Many Labelled Samples Do We Need?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pecher%2C+B">Branislav Pecher</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">When solving a task with limited labelled data, researchers can either use a
general large language model without further update, or use the few examples to
tune a specialised smaller model. When enough labels are available, the
specialised models outperform the general ones on many NLP tasks. In this work,
we aim to investigate how many labelled samples are required for the
specialised models to achieve this superior performance, while taking the
results variance into consideration. Observing the behaviour of prompting,
in-context learning, fine-tuning and instruction-tuning, identifying their
break-even points when increasing number of labelled training samples across
three tasks of varying complexity, we find that the specialised models often
need only few samples ($100-1000$) to be on par or better than the general
ones. At the same time, the amount of required labelled data strongly depends
on the task complexity and results variance.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12820" title="Abstract">arXiv:2402.12820</a> [<a href="/pdf/2402.12820" title="Download PDF">pdf</a>, <a href="/format/2402.12820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASCEND: Accurate yet Efficient End-to-End Stochastic Computing  Acceleration of Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+T">Tong Xie</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yixuan Hu</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+R">Renjie Wei</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+R">Runsheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Ru Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in DATE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Stochastic computing (SC) has emerged as a promising computing paradigm for
neural acceleration. However, how to accelerate the state-of-the-art Vision
Transformer (ViT) with SC remains unclear. Unlike convolutional neural
networks, ViTs introduce notable compatibility and efficiency challenges
because of their nonlinear functions, e.g., softmax and Gaussian Error Linear
Units (GELU). In this paper, for the first time, a ViT accelerator based on
end-to-end SC, dubbed ASCEND, is proposed. ASCEND co-designs the SC circuits
and ViT networks to enable accurate yet efficient acceleration. To overcome the
compatibility challenges, ASCEND proposes a novel deterministic SC block for
GELU and leverages an SC-friendly iterative approximate algorithm to design an
accurate and efficient softmax circuit. To improve inference efficiency, ASCEND
develops a two-stage training pipeline to produce accurate low-precision ViTs.
With extensive experiments, we show the proposed GELU and softmax blocks
achieve 56.3% and 22.6% error reduction compared to existing SC designs,
respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x,
respectively. Moreover, compared to the baseline low-precision ViTs, ASCEND
also achieves significant accuracy improvements on CIFAR10 and CIFAR100.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12821" title="Abstract">arXiv:2402.12821</a> [<a href="/pdf/2402.12821" title="Download PDF">pdf</a>, <a href="/format/2402.12821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Factual Inconsistency in Summaries: Towards Effective  Utilization of Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenlin Su</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+D">Jinho D. Choi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Factual inconsistency poses a significant hurdle for the commercial
deployment of abstractive summarizers. Under this Large Language Model (LLM)
era, this work focuses around two important questions: what is the best way to
leverage LLM for factual inconsistency detection, and how could we distill a
smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms
are firstly proposed and evaluated across five diverse datasets: direct
inference on the entire summary or each summary window; entity verification
through question generation and answering. Experiments suggest that LLM itself
is capable to resolve this task train-free under the proper paradigm design,
surpassing strong trained baselines by 2.8% on average. To further promote
practical utility, we then propose training strategies aimed at distilling
smaller open-source LLM that learns to score the entire summary at once with
high accuracy, which outperforms the zero-shot approaches by much larger LLM,
serving as an effective and efficient ready-to-use scorer.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12831" title="Abstract">arXiv:2402.12831</a> [<a href="/pdf/2402.12831" title="Download PDF">pdf</a>, <a href="/format/2402.12831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A sparse hierarchical $hp$-finite element method on disks and annuli
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Papadopoulos%2C+I+P+A">Ioannis P. A. Papadopoulos</a>, 
<a href="/search/math?searchtype=author&query=Olver%2C+S">Sheehan Olver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We develop a sparse hierarchical $hp$-finite element method ($hp$-FEM) for
the Helmholtz equation with rotationally invariant variable coefficients posed
on a two-dimensional disk or annulus. The mesh is an inner disk cell (omitted
if on an annulus domain) and concentric annuli cells. The discretization
preserves the Fourier mode decoupling of rotationally invariant operators, such
as the Laplacian, which manifests as block diagonal mass and stiffness
matrices. Moreover, the matrices have a sparsity pattern independent of the
order of the discretization and admit an optimal complexity factorization. The
sparse $hp$-FEM can handle radial discontinuities in the right-hand side and in
rotationally invariant Helmholtz coefficients. We consider examples such as a
high-frequency Helmholtz equation with radial discontinuities, the
time-dependent Schr\"odinger equation, and an extension to a three-dimensional
cylinder domain, with a quasi-optimal solve, via the Alternating Direction
Implicit (ADI) algorithm.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12832" title="Abstract">arXiv:2402.12832</a> [<a href="/pdf/2402.12832" title="Download PDF">pdf</a>, <a href="/ps/2402.12832" title="Download PostScript">ps</a>, <a href="/format/2402.12832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Optimal Fault Tolerant Distance Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+D">Dipan Dey</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Manoj Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We present an $f$-fault tolerant distance oracle for an undirected weighted
graph where each edge has an integral weight from $[1 \dots W]$. Given a set
$F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our
oracle returns the \emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf
\log (nW))^{O(f^2)})$ time, where $c &gt; 1$ is a constant. The space complexity
of our oracle is $O(f^4n^2\log^2 (nW))$. For a constant $f$, our oracle is
nearly optimal both in terms of space and time (barring some logarithmic
factor).
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12833" title="Abstract">arXiv:2402.12833</a> [<a href="/pdf/2402.12833" title="Download PDF">pdf</a>, <a href="/format/2402.12833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Multi-Preconditioned Conjugate Gradient with Additive  Multigrid Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kothari%2C+H">Hardik Kothari</a>, 
<a href="/search/math?searchtype=author&query=Nestola%2C+M+G+C">Maria Giuseppina Chiara Nestola</a>, 
<a href="/search/math?searchtype=author&query=Favino%2C+M">Marco Favino</a>, 
<a href="/search/math?searchtype=author&query=Krause%2C+R">Rolf Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Due to its optimal complexity, the multigrid (MG) method is one of the most
popular approaches for solving large-scale linear systems arising from the
discretization of partial differential equations. However, the parallel
implementation of standard MG methods, which are inherently multiplicative,
suffers from increasing communication complexity. In such cases, the additive
variants of MG methods provide a good alternative due to their inherently
parallel nature, although they exhibit slower convergence. This work combines
the additive multigrid method with the multipreconditioned conjugate gradient
(MPCG) method. In the proposed approach, the MPCG method employs the
corrections from the different levels of the MG hierarchy as separate
preconditioned search directions. In this approach, the MPCG method updates the
current iterate by using the linear combination of the preconditioned search
directions, where the optimal coefficients for the linear combination are
computed by exploiting the energy norm minimization of the CG method. The idea
behind our approach is to combine the $A$-conjugacy of the search directions of
the MPCG method and the quasi $H_1$-orthogonality of the corrections from the
MG hierarchy. In the numerical section, we study the performance of the
proposed method compared to the standard additive and multiplicative MG methods
used as preconditioners for the CG method.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12834" title="Abstract">arXiv:2402.12834</a> [<a href="/pdf/2402.12834" title="Download PDF">pdf</a>, <a href="/format/2402.12834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tirelli%2C+C">Cristian Tirelli</a>, 
<a href="/search/cs?searchtype=author&query=Sapriza%2C+J">Juan Sapriza</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez%2C+R+R">Rub&#xe9;n Rodr&#xed;guez &#xc1;lvarez</a>, 
<a href="/search/cs?searchtype=author&query=Ferretti%2C+L">Lorenzo Ferretti</a>, 
<a href="/search/cs?searchtype=author&query=Denkinger%2C+B">Beno&#xee;t Denkinger</a>, 
<a href="/search/cs?searchtype=author&query=Ansaloni%2C+G">Giovanni Ansaloni</a>, 
<a href="/search/cs?searchtype=author&query=Calero%2C+J+M">Jos&#xe9; Miranda Calero</a>, 
<a href="/search/cs?searchtype=author&query=Atienza%2C+D">David Atienza</a>, 
<a href="/search/cs?searchtype=author&query=Pozzi%2C+L">Laura Pozzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power
architectures designed to accelerate Compute-Intensive Loops (CILs). The
effectiveness of CGRAs in providing acceleration relies on the quality of
mapping: how efficiently the CIL is compiled onto the platform. State of the
Art (SoA) compilation techniques utilize modulo scheduling to minimize the
Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to
address mapping challenges. Our work approaches the mapping problem through a
satisfiability (SAT) formulation. We introduce the Kernel Mobility Schedule
(KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural
information to generate Boolean statements that, when satisfied, yield a valid
mapping. Experimental results demonstrate SAT-MapIt outperforming SoA
alternatives in almost 50\% of explored benchmarks. Additionally, we evaluated
the mapping results in a synthesizable CGRA design and emphasized the run-time
metrics trends, i.e. energy efficiency and latency, across different CILs and
CGRA sizes. We show that a hardware-agnostic analysis performed on
compiler-level metrics can optimally prune the architectural design space,
while still retaining Pareto-optimal configurations. Moreover, by exploring how
implementation details impact cost and performance on real hardware, we
highlight the importance of holistic software-to-hardware mapping flows, as the
one presented herein.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12835" title="Abstract">arXiv:2402.12835</a> [<a href="/pdf/2402.12835" title="Download PDF">pdf</a>, <a href="/format/2402.12835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">An Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qingyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Large language models (LLMs) have demonstrated considerable
capabilities across various natural language tasks, they often fall short of
the performance achieved by domain-specific state-of-the-art models. One
potential approach to enhance domain-specific capabilities of LLMs involves
fine-tuning them using corresponding datasets. However, this method can be both
resource and time-intensive, and not applicable to closed-source commercial
LLMs. In this paper, we propose Preference Adaptation for Enhancing
Domain-specific Abilities of LLMs (PANDA), a method designed to augment the
domain-specific capabilities of LLMs by leveraging insights from the response
preference of expert models without requiring fine-tuning. Our experimental
results reveal that PANDA significantly enhances the domain-specific ability of
LLMs on text classification and interactive decision tasks. Moreover, LLM with
PANDA even outperforms the expert model that being learned on 4 tasks of
ScienceWorld. This finding highlights the potential of exploring tuning-free
approaches to achieve weak-to-strong generalization.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12840" title="Abstract">arXiv:2402.12840</a> [<a href="/pdf/2402.12840" title="Download PDF">pdf</a>, <a href="/format/2402.12840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koto%2C+F">Fajri Koto</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Shatnawi%2C+S">Sara Shatnawi</a>, 
<a href="/search/cs?searchtype=author&query=Doughman%2C+J">Jad Doughman</a>, 
<a href="/search/cs?searchtype=author&query=Sadallah%2C+A+B">Abdelrahman Boda Sadallah</a>, 
<a href="/search/cs?searchtype=author&query=Alraeesi%2C+A">Aisha Alraeesi</a>, 
<a href="/search/cs?searchtype=author&query=Almubarak%2C+K">Khalid Almubarak</a>, 
<a href="/search/cs?searchtype=author&query=Alyafeai%2C+Z">Zaid Alyafeai</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+N">Neha Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Shehata%2C+S">Shady Shehata</a>, 
<a href="/search/cs?searchtype=author&query=Habash%2C+N">Nizar Habash</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The focus of language model evaluation has transitioned towards reasoning and
knowledge-intensive tasks, driven by advancements in pretraining large models.
While state-of-the-art models are partially trained on large Arabic texts,
evaluating their performance in Arabic remains challenging due to the limited
availability of relevant datasets. To bridge this gap, we present ArabicMMLU,
the first multi-task language understanding benchmark for Arabic language,
sourced from school exams across diverse educational levels in different
countries spanning North Africa, the Levant, and the Gulf regions. Our data
comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard
Arabic (MSA), and is carefully constructed by collaborating with native
speakers in the region. Our comprehensive evaluations of 35 models reveal
substantial room for improvement, particularly among the best open-source
models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of
50%, while even the top-performing Arabic-centric model only achieves a score
of 62.3%.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12842" title="Abstract">arXiv:2402.12842</a> [<a href="/pdf/2402.12842" title="Download PDF">pdf</a>, <a href="/format/2402.12842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptKD: Distilling Student-Friendly Knowledge for Generative Language  Models via Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gyeongman Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+D">Doohyuk Jang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Eunho Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have raised concerns
about inference costs, increasing the need for research into model compression.
While knowledge distillation (KD) is a prominent method for this, research on
KD for generative language models like LLMs is relatively sparse, and the
approach of distilling student-friendly knowledge, which has shown promising
performance in KD for classification models, remains unexplored in generative
language models. To explore this approach, we propose PromptKD, a simple yet
effective method that utilizes prompt tuning - for the first time in KD - to
enable generative language models to transfer student-friendly knowledge.
Unlike previous works in classification that require fine-tuning the entire
teacher model for extracting student-friendly knowledge, PromptKD achieves
similar effects by adding a small number of prompt tokens and tuning only the
prompt with student guidance. Extensive experiments on instruction-following
datasets using the GPT-2 model family show that PromptKD achieves
state-of-the-art performance while adding only 0.0007% of the teacher's
parameters as prompts. Further analysis suggests that distilling
student-friendly knowledge alleviates exposure bias effectively throughout the
entire training process, leading to performance enhancements.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12843" title="Abstract">arXiv:2402.12843</a> [<a href="/pdf/2402.12843" title="Download PDF">pdf</a>, <a href="/format/2402.12843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sagaram%2C+S">Sankarshanaa Sagaram</a>, 
<a href="/search/cs?searchtype=author&query=Kasliwal%2C+A">Aditya Kasliwal</a>, 
<a href="/search/cs?searchtype=author&query=Didwania%2C+K">Krish Didwania</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+L">Laven Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Kailas%2C+P">Pallavi Kailas</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+U">Ujjwal Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR Tiny Paper 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The increasing adoption of solar energy necessitates advanced methodologies
for monitoring and maintenance to ensure optimal performance of solar panel
installations. A critical component in this context is the accurate
segmentation of solar panels from aerial or satellite imagery, which is
essential for identifying operational issues and assessing efficiency. This
paper addresses the significant challenges in panel segmentation, particularly
the scarcity of annotated data and the labour-intensive nature of manual
annotation for supervised learning. We explore and apply Self-Supervised
Learning (SSL) to solve these challenges. We demonstrate that SSL significantly
enhances model generalization under various conditions and reduces dependency
on manually annotated data, paving the way for robust and adaptable solar panel
segmentation solutions.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12844" title="Abstract">arXiv:2402.12844</a> [<a href="/pdf/2402.12844" title="Download PDF">pdf</a>, <a href="/format/2402.12844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICON: Improving Inter-Report Consistency of Radiology Report Generation  via Lesion-aware Mix-up Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wenjun Hou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaishuai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Previous research on radiology report generation has made significant
progress in terms of increasing the clinical accuracy of generated reports. In
this paper, we emphasize another crucial quality that it should possess, i.e.,
inter-report consistency, which refers to the capability of generating
consistent reports for semantically equivalent radiographs. This quality is
even of greater significance than the overall report accuracy in terms of
ensuring the system's credibility, as a system prone to providing conflicting
results would severely erode users' trust. Regrettably, existing approaches
struggle to maintain inter-report consistency, exhibiting biases towards common
patterns and susceptibility to lesion variants. To address this issue, we
propose ICON, which improves the inter-report consistency of radiology report
generation. Aiming at enhancing the system's ability to capture the
similarities in semantically equivalent lesions, our approach involves first
extracting lesions from input images and examining their characteristics. Then,
we introduce a lesion-aware mix-up augmentation technique to ensure that the
representations of the semantically equivalent lesions align with the same
attributes, by linearly interpolating them during the training phase. Extensive
experiments on three publicly available chest X-ray datasets verify the
effectiveness of our approach, both in terms of improving the consistency and
accuracy of the generated reports.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12845" title="Abstract">arXiv:2402.12845</a> [<a href="/pdf/2402.12845" title="Download PDF">pdf</a>, <a href="/format/2402.12845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared  Semantic Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+M">Ming Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Drawing upon the intuition that aligning different modalities to the same
semantic embedding space would allow models to understand states and actions
more easily, we propose a new perspective to the offline reinforcement learning
(RL) challenge. More concretely, we transform it into a supervised learning
task by integrating multimodal and pre-trained language models. Our approach
incorporates state information derived from images and action-related data
obtained from text, thereby bolstering RL training performance and promoting
long-term strategic thinking. We emphasize the contextual understanding of
language and demonstrate how decision-making in RL can benefit from aligning
states' and actions' representation with languages' representation. Our method
significantly outperforms current baselines as evidenced by evaluations
conducted on Atari and OpenAI Gym environments. This contributes to advancing
offline RL performance and efficiency while providing a novel perspective on
offline RL.Our code and data are available at
https://github.com/Zheng0428/MORE_.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12846" title="Abstract">arXiv:2402.12846</a> [<a href="/pdf/2402.12846" title="Download PDF">pdf</a>, <a href="/format/2402.12846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConVQG: Contrastive Visual Question Generation with Multimodal Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Li Mi</a>, 
<a href="/search/cs?searchtype=author&query=Montariol%2C+S">Syrielle Montariol</a>, 
<a href="/search/cs?searchtype=author&query=Castillo-Navarro%2C+J">Javiera Castillo-Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xianjie Dai</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=Tuia%2C+D">Devis Tuia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024. Project page at <a href="https://limirs.github.io/ConVQG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Asking questions about visual environments is a crucial way for intelligent
agents to understand rich multi-faceted scenes, raising the importance of
Visual Question Generation (VQG) systems. Apart from being grounded to the
image, existing VQG systems can use textual constraints, such as expected
answers or knowledge triplets, to generate focused questions. These constraints
allow VQG systems to specify the question content or leverage external
commonsense knowledge that can not be obtained from the image content only.
However, generating focused questions using textual constraints while enforcing
a high relevance to the image content remains a challenge, as VQG systems often
ignore one or both forms of grounding. In this work, we propose Contrastive
Visual Question Generation (ConVQG), a method using a dual contrastive
objective to discriminate questions generated using both modalities from those
based on a single one. Experiments on both knowledge-aware and standard VQG
benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and
generates image-grounded, text-guided, and knowledge-rich questions. Our human
evaluation results also show preference for ConVQG questions compared to
non-contrastive baselines.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12847" title="Abstract">arXiv:2402.12847</a> [<a href="/pdf/2402.12847" title="Download PDF">pdf</a>, <a href="/format/2402.12847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction-tuned Language Models are Better Knowledge Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhengbao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhiqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pedro Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+W">Wen-tau Yih</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+S">Srinivasan Iyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In order for large language model (LLM)-based assistants to effectively adapt
to evolving information needs, it must be possible to update their factual
knowledge through continued training on new data. The standard recipe for doing
so involves continued pre-training on new documents followed by
instruction-tuning on question-answer (QA) pairs. However, we find that LLMs
trained with this recipe struggle to answer questions, even though the
perplexity of documents is minimized. We found that QA pairs are generally
straightforward, while documents are more complex, weaving many factual
statements together in an intricate manner. Therefore, we hypothesize that it
is beneficial to expose LLMs to QA pairs before continued pre-training on
documents so that the process of encoding knowledge from complex documents
takes into account how this knowledge is accessed through questions. Based on
this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes
on questions prior to training on documents. This contrasts with standard
instruction-tuning, which learns how to extract knowledge after training on
documents. Extensive experiments and ablation studies demonstrate that PIT
significantly enhances the ability of LLMs to absorb knowledge from new
documents, outperforming standard instruction-tuning by 17.8%.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12851" title="Abstract">arXiv:2402.12851</a> [<a href="/pdf/2402.12851" title="Download PDF">pdf</a>, <a href="/format/2402.12851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoELoRA: Contrastive Learning Guided Mixture of Experts on  Parameter-Efficient Fine-Tuning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tongxu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jiahe Lei</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fine-tuning is often necessary to enhance the adaptability of Large Language
Models (LLM) to downstream tasks. Nonetheless, the process of updating billions
of parameters demands significant computational resources and training time,
which poses a substantial obstacle to the widespread application of large-scale
models in various scenarios. To address this issue, Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.
However, current PEFT approaches that employ a limited set of global parameters
(such as LoRA, which adds low-rank approximation matrices to all weights) face
challenges in flexibly combining different computational modules in downstream
tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider
LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon
observed in MoE, we propose the utilization of contrastive learning to
encourage experts to learn distinct features. We conducted experiments on 11
tasks in math reasoning and common-sense reasoning benchmarks. With the same
number of parameters, our approach outperforms LoRA significantly. In math
reasoning, MoELoRA achieved an average performance that was 4.2% higher than
LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on
several benchmarks.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12852" title="Abstract">arXiv:2402.12852</a> [<a href="/pdf/2402.12852" title="Download PDF">pdf</a>, <a href="/format/2402.12852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCFC++: Enhancing Federated Clustering through Feature Decorrelation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yi-Zi Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong-Yuan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In federated clustering, multiple data-holding clients collaboratively group
data without exchanging raw data. This field has seen notable advancements
through its marriage with contrastive learning, exemplified by
Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from
heterogeneous data across clients, leading to poor and unrobust performance.
Our study conducts both empirical and theoretical analyses to understand the
impact of heterogeneous data on CCFC. Findings indicate that increased data
heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased
correlations across multiple dimensions of the learned representations. To
address this, we introduce a decorrelation regularizer to CCFC. Benefiting from
the regularizer, the improved method effectively mitigates the detrimental
effects of data heterogeneity, and achieves superior performance, as evidenced
by a marked increase in NMI scores, with the gain reaching as high as 0.32 in
the most pronounced case.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12854" title="Abstract">arXiv:2402.12854</a> [<a href="/pdf/2402.12854" title="Download PDF">pdf</a>, <a href="/format/2402.12854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Mapper For Topological Optimization Of Data  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oulhaj%2C+Z">Ziyad Oulhaj</a>, 
<a href="/search/cs?searchtype=author&query=Carri%C3%A8re%2C+M">Mathieu Carri&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+B">Bertrand Michel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG); Algebraic Topology (math.AT)

</div>
<p class="mathjax">Unsupervised data representation and visualization using tools from topology
is an active and growing field of Topological Data Analysis (TDA) and data
science. Its most prominent line of work is based on the so-called Mapper
graph, which is a combinatorial graph whose topological structures (connected
components, branches, loops) are in correspondence with those of the data
itself. While highly generic and applicable, its use has been hampered so far
by the manual tuning of its many parameters-among these, a crucial one is the
so-called filter: it is a continuous function whose variations on the data set
are the main ingredient for both building the Mapper representation and
assessing the presence and sizes of its topological structures. However, while
a few parameter tuning methods have already been investigated for the other
Mapper parameters (i.e., resolution, gain, clustering), there is currently no
method for tuning the filter itself. In this work, we build on a recently
proposed optimization framework incorporating topology to provide the first
filter optimization scheme for Mapper graphs. In order to achieve this, we
propose a relaxed and more general version of the Mapper graph, whose
convergence properties are investigated. Finally, we demonstrate the usefulness
of our approach by optimizing Mapper graph representations on several datasets,
and showcasing the superiority of the optimized representation over arbitrary
ones.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12856" title="Abstract">arXiv:2402.12856</a> [<a href="/pdf/2402.12856" title="Download PDF">pdf</a>, <a href="/format/2402.12856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new simplified MOPSO based on Swarm Elitism and Swarm Memory: MO-ETPSO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fitas%2C+R">Ricardo Fitas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">This paper presents an algorithm based on Particle Swarm Optimization (PSO),
adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO).
The proposed algorithm integrates core strategies from the well-established
NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the
advantages of Swarm Intelligence in terms of individual and social cognition. A
novel aspect of the algorithm is the introduction of a swarm memory and swarm
elitism, which may turn the adoption of NSGA-II strategies in PSO. These
features enhance the algorithm's ability to retain and utilize high-quality
solutions throughout optimization. Furthermore, all operators within the
algorithm are intentionally designed for simplicity, ensuring ease of
replication and implementation in various settings. Preliminary comparisons
with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms
of solutions found and convergence, have yielded promising results in favor of
MO-ETPSO.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12861" title="Abstract">arXiv:2402.12861</a> [<a href="/pdf/2402.12861" title="Download PDF">pdf</a>, <a href="/format/2402.12861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding Reconstruction Attack Success of Adversaries Without Data  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziller%2C+A">Alexander Ziller</a>, 
<a href="/search/cs?searchtype=author&query=Riess%2C+A">Anneliese Riess</a>, 
<a href="/search/cs?searchtype=author&query=Schwethelm%2C+K">Kristian Schwethelm</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+T+T">Tamara T. Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/cs?searchtype=author&query=Kaissis%2C+G">Georgios Kaissis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Reconstruction attacks on machine learning (ML) models pose a strong risk of
leakage of sensitive data. In specific contexts, an adversary can (almost)
perfectly reconstruct training data samples from a trained model using the
model's gradients. When training ML models with differential privacy (DP),
formal upper bounds on the success of such reconstruction attacks can be
provided. So far, these bounds have been formulated under worst-case
assumptions that might not hold high realistic practicality. In this work, we
provide formal upper bounds on reconstruction success under realistic
adversarial settings against ML models trained with DP and support these bounds
with empirical results. With this, we show that in realistic scenarios, (a) the
expected reconstruction success can be bounded appropriately in different
contexts and by different metrics, which (b) allows for a more educated choice
of a privacy parameter.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12862" title="Abstract">arXiv:2402.12862</a> [<a href="/pdf/2402.12862" title="Download PDF">pdf</a>, <a href="/format/2402.12862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Ambiguity in Emotion: From Out-of-Domain Detection to  Distribution Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+C">Chung-Cheng Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiujia Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Junwen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Sainath%2C+T+N">Tara N. Sainath</a>, 
<a href="/search/cs?searchtype=author&query=Woodland%2C+P+C">Philip C. Woodland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The subjective perception of emotion leads to inconsistent labels from human
annotators. Typically, utterances lacking majority-agreed labels are excluded
when training an emotion classifier, which cause problems when encountering
ambiguous emotional expressions during testing. This paper investigates three
methods to handle ambiguous emotion. First, we show that incorporating
utterances without majority-agreed labels as an additional class in the
classifier reduces the classification performance of the other emotion classes.
Then, we propose detecting utterances with ambiguous emotions as out-of-domain
samples by quantifying the uncertainty in emotion classification using
evidential deep learning. This approach retains the classification accuracy
while effectively detects ambiguous emotion expressions. Furthermore, to obtain
fine-grained distinctions among ambiguous emotions, we propose representing
emotion as a distribution instead of a single class label. The task is thus
re-framed from classification to distribution estimation where every individual
annotation is taken into account, not just the majority opinion. The evidential
uncertainty measure is extended to quantify the uncertainty in emotion
distribution estimation. Experimental results on the IEMOCAP and CREMA-D
datasets demonstrate the superior capability of the proposed method in terms of
majority class prediction, emotion distribution estimation, and uncertainty
estimation.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12863" title="Abstract">arXiv:2402.12863</a> [<a href="/pdf/2402.12863" title="Download PDF">pdf</a>, <a href="/format/2402.12863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Cross-rule Optimization Bugs in Datalog Engines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Linzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rigger%2C+M">Manuel Rigger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The ACM SIGPLAN Conference on Object Oriented Programming, Systems, Languages, and Applications (2024), Pasadena, California, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Datalog is a popular and widely-used declarative logic programming language.
Datalog engines apply many cross-rule optimizations; bugs in them can cause
incorrect results. To detect such optimization bugs, we propose an automated
testing approach called Incremental Rule Evaluation (IRE), which
synergistically tackles the test oracle and test case generation problem. The
core idea behind the test oracle is to compare the results of an optimized
program and a program without cross-rule optimization; any difference indicates
a bug in the Datalog engine. Our core insight is that, for an optimized,
incrementally-generated Datalog program, we can evaluate all rules individually
by constructing a reference program to disable the optimizations that are
performed among multiple rules. Incrementally generating test cases not only
allows us to apply the test oracle for every new rule generated-we also can
ensure that every newly added rule generates a non-empty result with a given
probability and eschew recomputing already-known facts. We implemented IRE as a
tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely
Souffl\'e, CozoDB, $\mu$Z, and DDlog, and discovered a total of 30 bugs. Of
these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt
can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the
bugs identified by Deopt, queryFuzz might be unable to detect 5. Our
incremental test case generation approach is efficient; for example, for test
cases containing 60 rules, our incremental approach can produce 1.17$\times$
(for DDlog) to 31.02$\times$ (for Souffl\'e) as many valid test cases with
non-empty results as the naive random method. We believe that the simplicity
and the generality of the approach will lead to its wide adoption in practice.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12864" title="Abstract">arXiv:2402.12864</a> [<a href="/pdf/2402.12864" title="Download PDF">pdf</a>, <a href="/format/2402.12864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Protocol Using Captive Portals for FIDO2 Network Authentication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rivera-Dourado%2C+M">Marti&#xf1;o Rivera-Dourado</a>, 
<a href="/search/cs?searchtype=author&query=Gestal%2C+M">Marcos Gestal</a>, 
<a href="/search/cs?searchtype=author&query=Pazos%2C+A">Alejandro Pazos</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1zquez-Naya%2C+J">Jose V&#xe1;zquez-Naya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an author version. It has not been peer reviewed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Software Engineering (cs.SE)

</div>
<p class="mathjax">FIDO2 authentication is starting to be applied in numerous web authentication
services, aiming to replace passwords and their known vulnerabilities. However,
this new authentication method has not been integrated yet with network
authentication systems. In this paper, we introduce FIDO2CAP: FIDO2
Captive-portal Authentication Protocol. Our proposal describes a novel protocol
for captive-portal network authentication using FIDO2 authenticators, as
security keys and passkeys. For validating our proposal, we have developed a
prototype of FIDO2CAP authentication in a mock scenario. Using this prototype,
we performed an usability experiment with 15 real users. This work makes the
first systematic approach for adapting network authentication to the new
authentication paradigm relying on FIDO2 authentication.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12865" title="Abstract">arXiv:2402.12865</a> [<a href="/pdf/2402.12865" title="Download PDF">pdf</a>, <a href="/format/2402.12865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backward Lens: Projecting Language Model Gradients into the Vocabulary  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katz%2C+S">Shahar Katz</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding how Transformer-based Language Models (LMs) learn and recall
information is a key goal of the deep learning community. Recent
interpretability methods project weights and hidden states obtained from the
forward pass to the models' vocabularies, helping to uncover how information
flows within LMs. In this work, we extend this methodology to LMs' backward
pass and gradients. We first prove that a gradient matrix can be cast as a
low-rank linear combination of its forward and backward passes' inputs. We then
develop methods to project these gradients into vocabulary items and explore
the mechanics of how new information is stored in the LMs' neurons.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12867" title="Abstract">arXiv:2402.12867</a> [<a href="/pdf/2402.12867" title="Download PDF">pdf</a>, <a href="/ps/2402.12867" title="Download PostScript">ps</a>, <a href="/format/2402.12867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards MLOps: A DevOps Tools Recommender System for Machine Learning  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+P+S+U">Pir Sami Ullah Shah</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+N">Naveed Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Beg%2C+M+O">Mirza Omer Beg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Applying DevOps practices to machine learning system is termed as MLOps and
machine learning systems evolve on new data unlike traditional systems on
requirements. The objective of MLOps is to establish a connection between
different open-source tools to construct a pipeline that can automatically
perform steps to construct a dataset, train the machine learning model and
deploy the model to the production as well as store different versions of model
and dataset. Benefits of MLOps is to make sure the fast delivery of the new
trained models to the production to have accurate results. Furthermore, MLOps
practice impacts the overall quality of the software products and is completely
dependent on open-source tools and selection of relevant open-source tools is
considered as challenged while a generalized method to select an appropriate
open-source tools is desirable. In this paper, we present a framework for
recommendation system that processes the contextual information (e.g., nature
of data, type of the data) of the machine learning project and recommends a
relevant toolchain (tech-stack) for the operationalization of machine learning
systems. To check the applicability of the proposed framework, four different
approaches i.e., rule-based, random forest, decision trees and k-nearest
neighbors were investigated where precision, recall and f-score is measured,
the random forest out classed other approaches with highest f-score value of
0.66.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12868" title="Abstract">arXiv:2402.12868</a> [<a href="/pdf/2402.12868" title="Download PDF">pdf</a>, <a href="/format/2402.12868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Rates in Online Convex Optimization by Exploiting the Curvature of  Feasible Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsuchiya%2C+T">Taira Tsuchiya</a>, 
<a href="/search/cs?searchtype=author&query=Ito%2C+S">Shinji Ito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we explore online convex optimization (OCO) and introduce a
new analysis that provides fast rates by exploiting the curvature of feasible
sets. In online linear optimization, it is known that if the average gradient
of loss functions is larger than a certain value, the curvature of feasible
sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a
logarithmic regret. This paper reveals that algorithms adaptive to the
curvature of loss functions can also leverage the curvature of feasible sets.
We first prove that if an optimal decision is on the boundary of a feasible set
and the gradient of an underlying loss function is non-zero, then the algorithm
achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments.
Here, $\rho &gt; 0$ is the radius of the smallest sphere that includes the optimal
decision and encloses the feasible set. Our approach, unlike existing ones, can
work directly with convex loss functions, exploiting the curvature of loss
functions simultaneously, and can achieve the logarithmic regret only with a
local property of feasible sets. Additionally, it achieves an $O(\sqrt{T})$
regret even in adversarial environments where FTL suffers an $\Omega(T)$
regret, and attains an $O(\rho \log T + \sqrt{C \rho \log T})$ regret bound in
corrupted stochastic environments with corruption level $C$. Furthermore, by
extending our analysis, we establish a regret upper bound of
$O\Big(T^{\frac{q-2}{2(q-1)}} (\log T)^{\frac{q}{2(q-1)}}\Big)$ for
$q$-uniformly convex feasible sets, where uniformly convex sets include
strongly convex sets and $\ell_p$-balls for $p \in [1,\infty)$. This bound
bridges the gap between the $O(\log T)$ regret bound for strongly convex sets
($q=2$) and the $O(\sqrt{T})$ regret bound for non-curved sets ($q\to\infty$).
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12869" title="Abstract">arXiv:2402.12869</a> [<a href="/pdf/2402.12869" title="Download PDF">pdf</a>, <a href="/format/2402.12869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based  Question Answering with Domain Hybrid Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+D">Dehai Min</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+N">Nan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rihui Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+N">Nuo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianren Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Augmenting Large Language Models (LLMs) for Question Answering (QA) with
domain specific data has attracted wide attention. However, domain data often
exists in a hybrid format, including text and semi-structured tables, posing
challenges for the seamless integration of information. Table-to-Text
Generation is a promising solution by facilitating the transformation of hybrid
data into a uniformly text-formatted corpus. Although this technique has been
widely studied by the NLP community, there is currently no comparative analysis
on how corpora generated by different table-to-text methods affect the
performance of QA systems. In this paper, we address this research gap in two
steps. First, we innovatively integrate table-to-text generation into the
framework of enhancing LLM-based QA systems with domain hybrid data. Then, we
utilize this framework in real-world industrial data to conduct extensive
experiments on two types of QA systems (DSFT and RAG frameworks) with four
representative methods: Markdown format, Template serialization, TPLM-based
method, and LLM-based method. Based on the experimental results, we draw some
empirical findings and explore the underlying reasons behind the success of
some methods. We hope the findings of this work will provide a valuable
reference for the academic and industrial communities in developing robust QA
systems.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12874" title="Abstract">arXiv:2402.12874</a> [<a href="/pdf/2402.12874" title="Download PDF">pdf</a>, <a href="/format/2402.12874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skill or Luck? Return Decomposition via Advantage Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hsiao-Ru Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning from off-policy data is essential for sample-efficient reinforcement
learning. In the present work, we build on the insight that the advantage
function can be understood as the causal effect of an action on the return, and
show that this allows us to decompose the return of a trajectory into parts
caused by the agent's actions (skill) and parts outside of the agent's control
(luck). Furthermore, this decomposition enables us to naturally extend Direct
Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The
resulting method can learn from off-policy trajectories without relying on
importance sampling techniques or truncating off-policy actions. We draw
connections between Off-policy DAE and previous methods to demonstrate how it
can speed up learning and when the proposed off-policy corrections are
important. Finally, we use the MinAtar environments to illustrate how ignoring
off-policy corrections can lead to suboptimal policy optimization performance.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12875" title="Abstract">arXiv:2402.12875</a> [<a href="/pdf/2402.12875" title="Download PDF">pdf</a>, <a href="/format/2402.12875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Thought Empowers Transformers to Solve Inherently Serial  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages. ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Instructing the model to generate a sequence of intermediate steps, a.k.a., a
chain of thought (CoT), is a highly effective method to improve the accuracy of
large language models (LLMs) on arithmetics and symbolic reasoning tasks.
However, the mechanism behind CoT remains unclear. This work provides a
theoretical understanding of the power of CoT for decoder-only transformers
through the lens of expressiveness. Conceptually, CoT empowers the model with
the ability to perform inherently serial computation, which is otherwise
lacking in transformers, especially when depth is low. Given input length $n$,
previous works have shown that constant-depth transformers with finite
precision $\mathsf{poly}(n)$ embedding size can only solve problems in
$\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper
bound for constant-depth transformers with constant-bit precision, which can
only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$.
However, with $T$ steps of CoT, constant-depth transformers using constant-bit
precision and $O(\log n)$ embedding size can solve any problem solvable by
boolean circuits of size $T$. Empirically, enabling CoT dramatically improves
the accuracy for tasks that are hard for parallel computation, including the
composition of permutation groups, iterated squaring, and circuit value
problems, especially for low-depth transformers.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12876" title="Abstract">arXiv:2402.12876</a> [<a href="/pdf/2402.12876" title="Download PDF">pdf</a>, <a href="/format/2402.12876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Multi-Task Learning on Non-IID Data Silos: An Experimental  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxiang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Suizhi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sirejiding%2C+S">Shalayiding Sirejiding</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongtao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yue Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The innovative Federated Multi-Task Learning (FMTL) approach consolidates the
benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling
collaborative model training on multi-task learning datasets. However, a
comprehensive evaluation method, integrating the unique features of both FL and
MTL, is currently absent in the field. This paper fills this void by
introducing a novel framework, FMTL-Bench, for systematic evaluation of the
FMTL paradigm. This benchmark covers various aspects at the data, model, and
optimization algorithm levels, and comprises seven sets of comparative
experiments, encapsulating a wide array of non-independent and identically
distributed (Non-IID) data partitioning scenarios. We propose a systematic
process for comparing baselines of diverse indicators and conduct a case study
on communication expenditure, time, and energy consumption. Through our
exhaustive experiments, we aim to provide valuable insights into the strengths
and limitations of existing baseline methods, contributing to the ongoing
discourse on optimal FMTL application in practical scenarios. The source code
will be made available for results replication.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12880" title="Abstract">arXiv:2402.12880</a> [<a href="/pdf/2402.12880" title="Download PDF">pdf</a>, <a href="/format/2402.12880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autism Detection in Speech - A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Probol%2C+N">Nadine Probol</a>, 
<a href="/search/cs?searchtype=author&query=Mieskes%2C+M">Margot Mieskes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">There has been a range of studies of how autism is displayed in voice,
speech, and language. We analyse studies from the biomedical, as well as the
psychological domain, but also from the NLP domain in order to find linguistic,
prosodic and acoustic cues that could indicate autism. Our survey looks at all
three domains. We define autism and which comorbidities might influence the
correct detection of the disorder. We especially look at observations such as
verbal and semantic fluency, prosodic features, but also disfluencies and
speaking rate. We also show word-based approaches and describe machine learning
and transformer-based approaches both on the audio data as well as the
transcripts. Lastly, we conclude, while there already is a lot of research,
female patients seem to be severely under-researched. Also, most NLP research
focuses on traditional machine learning methods instead of transformers which
could be beneficial in this context. Additionally, we were unable to find
research combining both features from audio and transcripts.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12881" title="Abstract">arXiv:2402.12881</a> [<a href="/pdf/2402.12881" title="Download PDF">pdf</a>, <a href="/format/2402.12881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object  Affordances of Language and Vision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adak%2C+S">Sayantan Adak</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+D">Daivik Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We investigate the knowledge of object affordances in pre-trained language
models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based
large pre-trained language models (PTLM) learn contextual representation from
massive amounts of unlabeled text and are shown to perform impressively in
downstream NLU tasks. In parallel, a growing body of literature shows that
PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and
grounding. To take a first step toward quantifying the effect of grounding (or
lack thereof), we curate a novel and comprehensive dataset of object
affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike
affordance datasets collected in vision and language domains, we annotate
in-the-wild sentences with objects and affordances. Experimental results reveal
that PTLMs exhibit limited reasoning abilities when it comes to uncommon object
affordances. We also observe that pre-trained VLMs do not necessarily capture
object affordances effectively. Through few-shot fine-tuning, we demonstrate
improvement in affordance knowledge in PTLMs and VLMs. Our research contributes
a novel dataset for language grounding tasks, and presents insights into LM
capabilities, advancing the understanding of object affordances. Codes and data
are available at https://github.com/sayantan11995/Affordance
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12882" title="Abstract">arXiv:2402.12882</a> [<a href="/pdf/2402.12882" title="Download PDF">pdf</a>, <a href="/ps/2402.12882" title="Download PostScript">ps</a>, <a href="/format/2402.12882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Theory: A Geometrical Interpretation of Multivectorial Apparent  Power
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Castilla%2C+M">M. Castilla</a>, 
<a href="/search/eess?searchtype=author&query=Bravo%2C+J+C">Juan Carlos Bravo</a>, 
<a href="/search/eess?searchtype=author&query=Ordo%C3%B1ez%2C+M">M. Ordo&#xf1;ez</a>, 
<a href="/search/eess?searchtype=author&query=Monta%C3%B1o%2C+J+C">Juan Carlos Monta&#xf1;o</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Circuits and Systems I-Regular Papers, (
  Volume: 55, Issue: 10, November 2008)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, a generalization of the concept of electrical power for
periodic current and voltage waveforms based on a new generalized complex
geometric algebra (GCGA) is proposed. This powerful tool permits, in
n-sinusoidal/nonlinear situations, representing and calculating the voltage,
current, and apparent power in a single-port electrical network in terms of
multivectors. The new expressions result in a novel representation of the
apparent power, similar to the Steinmetz's phasor model, based on complex
numbers, but limited to the purely sinusoidal case. The multivectorial approach
presented is based on the frequency-domain decomposition of the apparent power
into three components: the real part and the imaginary part of the
complex-scalar associated to active and reactive power respectively, and
distortion power, associated to the complex-bivector. A geometrical
interpretation of the multivectorial components of apparent power is discussed.
Numerical examples illustrate the clear advantages of the suggested approach.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12886" title="Abstract">arXiv:2402.12886</a> [<a href="/pdf/2402.12886" title="Download PDF">pdf</a>, <a href="/format/2402.12886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time High-resolution View Synthesis of Complex Scenes with Explicit  3D Visibility Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tiansong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xuangeng Chu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chengkun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Changyin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Rendering photo-realistic novel-view images of complex scenes has been a
long-standing challenge in computer graphics. In recent years, great research
progress has been made on enhancing rendering quality and accelerating
rendering speed in the realm of view synthesis. However, when rendering complex
dynamic scenes with sparse views, the rendering quality remains limited due to
occlusion problems. Besides, for rendering high-resolution images on dynamic
scenes, the rendering speed is still far from real-time. In this work, we
propose a generalizable view synthesis method that can render high-resolution
novel-view images of complex static and dynamic scenes in real-time from sparse
views. To address the occlusion problems arising from the sparsity of input
views and the complexity of captured scenes, we introduce an explicit 3D
visibility reasoning approach that can efficiently estimate the visibility of
sampled 3D points to the input views. The proposed visibility reasoning
approach is fully differentiable and can gracefully fit inside the volume
rendering pipeline, allowing us to train our networks with only multi-view
images as supervision while refining geometry and texture simultaneously.
Besides, each module in our pipeline is carefully designed to bypass the
time-consuming MLP querying process and enhance the rendering quality of
high-resolution images, enabling us to render high-resolution novel-view images
in real-time.Experimental results show that our method outperforms previous
view synthesis methods in both rendering quality and speed, particularly when
dealing with complex dynamic scenes with sparse views.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12887" title="Abstract">arXiv:2402.12887</a> [<a href="/pdf/2402.12887" title="Download PDF">pdf</a>, <a href="/format/2402.12887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The practice of qualitative parameterisation in the development of  Bayesian networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mascaro%2C+S">Steven Mascaro</a>, 
<a href="/search/cs?searchtype=author&query=Woodberry%2C+O">Owen Woodberry</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nicholson%2C+A+E">Ann E. Nicholson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, technical note
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The typical phases of Bayesian network (BN) structured development include
specification of purpose and scope, structure development, parameterisation and
validation. Structure development is typically focused on qualitative issues
and parameterisation quantitative issues, however there are qualitative and
quantitative issues that arise in both phases. A common step that occurs after
the initial structure has been developed is to perform a rough parameterisation
that only captures and illustrates the intended qualitative behaviour of the
model. This is done prior to a more rigorous parameterisation, ensuring that
the structure is fit for purpose, as well as supporting later development and
validation. In our collective experience and in discussions with other
modellers, this step is an important part of the development process, but is
under-reported in the literature. Since the practice focuses on qualitative
issues, despite being quantitative in nature, we call this step qualitative
parameterisation and provide an outline of its role in the BN development
process.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12889" title="Abstract">arXiv:2402.12889</a> [<a href="/pdf/2402.12889" title="Download PDF">pdf</a>, <a href="/format/2402.12889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BFT-DSN: A Byzantine Fault Tolerant Decentralized Storage Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hechuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minghui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Rajiv Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dongxiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuzhen Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the rapid development of blockchain and its applications, the amount of
data stored on decentralized storage networks (DSNs) has grown exponentially.
DSNs bring together affordable storage resources from around the world to
provide robust, decentralized storage services for tens of thousands of
decentralized applications (dApps). However, existing DSNs do not offer
verifiability when implementing erasure coding for redundant storage, making
them vulnerable to Byzantine encoders. Additionally, there is a lack of
Byzantine fault-tolerant consensus for optimal resilience in DSNs. This paper
introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network
designed to address these challenges. BFT-DSN combines storage-weighted BFT
consensus with erasure coding and incorporates homomorphic fingerprints and
weighted threshold signatures for decentralized verification. The
implementation of BFT-DSN demonstrates its comparable performance in terms of
storage cost and latency as well as superior performance in Byzantine
resilience when compared to existing industrial decentralized storage networks.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12890" title="Abstract">arXiv:2402.12890</a> [<a href="/pdf/2402.12890" title="Download PDF">pdf</a>, <a href="/format/2402.12890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Discriminative Sentence Embeddings via Semantic Graph Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fettal%2C+C">Chakib Fettal</a>, 
<a href="/search/cs?searchtype=author&query=Labiod%2C+L">Lazhar Labiod</a>, 
<a href="/search/cs?searchtype=author&query=Nadif%2C+M">Mohamed Nadif</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper explores an empirical approach to learn more discriminantive
sentence representations in an unsupervised fashion. Leveraging semantic graph
smoothing, we enhance sentence embeddings obtained from pretrained models to
improve results for the text clustering and classification tasks. Our method,
validated on eight benchmarks, demonstrates consistent improvements, showcasing
the potential of semantic graph smoothing in improving sentence embeddings for
the supervised and unsupervised document categorization tasks.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12891" title="Abstract">arXiv:2402.12891</a> [<a href="/pdf/2402.12891" title="Download PDF">pdf</a>, <a href="/format/2402.12891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard  Plenoptic Camera
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michels%2C+T">Tim Michels</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A4ckelmann%2C+D">Daniel M&#xe4;ckelmann</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+R">Reinhard Koch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 15 figures, Submitted to MDPI Sensors journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Among the common applications of plenoptic cameras are depth reconstruction
and post-shot refocusing. These require a calibration relating the camera-side
light field to that of the scene. Numerous methods with this goal have been
developed based on thin lens models for the plenoptic camera's main lens and
microlenses. Our work addresses the often-overlooked role of the main lens exit
pupil in these models and specifically in the decoding process of standard
plenoptic camera (SPC) images. We formally deduce the connection between the
refocusing distance and the resampling parameter for the decoded light field
and provide an analysis of the errors that arise when the exit pupil is not
considered. In addition, previous work is revisited with respect to the exit
pupil's role and all theoretical results are validated through a
ray-tracing-based simulation. With the public release of the evaluated SPC
designs alongside our simulation and experimental data we aim to contribute to
a more accurate and nuanced understanding of plenoptic camera optics.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12905" title="Abstract">arXiv:2402.12905</a> [<a href="/pdf/2402.12905" title="Download PDF">pdf</a>, <a href="/ps/2402.12905" title="Download PostScript">ps</a>, <a href="/format/2402.12905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Rainbow Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fluschnik%2C+T">Till Fluschnik</a>, 
<a href="/search/cs?searchtype=author&query=Kellerhals%2C+L">Leon Kellerhals</a>, 
<a href="/search/cs?searchtype=author&query=Renken%2C+M">Malte Renken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We introduce the algorithmic problem of finding a locally rainbow path of
length $\ell$ connecting two distinguished vertices $s$ and $t$ in a
vertex-colored directed graph. Herein, a path is locally rainbow if between any
two visits of equally colored vertices, the path traverses consecutively at
least $r$ differently colored vertices. This problem generalizes the well-known
problem of finding a rainbow path. It finds natural applications whenever there
are different types of resources that must be protected from overuse, such as
crop sequence optimization or production process scheduling. We show that the
problem is computationally intractable even if $r=2$ or if one looks for a
locally rainbow among the shortest paths. On the positive side, if one looks
for a path that takes only a short detour (i.e., it is slightly longer than the
shortest path) and if $r$ is small, the problem can be solved efficiently.
Indeed, the running time of the respective algorithm is near-optimal unless the
ETH fails.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12906" title="Abstract">arXiv:2402.12906</a> [<a href="/pdf/2402.12906" title="Download PDF">pdf</a>, <a href="/format/2402.12906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fog enabled distributed training architecture for federated learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aditya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Srirama%2C+S+N">Satish Narayana Srirama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference paper accepted at BDA 2021
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Big Data Analytics 9th International Conference, BDA 2021, Virtual
  Event, December 15-18, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The amount of data being produced at every epoch of second is increasing
every moment. Various sensors, cameras and smart gadgets produce continuous
data throughout its installation. Processing and analyzing raw data at a cloud
server faces several challenges such as bandwidth, congestion, latency, privacy
and security. Fog computing brings computational resources closer to IoT that
addresses some of these issues. These IoT devices have low computational
capability, which is insufficient to train machine learning. Mining hidden
patterns and inferential rules from continuously growing data is crucial for
various applications. Due to growing privacy concerns, privacy preserving
machine learning is another aspect that needs to be inculcated. In this paper,
we have proposed a fog enabled distributed training architecture for machine
learning tasks using resources constrained devices. The proposed architecture
trains machine learning model on rapidly changing data using online learning.
The network is inlined with privacy preserving federated learning training.
Further, the learning capability of architecture is tested on a real world IIoT
use case. We trained a neural network model for human position detection in
IIoT setup on rapidly changing data.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12907" title="Abstract">arXiv:2402.12907</a> [<a href="/pdf/2402.12907" title="Download PDF">pdf</a>, <a href="/format/2402.12907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive Compatibility for AI Alignment in Sociotechnical Systems:  Positions and Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fengshuo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haoyang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengdong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The burgeoning integration of artificial intelligence (AI) into human society
brings forth significant implications for societal governance and safety. While
considerable strides have been made in addressing AI alignment challenges,
existing methodologies primarily focus on technical facets, often neglecting
the intricate sociotechnical nature of AI systems, which can lead to a
misalignment between the development and deployment contexts. To this end, we
posit a new problem worth exploring: Incentive Compatibility Sociotechnical
Alignment Problem (ICSAP). We hope this can call for more researchers to
explore how to leverage the principles of Incentive Compatibility (IC) from
game theory to bridge the gap between technical and societal components to
maintain AI consensus with human societies in different contexts. We further
discuss three classical game problems for achieving IC: mechanism design,
contract theory, and Bayesian persuasion, in addressing the perspectives,
potentials, and challenges of solving ICSAP, and provide preliminary
implementation conceptions.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12908" title="Abstract">arXiv:2402.12908</a> [<a href="/pdf/2402.12908" title="Download PDF">pdf</a>, <a href="/format/2402.12908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RealCompo: Dynamic Equilibrium between Realism and Compositionality  Improves Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Ling Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yaqi Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiake Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+B">Bin Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project: <a href="https://github.com/YangLing0818/RealCompo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have achieved remarkable advancements in text-to-image
generation. However, existing models still have many difficulties when faced
with multiple-object compositional generation. In this paper, we propose a new
training-free and transferred-friendly text-to-image generation framework,
namely RealCompo, which aims to leverage the advantages of text-to-image and
layout-to-image models to enhance both realism and compositionality of the
generated images. An intuitive and novel balancer is proposed to dynamically
balance the strengths of the two models in denoising process, allowing
plug-and-play use of any model without extra training. Extensive experiments
show that our RealCompo consistently outperforms state-of-the-art text-to-image
models and layout-to-image models in multiple-object compositional generation
while keeping satisfactory realism and compositionality of the generated
images. Code is available at https://github.com/YangLing0818/RealCompo
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12913" title="Abstract">arXiv:2402.12913</a> [<a href="/pdf/2402.12913" title="Download PDF">pdf</a>, <a href="/format/2402.12913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination  Detection with Weakly Supervised Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chengcheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Songtan Fang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiarong He</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Max Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper mainly describes a unified system for hallucination detection of
LLMs, which wins the second prize in the model-agnostic track of the
SemEval-2024 Task 6, and also achieves considerable results in the model-aware
track. This task aims to detect hallucination with LLMs for three different
text-generation tasks without labeled training data. We utilize prompt
engineering and few-shot learning to verify the performance of different LLMs
on the validation data. Then we select the LLMs with better performance to
generate high-quality weakly supervised training data, which not only satisfies
the consistency of different LLMs, but also satisfies the consistency of the
optimal LLM with different sampling parameters. Furthermore, we finetune
different LLMs by using the constructed training data, and finding that a
relatively small LLM can achieve a competitive level of performance in
hallucination detection, when compared to the large LLMs and the prompt-based
approaches using GPT-4.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12914" title="Abstract">arXiv:2402.12914</a> [<a href="/pdf/2402.12914" title="Download PDF">pdf</a>, <a href="/format/2402.12914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-based Human-Agent Collaboration for Complex Task  Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xueyang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhi-Yuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In recent developments within the research community, the integration of
Large Language Models (LLMs) in creating fully autonomous agents has garnered
significant interest. Despite this, LLM-based agents frequently demonstrate
notable shortcomings in adjusting to dynamic environments and fully grasping
human needs. In this work, we introduce the problem of LLM-based human-agent
collaboration for complex task-solving, exploring their synergistic potential.
In addition, we propose a Reinforcement Learning-based Human-Agent
Collaboration method, ReHAC. This approach includes a policy model designed to
determine the most opportune stages for human intervention within the
task-solving process. We construct a human-agent collaboration dataset to train
this policy model in an offline reinforcement learning environment. Our
validation tests confirm the model's effectiveness. The results demonstrate
that the synergistic efforts of humans and LLM-based agents significantly
improve performance in complex tasks, primarily through well-planned, limited
human intervention. Datasets and code are available at:
https://github.com/XueyangFeng/ReHAC.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12916" title="Abstract">arXiv:2402.12916</a> [<a href="/pdf/2402.12916" title="Download PDF">pdf</a>, <a href="/ps/2402.12916" title="Download PostScript">ps</a>, <a href="/format/2402.12916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of  Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+C">Chunhe Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wenran Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data Pipeline plays an indispensable role in tasks such as modeling machine
learning and developing data products. With the increasing diversification and
complexity of Data sources, as well as the rapid growth of data volumes,
building an efficient Data Pipeline has become crucial for improving work
efficiency and solving complex problems. This paper focuses on exploring how to
optimize data flow through automated machine learning methods by integrating
AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to
enhance the intelligence of Data Pipeline, thereby achieving better results in
machine learning tasks. By delving into the automation and optimization of Data
flows, we uncover key strategies for constructing efficient data pipelines that
can adapt to the ever-changing data landscape. This not only accelerates the
modeling process but also provides innovative solutions to complex problems,
enabling more significant outcomes in increasingly intricate data domains.
Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12921" title="Abstract">arXiv:2402.12921</a> [<a href="/pdf/2402.12921" title="Download PDF">pdf</a>, <a href="/format/2402.12921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Right on Time: Revising Time Series Models by Constraining their  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Maurice Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Steinmann%2C+D">David Steinmann</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%BCst%2C+A">Antonia W&#xfc;st</a>, 
<a href="/search/cs?searchtype=author&query=Kokozinski%2C+A">Andre Kokozinski</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The reliability of deep time series models is often compromised by their
tendency to rely on confounding factors, which may lead to misleading results.
Our newly recorded, naturally confounded dataset named P2S from a real
mechanical production line emphasizes this. To tackle the challenging problem
of mitigating confounders in time series data, we introduce Right on Time
(RioT). Our method enables interactions with model explanations across both the
time and frequency domain. Feedback on explanations in both domains is then
used to constrain the model, steering it away from the annotated confounding
factors. The dual-domain interaction strategy is crucial for effectively
addressing confounders in time series datasets. We empirically demonstrate that
RioT can effectively guide models away from the wrong reasons in P2S as well as
popular time series classification and forecasting datasets.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12922" title="Abstract">arXiv:2402.12922</a> [<a href="/pdf/2402.12922" title="Download PDF">pdf</a>, <a href="/ps/2402.12922" title="Download PostScript">ps</a>, <a href="/format/2402.12922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with  a 1DoF Gimbaled-Thruster and two Reaction Wheels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kouhi%2C+H">Hamed Kouhi</a>, 
<a href="/search/eess?searchtype=author&query=Kabganian%2C+M">Mansour Kabganian</a>, 
<a href="/search/eess?searchtype=author&query=Saberi%2C+F+F">Farhad Fani Saberi</a>, 
<a href="/search/eess?searchtype=author&query=Ghorbani%2C+F">Fatemeh Ghorbani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 5th National Conference on Application of Novel Technologies in Engineering Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Attitude control of spacecraft during an impulsive orbital maneuver is a
vital task. Many spacecraft and launchers use the gimbaled thrust vector
control (TVC) in their attitude control system during an orbital maneuver.
Mathematical modeling of the gimbal actuator is an important task because we
should show the applicability of the gimbaled-TVC in a spacecraft. In this
paper, a spin-stabilized spacecraft equipped with one degree of freedom (DoF)
gimbaled-thruster and two reaction wheels (RWs) is considered. The control
goals are disturbance rejection and thrust vector (spin-axis) stabilization
based on one DoF gimbal actuator and two RWs. The gimbal is assumed to be
equipped with a gearbox and a DC electric motor. This actuator must supply the
gimbal torque to rotate the spacecraft nozzle. The mathematical model of the
mentioned spacecraft is extended with respect to the DC motor equations. In
order to investigate the applicability of the proposed method, an industrial DC
electric motor is considered for the gimbal actuator. The simulation results
prove that an industrial DC electric motor is able to be used for attitude
control of the mentioned spacecraft. The simulation results indicate the
applicability of the proposed control method in an impulsive orbital maneuver.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12923" title="Abstract">arXiv:2402.12923</a> [<a href="/pdf/2402.12923" title="Download PDF">pdf</a>, <a href="/format/2402.12923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Point Cloud-Based 3D Defect Detection and Classification  for Industrial Systems: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rani%2C+A">Anju Rani</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz-Arroyo%2C+D">Daniel Ortiz-Arroyo</a>, 
<a href="/search/cs?searchtype=author&query=Durdevic%2C+P">Petar Durdevic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 9 figures, 3 tables, review paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, 3D point clouds (PCs) have gained significant attention due
to their diverse applications across various fields such as computer vision
(CV), condition monitoring, virtual reality, robotics, autonomous driving etc.
Deep learning (DL) has proven effective in leveraging 3D PCs to address various
challenges previously encountered in 2D vision. However, the application of
deep neural networks (DNN) to process 3D PCs presents its own set of
challenges. To address these challenges, numerous methods have been proposed.
This paper provides an in-depth review of recent advancements in DL-based
condition monitoring (CM) using 3D PCs, with a specific focus on defect shape
classification and segmentation within industrial applications for operational
and maintenance purposes. Recognizing the crucial role of these aspects in
industrial maintenance, the paper provides insightful observations that offer
perspectives on the strengths and limitations of the reviewed DL-based PC
processing methods. This synthesis of knowledge aims to contribute to the
understanding and enhancement of CM processes, particularly within the
framework of remaining useful life (RUL), in industrial systems.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12927" title="Abstract">arXiv:2402.12927</a> [<a href="/pdf/2402.12927" title="Download PDF">pdf</a>, <a href="/format/2402.12927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIPping the Deception: Adapting Vision-Language Models for Universal  Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Sohail Ahmed Khan</a>, 
<a href="/search/cs?searchtype=author&query=Dang-Nguyen%2C+D">Duc-Tien Dang-Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent advancements in Generative Adversarial Networks (GANs) and the
emergence of Diffusion models have significantly streamlined the production of
highly realistic and widely accessible synthetic content. As a result, there is
a pressing need for effective general purpose detection mechanisms to mitigate
the potential risks posed by deepfakes. In this paper, we explore the
effectiveness of pre-trained vision-language models (VLMs) when paired with
recent adaptation methods for universal deepfake detection. Following previous
studies in this domain, we employ only a single dataset (ProGAN) in order to
adapt CLIP for deepfake detection. However, in contrast to prior research,
which rely solely on the visual part of CLIP while ignoring its textual
component, our analysis reveals that retaining the text part is crucial.
Consequently, the simple and lightweight Prompt Tuning based adaptation
strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and
6.61% accuracy while utilizing less than one third of the training data (200k
images as compared to 720k). To assess the real-world applicability of our
proposed models, we conduct a comprehensive evaluation across various
scenarios. This involves rigorous testing on images sourced from 21 distinct
datasets, including those generated by GANs-based, Diffusion-based and
Commercial tools.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12928" title="Abstract">arXiv:2402.12928</a> [<a href="/pdf/2402.12928" title="Download PDF">pdf</a>, <a href="/format/2402.12928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Literature Review of Literature Reviews in Pattern Analysis and  Machine Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Penghai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages,9 figures, 5 tables. [February 19, 2024]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">By consolidating scattered knowledge, the literature review provides a
comprehensive understanding of the investigated topic. However, excessive
reviews, especially in the booming field of pattern analysis and machine
intelligence (PAMI), raise concerns for both researchers and reviewers. In
response to these concerns, this Analysis aims to provide a thorough review of
reviews in the PAMI field from diverse perspectives. First, large language
model-empowered bibliometric indicators are proposed to evaluate literature
reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI,
and a topic dataset are constructed, which are utilized to obtain statistical
characteristics of PAMI reviews. Unlike traditional bibliometric measurements,
the proposed article-level indicators provide real-time and field-normalized
quantified assessments of reviews without relying on user-defined keywords.
Second, based on these indicators, the study presents comparative analyses of
different reviews, unveiling the characteristics of publications across various
fields, periods, and journals. The newly emerging AI-generated literature
reviews are also appraised, and the observed differences suggest that most
AI-generated reviews still lag behind human-authored reviews in several
aspects. Third, we briefly provide a subjective evaluation of representative
PAMI reviews and introduce a paper structure-based typology of literature
reviews. This typology may improve the clarity and effectiveness for scholars
in reading and writing reviews, while also serving as a guide for AI systems in
generating well-organized reviews. Finally, this Analysis offers insights into
the current challenges of literature reviews and envisions future directions
for their development.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12930" title="Abstract">arXiv:2402.12930</a> [<a href="/pdf/2402.12930" title="Download PDF">pdf</a>, <a href="/format/2402.12930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sascha Xu</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+N+P">Nils Philipp Walter</a>, 
<a href="/search/cs?searchtype=author&query=Kalofolias%2C+J">Janis Kalofolias</a>, 
<a href="/search/cs?searchtype=author&query=Vreeken%2C+J">Jilles Vreeken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Finding and describing sub-populations that are exceptional regarding a
target property has important applications in many scientific disciplines, from
identifying disadvantaged demographic groups in census data to finding
conductive molecules within gold nanoparticles. Current approaches to finding
such subgroups require pre-discretized predictive variables, do not permit
non-trivial target distributions, do not scale to large datasets, and struggle
to find diverse results.
<br />To address these limitations, we propose Syflow, an end-to-end optimizable
approach in which we leverage normalizing flows to model arbitrary target
distributions, and introduce a novel neural layer that results in easily
interpretable subgroup descriptions. We demonstrate on synthetic and real-world
data, including a case study, that Syflow reliably finds highly exceptional
subgroups accompanied by insightful descriptions.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12936" title="Abstract">arXiv:2402.12936</a> [<a href="/pdf/2402.12936" title="Download PDF">pdf</a>, <a href="/format/2402.12936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Impacts of Poisoning on Model Parameters and Neuron  Activations: A Case Study of Poisoning CodeBERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Aftab Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
<a href="/search/cs?searchtype=author&query=Ayoobi%2C+N">Navid Ayoobi</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+M+A">Mohammad Amin Alipour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized software development
practices, yet concerns about their safety have arisen, particularly regarding
hidden backdoors, aka trojans. Backdoor attacks involve the insertion of
triggers into training data, allowing attackers to manipulate the behavior of
the model maliciously. In this paper, we focus on analyzing the model
parameters to detect potential backdoor signals in code models. Specifically,
we examine attention weights and biases, activation values, and context
embeddings of the clean and poisoned CodeBERT models. Our results suggest
noticeable patterns in activation values and context embeddings of poisoned
samples for the poisoned CodeBERT model; however, attention weights and biases
do not show any significant differences. This work contributes to ongoing
efforts in white-box detection of backdoor signals in LLMs of code through the
analysis of parameters and activations.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12937" title="Abstract">arXiv:2402.12937</a> [<a href="/pdf/2402.12937" title="Download PDF">pdf</a>, <a href="/format/2402.12937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sirohi%2C+A+K">Anuj Kumar Sirohi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anjali Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sandeep Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Bagchi%2C+A">Amitabha Bagchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We address the growing apprehension that GNNs, in the absence of fairness
constraints, might produce biased decisions that disproportionately affect
underprivileged groups or individuals. Departing from previous work, we
introduce for the first time a method for incorporating the Gini coefficient as
a measure of fairness to be used within the GNN framework. Our proposal,
GRAPHGINI, works with the two different goals of individual and group fairness
in a single system, while maintaining high prediction accuracy. GRAPHGINI
enforces individual fairness through learnable attention scores that help in
aggregating more information through similar nodes. A heuristic-based maximum
Nash social welfare constraint ensures the maximum possible group fairness.
Both the individual fairness constraint and the group fairness constraint are
stated in terms of a differentiable approximation of the Gini coefficient. This
approximation is a contribution that is likely to be of interest even beyond
the scope of the problem studied in this paper. Unlike other state-of-the-art,
GRAPHGINI automatically balances all three optimization objectives (utility,
individual, and group fairness) of the GNN and is free from any manual tuning
of weight parameters. Extensive experimentation on real-world datasets
showcases the efficacy of GRAPHGINI in making significant improvements in
individual fairness compared to all currently available state-of-the-art
methods while maintaining utility and group equality.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12938" title="Abstract">arXiv:2402.12938</a> [<a href="/pdf/2402.12938" title="Download PDF">pdf</a>, <a href="/format/2402.12938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniCell: Universal Cell Nucleus Classification via Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024, Code and models are available at <a href="https://github.com/lhaof/UniCell">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recognition of multi-class cell nuclei can significantly facilitate the
process of histopathological diagnosis. Numerous pathological datasets are
currently available, but their annotations are inconsistent. Most existing
methods require individual training on each dataset to deduce the relevant
labels and lack the use of common knowledge across datasets, consequently
restricting the quality of recognition. In this paper, we propose a universal
cell nucleus classification framework (UniCell), which employs a novel prompt
learning mechanism to uniformly predict the corresponding categories of
pathological images from different dataset domains. In particular, our
framework adopts an end-to-end architecture for nuclei detection and
classification, and utilizes flexible prediction heads for adapting various
datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the
properties of multiple datasets to enhance features. The DPM first integrates
the embeddings of datasets and semantic categories, and then employs the
integrated prompts to refine image representations, efficiently harvesting the
shared knowledge among the related cell types and data sources. Experimental
results demonstrate that the proposed method effectively achieves the
state-of-the-art results on four nucleus detection and classification
benchmarks. Code and models are available at https://github.com/lhaof/UniCell
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12939" title="Abstract">arXiv:2402.12939</a> [<a href="/pdf/2402.12939" title="Download PDF">pdf</a>, <a href="/format/2402.12939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Behavioral Modes in Deep Reinforcement Learning Policies  Using Trajectory Clustering in Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Remman%2C+S+B">Sindre Benjamin Remman</a>, 
<a href="/search/cs?searchtype=author&query=Lekkas%2C+A+M">Anastasios M. Lekkas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the European Control Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Understanding the behavior of deep reinforcement learning (DRL) agents is
crucial for improving their performance and reliability. However, the
complexity of their policies often makes them challenging to understand. In
this paper, we introduce a new approach for investigating the behavior modes of
DRL policies, which involves utilizing dimensionality reduction and trajectory
clustering in the latent space of neural networks. Specifically, we use
Pairwise Controlled Manifold Approximation Projection (PaCMAP) for
dimensionality reduction and TRACLUS for trajectory clustering to analyze the
latent space of a DRL policy trained on the Mountain Car control task. Our
methodology helps identify diverse behavior patterns and suboptimal choices by
the policy, thus allowing for targeted improvements. We demonstrate how our
approach, combined with domain knowledge, can enhance a policy's performance in
specific regions of the state space.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12940" title="Abstract">arXiv:2402.12940</a> [<a href="/pdf/2402.12940" title="Download PDF">pdf</a>, <a href="/ps/2402.12940" title="Download PostScript">ps</a>, <a href="/format/2402.12940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalized Orthography for Tunisian Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turki%2C+H">Houcemeddine Turki</a>, 
<a href="/search/cs?searchtype=author&query=Ellouze%2C+K">Kawthar Ellouze</a>, 
<a href="/search/cs?searchtype=author&query=Ammar%2C+H+B">Hager Ben Ammar</a>, 
<a href="/search/cs?searchtype=author&query=Taieb%2C+M+A+H">Mohamed Ali Hadj Taieb</a>, 
<a href="/search/cs?searchtype=author&query=Adel%2C+I">Imed Adel</a>, 
<a href="/search/cs?searchtype=author&query=Aouicha%2C+M+B">Mohamed Ben Aouicha</a>, 
<a href="/search/cs?searchtype=author&query=Farri%2C+P+L">Pier Luigi Farri</a>, 
<a href="/search/cs?searchtype=author&query=Bennour%2C+A">Abderrezak Bennour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final Report for the Derja Association
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to
Tunisia, initially stemmed from the Arabic language and enriched by a multitude
of historical influences. This research introduces the "Normalized Orthography
for Tunisian Arabic" (NOTA), an adaptation of CODA* guidelines tailored for
transcribing Tunisian Arabic using the Arabic script for language resource
development purposes, with an emphasis on user-friendliness and consistency.
The updated standard seeks to address challenges related to accurately
representing the unique characteristics of Tunisian phonology and morphology.
This will be achieved by rectifying problems arising from transcriptions based
on resemblances to Modern Standard Arabic.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12945" title="Abstract">arXiv:2402.12945</a> [<a href="/pdf/2402.12945" title="Download PDF">pdf</a>, <a href="/format/2402.12945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Approximation Approach to Federated Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%2C+S+P">Srihari P V</a>, 
<a href="/search/cs?searchtype=author&query=Bhikkaji%2C+B">Bharath Bhikkaji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper examines Federated learning (FL) in a Stochastic Approximation
(SA) framework. FL is a collaborative way to train neural network models across
various participants or clients without centralizing their data. Each client
will train a model on their respective data and send the weights across to a
the server periodically for aggregation. The server aggregates these weights
which are then used by the clients to re-initialize their neural network and
continue the training. SA is an iterative algorithm that uses approximate
sample gradients and tapering step size to locate a minimizer of a cost
function. In this paper the clients use a stochastic approximation iterate to
update the weights of its neural network. It is shown that the aggregated
weights track an autonomous ODE. Numerical simulations are performed and the
results are compared with standard algorithms like FedAvg and FedProx. It is
observed that the proposed algorithm is robust and gives more reliable
estimates of the weights, in particular when the clients data are not
identically distributed.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12946" title="Abstract">arXiv:2402.12946</a> [<a href="/pdf/2402.12946" title="Download PDF">pdf</a>, <a href="/format/2402.12946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell Graph Transformer for Nuclei Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+W">Wei Lou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024, Code and models are available at <a href="https://github.com/lhaof/CGT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nuclei classification is a critical step in computer-aided diagnosis with
histopathology images. In the past, various methods have employed graph neural
networks (GNN) to analyze cell graphs that model inter-cell relationships by
considering nuclei as vertices. However, they are limited by the GNN mechanism
that only passes messages among local nodes via fixed edges. To address the
issue, we develop a cell graph transformer (CGT) that treats nodes and edges as
input tokens to enable learnable adjacency and information exchange among all
nodes. Nevertheless, training the transformer with a cell graph presents
another challenge. Poorly initialized features can lead to noisy self-attention
scores and inferior convergence, particularly when processing the cell graphs
with numerous connections. Thus, we further propose a novel topology-aware
pretraining method that leverages a graph convolutional network (GCN) to learn
a feature extractor. The pre-trained features may suppress unreasonable
correlations and hence ease the finetuning of CGT. Experimental results suggest
that the proposed cell graph transformer with topology-aware pretraining
significantly improves the nuclei classification results, and achieves the
state-of-the-art performance. Code and models are available at
https://github.com/lhaof/CGT
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12947" title="Abstract">arXiv:2402.12947</a> [<a href="/pdf/2402.12947" title="Download PDF">pdf</a>, <a href="/format/2402.12947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multigrid on unstructured meshes with regions of low quality cells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wells%2C+G+N">Garth N. Wells</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The convergence of multigrid methods degrades significantly if a small number
of low quality cells are present in a finite element mesh, and this can be a
barrier to the efficient and robust application of multigrid on complicated
geometric domains. The degraded performance is observed also if intermediate
levels in a non-nested geometric multigrid problem have low quality cells, even
when the fine grid is high quality. It is demonstrated for geometric multigrid
methods that the poor convergence is due to the local failure of smoothers to
eliminate parts of error around cells of low quality. To overcome this, a
global--local combined smoother is developed to maintain effective relaxation
in the presence of a small number of poor quality cells. The smoother involves
the application of a standard smoother on the whole domain, followed by local
corrections for small subdomains with low quality cells. Two- and
three-dimensional numerical experiments demonstrate that the degraded
convergence of multigrid for low quality meshes can be restored to the high
quality mesh reference case using the proposed smoother. The effect is
particularly pronounced for higher-order finite elements. The results provide a
basis for developing efficient, non-nested geometric multigrid methods for
complicated engineering geometries.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12948" title="Abstract">arXiv:2402.12948</a> [<a href="/pdf/2402.12948" title="Download PDF">pdf</a>, <a href="/format/2402.12948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GumbelSoft: Diversified Language Model Watermarking via the  GumbelMax-trick
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jiayi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) excellently generate human-like text, but also
raise concerns about misuse in fake news and academic dishonesty.
Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM
watermark), is a standout solution for safeguarding machine-generated texts due
to its notable detectability. However, GM watermark encounters a major
challenge with generation diversity, always yielding identical outputs for the
same prompt, negatively impacting generation diversity and user experience. To
overcome this limitation, we propose a new type of GM watermark, the
Logits-Addition watermark, and its three variants, specifically designed to
enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of
the Logits-Addition watermark) demonstrates superior performance in high
diversity settings, with its AUROC score outperforming those of the two
alternative variants by 0.1 to 0.3 and surpassing other decoding-based
watermarking methods by a minimum of 0.1.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12950" title="Abstract">arXiv:2402.12950</a> [<a href="/pdf/2402.12950" title="Download PDF">pdf</a>, <a href="/format/2402.12950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jinjing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zimeng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Heyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with
the fundamental theory of quantum mechanics to achieve machine learning tasks
with quantum acceleration. Recently, QNN systems have been found to manifest
robustness issues similar to classical DL systems. There is an urgent need for
ways to test their correctness and security. However, QNN systems differ
significantly from traditional quantum software and classical DL systems,
posing critical challenges for QNN testing. These challenges include the
inapplicability of traditional quantum software testing methods, the dependence
of quantum test sample generation on perturbation operators, and the absence of
effective information in quantum neurons. In this paper, we propose QuanTest, a
quantum entanglement-guided adversarial testing framework to uncover potential
erroneous behaviors in QNN systems. We design a quantum entanglement adequacy
criterion to quantify the entanglement acquired by the input quantum states
from the QNN system, along with two similarity metrics to measure the proximity
of generated quantum adversarial examples to the original inputs. Subsequently,
QuanTest formulates the problem of generating test inputs that maximize the
quantum entanglement sufficiency and capture incorrect behaviors of the QNN
system as a joint optimization problem and solves it in a gradient-based manner
to generate quantum adversarial examples. Experimental results demonstrate that
QuanTest possesses the capability to capture erroneous behaviors in QNN systems
(generating 67.48%-96.05% more test samples than the random noise under the
same perturbation size constraints). The entanglement-guided approach proves
effective in adversarial testing, generating more adversarial examples (maximum
increase reached 21.32%).
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12952" title="Abstract">arXiv:2402.12952</a> [<a href="/pdf/2402.12952" title="Download PDF">pdf</a>, <a href="/ps/2402.12952" title="Download PostScript">ps</a>, <a href="/format/2402.12952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral collocation for functional and delay differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hale%2C+N">Nicholas Hale</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A framework for Chebyshev spectral collocation methods for the numerical
solution of functional and delay differential equations (FDEs and DDEs) is
described. The framework combines interpolation via the barycentric resampling
matrix with a multidomain approach used to resolve isolated discontinuities
propagated by non-smooth initial data. Geometric convergence is demonstrated
for several examples of linear and nonlinear FDEs and DDEs with various delay
types, including discrete, proportional, continuous, and state-dependent delay.
The framework is a natural extension of standard spectral collocation methods
based on polynomial interpolants and can be readily incorporated into existing
spectral discretisations, such as in Chebfun/Chebop, allowing the automated and
efficient solution of a wide class of nonlinear functional and delay
differential equations.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12954" title="Abstract">arXiv:2402.12954</a> [<a href="/pdf/2402.12954" title="Download PDF">pdf</a>, <a href="/format/2402.12954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Logical Message Passing Transformer for Complex Query  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhiping Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Junhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, and 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging
task. Given that KGs are usually incomplete, neural models are proposed to
solve CQA by performing multi-hop logical reasoning. However, most of them
cannot perform well on both one-hop and multi-hop queries simultaneously.
Recent work proposes a logical message passing mechanism based on the
pre-trained neural link predictors. While effective on both one-hop and
multi-hop queries, it ignores the difference between the constant and variable
nodes in a query graph. In addition, during the node embedding update stage,
this mechanism cannot dynamically measure the importance of different messages,
and whether it can capture the implicit logical dependencies related to a node
and received messages remains unclear. In this paper, we propose Conditional
Logical Message Passing Transformer (CLMPT), which considers the difference
between constants and variables in the case of using pre-trained neural link
predictors and performs message passing conditionally on the node type. We
empirically verified that this approach can reduce computational costs without
affecting performance. Furthermore, CLMPT uses the transformer to aggregate
received messages and update the corresponding node embedding. Through the
self-attention mechanism, CLMPT can assign adaptive weights to elements in an
input set consisting of received messages and the corresponding node and
explicitly model logical dependencies between various elements. Experimental
results show that CLMPT is a new state-of-the-art neural CQA model.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12957" title="Abstract">arXiv:2402.12957</a> [<a href="/pdf/2402.12957" title="Download PDF">pdf</a>, <a href="/format/2402.12957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Wireless Federated Learning via Doubly Adaptive  Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xuefeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiumei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Z">Zhen Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Federated learning (FL) has been recognized as a viable distributed learning
paradigm for training a machine learning model across distributed clients
without uploading raw data. However, FL in wireless networks still faces two
major challenges, i.e., large communication overhead and high energy
consumption, which are exacerbated by client heterogeneity in dataset sizes and
wireless channels. While model quantization is effective for energy reduction,
existing works ignore adapting quantization to heterogeneous clients and FL
convergence. To address these challenges, this paper develops an energy
optimization problem of jointly designing quantization levels, scheduling
clients, allocating channels, and controlling computation frequencies (QCCF) in
wireless FL. Specifically, we derive an upper bound identifying the influence
of client scheduling and quantization errors on FL convergence. Under the
longterm convergence constraints and wireless constraints, the problem is
established and transformed into an instantaneous problem with Lyapunov
optimization. Solving Karush-Kuhn-Tucker conditions, our closed-form solution
indicates that the doubly adaptive quantization level rises with the training
process and correlates negatively with dataset sizes. Experiment results
validate our theoretical results, showing that QCCF consumes less energy with
faster convergence compared with state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12958" title="Abstract">arXiv:2402.12958</a> [<a href="/pdf/2402.12958" title="Download PDF">pdf</a>, <a href="/format/2402.12958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Go Static: Contextualized Logging Statement Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R">Renyi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiazhen Gu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pinjia He</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R.Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted by The ACM International Conference on the Foundations of Software Engineering (FSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Logging practices have been extensively investigated to assist developers in
writing appropriate logging statements for documenting software behaviors.
Although numerous automatic logging approaches have been proposed, their
performance remains unsatisfactory due to the constraint of the single-method
input, without informative programming context outside the method.
Specifically, we identify three inherent limitations with single-method
context: limited static scope of logging statements, inconsistent logging
styles, and missing type information of logging variables. To tackle these
limitations, we propose SCLogger, the first contextualized logging statement
generation approach with inter-method static contexts. First, SCLogger extracts
inter-method contexts with static analysis to construct the contextualized
prompt for language models to generate a tentative logging statement. The
contextualized prompt consists of an extended static scope and sampled similar
methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger
refines the access of logging variables by formulating a new refinement prompt
for language models, which incorporates detailed type information of variables
in the tentative logging statement. The evaluation results show that SCLogger
surpasses the state-of-the-art approach by 8.7% in logging position accuracy,
32.1% in level accuracy, 19.6% in variable precision, and 138.4% in text BLEU-4
score. Furthermore, SCLogger consistently boosts the performance of logging
statement generation across a range of large language models, thereby
showcasing the generalizability of this approach.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12959" title="Abstract">arXiv:2402.12959</a> [<a href="/pdf/2402.12959" title="Download PDF">pdf</a>, <a href="/format/2402.12959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Stealing Attacks Against Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sha%2C+Z">Zeyang Sha</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The increasing reliance on large language models (LLMs) such as ChatGPT in
various fields emphasizes the importance of ``prompt engineering,'' a
technology to improve the quality of model outputs. With companies investing
significantly in expert prompt engineers and educational resources rising to
meet market demand, designing high-quality prompts has become an intriguing
challenge. In this paper, we propose a novel attack against LLMs, named prompt
stealing attacks. Our proposed prompt stealing attack aims to steal these
well-designed prompts based on the generated answers. The prompt stealing
attack contains two primary modules: the parameter extractor and the prompt
reconstruction. The goal of the parameter extractor is to figure out the
properties of the original prompts. We first observe that most prompts fall
into one of three categories: direct prompt, role-based prompt, and in-context
prompt. Our parameter extractor first tries to distinguish the type of prompts
based on the generated answers. Then, it can further predict which role or how
many contexts are used based on the types of prompts. Following the parameter
extractor, the prompt reconstructor can be used to reconstruct the original
prompts based on the generated answers and the extracted features. The final
goal of the prompt reconstructor is to generate the reversed prompts, which are
similar to the original prompts. Our experimental results show the remarkable
performance of our proposed attacks. Our proposed attacks add a new dimension
to the study of prompt engineering and call for more attention to the security
issues on LLMs.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12960" title="Abstract">arXiv:2402.12960</a> [<a href="/pdf/2402.12960" title="Download PDF">pdf</a>, <a href="/ps/2402.12960" title="Download PostScript">ps</a>, <a href="/format/2402.12960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Non-Failure Conditions for Declarative Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanus%2C+M">Michael Hanus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of a paper presented at the 17th International Symposium on Functional and Logic Programming (FLOPS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Unintended failures during a computation are painful but frequent during
software development. Failures due to external reasons (e.g., missing files, no
permissions) can be caught by exception handlers. Programming failures, such as
calling a partially defined operation with unintended arguments, are often not
caught due to the assumption that the software is correct. This paper presents
an approach to verify such assumptions. For this purpose, non-failure
conditions for operations are inferred and then checked in all uses of
partially defined operations. In the positive case, the absence of such
failures is ensured. In the negative case, the programmer could adapt the
program to handle possibly failing situations and check the program again. Our
method is fully automatic and can be applied to larger declarative programs.
The results of an implementation for functional logic Curry programs are
presented.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12962" title="Abstract">arXiv:2402.12962</a> [<a href="/pdf/2402.12962" title="Download PDF">pdf</a>, <a href="/format/2402.12962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost  Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chunyang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Haogang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+M">Maolin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Autoscaling is a technology to automatically scale the resources provided to
their applications without human intervention to guarantee runtime Quality of
Service (QoS) while saving costs. However, user-facing cloud applications serve
dynamic workloads that often exhibit variable and contain bursts, posing
challenges to autoscaling for maintaining QoS within Service-Level Objectives
(SLOs). Conservative strategies risk over-provisioning, while aggressive ones
may cause SLO violations, making it more challenging to design effective
autoscaling. This paper introduces BAScaler, a Burst-Aware Autoscaling
framework for containerized cloud services or applications under complex
workloads, combining multi-level machine learning (ML) techniques to mitigate
SLO violations while saving costs. BAScaler incorporates a novel
prediction-based burst detection mechanism that distinguishes between
predictable periodic workload spikes and actual bursts. When bursts are
detected, BAScaler appropriately overestimates them and allocates resources
accordingly to address the rapid growth in resource demand. On the other hand,
BAScaler employs reinforcement learning to rectify potential inaccuracies in
resource estimation, enabling more precise resource allocation during
non-bursts. Experiments across ten real-world workloads demonstrate BAScaler's
effectiveness, achieving a 57% average reduction in SLO violations and cutting
resource costs by 10% compared to other prominent methods.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12967" title="Abstract">arXiv:2402.12967</a> [<a href="/pdf/2402.12967" title="Download PDF">pdf</a>, <a href="/format/2402.12967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Privacy via Information Density
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grosse%2C+L">Leonhard Grosse</a>, 
<a href="/search/cs?searchtype=author&query=Saeidian%2C+S">Sara Saeidian</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+P">Parastoo Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Oechtering%2C+T+J">Tobias J. Oechtering</a>, 
<a href="/search/cs?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We examine the relationship between privacy metrics that utilize information
density to measure information leakage between a private and a disclosed random
variable. Firstly, we prove that bounding the information density from above or
below in turn implies a lower or upper bound on the information density,
respectively. Using this result, we establish new relationships between local
information privacy, asymmetric local information privacy, pointwise maximal
leakage and local differential privacy. We further provide applications of
these relations to privacy mechanism design. Furthermore, we provide statements
showing the equivalence between a lower bound on information density and
risk-averse adversaries. More specifically, we prove an equivalence between a
guessing framework and a cost-function framework that result in the desired
lower bound on the information density.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12968" title="Abstract">arXiv:2402.12968</a> [<a href="/pdf/2402.12968" title="Download PDF">pdf</a>, <a href="/format/2402.12968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MapTrack: Tracking in the Map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruohui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chenglin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yun Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted
trajectories for each target. Most state-of-the-art approaches first detect
objects in each frame and then implement data association between new
detections and existing tracks using motion models and appearance similarities.
Despite achieving satisfactory results, occlusion and crowds can easily lead to
missing and distorted detections, followed by missing and false associations.
In this paper, we first revisit the classic tracker DeepSORT, enhancing its
robustness over crowds and occlusion significantly by placing greater trust in
predictions when detections are unavailable or of low quality in crowded and
occluded scenes. Specifically, we propose a new framework comprising of three
lightweight and plug-and-play algorithms: the probability map, the prediction
map, and the covariance adaptive Kalman filter. The probability map identifies
whether undetected objects have genuinely disappeared from view (e.g., out of
the image or entered a building) or are only temporarily undetected due to
occlusion or other reasons. Trajectories of undetected targets that are still
within the probability map are extended by state estimations directly. The
prediction map determines whether an object is in a crowd, and we prioritize
state estimations over observations when severe deformation of observations
occurs, accomplished through the covariance adaptive Kalman filter. The
proposed method, named MapTrack, achieves state-of-the-art results on popular
multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior
performance, our method remains simple, online, and real-time. The code will be
open-sourced later.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12969" title="Abstract">arXiv:2402.12969</a> [<a href="/pdf/2402.12969" title="Download PDF">pdf</a>, <a href="/format/2402.12969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gl&#xf3;rIA - A Generative and Open Large Language Model for Portuguese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopes%2C+R">Ricardo Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Magalh%C3%A3es%2C+J">Jo&#xe3;o Magalh&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Semedo%2C+D">David Semedo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at PROPOR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Significant strides have been made in natural language tasks, largely
attributed to the emergence of powerful large language models (LLMs). These
models, pre-trained on extensive and diverse corpora, have become increasingly
capable of comprehending the intricacies of language. Despite the abundance of
LLMs for many high-resource languages, the availability of such models remains
limited for European Portuguese. We introduce Gl\'orIA, a robust European
Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive
PT-PT text corpus comprising 35 billion tokens from various sources. We present
our pre-training methodology, followed by an assessment of the model's
effectiveness on multiple downstream tasks. Additionally, to evaluate our
models' language modeling capabilities, we introduce CALAME-PT (Context-Aware
LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot
language-modeling benchmark. Evaluation shows that Gl\'orIA significantly
outperforms existing open PT decoder models in language modeling and that it
can generate sound, knowledge-rich, and coherent PT-PT text. The model also
exhibits strong potential for various downstream tasks.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12973" title="Abstract">arXiv:2402.12973</a> [<a href="/pdf/2402.12973" title="Download PDF">pdf</a>, <a href="/format/2402.12973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Between Green Hills and Green Bills: Unveiling the Green Shades of  Sustainability and Burden Shifting through Multi-Objective Optimization in  Swiss Energy System Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schnidrig%2C+J">Jonas Schnidrig</a>, 
<a href="/search/cs?searchtype=author&query=Souttre%2C+M">Matthieu Souttre</a>, 
<a href="/search/cs?searchtype=author&query=Chuat%2C+A">Arthur Chuat</a>, 
<a href="/search/cs?searchtype=author&query=Mar%C3%A9chal%2C+F">Fran&#xe7;ois Mar&#xe9;chal</a>, 
<a href="/search/cs?searchtype=author&query=Margni%2C+M">Manuele Margni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The Paris agreement is the first-ever universally accepted and legally
binding agreement on global climate change. It is a bridge between today's and
climate-neutrality policies and strategies before the end of the century.
Critical to this endeavor is energy system modeling, which, while adept at
devising cost-effective carbon-neutral strategies, often overlooks the broader
environmental and social implications. This study introduces an innovative
methodology that integrates life-cycle impact assessment indicators into energy
system modeling, enabling a comprehensive assessment of both economic and
environmental outcomes.
<br />Focusing on Switzerland's energy system as a case study, our model reveals
that optimizing key environomic indicators can lead to significant economic
advantages, with system costs potentially decreasing by 15% to 47% by
minimizing potential impacts from operating fossil technologies to the indirect
impact related to the construction of the renewable infrastructure. However, a
system optimized solely for economic efficiency, despite achieving 63%
reduction in carbon footprint compared to 2020, our results show a potential
risk of burden shift to other environmental issues.
<br />The adoption of multi-objective optimization in our approach nuances the
exploration of the complex interplay between environomic objectives and
technological choices. Our results illuminate pathways towards more
holistically optimized energy systems, effectively addressing trade-offs across
environmental problems and enhancing societal acceptance of the solutions to
this century's defining challenge.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12974" title="Abstract">arXiv:2402.12974</a> [<a href="/pdf/2402.12974" title="Download PDF">pdf</a>, <a href="/format/2402.12974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Style Prompting with Swapping Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jaeseok Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yunjey Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gayoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the evolving domain of text-to-image generation, diffusion models have
emerged as powerful tools in content creation. Despite their remarkable
capability, existing models still face challenges in achieving controlled
generation with a consistent style, requiring costly fine-tuning or often
inadequately transferring the visual elements due to content leakage. To
address these challenges, we propose a novel approach, \ours, to produce a
diverse range of images while maintaining specific style elements and nuances.
During the denoising process, we keep the query from original features while
swapping the key and value with those from reference features in the late
self-attention layers. This approach allows for the visual style prompting
without any fine-tuning, ensuring that generated images maintain a faithful
style. Through extensive evaluation across various styles and text prompts, our
method demonstrates superiority over existing approaches, best reflecting the
style of the references and ensuring that resulting images match the text
prompts most accurately. Our project page is available
\href{https://curryjung.github.io/VisualStylePrompt/}{here}.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12976" title="Abstract">arXiv:2402.12976</a> [<a href="/pdf/2402.12976" title="Download PDF">pdf</a>, <a href="/format/2402.12976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Demonstrations on Multilingual In-Context Learning: A  Multidimensional Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miaoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+V">Vagrant Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Alabi%2C+J+O">Jesujoba O. Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Klakow%2C+D">Dietrich Klakow</a>, 
<a href="/search/cs?searchtype=author&query=Mosbach%2C+M">Marius Mosbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In-context learning is a popular inference strategy where large language
models solve a task using only a few labelled demonstrations without needing
any parameter updates. Compared to work on monolingual (English) in-context
learning, multilingual in-context learning is under-explored, and we lack an
in-depth understanding of the role of demonstrations in this context. To
address this gap, we conduct a multidimensional analysis of multilingual
in-context learning, experimenting with 5 models from different model families,
9 datasets covering classification and generation tasks, and 56 typologically
diverse languages. Our results reveal that the effectiveness of demonstrations
varies significantly across models, tasks, and languages. We also find that
Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of
demonstrations. Instead, a carefully crafted template often eliminates the
benefits of demonstrations for some tasks and languages altogether. These
findings show that the importance of demonstrations might be overestimated. Our
work highlights the need for granular evaluation across multiple axes towards a
better understanding of in-context learning.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12984" title="Abstract">arXiv:2402.12984</a> [<a href="/pdf/2402.12984" title="Download PDF">pdf</a>, <a href="/format/2402.12984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can GNN be Good Adapter for LLMs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kaiqiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+D">Dezheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Q">Quanjin Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Z">Ziwei Chai</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, large language models (LLMs) have demonstrated superior
capabilities in understanding and zero-shot learning on textual data, promising
significant advances for many text-related domains. In the graph domain,
various real-world scenarios also involve textual data, where tasks and node
features can be described by text. These text-attributed graphs (TAGs) have
broad applications in social media, recommendation systems, etc. Thus, this
paper explores how to utilize LLMs to model TAGs. Previous methods for TAG
modeling are based on million-scale LMs. When scaled up to billion-scale LLMs,
they face huge challenges in computational costs. Additionally, they also
ignore the zero-shot inference capabilities of LLMs. Therefore, we propose
GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter
in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN
adapter introduces only a few trainable parameters and can be trained with low
computation costs. The entire framework is trained using auto-regression on
node text (next token prediction). Once trained, GraphAdapter can be seamlessly
fine-tuned with task-specific prompts for various downstream tasks. Through
extensive experiments across multiple real-world TAGs, GraphAdapter based on
Llama 2 gains an average improvement of approximately 5\% in terms of node
classification. Furthermore, GraphAdapter can also adapt to other language
models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs
can serve as effective adapters for LLMs in TAG modeling.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12986" title="Abstract">arXiv:2402.12986</a> [<a href="/pdf/2402.12986" title="Download PDF">pdf</a>, <a href="/format/2402.12986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory  Manycore Clusters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzola%2C+S">Sergio Mazzola</a>, 
<a href="/search/cs?searchtype=author&query=Riedel%2C+S">Samuel Riedel</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Systolic arrays and shared L1-memory manycore clusters are commonly used
architectural paradigms that offer different trade-offs to accelerate parallel
workloads. While the first excel with regular dataflow at the cost of rigid
architectures and complex programming models, the second are versatile and easy
to program but require explicit data flow management and synchronization. This
work aims at enabling efficient systolic execution on shared L1-memory manycore
clusters. We devise a flexible architecture where small and energy-efficient
RISC-V cores act as the systolic array's processing elements (PEs) and can form
diverse, reconfigurable systolic topologies through queues mapped in the
cluster's shared memory. We introduce two low-overhead RISC-V ISA extensions
for efficient systolic execution, namely Xqueue and Queue-linked registers
(QLRs), which support queue management in hardware. The Xqueue extension
enables single-instruction access to shared-memory-mapped queues, while QLRs
allow implicit and autonomous access to them, relieving the cores of explicit
communication instructions. We demonstrate Xqueue and QLRs in MemPool, an
open-source manycore cluster with 256 PEs, and analyze the hybrid
systolic-shared-memory architecture's trade-offs on matrix multiplication,
convolution, and FFT kernels. For an area increase of just 6%, our hybrid
architecture almost doubles MemPool's compute unit utilization to up to 95% and
significantly improves energy efficiency, achieving up to 63% of power spent in
the PEs. In typical conditions (TT/0.80V/25{\deg}C) in a 22nm FDX technology,
our hybrid architecture runs at 600MHz with no frequency degradation and is up
to 64% more energy efficient than the shared-memory baseline, achieving up to
208GOPS/W.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12987" title="Abstract">arXiv:2402.12987</a> [<a href="/pdf/2402.12987" title="Download PDF">pdf</a>, <a href="/format/2402.12987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Graph Incremental Learning on Evolving Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Difan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Incremental learning is a machine learning approach that involves training a
model on a sequence of tasks, rather than all tasks at once. This ability to
learn incrementally from a stream of tasks is crucial for many real-world
applications. However, incremental learning is a challenging problem on
graph-structured data, as many graph-related problems involve prediction tasks
for each individual node, known as Node-wise Graph Incremental Learning (NGIL).
This introduces non-independent and non-identically distributed characteristics
in the sample data generation process, making it difficult to maintain the
performance of the model as new tasks are added. In this paper, we focus on the
inductive NGIL problem, which accounts for the evolution of graph structure
(structural shift) induced by emerging tasks. We provide a formal formulation
and analysis of the problem, and propose a novel regularization-based technique
called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the
structural shift on catastrophic forgetting of the inductive NGIL problem. We
show that the structural shift can lead to a shift in the input distribution
for the existing tasks, and further lead to an increased risk of catastrophic
forgetting. Through comprehensive empirical studies with several benchmark
datasets, we demonstrate that our proposed method,
Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to
improve the performance of state-of-the-art GNN incremental learning frameworks
in the inductive setting.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12989" title="Abstract">arXiv:2402.12989</a> [<a href="/pdf/2402.12989" title="Download PDF">pdf</a>, <a href="/format/2402.12989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile Perception in Upper Limb Prostheses: Mechanical  Characterization, Human Experiments, and Computational Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivani%2C+A+S">Alessia Silvia Ivani</a>, 
<a href="/search/cs?searchtype=author&query=Catalano%2C+M+G">Manuel G. Catalano</a>, 
<a href="/search/cs?searchtype=author&query=Grioli%2C+G">Giorgio Grioli</a>, 
<a href="/search/cs?searchtype=author&query=Bianchi%2C+M">Matteo Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Visell%2C+Y">Yon Visell</a>, 
<a href="/search/cs?searchtype=author&query=Bicchi%2C+A">Antonio Bicchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Our research investigates vibrotactile perception in four prosthetic hands
with distinct kinematics and mechanical characteristics. We found that rigid
and simple socket-based prosthetic devices can transmit tactile information and
surprisingly enable users to identify the stimulated finger with high
reliability. This ability decreases with more advanced prosthetic hands with
additional articulations and softer mechanics. We conducted experiments to
understand the underlying mechanisms. We assessed a prosthetic user's ability
to discriminate finger contacts based on vibrations transmitted through the
four prosthetic hands. We also performed numerical and mechanical vibration
tests on the prostheses and used a machine learning classifier to identify the
contacted finger. Our results show that simpler and rigid prosthetic hands
facilitate contact discrimination (for instance, a user of a purely cosmetic
hand can distinguish a contact on the index finger from other fingers with 83%
accuracy), but all tested hands, including soft advanced ones, performed above
chance level. Despite advanced hands reducing vibration transmission, a machine
learning algorithm still exceeded human performance in discriminating finger
contacts. These findings suggest the potential for enhancing vibrotactile
feedback in advanced prosthetic hands and lay the groundwork for future
integration of such feedback in prosthetic devices.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12991" title="Abstract">arXiv:2402.12991</a> [<a href="/pdf/2402.12991" title="Download PDF">pdf</a>, <a href="/format/2402.12991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gubri%2C+M">Martin Gubri</a>, 
<a href="/search/cs?searchtype=author&query=Ulmer%2C+D">Dennis Ulmer</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Model (LLM) services and models often come with legal rules on
who can use them and how they must use them. Assessing the compliance of the
released LLMs is crucial, as these rules protect the interests of the LLM
contributor and prevent misuse. In this context, we describe the novel problem
of Black-box Identity Verification (BBIV). The goal is to determine whether a
third-party application uses a certain LLM through its chat function. We
propose a method called Targeted Random Adversarial Prompt (TRAP) that
identifies the specific LLM in use. We repurpose adversarial suffixes,
originally proposed for jailbreaking, to get a pre-defined answer from the
target LLM, while other models give random answers. TRAP detects the target
LLMs with over 95% true positive rate at under 0.2% false positive rate even
after a single interaction. TRAP remains effective even if the LLM has minor
changes that do not significantly alter the original function.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12993" title="Abstract">arXiv:2402.12993</a> [<a href="/pdf/2402.12993" title="Download PDF">pdf</a>, <a href="/format/2402.12993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Autonomous Large Language Model Agent for Chemical Literature Data  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hanqun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Menghao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lanqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiezhong Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P+A">Pheng Ann Heng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Chemical synthesis, which is crucial for advancing material synthesis and
drug discovery, impacts various sectors including environmental science and
healthcare. The rise of technology in chemistry has generated extensive
chemical data, challenging researchers to discern patterns and refine synthesis
processes. Artificial intelligence (AI) helps by analyzing data to optimize
synthesis and increase yields. However, AI faces challenges in processing
literature data due to the unstructured format and diverse writing style of
chemical literature. To overcome these difficulties, we introduce an end-to-end
AI agent framework capable of high-fidelity extraction from extensive chemical
literature. This AI agent employs large language models (LLMs) for prompt
generation and iterative optimization. It functions as a chemistry assistant,
automating data collection and analysis, thereby saving manpower and enhancing
performance. Our framework's efficacy is evaluated using accuracy, recall, and
F1 score of reaction condition data, and we compared our method with human
experts in terms of content correctness and time efficiency. The proposed
approach marks a significant advancement in automating chemical literature
extraction and demonstrates the potential for AI to revolutionize data
management and utilization in chemistry.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12994" title="Abstract">arXiv:2402.12994</a> [<a href="/pdf/2402.12994" title="Download PDF">pdf</a>, <a href="/format/2402.12994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Graph-based Recommendation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bohao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qihao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">With the capacity to capture high-order collaborative signals, Graph Neural
Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS).
However, their efficacy often hinges on the assumption that training and
testing data share the same distribution (a.k.a. IID assumption), and exhibits
significant declines under distribution shifts. Distribution shifts commonly
arises in RS, often attributed to the dynamic nature of user preferences or
ubiquitous biases during data collection in RS. Despite its significance,
researches on GNN-based recommendation against distribution shift are still
sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN)
that incorporates Distributional Robust Optimization (DRO) into the GNN-based
recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater
to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing
regularizer, thereby facilitating the nuanced application of DRO; 2) Given the
typically sparse nature of recommendation data, which might impede robust
optimization, we introduce slight perturbations in the training distribution to
expand its support. Notably, while DR-GNN involves complex optimization, it can
be implemented easily and efficiently. Our extensive experiments validate the
effectiveness of DR-GNN against three typical distribution shifts. The code is
available at https://github.com/WANGBohaO-jpg/DR-GNN .
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12997" title="Abstract">arXiv:2402.12997</a> [<a href="/pdf/2402.12997" title="Download PDF">pdf</a>, <a href="/format/2402.12997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Trustworthy Reranking: A Simple yet Effective Abstention  Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gisserot-Boukhlef%2C+H">Hippolyte Gisserot-Boukhlef</a>, 
<a href="/search/cs?searchtype=author&query=Faysse%2C+M">Manuel Faysse</a>, 
<a href="/search/cs?searchtype=author&query=Malherbe%2C+E">Emmanuel Malherbe</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12998" title="Abstract">arXiv:2402.12998</a> [<a href="/pdf/2402.12998" title="Download PDF">pdf</a>, <a href="/ps/2402.12998" title="Download PostScript">ps</a>, <a href="/format/2402.12998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phonotactic Complexity across Dialects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shim%2C+R+S">Ryan Soh-Eun Shim</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kalvin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D+R">David R. Mortensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to COLING-LREC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Received wisdom in linguistic typology holds that if the structure of a
language becomes more complex in one dimension, it will simplify in another,
building on the assumption that all languages are equally complex (Joseph and
Newmeyer, 2012). We study this claim on a micro-level, using a
tightly-controlled sample of Dutch dialects (across 366 collection sites) and
Min dialects (across 60 sites), which enables a more fair comparison across
varieties. Even at the dialect level, we find empirical evidence for a tradeoff
between word length and a computational measure of phonotactic complexity from
a LSTM-based phone-level language model-a result previously documented only at
the language level. A generalized additive model (GAM) shows that dialects with
low phonotactic complexity concentrate around the capital regions, which we
hypothesize to correspond to prior hypotheses that language varieties of
greater or more diverse populations show reduced phonotactic complexity. We
also experiment with incorporating the auxiliary task of predicting syllable
constituency, but do not find an increase in the negative correlation observed.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13000" title="Abstract">arXiv:2402.13000</a> [<a href="/pdf/2402.13000" title="Download PDF">pdf</a>, <a href="/format/2402.13000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Context-Aware Contrastive Explanations in Rule-based Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbold%2C+L">Lars Herbold</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mersedeh Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Vogelsang%2C+A">Andreas Vogelsang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 Workshop on Explainability Engineering (ExEn '24 )
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Human explanations are often contrastive, meaning that they do not answer the
indeterminate "Why?" question, but instead "Why P, rather than Q?".
Automatically generating contrastive explanations is challenging because the
contrastive event (Q) represents the expectation of a user in contrast to what
happened. We present an approach that predicts a potential contrastive event in
situations where a user asks for an explanation in the context of rule-based
systems. Our approach analyzes a situation that needs to be explained and then
selects the most likely rule a user may have expected instead of what the user
has observed. This contrastive event is then used to create a contrastive
explanation that is presented to the user. We have implemented the approach as
a plugin for a home automation system and demonstrate its feasibility in four
test scenarios.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13004" title="Abstract">arXiv:2402.13004</a> [<a href="/pdf/2402.13004" title="Download PDF">pdf</a>, <a href="/format/2402.13004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Conventional Hybrid and CTC/Attention Decoders for  Continuous Visual Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gimeno-G%C3%B3mez%2C+D">David Gimeno-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Hinarejos%2C+C">Carlos-D. Mart&#xed;nez-Hinarejos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Thanks to the rise of deep learning and the availability of large-scale
audio-visual databases, recent advances have been achieved in Visual Speech
Recognition (VSR). Similar to other speech processing tasks, these end-to-end
VSR systems are usually based on encoder-decoder architectures. While encoders
are somewhat general, multiple decoding approaches have been explored, such as
the conventional hybrid model based on Deep Neural Networks combined with
Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification
(CTC) paradigm. However, there are languages and tasks in which data is scarce,
and in this situation, there is not a clear comparison between different types
of decoders. Therefore, we focused our study on how the conventional DNN-HMM
decoder and its state-of-the-art CTC/Attention counterpart behave depending on
the amount of data used for their estimation. We also analyzed to what extent
our visual speech features were able to adapt to scenarios for which they were
not explicitly trained, either considering a similar dataset or another
collected for a different language. Results showed that the conventional
paradigm reached recognition rates that improve the CTC/Attention model in
data-scarcity scenarios along with a reduced training time and fewer
parameters.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13006" title="Abstract">arXiv:2402.13006</a> [<a href="/pdf/2402.13006" title="Download PDF">pdf</a>, <a href="/format/2402.13006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Impact of Model Instability on Explanations and  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marjanovi%C4%87%2C+S+V">Sara Vera Marjanovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=Lioma%2C+C">Christina Lioma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Explainable AI methods facilitate the understanding of model behaviour, yet,
small, imperceptible perturbations to inputs can vastly distort explanations.
As these explanations are typically evaluated holistically, before model
deployment, it is difficult to assess when a particular explanation is
trustworthy. Some studies have tried to create confidence estimators for
explanations, but none have investigated an existing link between uncertainty
and explanation quality. We artificially simulate epistemic uncertainty in text
input by introducing noise at inference time. In this large-scale empirical
study, we insert different levels of noise perturbations and measure the effect
on the output of pre-trained language models and different uncertainty metrics.
Realistic perturbations have minimal effect on performance and explanations,
yet masking has a drastic effect. We find that high uncertainty doesn't
necessarily imply low explanation plausibility; the correlation between the two
metrics can be moderately positive when noise is exposed during the training
process. This suggests that noise-augmented models may be better at identifying
salient tokens when uncertain. Furthermore, when predictive and epistemic
uncertainty measures are over-confident, the robustness of a saliency map to
perturbation can indicate model stability issues. Integrated Gradients shows
the overall greatest robustness to perturbation, while still showing
model-specific patterns in performance; however, this phenomenon is limited to
smaller Transformer-based language models.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13007" title="Abstract">arXiv:2402.13007</a> [<a href="/pdf/2402.13007" title="Download PDF">pdf</a>, <a href="/format/2402.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improve Cross-Architecture Generalization on Dataset Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Binglin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Linhao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wentao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Dataset distillation, a pragmatic approach in machine learning, aims to
create a smaller synthetic dataset from a larger existing dataset. However,
existing distillation methods primarily adopt a model-based paradigm, where the
synthetic dataset inherits model-specific biases, limiting its generalizability
to alternative models. In response to this constraint, we propose a novel
methodology termed "model pool". This approach involves selecting models from a
diverse model pool based on a specific probability distribution during the data
distillation process. Additionally, we integrate our model pool with the
established knowledge distillation approach and apply knowledge distillation to
the test process of the distilled dataset. Our experimental results validate
the effectiveness of the model pool approach across a range of existing models
while testing, demonstrating superior performance compared to existing
methodologies.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13008" title="Abstract">arXiv:2402.13008</a> [<a href="/pdf/2402.13008" title="Download PDF">pdf</a>, <a href="/format/2402.13008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Enumeration of Large Maximal k-Plexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qihao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Da Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lyuheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Ji Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Finding cohesive subgraphs in a large graph has many important applications,
such as community detection and biological network analysis. Clique is often a
too strict cohesive structure since communities or biological modules rarely
form as cliques for various reasons such as data noise. Therefore, $k$-plex is
introduced as a popular clique relaxation, which is a graph where every vertex
is adjacent to all but at most $k$ vertices. In this paper, we propose an
efficient branch-and-bound algorithm as well as its task-based parallel version
to enumerate all maximal $k$-plexes with at least $q$ vertices. Our algorithm
adopts an effective search space partitioning approach that provides a good
time complexity, a new pivot vertex selection method that reduces candidate
vertex size, an effective upper-bounding technique to prune useless branches,
and three novel pruning techniques by vertex pairs. Our parallel algorithm uses
a timeout mechanism to eliminate straggler tasks, and maximizes cache locality
while ensuring load balancing. Extensive experiments show that compared with
the state-of-the-art algorithms, our sequential and parallel algorithms
enumerate large maximal $k$-plexes with up to $5 \times$ and $18.9 \times$
speedup, respectively. Ablation results also demonstrate that our pruning
techniques bring up to $7 \times$ speedup compared with our basic algorithm.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13013" title="Abstract">arXiv:2402.13013</a> [<a href="/pdf/2402.13013" title="Download PDF">pdf</a>, <a href="/format/2402.13013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Needs Comments: Enhancing Code LLMs with Comment Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Demin Song</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Honglin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+S">Shuhao Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yudong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The programming skill is one crucial ability for Large Language Models
(LLMs), necessitating a deep understanding of programming languages (PLs) and
their correlation with natural languages (NLs). We examine the impact of
pre-training data on code-focused LLMs' performance by assessing the comment
density as a measure of PL-NL alignment. Given the scarcity of code-comment
aligned data in pre-training corpora, we introduce a novel data augmentation
method that generates comments for existing code, coupled with a data filtering
strategy that filters out code data poorly correlated with natural language. We
conducted experiments on three code-focused LLMs and observed consistent
improvements in performance on two widely-used programming skill benchmarks.
Notably, the model trained on the augmented data outperformed both the model
used for generating comments and the model further trained on the data without
augmentation.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13016" title="Abstract">arXiv:2402.13016</a> [<a href="/pdf/2402.13016" title="Download PDF">pdf</a>, <a href="/format/2402.13016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the effects of language-specific class imbalance in  multilingual fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+V">Vincent Jung</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Plas%2C+L">Lonneke van der Plas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in: Findings of the Association for Computational Linguistics: EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We study the effect of one type of imbalance often present in real-life
multilingual classification datasets: an uneven distribution of labels across
languages. We show evidence that fine-tuning a transformer-based Large Language
Model (LLM) on a dataset with this imbalance leads to worse performance, a more
pronounced separation of languages in the latent space, and the promotion of
uninformative features. We modify the traditional class weighing approach to
imbalance by calculating class weights separately for each language and show
that this helps mitigate those detrimental effects. These results create
awareness of the negative effects of language-specific class imbalance in
multilingual fine-tuning and the way in which the model learns to rely on the
separation of languages to perform the task.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13019" title="Abstract">arXiv:2402.13019</a> [<a href="/pdf/2402.13019" title="Download PDF">pdf</a>, <a href="/format/2402.13019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Neural-based Classification with Logical Background Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ledaguenel%2C+A">Arthur Ledaguenel</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Khouadjia%2C+M">Mostepha Khouadjia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, submitted to IJCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">Neurosymbolic AI is a growing field of research aiming to combine neural
networks learning capabilities with the reasoning abilities of symbolic
systems. This hybridization can take many shapes. In this paper, we propose a
new formalism for supervised multi-label classification with propositional
background knowledge. We introduce a new neurosymbolic technique called
semantic conditioning at inference, which only constrains the system during
inference while leaving the training unaffected. We discuss its theoritical and
practical advantages over two other popular neurosymbolic techniques: semantic
conditioning and semantic regularization. We develop a new multi-scale
methodology to evaluate how the benefits of a neurosymbolic technique evolve
with the scale of the network. We then evaluate experimentally and compare the
benefits of all three techniques across model scales on several datasets. Our
results demonstrate that semantic conditioning at inference can be used to
build more accurate neural-based systems with fewer resources while
guaranteeing the semantic consistency of outputs.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13022" title="Abstract">arXiv:2402.13022</a> [<a href="/pdf/2402.13022" title="Download PDF">pdf</a>, <a href="/format/2402.13022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoMeLVLM: A Large Vision Language Model for Social Media Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinnong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+H">Haoyu Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+X">Xinyi Mou</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+H">Hanjia Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiebo Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">The growth of social media, characterized by its multimodal nature, has led
to the emergence of diverse phenomena and challenges, which calls for an
effective approach to uniformly solve automated tasks. The powerful Large
Vision Language Models make it possible to handle a variety of tasks
simultaneously, but even with carefully designed prompting methods, the general
domain models often fall short in aligning with the unique speaking style and
context of social media tasks. In this paper, we introduce a Large Vision
Language Model for Social Media Processing (SoMeLVLM), which is a cognitive
framework equipped with five key capabilities including knowledge &amp;
comprehension, application, analysis, evaluation, and creation. SoMeLVLM is
designed to understand and generate realistic social media behavior. We have
developed a 654k multimodal social media instruction-tuning dataset to support
our cognitive framework and fine-tune our model. Our experiments demonstrate
that SoMeLVLM achieves state-of-the-art performance in multiple social media
tasks. Further analysis shows its significant advantages over baselines in
terms of cognitive abilities.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13024" title="Abstract">arXiv:2402.13024</a> [<a href="/pdf/2402.13024" title="Download PDF">pdf</a>, <a href="/format/2402.13024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmartEx: A Framework for Generating User-Centric Explanations in Smart  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mersedeh Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Herbold%2C+L">Lars Herbold</a>, 
<a href="/search/cs?searchtype=author&query=Unterbusch%2C+M">Max Unterbusch</a>, 
<a href="/search/cs?searchtype=author&query=Vogelsang%2C+A">Andreas Vogelsang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22nd International Conference on Pervasive Computing and Communications (PerCom 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Explainability is crucial for complex systems like pervasive smart
environments, as they collect and analyze data from various sensors, follow
multiple rules, and control different devices resulting in behavior that is not
trivial and, thus, should be explained to the users. The current approaches,
however, offer flat, static, and algorithm-focused explanations. User-centric
explanations, on the other hand, consider the recipient and context, providing
personalized and context-aware explanations. To address this gap, we propose an
approach to incorporate user-centric explanations into smart environments. We
introduce a conceptual model and a reference architecture for characterizing
and generating such explanations. Our work is the first technical solution for
generating context-aware and granular explanations in smart environments. Our
architecture implementation demonstrates the feasibility of our approach
through various scenarios.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13025" title="Abstract">arXiv:2402.13025</a> [<a href="/pdf/2402.13025" title="Download PDF">pdf</a>, <a href="/format/2402.13025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFEVER: A Chinese Fact Extraction and VERification Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Ying-Jia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chun-Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chia-Jen Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi-Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yun-Yu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chih-Hao Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mei-Feng Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+H">Hung-Yu Kao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present CFEVER, a Chinese dataset designed for Fact Extraction and
VERification. CFEVER comprises 30,012 manually created claims based on content
in Chinese Wikipedia. Each claim in CFEVER is labeled as "Supports", "Refutes",
or "Not Enough Info" to depict its degree of factualness. Similar to the FEVER
dataset, claims in the "Supports" and "Refutes" categories are also annotated
with corresponding evidence sentences sourced from single or multiple pages in
Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934
for five-way inter-annotator agreement. In addition, through the experiments
with the state-of-the-art approaches developed on the FEVER dataset and a
simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous
benchmark for factual extraction and verification, which can be further used
for developing automated systems to alleviate human fact-checking efforts.
CFEVER is available at https://ikmlab.github.io/CFEVER.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13027" title="Abstract">arXiv:2402.13027</a> [<a href="/pdf/2402.13027" title="Download PDF">pdf</a>, <a href="/format/2402.13027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the decision-making analysis differential equation using eye  fixation data in Unity software with Hermite Long-Short-Term Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parand%2C+K">Kourosh Parand</a>, 
<a href="/search/cs?searchtype=author&query=Setayeshi%2C+S">Saeed Setayeshi</a>, 
<a href="/search/cs?searchtype=author&query=Pedram%2C+M+M">Mir Mohsen Pedram</a>, 
<a href="/search/cs?searchtype=author&query=Yoonesi%2C+A">Ali Yoonesi</a>, 
<a href="/search/cs?searchtype=author&query=Pakniyat%2C+A">Aida Pakniyat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Decision-making is a fundamental component of our personal and professional
lives. To analyze decision-making accuracy, this study proposes a virtual
environment designed as an industrial town to investigate the relationship
between eye movements and decision-making. Eye tracking provides a tool to
examine eye movements, which contain information related to eye position, head
position, and gaze direction. The game is designed using Unity software, with
the collected data being analyzed using a differential equation and the Hermite
neural network method. The game is used to identify the behaviors exhibited by
bad and good individuals and differentiate between them before taking action.
This paper investigates the accuracy of an individual's decision-making process
by analyzing their eye movements and the correctness of the decisions made.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13028" title="Abstract">arXiv:2402.13028</a> [<a href="/pdf/2402.13028" title="Download PDF">pdf</a>, <a href="/format/2402.13028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Haisong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weizhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=wu%2C+S">Shu wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 38th Association for the Advancement of Artificial Intelligence, AAAI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fact checking aims to predict claim veracity by reasoning over multiple
evidence pieces. It usually involves evidence retrieval and veracity reasoning.
In this paper, we focus on the latter, reasoning over unstructured text and
structured table information. Previous works have primarily relied on
fine-tuning pretrained language models or training homogeneous-graph-based
models. Despite their effectiveness, we argue that they fail to explore the
rich semantic information underlying the evidence with different structures. To
address this, we propose a novel word-level Heterogeneous-graph-based model for
Fact Checking over unstructured and structured information, namely HeterFC. Our
approach leverages a heterogeneous evidence graph, with words as nodes and
thoughtfully designed edges representing different evidence properties. We
perform information propagation via a relational graph neural network,
facilitating interactions between claims and evidence. An attention-based
method is utilized to integrate information, combined with a language model for
generating predictions. We introduce a multitask loss function to account for
potential inaccuracies in evidence retrieval. Comprehensive experiments on the
large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC.
Code will be released at: https://github.com/Deno-V/HeterFC.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13029" title="Abstract">arXiv:2402.13029</a> [<a href="/pdf/2402.13029" title="Download PDF">pdf</a>, <a href="/ps/2402.13029" title="Download PostScript">ps</a>, <a href="/format/2402.13029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning for Iot/Edge/Fog Computing Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+B+T">Balqees Talal Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Idrees%2C+A+K">Ali Kadhum Idrees</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures, Book chapter
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Hasan, B.T., Idrees, A.K. (2024).Federated Learning for
  Iot/Edge/Fog Computing Systems. In: Jayakrushna Sahoo, Mariya Ouaissa, Akarsh
  K. Nair(eds) Federated Learning Principles, Paradigms, and Applications.
  Apple Academic Press
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">With the help of a new architecture called Edge/Fog (E/F) computing, cloud
computing services can now be extended nearer to data generator devices. E/F
computing in combination with Deep Learning (DL) is a promisedtechnique that is
vastly applied in numerous fields. To train their models, data producers in
conventional DL architectures with E/F computing enable them to repeatedly
transmit and communicate data with third-party servers, like Edge/Fog or cloud
servers. Due to the extensive bandwidth needs, legal issues, and privacy risks,
this architecture is frequently impractical. Through a centralized server, the
models can be co-trained by FL through distributed clients, including cars,
hospitals, and mobile phones, while preserving data localization. As it
facilitates group learning and model optimization, FL can therefore be seen as
a motivating element in the E/F computing paradigm. Although FL applications in
E/F computing environments have been considered in previous studies, FL
execution and hurdles in the E/F computing framework have not been thoroughly
covered. In order to identify advanced solutions, this chapter will provide a
review of the application of FL in E/F computing systems. We think that by
doing this chapter, researchers will learn more about how E/F computing and FL
enable related concepts and technologies. Some case studies about the
implementation of federated learning in E/F computing are being investigated.
The open issues and future research directions are introduced.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13033" title="Abstract">arXiv:2402.13033</a> [<a href="/pdf/2402.13033" title="Download PDF">pdf</a>, <a href="/format/2402.13033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Real-World Complex Network Representations with Hyperedge  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zehui Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingzhu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Stan%2C+G">Guy-Bart Stan</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review. 17 pages, 4 figures, 14 tables. arXiv admin note: text overlap with <a href="/abs/2306.05108">arXiv:2306.05108</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph augmentation methods play a crucial role in improving the performance
and enhancing generalisation capabilities in Graph Neural Networks (GNNs).
Existing graph augmentation methods mainly perturb the graph structures and are
usually limited to pairwise node relations. These methods cannot fully address
the complexities of real-world large-scale networks that often involve
higher-order node relations beyond only being pairwise. Meanwhile, real-world
graph datasets are predominantly modelled as simple graphs, due to the scarcity
of data that can be used to form higher-order edges. Therefore, reconfiguring
the higher-order edges as an integration into graph augmentation strategies
lights up a promising research path to address the aforementioned issues. In
this paper, we present Hyperedge Augmentation (HyperAug), a novel graph
augmentation method that constructs virtual hyperedges directly form the raw
data, and produces auxiliary node features by extracting from the virtual
hyperedge information, which are used for enhancing GNN performances on
downstream tasks. We design three diverse virtual hyperedge construction
strategies to accompany the augmentation scheme: (1) via graph statistics, (2)
from multiple data perspectives, and (3) utilising multi-modality. Furthermore,
to facilitate HyperAug evaluation, we provide 23 novel real-world graph
datasets across various domains including social media, biology, and
e-commerce. Our empirical study shows that HyperAug consistently and
significantly outperforms GNN baselines and other graph augmentation methods,
across a variety of application contexts, which clearly indicates that it can
effectively incorporate higher-order node relations into graph augmentation
methods for real-world complex networks.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13035" title="Abstract">arXiv:2402.13035</a> [<a href="/pdf/2402.13035" title="Download PDF">pdf</a>, <a href="/format/2402.13035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Check: Unleashing Potentials for Self-Correction in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Che Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhenyang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuejian Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have made significant strides in reasoning
capabilities, with ongoing efforts to refine their reasoning through
self-correction. However, recent studies suggest that self-correction can be
limited or even counterproductive without external accurate knowledge, raising
questions about the limits and effectiveness of self-correction. In this paper,
we aim to enhance LLM's self-checking capabilities by meticulously designing
training data, thereby improving the accuracy of self-correction. We conduct a
detailed analysis of error types in mathematical reasoning and develop a
tailored prompt, termed ``Step CoT Check''. Then we construct a
checking-correction dataset for training models. After integrating the original
CoT data and checking-correction data for training, we observe that models
could improve their self-checking capabilities, thereby enhancing their
self-correction capacity and eliminating the need for external feedback or
ground truth labels to ascertain the endpoint of correction. We compare the
performance of models fine-tuned with the ``Step CoT Check'' prompt against
those refined using other promps within the context of checking-correction
data. The ``Step CoT Check'' outperforms the other two check formats in model
with lager parameters, providing more precise feedback thus achieving a higher
rate of correctness. For reproducibility, all the datasets and codes are
provided in \url{https://github.com/bammt/Learn-to-check}.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13036" title="Abstract">arXiv:2402.13036</a> [<a href="/pdf/2402.13036" title="Download PDF">pdf</a>, <a href="/format/2402.13036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SiLLM: Large Language Models for Simultaneous Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shoutao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 tables, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Simultaneous Machine Translation (SiMT) generates translations while reading
the source sentence, necessitating a policy to determine the optimal timing for
reading and generating words. Despite the remarkable performance achieved by
Large Language Models (LLM) across various NLP tasks, existing SiMT methods
predominantly focus on conventional transformers, employing a single model to
concurrently determine the policy and generate the translations. However, given
the complexity of SiMT, it is challenging to effectively address both tasks
with a single model. Therefore, there is a need to decouple the SiMT task into
policy-decision and translation sub-tasks. We propose SiLLM, which delegates
the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The
policy-decision agent is managed by a conventional SiMT model, responsible for
determining the translation policy. The translation agent, leveraging the
capabilities of LLM, generates translation using the partial source sentence.
The two agents collaborate to accomplish SiMT. To facilitate the application of
token-level policies determined by conventional SiMT models to LLM, we propose
a word-level policy adapted for LLM. Experiments on two datasets demonstrate
that, with a small amount of data for fine-tuning LLM, SiLLM attains
state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13037" title="Abstract">arXiv:2402.13037</a> [<a href="/pdf/2402.13037" title="Download PDF">pdf</a>, <a href="/format/2402.13037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Align Your Intents: Offline Imitation Learning via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bobrin%2C+M">Maksim Bobrin</a>, 
<a href="/search/cs?searchtype=author&query=Buzun%2C+N">Nazar Buzun</a>, 
<a href="/search/cs?searchtype=author&query=Krylov%2C+D">Dmitrii Krylov</a>, 
<a href="/search/cs?searchtype=author&query=Dylov%2C+D+V">Dmitry V. Dylov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning (RL) addresses the problem of sequential
decision-making by learning optimal policy through pre-collected data, without
interacting with the environment. As yet, it has remained somewhat impractical,
because one rarely knows the reward explicitly and it is hard to distill it
retrospectively. Here, we show that an imitating agent can still learn the
desired behavior merely from observing the expert, despite the absence of
explicit rewards or action labels. In our method, AILOT (Aligned Imitation
Learning via Optimal Transport), we involve special representation of states in
a form of intents that incorporate pairwise spatial distances within the data.
Given such representations, we define intrinsic reward function via optimal
transport distance between the expert's and the agent's trajectories. We report
that AILOT outperforms state-of-the art offline imitation learning algorithms
on D4RL benchmarks and improves the performance of other offline RL algorithms
in the sparse-reward tasks.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13038" title="Abstract">arXiv:2402.13038</a> [<a href="/pdf/2402.13038" title="Download PDF">pdf</a>, <a href="/format/2402.13038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacquet%2C+M">Martin Jacquet</a>, 
<a href="/search/cs?searchtype=author&query=Alexis%2C+K">Kostas Alexis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework
exploiting a Deep Neural Network for processing onboard-captured depth images
for collision avoidance in trajectory-tracking tasks with UAVs. The network is
trained on simulated depth images to output a collision score for queried 3D
points within the sensor field of view. Then, this network is translated into
an algebraic symbolic equation and included in the N-MPC, explicitly
constraining predicted positions to be collision-free throughout the receding
horizon. The N-MPC achieves real time control of a UAV with a control frequency
of 100Hz. The proposed framework is validated through statistical analysis of
the collision classifier network, as well as Gazebo simulations and real
experiments to assess the resulting capabilities of the N-MPC to effectively
avoid collisions in cluttered environments. The associated code is released
open-source along with the training images.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13040" title="Abstract">arXiv:2402.13040</a> [<a href="/pdf/2402.13040" title="Download PDF">pdf</a>, <a href="/format/2402.13040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Guided Molecule Generation with Diffusion Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Haisong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 38th Association for the Advancement of Artificial Intelligence, AAAI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Text-guided molecule generation is a task where molecules are generated to
match specific textual descriptions. Recently, most existing SMILES-based
molecule generation methods rely on an autoregressive architecture. In this
work, we propose the Text-Guided Molecule Generation with Diffusion Language
Model (TGM-DLM), a novel approach that leverages diffusion models to address
the limitations of autoregressive methods. TGM-DLM updates token embeddings
within the SMILES string collectively and iteratively, using a two-phase
diffusion generation process. The first phase optimizes embeddings from random
noise, guided by the text description, while the second phase corrects invalid
SMILES strings to form valid molecular representations. We demonstrate that
TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for
additional data resources. Our findings underscore the remarkable effectiveness
of TGM-DLM in generating coherent and precise molecules with specific
properties, opening new avenues in drug discovery and related scientific
domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13043" title="Abstract">arXiv:2402.13043</a> [<a href="/pdf/2402.13043" title="Download PDF">pdf</a>, <a href="/format/2402.13043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Efficient Conversation Retrieval for Dialogue State  Tracking with Implicit Text Summaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Driesen%2C+J">Joris Driesen</a>, 
<a href="/search/cs?searchtype=author&query=Coca%2C+A">Alexandru Coca</a>, 
<a href="/search/cs?searchtype=author&query=Johannsen%2C+A">Anders Johannsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Few-shot dialogue state tracking (DST) with Large Language Models (LLM)
relies on an effective and efficient conversation retriever to find similar
in-context examples for prompt learning. Previous works use raw dialogue
context as search keys and queries, and a retriever is fine-tuned with
annotated dialogues to achieve superior performance. However, the approach is
less suited for scaling to new domains or new annotation languages, where
fine-tuning data is unavailable. To address this problem, we handle the task of
conversation retrieval based on text summaries of the conversations. A
LLM-based conversation summarizer is adopted for query and key generation,
which enables effective maximum inner product search. To avoid the extra
inference cost brought by LLM-based conversation summarization, we further
distill a light-weight conversation encoder which produces query embeddings
without decoding summaries for test conversations. We validate our retrieval
approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The
experimental results show a significant improvement over relevant baselines in
real few-shot DST settings.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13045" title="Abstract">arXiv:2402.13045</a> [<a href="/pdf/2402.13045" title="Download PDF">pdf</a>, <a href="/format/2402.13045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human  Motion Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wansong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Sibo Tian</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Boyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Minghui Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a deep learning enhanced adaptive unscented Kalman filter
(UKF) for predicting human arm motion in the context of manufacturing. Unlike
previous network-based methods that solely rely on captured human motion data,
which is represented as bone vectors in this paper, we incorporate a human arm
dynamic model into the motion prediction algorithm and use the UKF to
iteratively forecast human arm motions. Specifically, a
Lagrangian-mechanics-based physical model is employed to correlate arm motions
with associated muscle forces. Then a Recurrent Neural Network (RNN) is
integrated into the framework to predict future muscle forces, which are
transferred back to future arm motions based on the dynamic model. Given the
absence of measurement data for future human motions that can be input into the
UKF to update the state, we integrate another RNN to directly predict human
future motions and treat the prediction as surrogate measurement data fed into
the UKF. A noteworthy aspect of this study involves the quantification of
uncertainties associated with both the data-driven and physical models in one
unified framework. These quantified uncertainties are used to dynamically adapt
the measurement and process noises of the UKF over time. This adaption, driven
by the uncertainties of the RNN models, addresses inaccuracies stemming from
the data-driven model and mitigates discrepancies between the assumed and true
physical models, ultimately enhancing the accuracy and robustness of our
predictions. Compared to the traditional RNN-based prediction, our method
demonstrates improved accuracy and robustness in extensive experimental
validations of various types of human motions.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13048" title="Abstract">arXiv:2402.13048</a> [<a href="/pdf/2402.13048" title="Download PDF">pdf</a>, <a href="/format/2402.13048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Knowledge Editing in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zihao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hanxing Ding</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingcheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Efficient knowledge editing of large language models is crucial for replacing
obsolete information or incorporating specialized knowledge on a large scale.
However, previous methods implicitly assume that knowledge is localized and
isolated within the model, an assumption that oversimplifies the interconnected
nature of model knowledge. The premise of localization results in an incomplete
knowledge editing, whereas an isolated assumption may impair both other
knowledge and general abilities. It introduces instability to the performance
of the knowledge editing method. To transcend these assumptions, we introduce
StableKE, a method adopts a novel perspective based on knowledge augmentation
rather than knowledge localization. To overcome the expense of human labeling,
StableKE integrates two automated knowledge augmentation strategies: Semantic
Paraphrase Enhancement strategy, which diversifies knowledge descriptions to
facilitate the teaching of new information to the model, and Contextual
Description Enrichment strategy, expanding the surrounding knowledge to prevent
the forgetting of related information. StableKE surpasses other knowledge
editing methods, demonstrating stability both edited knowledge and multi-hop
knowledge, while also preserving unrelated knowledge and general abilities.
Moreover, StableKE can edit knowledge on ChatGPT.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13049" title="Abstract">arXiv:2402.13049</a> [<a href="/pdf/2402.13049" title="Download PDF">pdf</a>, <a href="/ps/2402.13049" title="Download PostScript">ps</a>, <a href="/format/2402.13049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Quantum Paradigms, but Still No Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+S">Samuel Epstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">An overwhelming majority of quantum (pure and mixed) states, when undertaking
a POVM measurement, will result in a classical probability with no algorithmic
information. Thus most quantum states produce white noise when measured.
Furthermore most non-pointer states, when undergoing the decoherence process,
will produce white noise. These results can be seen as consequences of the
vastness of Hilbert spaces.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13055" title="Abstract">arXiv:2402.13055</a> [<a href="/pdf/2402.13055" title="Download PDF">pdf</a>, <a href="/format/2402.13055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Semantic Induction Heads to Understand In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although large language models (LLMs) have demonstrated remarkable
performance, the lack of transparency in their inference logic raises concerns
about their trustworthiness. To gain a better understanding of LLMs, we conduct
a detailed analysis of the operations of attention heads and aim to better
understand the in-context learning of LLMs. Specifically, we investigate
whether attention heads encode two types of relationships between tokens
present in natural languages: the syntactic dependency parsed from sentences
and the relation within knowledge graphs. We find that certain attention heads
exhibit a pattern where, when attending to head tokens, they recall tail tokens
and increase the output logits of those tail tokens. More crucially, the
formulation of such semantic induction heads has a close correlation with the
emergence of the in-context learning ability of language models. The study of
semantic attention heads advances our understanding of the intricate operations
of attention heads in transformers, and further provides new insights into the
in-context learning of LLMs.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13056" title="Abstract">arXiv:2402.13056</a> [<a href="/pdf/2402.13056" title="Download PDF">pdf</a>, <a href="/format/2402.13056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Computing for IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+B+T">Balqees Talal Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Idrees%2C+A+K">Ali Kadhum Idrees</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures, Book Chapter
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Donta, P.K., Hazra, A., Lov\'en, L. (eds) Learning Techniques
  for the Internet of Things. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Over the past few years, The idea of edge computing has seen substantial
expansion in both academic and industrial circles. This computing approach has
garnered attention due to its integrating role in advancing various
state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial
intelligence, and aug- mented reality. In this chapter, we introduce computing
paradigms for IoT, offering an overview of the current cutting-edge computing
approaches that can be used with IoT. Furthermore, we go deeper into edge
computing paradigms, specifically focusing on cloudlet and mobile edge
computing. After that, we investigate the archi- tecture of edge
computing-based IoT, its advantages, and the technologies that make Edge
computing-based IoT possible, including artificial intelligence and lightweight
virtualization. Additionally, we review real-life case studies of how edge
computing is applied in IoT-based Intelligent Systems, including areas like
healthcare, manufac- turing, agriculture, and transportation. Finally, we
discuss current research obstacles and outline potential future directions for
further investigation in this domain.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13058" title="Abstract">arXiv:2402.13058</a> [<a href="/pdf/2402.13058" title="Download PDF">pdf</a>, <a href="/format/2402.13058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Graph Set and Evidence Pattern Reasoning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+T">Tianxiang Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yong Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Evidence theory is widely used in decision-making and reasoning systems. In
previous research, Transferable Belief Model (TBM) is a commonly used
evidential decision making model, but TBM is a non-preference model. In order
to better fit the decision making goals, the Evidence Pattern Reasoning Model
(EPRM) is proposed. By defining pattern operators and decision making
operators, corresponding preferences can be set for different tasks. Random
Permutation Set (RPS) expands order information for evidence theory. It is hard
for RPS to characterize the complex relationship between samples such as
cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were
proposed to model complex relationships and represent more event types. In
order to illustrate the significance of RGS and EPRM, an experiment of aircraft
velocity ranking was designed and 10,000 cases were simulated. The
implementation of EPRM called Conflict Resolution Decision optimized 18.17\% of
the cases compared to Mean Velocity Decision, effectively improving the
aircraft velocity ranking. EPRM provides a unified solution for evidence-based
decision making.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13061" title="Abstract">arXiv:2402.13061</a> [<a href="/pdf/2402.13061" title="Download PDF">pdf</a>, <a href="/format/2402.13061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Fairness via Maximum Mean Discrepancy Regularization on Logits  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hao-Wei Chung</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+C">Ching-Hao Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Jen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fairness has become increasingly pivotal in machine learning for high-risk
applications such as machine learning in healthcare and facial recognition.
However, we see the deficiency in the previous logits space constraint methods.
Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness
condition by imposing constraints on output logits with Maximum Mean
Discrepancy. Moreover, quantitative analysis and experimental results show that
our framework has a better property that outperforms previous methods and
achieves state-of-the-art on two facial recognition datasets and one animal
dataset. Finally, we show experimental results and demonstrate that our debias
approach achieves the fairness condition effectively.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13064" title="Abstract">arXiv:2402.13064</a> [<a href="/pdf/2402.13064" title="Download PDF">pdf</a>, <a href="/format/2402.13064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhengyang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaojun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zeqiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuxian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si-Qing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce Generalized Instruction Tuning (called GLAN), a general and
scalable method for instruction tuning of Large Language Models (LLMs). Unlike
prior work that relies on seed examples or existing datasets to construct
instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of
human knowledge and capabilities as input and generates large-scale synthetic
instruction data across all disciplines. Specifically, inspired by the
systematic structure in human education system, we build the taxonomy by
decomposing human knowledge and capabilities to various fields, sub-fields and
ultimately, distinct disciplines semi-automatically, facilitated by LLMs.
Subsequently, we generate a comprehensive list of subjects for every discipline
and proceed to design a syllabus tailored to each subject, again utilizing
LLMs. With the fine-grained key concepts detailed in every class session of the
syllabus, we are able to generate diverse instructions with a broad coverage
across the entire spectrum of human knowledge and skills. Extensive experiments
on large language models (e.g., Mistral) demonstrate that GLAN excels in
multiple dimensions from mathematical reasoning, coding, academic exams,
logical reasoning to general instruction following without using task-specific
training data of these tasks. In addition, GLAN allows for easy customization
and new fields or skills can be added by simply incorporating a new node into
our taxonomy.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13065" title="Abstract">arXiv:2402.13065</a> [<a href="/pdf/2402.13065" title="Download PDF">pdf</a>, <a href="/format/2402.13065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Pattern Matching in Computation Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondada%2C+L">Luca Mondada</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9s-Mart%C3%ADnez%2C+P">Pablo Andr&#xe9;s-Mart&#xed;nez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Graph rewriting is a popular tool for the optimisation and modification of
graph expressions in domains such as compilers, machine learning and quantum
computing. The underlying data structures are often port graphs - graphs with
labels at edge endpoints. These port labels greatly simplify pattern matching.
<br />A pre-requisite for graph rewriting is the ability to find subgraphs of the
input that match known graph identities: the pattern matching problem. We
propose a new solution to pattern matching in port graphs. Its novelty lies in
the use of a pre-computed data structure that makes the pattern matching
runtime complexity independent of the number of patterns. The runtime is bound
by the maximum width $w$ and depth $d$ of the patterns, as well as the input
graph size $|G|$ as $O(|G| \cdot c^w / w^{1/2} \cdot d)$ with $c = 6.75$. This
offers a significant advantage over existing solutions for use cases where
patterns have low width and the set of patterns is large and fixed ahead of
time.
<br />In the context of quantum circuits, pattern width can be limited to qubit
number. Quantum superoptimisers may use thousands of rewrite rules on circuits
with less than 5 qubits, making them an ideal use case. We provide benchmarks
showing that our algorithm offers a 20x speedup over current implementations on
a dataset of 10'000 real world patterns describing quantum circuits.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13068" title="Abstract">arXiv:2402.13068</a> [<a href="/pdf/2402.13068" title="Download PDF">pdf</a>, <a href="/format/2402.13068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Completion with BMD Factor Nuclear Norm Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tian%2C+F">Fan Tian</a>, 
<a href="/search/math?searchtype=author&query=Pasha%2C+M">Mirjeta Pasha</a>, 
<a href="/search/math?searchtype=author&query=Kilmer%2C+M+E">Misha E. Kilmer</a>, 
<a href="/search/math?searchtype=author&query=Miller%2C+E">Eric Miller</a>, 
<a href="/search/math?searchtype=author&query=Patra%2C+A">Abani Patra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper is concerned with the problem of recovering third-order tensor
data from limited samples. A recently proposed tensor decomposition (BMD)
method has been shown to efficiently compress third-order spatiotemporal data.
Using the BMD, we formulate a slicewise nuclear norm penalized algorithm to
recover a third-order tensor from limited observed samples. We develop an
efficient alternating direction method of multipliers (ADMM) scheme to solve
the resulting minimization problem. Experimental results on real data show our
method to give reconstruction comparable to those of HaLRTC (Liu et al., IEEE
Trans Ptrn Anal Mchn Int, 2012), a well-known tensor completion method, in
about the same number of iterations. However, our method has the advantage of
smaller subproblems and higher parallelizability per iteration.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13075" title="Abstract">arXiv:2402.13075</a> [<a href="/pdf/2402.13075" title="Download PDF">pdf</a>, <a href="/format/2402.13075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Synthesis of Controllers for Safety-Critical Autonomous Systems:  Developments and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiang Yin</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+B">Bingzhao Gao</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xiao Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In recent years, formal methods have been extensively used in the design of
autonomous systems. By employing mathematically rigorous techniques, formal
methods can provide fully automated reasoning processes with provable safety
guarantees for complex dynamic systems with intricate interactions between
continuous dynamics and discrete logics. This paper provides a comprehensive
review of formal controller synthesis techniques for safety-critical autonomous
systems. Specifically, we categorize the formal control synthesis problem based
on diverse system models, encompassing deterministic, non-deterministic, and
stochastic, and various formal safety-critical specifications involving logic,
real-time, and real-valued domains. The review covers fundamental formal
control synthesis techniques, including abstraction-based approaches and
abstraction-free methods. We explore the integration of data-driven synthesis
approaches in formal control synthesis. Furthermore, we review formal
techniques tailored for multi-agent systems (MAS), with a specific focus on
various approaches to address the scalability challenges in large-scale
systems. Finally, we discuss some recent trends and highlight research
challenges in this area.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13076" title="Abstract">arXiv:2402.13076</a> [<a href="/pdf/2402.13076" title="Download PDF">pdf</a>, <a href="/format/2402.13076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Weights Are Created Equal: Enhancing Energy Efficiency in  On-Device Streaming Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+Y">Yuan Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+L">Liangzhen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ernie Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Changsheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yangyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+V">Vikas Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Power consumption plays an important role in on-device streaming speech
recognition, as it has a direct impact on the user experience. This study
delves into how weight parameters in speech recognition models influence the
overall power consumption of these models. We discovered that the impact of
weight parameters on power consumption varies, influenced by factors including
how often they are invoked and their placement in memory. Armed with this
insight, we developed design guidelines aimed at optimizing on-device speech
recognition models. These guidelines focus on minimizing power use without
substantially affecting accuracy. Our method, which employs targeted
compression based on the varying sensitivities of weight parameters,
demonstrates superior performance compared to state-of-the-art compression
methods. It achieves a reduction in energy usage of up to 47% while maintaining
similar model accuracy and improving the real-time factor.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13077" title="Abstract">arXiv:2402.13077</a> [<a href="/pdf/2402.13077" title="Download PDF">pdf</a>, <a href="/format/2402.13077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanistic Neural Networks for Scientific Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pervez%2C+A">Adeel Pervez</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Gavves%2C+E">Efstratios Gavves</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This paper presents Mechanistic Neural Networks, a neural network design for
machine learning applications in the sciences. It incorporates a new
Mechanistic Block in standard architectures to explicitly learn governing
differential equations as representations, revealing the underlying dynamics of
data and enhancing interpretability and efficiency in data modeling. Central to
our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by
a technique that reduces solving linear ODEs to solving linear programs. This
integrates well with neural networks and surpasses the limitations of
traditional ODE solvers enabling scalable GPU parallel processing. Overall,
Mechanistic Neural Networks demonstrate their versatility for scientific
machine learning applications, adeptly managing tasks from equation discovery
to dynamic systems modeling. We prove their comprehensive capabilities in
analyzing and interpreting complex scientific data across various applications,
showing significant performance against specialized state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13081" title="Abstract">arXiv:2402.13081</a> [<a href="/pdf/2402.13081" title="Download PDF">pdf</a>, <a href="/format/2402.13081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IT Intrusion Detection Using Statistical Learning and Testbed  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Stadler%2C+R">Rolf Stadler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A shortened version of this paper will appear in the conference proceedings of NOMS 2024 (IEEE/IFIP Network Operations and Management Symposium)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We study automated intrusion detection in an IT infrastructure, specifically
the problem of identifying the start of an attack, the type of attack, and the
sequence of actions an attacker takes, based on continuous measurements from
the infrastructure. We apply statistical learning methods, including Hidden
Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier
(RFC) to map sequences of observations to sequences of predicted attack
actions. In contrast to most related research, we have abundant data to train
the models and evaluate their predictive power. The data comes from traces we
generate on an in-house testbed where we run attacks against an emulated IT
infrastructure. Central to our work is a machine-learning pipeline that maps
measurements from a high-dimensional observation space to a space of low
dimensionality or to a small set of observation symbols. Investigating
intrusions in offline as well as online scenarios, we find that both HMM and
LSTM can be effective in predicting attack start time, attack type, and attack
actions. If sufficient training data is available, LSTM achieves higher
prediction accuracy than HMM. HMM, on the other hand, requires less
computational resources and less training data for effective prediction. Also,
we find that the methods we study benefit from data produced by traditional
intrusion detection systems like SNORT.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13085" title="Abstract">arXiv:2402.13085</a> [<a href="/pdf/2402.13085" title="Download PDF">pdf</a>, <a href="/ps/2402.13085" title="Download PostScript">ps</a>, <a href="/format/2402.13085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kleene Theorems for Lasso Languages and $&#x3c9;$-Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cruchten%2C+M">Mike Cruchten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Automata operating on pairs of words were introduced as an alternative way of
capturing acceptance of regular $\omega$-languages. Families of DFAs and lasso
automata operating on such pairs followed, giving rise to minimisation
algorithms, a Myhill-Nerode theorem and language learning algorithms. Yet
Kleene theorems for such a well-established class are still missing. Here, we
introduce rational lasso languages and expressions, show a Kleene theorem for
lasso languages and explore the connection between rational lasso and
$\omega$-expressions, which yields a Kleene theorem for $\omega$-languages with
respect to saturated lasso automata. For one direction of the Kleene theorems,
we also provide a Brzozowski construction for lasso automata from rational
lasso expressions.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13086" title="Abstract">arXiv:2402.13086</a> [<a href="/pdf/2402.13086" title="Download PDF">pdf</a>, <a href="/format/2402.13086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Profinite trees, through monads and the lambda-calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreau%2C+V">Vincent Moreau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL); Category Theory (math.CT)

</div>
<p class="mathjax">In its simplest form, the theory of regular languages is the study of sets of
finite words recognized by finite monoids. The finiteness condition on monoids
gives rise to a topological space whose points, called profinite words, encode
the limiting behavior of words with respect to finite monoids. Yet, some
aspects of the theory of regular languages are not particular to monoids and
can be described in a general setting. On the one hand, Boja\'{n}czyk has shown
how to use monads to generalize the theory of regular languages and has given
an abstract definition of the free profinite structure, defined by codensity,
given a fixed monad and a notion of finite structure. On the other hand,
Salvati has introduced the notion of language of $\lambda$-terms, using
denotational semantics, which generalizes the case of words and trees through
the Church encoding. In recent work, the author and collaborators defined the
notion of profinite $\lambda$-term using semantics in finite sets and
functions, which extend the Church encoding to profinite words.
<br />In this article, we prove that these two generalizations, based on monads and
denotational semantics, coincide in the case of trees. To do so, we consider
the monad of abstract clones which, when applied to a ranked alphabet, gives
the associated clone of ranked trees. This induces a notion of free profinite
clone, and hence of profinite trees. The main contribution is a categorical
proof that the free profinite clone on a ranked alphabet is isomorphic, as a
Stone-enriched clone, to the clone of profinite $\lambda$-terms of Church type.
Moreover, we also prove a parametricity theorem on families of semantic
elements which provides another equivalent formulation of profinite trees in
terms of Reynolds parametricity.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13087" title="Abstract">arXiv:2402.13087</a> [<a href="/pdf/2402.13087" title="Download PDF">pdf</a>, <a href="/format/2402.13087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Does Selection Leak Privacy: Revisiting Private Selection and  Improved Results for Hyper-parameter Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zihang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenglong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We study the problem of guaranteeing Differential Privacy (DP) in
hyper-parameter tuning, a crucial process in machine learning involving the
selection of the best run from several. Unlike many private algorithms,
including the prevalent DP-SGD, the privacy implications of tuning remain
insufficiently understood. Recent works propose a generic private solution for
the tuning process, yet a fundamental question still persists: is the current
privacy bound for this solution tight?
<br />This paper contributes both positive and negative answers to this question.
Initially, we provide studies affirming the current privacy analysis is indeed
tight in a general sense. However, when we specifically study the
hyper-parameter tuning problem, such tightness no longer holds. This is first
demonstrated by applying privacy audit on the tuning process. Our findings
underscore a substantial gap between the current theoretical privacy bound and
the empirical bound derived even under the strongest audit setup.
<br />The gap found is not a fluke. Our subsequent study provides an improved
privacy result for private hyper-parameter tuning due to its distinct
properties. Our privacy results are also more generalizable compared to prior
analyses that are only easily applicable in specific setups.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13088" title="Abstract">arXiv:2402.13088</a> [<a href="/pdf/2402.13088" title="Download PDF">pdf</a>, <a href="/format/2402.13088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Slot-VLM: SlowFast Slots for Video-Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Cuiling Lan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wenxuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuejin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video-Language Models (VLMs), powered by the advancements in Large Language
Models (LLMs), are charting new frontiers in video understanding. A pivotal
challenge is the development of an efficient method to encapsulate video
content into a set of representative tokens to align with LLMs. In this work,
we introduce Slot-VLM, a novel framework designed to generate semantically
decomposed video tokens, in terms of object-wise and event-wise visual
representations, to facilitate LLM inference. Particularly, we design a
SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense
video tokens from the CLIP vision encoder to a set of representative slots. In
order to take into account both the spatial object details and the varied
temporal dynamics, SF-Slots is built with a dual-branch structure. The
Slow-Slots branch focuses on extracting object-centric slots from features at
high spatial resolution but low (slow) frame sample rate, emphasizing detailed
object information. Conversely, Fast-Slots branch is engineered to learn
event-centric slots from high temporal sample rate but low spatial resolution
features. These complementary slots are combined to form the vision context,
serving as the input to the LLM for efficient question answering. Our
experimental results demonstrate the effectiveness of our Slot-VLM, which
achieves the state-of-the-art performance on video question-answering.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13089" title="Abstract">arXiv:2402.13089</a> [<a href="/pdf/2402.13089" title="Download PDF">pdf</a>, <a href="/format/2402.13089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an empirical understanding of MoE design choices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Dongyang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Messmer%2C+B">Bettina Messmer</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In this study, we systematically evaluate the impact of common design choices
in Mixture of Experts (MoEs) on validation performance, uncovering distinct
influences at token and sequence levels. We also present empirical evidence
showing comparable performance between a learned router and a frozen, randomly
initialized router, suggesting that learned routing may not be essential. Our
study further reveals that Sequence-level routing can result in topic-specific
weak expert specialization, in contrast to syntax specialization observed with
Token-level routing.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13092" title="Abstract">arXiv:2402.13092</a> [<a href="/pdf/2402.13092" title="Download PDF">pdf</a>, <a href="/format/2402.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contractivity of neural ODEs: an eigenvalue optimization problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guglielmi%2C+N">Nicola Guglielmi</a>, 
<a href="/search/math?searchtype=author&query=De+Marinis%2C+A">Arturo De Marinis</a>, 
<a href="/search/math?searchtype=author&query=Savastianov%2C+A">Anton Savastianov</a>, 
<a href="/search/math?searchtype=author&query=Tudisco%2C+F">Francesco Tudisco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a novel methodology to solve a key eigenvalue optimization problem
which arises in the contractivity analysis of neural ODEs. When looking at
contractivity properties of a one layer weight-tied neural ODE
$\dot{u}(t)=\sigma(Au(t)+b)$ (with $u,b \in {\mathbb R}^n$, $A$ is a given $n
\times n$ matrix, $\sigma : {\mathbb R} \to {\mathbb R}^+$ denotes an
activation function and for a vector $z \in {\mathbb R}^n$, $\sigma(z) \in
{\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the
logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal
matrix such that ${\mathrm{diag}}(D) \in \sigma'({\mathbb R}^n)$. Specifically,
given a real number $c$ (usually $c=0$), the problem consists in finding the
largest positive interval $\chi\subseteq \mathbb [0,\infty)$ such that the
logarithmic norm $\mu(DA) \le c$ for all diagonal matrices $D$ with $D_{ii}\in
\chi$. We propose a two-level nested methodology: an inner level where, for a
given $\chi$, we compute an optimizer $D^\star(\chi)$ by a gradient system
approach, and an outer level where we tune $\chi$ so that the value $c$ is
reached by $\mu(D^\star(\chi)A)$. We extend the proposed two-level approach to
the general multilayer, and possibly time-dependent, case $\dot{u}(t) = \sigma(
A_k(t) \ldots \sigma ( A_{1}(t) u(t) + b_{1}(t) ) \ldots + b_{k}(t) )$ and we
propose several numerical examples to illustrate its behaviour, including its
stabilizing performance on a one-layer neural ODE applied to the classification
of the MNIST handwritten digits dataset.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13093" title="Abstract">arXiv:2402.13093</a> [<a href="/pdf/2402.13093" title="Download PDF">pdf</a>, <a href="/format/2402.13093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-level Knowledge Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Duo%2C+J">Jiangshan Duo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge editing aims at updating knowledge of large language models (LLMs)
to prevent them from becoming outdated. Existing work edits LLMs at the level
of factual knowledge triplets. However, natural knowledge updates in the real
world come from the occurrences of new events rather than direct changes in
factual triplets. In this paper, we propose a new task setting: event-level
knowledge editing, which directly edits new events into LLMs and improves over
conventional triplet-level editing on (1) Efficiency. A single event edit leads
to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond
updating factual knowledge, event-level editing also requires considering the
event influences and updating LLMs' knowledge about future trends. We construct
a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event
edits, 6,449 questions about factual knowledge, and 10,150 questions about
future tendencies. We systematically evaluate the performance of various
knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses
significant challenges to existing knowledge editing approaches. Our codes and
dataset are publicly released to facilitate further research.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13094" title="Abstract">arXiv:2402.13094</a> [<a href="/pdf/2402.13094" title="Download PDF">pdf</a>, <a href="/format/2402.13094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Comprehensibility Assessment of Simplified Texts among Persons  with Intellectual Disabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A4uberli%2C+A">Andreas S&#xe4;uberli</a>, 
<a href="/search/cs?searchtype=author&query=Holzknecht%2C+F">Franz Holzknecht</a>, 
<a href="/search/cs?searchtype=author&query=Haller%2C+P">Patrick Haller</a>, 
<a href="/search/cs?searchtype=author&query=Deilen%2C+S">Silvana Deilen</a>, 
<a href="/search/cs?searchtype=author&query=Schiffl%2C+L">Laura Schiffl</a>, 
<a href="/search/cs?searchtype=author&query=Hansen-Schirra%2C+S">Silvia Hansen-Schirra</a>, 
<a href="/search/cs?searchtype=author&query=Ebling%2C+S">Sarah Ebling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 2024 ACM Conference on Human Factors in Computing Systems (CHI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Text simplification refers to the process of increasing the comprehensibility
of texts. Automatic text simplification models are most commonly evaluated by
experts or crowdworkers instead of the primary target groups of simplified
texts, such as persons with intellectual disabilities. We conducted an
evaluation study of text comprehensibility including participants with and
without intellectual disabilities reading unsimplified, automatically and
manually simplified German texts on a tablet computer. We explored four
different approaches to measuring comprehensibility: multiple-choice
comprehension questions, perceived difficulty ratings, response time, and
reading speed. The results revealed significant variations in these
measurements, depending on the reader group and whether the text had undergone
automatic or manual simplification. For the target group of persons with
intellectual disabilities, comprehension questions emerged as the most reliable
measure, while analyzing reading speed provided valuable insights into
participants' reading behavior.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13096" title="Abstract">arXiv:2402.13096</a> [<a href="/pdf/2402.13096" title="Download PDF">pdf</a>, <a href="/format/2402.13096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching  in 6G HAPS Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%C3%A7%2C+G+B">G&#xf6;rkem Berkay Ko&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ilo%C4%9Flu%2C+B">Berk &#xc7;ilo&#x11f;lu</a>, 
<a href="/search/cs?searchtype=author&query=Ozturk%2C+M">Metin Ozturk</a>, 
<a href="/search/cs?searchtype=author&query=Yanikomeroglu%2C+H">Halim Yanikomeroglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This study investigates the integration of a high altitude platform station
(HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm
for energy saving. By doing so, the sustainability and ubiquitous connectivity
targets can be achieved. Besides, a delay-aware approach is also adopted, where
the delay profiles of users are respected in such a way that we attempt to meet
the latency requirements of users with a best-effort strategy. To this end, a
novel, simple, and lightweight Q-learning algorithm is designed to address the
cell-switching optimization problem. During the simulation campaigns, different
interference scenarios and delay situations between base stations are examined
in terms of energy consumption and quality-of-service (QoS), and the results
confirm the efficacy of the proposed Q-learning algorithm.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13098" title="Abstract">arXiv:2402.13098</a> [<a href="/pdf/2402.13098" title="Download PDF">pdf</a>, <a href="/format/2402.13098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ELAD: Explanation-Guided Large Language Models Active Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuntong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The deployment and application of Large Language Models (LLMs) is hindered by
their memory inefficiency, computational demands, and the high costs of API
inferences. Traditional distillation methods, which transfer the capabilities
of LLMs to smaller models, often fail to determine whether the knowledge has
been sufficiently transferred, potentially resulting in high costs or
incomplete distillation. In this paper, we propose an Explanation-Guided LLMs
Active Distillation (ELAD) framework that employs an active learning strategy
to optimize the balance between annotation costs and model performance. To
improve efficient sample selection, we introduce an explanation-guided sample
selection method that identifies samples challenging its reasoning by
exploiting uncertainties in explanation steps. Additionally, we present a
customized LLM-annotated explanation revision technique where the teacher model
detects and corrects flaws in the student model's reasoning. Our experiments
across various reasoning datasets demonstrate that our framework significantly
enhances the efficiency of LLM knowledge distillation.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13101" title="Abstract">arXiv:2402.13101</a> [<a href="/pdf/2402.13101" title="Download PDF">pdf</a>, <a href="/format/2402.13101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Microstructure-based Graph Neural Network for Accelerating Multiscale  Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Storm%2C+J">J. Storm</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+I+B+C+M">I. B. C. M. Rocha</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Meer%2C+F+P">F. P. van der Meer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Simulating the mechanical response of advanced materials can be done more
accurately using concurrent multiscale models than with single-scale
simulations. However, the computational costs stand in the way of the practical
application of this approach. The costs originate from microscale Finite
Element (FE) models that must be solved at every macroscopic integration point.
A plethora of surrogate modeling strategies attempt to alleviate this cost by
learning to predict macroscopic stresses from macroscopic strains, completely
replacing the microscale models. In this work, we introduce an alternative
surrogate modeling strategy that allows for keeping the multiscale nature of
the problem, allowing it to be used interchangeably with an FE solver for any
time step. Our surrogate provides all microscopic quantities, which are then
homogenized to obtain macroscopic quantities of interest. We achieve this for
an elasto-plastic material by predicting full-field microscopic strains using a
graph neural network (GNN) while retaining the microscopic constitutive
material model to obtain the stresses. This hybrid data-physics graph-based
approach avoids the high dimensionality originating from predicting full-field
responses while allowing non-locality to arise. By training the GNN on a
variety of meshes, it learns to generalize to unseen meshes, allowing a single
model to be used for a range of microstructures. The embedded microscopic
constitutive model in the GNN implicitly tracks history-dependent variables and
leads to improved accuracy. We demonstrate for several challenging scenarios
that the surrogate can predict complex macroscopic stress-strain paths. As the
computation time of our method scales favorably with the number of elements in
the microstructure compared to the FE method, our method can significantly
accelerate FE2 simulations.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13103" title="Abstract">arXiv:2402.13103</a> [<a href="/pdf/2402.13103" title="Download PDF">pdf</a>, <a href="/format/2402.13103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Functional Linear Discriminant Analysis for the  Classification of Short Time Series with Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bordoloi%2C+R">Rahul Bordoloi</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9da%2C+C">Cl&#xe9;mence R&#xe9;da</a>, 
<a href="/search/cs?searchtype=author&query=Trautmann%2C+O">Orell Trautmann</a>, 
<a href="/search/cs?searchtype=author&query=Bej%2C+S">Saptarshi Bej</a>, 
<a href="/search/cs?searchtype=author&query=Wolkenhauer%2C+O">Olaf Wolkenhauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Functional linear discriminant analysis (FLDA) is a powerful tool that
extends LDA-mediated multiclass classification and dimension reduction to
univariate time-series functions. However, in the age of large multivariate and
incomplete data, statistical dependencies between features must be estimated in
a computationally tractable way, while also dealing with missing data. There is
a need for a computationally tractable approach that considers the statistical
dependencies between features and can handle missing values. We here develop a
multivariate version of FLDA (MUDRA) to tackle this issue and describe an
efficient expectation/conditional-maximization (ECM) algorithm to infer its
parameters. We assess its predictive power on the "Articulary Word Recognition"
data set and show its improvement over the state-of-the-art, especially in the
case of missing data. MUDRA allows interpretable classification of data sets
with large proportions of missing data, which will be particularly useful for
medical or psychological data sets.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13104" title="Abstract">arXiv:2402.13104</a> [<a href="/pdf/2402.13104" title="Download PDF">pdf</a>, <a href="/format/2402.13104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Perception Versus Objective Driving Behavior: Subject Study of  Lateral Vehicle Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haselberger%2C+J">Johann Haselberger</a>, 
<a href="/search/eess?searchtype=author&query=Schick%2C+B">Bernhard Schick</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+S">Steffen M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Advancements in technology are steering attention toward creating comfortable
and acceptable driving characteristics in autonomous vehicles. Ensuring a safe
and comfortable ride experience is vital for the widespread adoption of
autonomous vehicles, as mismatches in driving styles between humans and
autonomous systems can impact passenger confidence. Current driving functions
have fixed parameters, and there is no universally agreed-upon driving style
for autonomous vehicles. Integrating driving style preferences into automated
vehicles may enhance acceptance and reduce uncertainty, expediting their
adoption. A controlled vehicle study (N = 62) was conducted with a variety of
German participants to identify the individual lateral driving behavior of
human drivers, specifically emphasizing rural roads. We introduce novel
indicators for assessing stationary and transient curve negotiation, directly
applicable in developing personalized lateral driving functions. To assess the
predictability of these indicators using self-reports, we introduce the
MDSI-DE, the German version of the Multidimensional Driving Style Inventory.
The correlation analysis between MDSI factor scores and proposed indicators
showed modest but significant associations, primarily with acceleration and
jerk statistics while the in-depth lateral driving behavior turned out to be
highly driver-heterogeneous. The dataset including the anonymized
socio-demographics and questionnaire responses, the raw vehicle measurements
including labels, and the derived driving behavior indicators are publicly
available at
https://www.kaggle.com/datasets/jhaselberger/spodb-subject-study-oflateral-vehicle-guidance.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13108" title="Abstract">arXiv:2402.13108</a> [<a href="/pdf/2402.13108" title="Download PDF">pdf</a>, <a href="/ps/2402.13108" title="Download PostScript">ps</a>, <a href="/format/2402.13108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Gradient Descent for Large Learning Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cr%C4%83ciun%2C+A">Alexandru Cr&#x103;ciun</a>, 
<a href="/search/cs?searchtype=author&query=Ghoshdastidar%2C+D">Debarghya Ghoshdastidar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">There currently is a significant interest in understanding the Edge of
Stability (EoS) phenomenon, which has been observed in neural networks
training, characterized by a non-monotonic decrease of the loss function over
epochs, while the sharpness of the loss (spectral norm of the Hessian)
progressively approaches and stabilizes around 2/(learning rate). Reasons for
the existence of EoS when training using gradient descent have recently been
proposed -- a lack of flat minima near the gradient descent trajectory together
with the presence of compact forward-invariant sets. In this paper, we show
that linear neural networks optimized under a quadratic loss function satisfy
the first assumption and also a necessary condition for the second assumption.
More precisely, we prove that the gradient descent map is non-singular, the set
of global minimizers of the loss function forms a smooth manifold, and the
stable minima form a bounded subset in parameter space. Additionally, we prove
that if the step-size is too big, then the set of initializations from which
gradient descent converges to a critical point has measure zero.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13109" title="Abstract">arXiv:2402.13109</a> [<a href="/pdf/2402.13109" title="Download PDF">pdf</a>, <a href="/format/2402.13109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the  Generalizability of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=LI%2C+Y">Yizhi LI</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiali Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoqun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zekun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruibin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yinghao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yiming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zuowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advancement of large language models (LLMs) has enhanced the ability to
generalize across a wide range of unseen natural language processing (NLP)
tasks through instruction-following. Yet, their effectiveness often diminishes
in low-resource languages like Chinese, exacerbated by biased evaluations from
data leakage, casting doubt on their true generalizability to new linguistic
territories. In response, we introduce the Chinese Instruction-Following
Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of
LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000
input-output pairs, developed by native speakers to test complex reasoning and
Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we
release only half of the dataset publicly, with the remainder kept private, and
introduce diversified instructions to minimize score variance, totaling 45,000
data instances. Our evaluation of 28 selected LLMs reveals a noticeable
performance gap, with the best model scoring only 52.9%, highlighting the
limitations of LLMs in less familiar language and task contexts. This work aims
to uncover the current limitations of LLMs in handling Chinese tasks, pushing
towards the development of more culturally informed and linguistically diverse
models with the released data and benchmark
(https://yizhilll.github.io/CIF-Bench/).
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13113" title="Abstract">arXiv:2402.13113</a> [<a href="/pdf/2402.13113" title="Download PDF">pdf</a>, <a href="/format/2402.13113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Only Time Will Tell: Interpreting How Transformers Process Local  Ambiguities Through the Lens of Restart-Incrementality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madureira%2C+B">Brielen Madureira</a>, 
<a href="/search/cs?searchtype=author&query=Kahardipraja%2C+P">Patrick Kahardipraja</a>, 
<a href="/search/cs?searchtype=author&query=Schlangen%2C+D">David Schlangen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Incremental models that process sentences one token at a time will sometimes
encounter points where more than one interpretation is possible. Causal models
are forced to output one interpretation and continue, whereas models that can
revise may edit their previous output as the ambiguity is resolved. In this
work, we look at how restart-incremental Transformers build and update internal
states, in an effort to shed light on what processes cause revisions not viable
in autoregressive models. We propose an interpretable way to analyse the
incremental states, showing that their sequential structure encodes information
on the garden path effect and its resolution. Our method brings insights on
various bidirectional encoders for contextualised meaning representation and
dependency parsing, contributing to show their advantage over causal models
when it comes to revisions.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13114" title="Abstract">arXiv:2402.13114</a> [<a href="/pdf/2402.13114" title="Download PDF">pdf</a>, <a href="/format/2402.13114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer  Nodes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zemin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Class imbalance in graph-structured data, where minor classes are
significantly underrepresented, poses a critical challenge for Graph Neural
Networks (GNNs). To address this challenge, existing studies generally generate
new minority nodes and edges connecting new nodes to the original graph to make
classes balanced. However, they do not solve the problem that majority classes
still propagate information to minority nodes by edges in the original graph
which introduces bias towards majority classes. To address this, we introduce
BuffGraph, which inserts buffer nodes into the graph, modulating the impact of
majority classes to improve minor class representation. Our extensive
experiments across diverse real-world datasets empirically demonstrate that
BuffGraph outperforms existing baseline methods in class-imbalanced node
classification in both natural settings and imbalanced settings. Code is
available at https://anonymous.4open.science/r/BuffGraph-730A.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13116" title="Abstract">arXiv:2402.13116</a> [<a href="/pdf/2402.13116" title="Download PDF">pdf</a>, <a href="/format/2402.13116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Knowledge Distillation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaohan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chongyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Can Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This survey presents an in-depth exploration of knowledge distillation (KD)
techniques within the realm of Large Language Models (LLMs), spotlighting the
pivotal role of KD in transferring sophisticated capabilities from proprietary
giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral.
Amidst the evolving AI landscape, this work elucidates the critical disparities
between proprietary and open-source LLMs, demonstrating how KD serves as an
essential conduit for imbuing the latter with the former's advanced
functionalities and nuanced understandings. Our survey is meticulously
structured around three foundational pillars: algorithm, skill, and
verticalization -- providing a comprehensive examination of KD mechanisms, the
enhancement of specific cognitive abilities, and their practical implications
across diverse fields. Crucially, the survey navigates the intricate interplay
between data augmentation (DA) and KD, illustrating how DA emerges as a
powerful paradigm within the KD framework to bolster LLMs' performance. By
leveraging DA to generate context-rich, skill-specific training data, KD
transcends traditional boundaries, enabling open-source models to approximate
the contextual adeptness, ethical alignment, and deep semantic insights
characteristic of their proprietary counterparts. This work aims to provide an
insightful guide for researchers and practitioners, offering a detailed
overview of current methodologies in knowledge distillation and proposing
future research directions. By bridging the gap between proprietary and
open-source LLMs, this survey underscores the potential for more accessible,
efficient, and sustainable AI solutions, fostering a more inclusive and
equitable landscape in AI advancements. An associated Github repository is
available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13117" title="Abstract">arXiv:2402.13117</a> [<a href="/pdf/2402.13117" title="Download PDF">pdf</a>, <a href="/format/2402.13117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster and Deterministic Subtrajectory Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Horst%2C+T">Thijs van der Horst</a>, 
<a href="/search/cs?searchtype=author&query=Ophelders%2C+T">Tim Ophelders</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Given a trajectory $T$ and a distance $\Delta$, we wish to find a set $C$ of
curves of complexity at most $\ell$, such that we can cover $T$ with subcurves
that each are within Fr\'echet distance $\Delta$ to at least one curve in $C$.
We call $C$ an $(\ell,\Delta)$-clustering and aim to find an
$(\ell,\Delta)$-clustering of minimum cardinality. This problem was introduced
by Akitaya $et$ $al.$ (2021) and shown to be NP-complete. The main focus has
therefore been on bicriterial approximation algorithms, allowing for the
clustering to be an $(\ell, \Theta(\Delta))$-clustering of roughly optimal
size. We present algorithms that construct $(\ell,4\Delta)$-clusterings of
$\mathcal{O}(k \log n)$ size, where $k$ is the size of the optimal $(\ell,
\Delta)$-clustering. For the discrete Fr\'echet distance, we use $\mathcal{O}(n
\ell \log n)$ space and $\mathcal{O}(k n^2 \log^3 n)$ deterministic worst case
time. For the continuous Fr\'echet distance, we use $\mathcal{O}(n^2 \log n)$
space and $\mathcal{O}(k n^3 \log^3 n)$ time. Our algorithms significantly
improve upon the clustering quality (improving the approximation factor in
$\Delta$) and size (whenever $\ell \in \Omega(\log n)$). We offer deterministic
running times comparable to known expected bounds. Additionally, in the
continuous setting, we give a near-linear improvement upon the space usage.
When compared only to deterministic results, we offer a near-linear speedup and
a near-quadratic improvement in the space usage. When we may restrict ourselves
to only considering clusters where all subtrajectories are vertex-to-vertex
subcurves, we obtain even better results under the continuous Fr\'echet
distance. Our algorithm becomes near quadratic and uses space that is near
linear in $n \ell$.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13120" title="Abstract">arXiv:2402.13120</a> [<a href="/pdf/2402.13120" title="Download PDF">pdf</a>, <a href="/format/2402.13120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile Weight Rendering: A Review for Researchers and Developers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Rodr%C3%ADguez%2C+R">Rub&#xe9;n Mart&#xed;n-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Ratschat%2C+A+L">Alexandre L. Ratschat</a>, 
<a href="/search/cs?searchtype=author&query=Marchal-Crespo%2C+L">Laura Marchal-Crespo</a>, 
<a href="/search/cs?searchtype=author&query=Vardar%2C+Y">Yasemin Vardar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 tables, 3 figures, survey
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Haptic rendering of weight plays an essential role in naturalistic object
interaction in virtual environments. While kinesthetic devices have
traditionally been used for this aim by applying forces on the limbs, tactile
interfaces acting on the skin have recently offered potential solutions to
enhance or substitute kinesthetic ones. Here, we aim to provide an in-depth
overview and comparison of existing tactile weight rendering approaches. We
categorized these approaches based on their type of stimulation into asymmetric
vibration and skin stretch, further divided according to the working mechanism
of the devices. Then, we compared these approaches using various criteria,
including physical, mechanical, and perceptual characteristics of the reported
devices and their potential applications. We found that asymmetric vibration
devices have the smallest form factor, while skin stretch devices relying on
the motion of flat surfaces, belts, or tactors present numerous mechanical and
perceptual advantages for scenarios requiring more accurate weight rendering.
Finally, we discussed the selection of the proposed categorization of devices
and their application scopes, together with the limitations and opportunities
for future research. We hope this study guides the development and use of
tactile interfaces to achieve a more naturalistic object interaction and
manipulation in virtual environments.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13122" title="Abstract">arXiv:2402.13122</a> [<a href="/pdf/2402.13122" title="Download PDF">pdf</a>, <a href="/format/2402.13122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable  Transfer from Black-Box to Lightweight Segmentation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cuttano%2C+C">Claudia Cuttano</a>, 
<a href="/search/cs?searchtype=author&query=Tavera%2C+A">Antonio Tavera</a>, 
<a href="/search/cs?searchtype=author&query=Cermelli%2C+F">Fabio Cermelli</a>, 
<a href="/search/cs?searchtype=author&query=Averta%2C+G">Giuseppe Averta</a>, 
<a href="/search/cs?searchtype=author&query=Caputo%2C+B">Barbara Caputo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, ICCV2023 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Many practical applications require training of semantic segmentation models
on unlabelled datasets and their execution on low-resource hardware.
Distillation from a trained source model may represent a solution for the first
but does not account for the different distribution of the training data.
Unsupervised domain adaptation (UDA) techniques claim to solve the domain
shift, but in most cases assume the availability of the source data or an
accessible white-box source model, which in practical applications are often
unavailable for commercial and/or safety reasons. In this paper, we investigate
a more challenging setting in which a lightweight model has to be trained on a
target unlabelled dataset for semantic segmentation, under the assumption that
we have access only to black-box source model predictions. Our method, named
CoRTe, consists of (i) a pseudo-labelling function that extracts reliable
knowledge from the black-box source model using its relative confidence, (ii) a
pseudo label refinement method to retain and enhance the novel information
learned by the student model on the target data, and (iii) a consistent
training of the model using the extracted pseudo labels. We benchmark CoRTe on
two synthetic-to-real settings, demonstrating remarkable results when using
black-box models to transfer knowledge on lightweight models for a target data
distribution.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13123" title="Abstract">arXiv:2402.13123</a> [<a href="/pdf/2402.13123" title="Download PDF">pdf</a>, <a href="/format/2402.13123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring AI-assisted Ideation and Prototyping for Choreography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yimeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+M">Misha Sra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Choreography creation is a multimodal endeavor, demanding cognitive abilities
to develop creative ideas and technical expertise to convert choreographic
ideas into physical dance movements. Previous endeavors have sought to reduce
the complexities in the choreography creation process in both dimensions. Among
them, non-AI-based systems have focused on reinforcing cognitive activities by
helping analyze and understand dance movements and augmenting physical
capabilities by enhancing body expressivity. On the other hand, AI-based
methods have helped the creation of novel choreographic materials with
generative AI algorithms. The choreography creation process is constrained by
time and requires a rich set of resources to stimulate novel ideas, but the
need for iterative prototyping and reduced physical dependence have not been
adequately addressed by prior research. Recognizing these challenges and the
research gap, we present an innovative AI-based choreography-support system.
Our goal is to facilitate rapid ideation by utilizing a generative AI model
that can produce diverse and novel dance sequences. The system is designed to
support iterative digital dance prototyping through an interactive web-based
user interface that enables the editing and modification of generated motion.
We evaluated our system by inviting six choreographers to analyze its
limitations and benefits and present the evaluation results along with
potential directions for future work.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13125" title="Abstract">arXiv:2402.13125</a> [<a href="/pdf/2402.13125" title="Download PDF">pdf</a>, <a href="/format/2402.13125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TreeEval: Benchmark-Free Evaluation of Large Language Models through  Tree Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, numerous new benchmarks have been established to evaluate the
performance of large language models (LLMs) via either computing a holistic
score or employing another LLM as a judge. However, these approaches suffer
from data leakage due to the open access of the benchmark and inflexible
evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a
benchmark-free evaluation method for LLMs that let a high-performance LLM host
an irreproducible evaluation session and essentially avoids the data leakage.
Moreover, this LLM performs as an examiner to raise up a series of questions
under a topic with a tree planing strategy, which considers the current
evaluation status to decide the next question generation and ensures the
completeness and efficiency of the evaluation process. We evaluate $6$ models
of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately
achieved the highest correlation coefficient with AlpacaEval2.0 using only
around $45$ questions. We also conduct more analysis to show the robustness and
reliability of TreeEval. Our code can be accessed via the provided
https://github.com/Ashura5/TreeEval.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13126" title="Abstract">arXiv:2402.13126</a> [<a href="/pdf/2402.13126" title="Download PDF">pdf</a>, <a href="/format/2402.13126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VGMShield: Mitigating Misuse of Video Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">With the rapid advancement in video generation, people can conveniently
utilize video generation models to create videos tailored to their specific
desires. Nevertheless, there are also growing concerns about their potential
misuse in creating and disseminating false information.
<br />In this work, we introduce VGMShield: a set of three straightforward but
pioneering mitigations through the lifecycle of fake video generation. We start
from \textit{fake video detection} trying to understand whether there is
uniqueness in generated videos and whether we can differentiate them from real
videos; then, we investigate the \textit{tracing} problem, which maps a fake
video back to a model that generates it. Towards these, we propose to leverage
pre-trained models that focus on {\it spatial-temporal dynamics} as the
backbone to identify inconsistencies in videos. Through experiments on seven
state-of-the-art open-source models, we demonstrate that current models still
cannot perfectly handle spatial-temporal relationships, and thus, we can
accomplish detection and tracing with nearly perfect accuracy.
<br />Furthermore, anticipating future generative model improvements, we propose a
{\it prevention} method that adds invisible perturbations to images to make the
generated videos look unreal. Together with fake video detection and tracing,
our multi-faceted set of solutions can effectively mitigate misuse of video
generative models.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13130" title="Abstract">arXiv:2402.13130</a> [<a href="/pdf/2402.13130" title="Download PDF">pdf</a>, <a href="/format/2402.13130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are ELECTRA&#x27;s Sentence Embeddings Beyond Repair? The Case of Semantic  Textual Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rep%2C+I">Ivan Rep</a>, 
<a href="/search/cs?searchtype=author&query=Duki%C4%87%2C+D">David Duki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0najder%2C+J">Jan &#x160;najder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While BERT produces high-quality sentence embeddings, its pre-training
computational cost is a significant drawback. In contrast, ELECTRA delivers a
cost-effective pre-training objective and downstream task performance
improvements, but not as performant sentence embeddings. The community tacitly
stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity
(STS). We notice a significant drop in performance when using the ELECTRA
discriminator's last layer in comparison to earlier layers. We explore this
drop and devise a way to repair ELECTRA's embeddings, proposing a novel
truncated model fine-tuning (TMFT) method. TMFT improves the Spearman
correlation coefficient by over 8 points while increasing parameter efficiency
on the STS benchmark dataset. We extend our analysis to various model sizes and
languages. Further, we discover the surprising efficacy of ELECTRA's generator
model, which performs on par with BERT, using significantly fewer parameters
and a substantially smaller embedding size. Finally, we observe further boosts
by combining TMFT with a word similarity task or domain adaptive pre-training.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13131" title="Abstract">arXiv:2402.13131</a> [<a href="/pdf/2402.13131" title="Download PDF">pdf</a>, <a href="/format/2402.13131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> exploreCOSMOS: Interactive Exploration of Conditional Statistical Shape  Models in the Web-Browser
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+M">Maximilian Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+B">Bernhard Egger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dies ist ein Vorabdruck des folgenden Beitrages, ver\"offentlicht in BVM 2024, herausgegeben von Maier, A. et al, 2024, Springer Nature, vervielf\"altigt mit Genehmigung von Springer Nature. Die finale authentifizierte Version ist online verf\"ugbar unter: <a href="https://doi.org/10.1007/978-3-658-44037-4_32">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Statistical Shape Models of faces and various body parts are heavily used in
medical image analysis, computer vision and visualization. Whilst the field is
well explored with many existing tools, all of them aim at experts, which
limits their applicability. We demonstrate the first tool that enables the
convenient exploration of statistical shape models in the browser, with the
capability to manipulate the faces in a targeted manner. This manipulation is
performed via a posterior model given partial observations. We release our code
and application on GitHub https://github.com/maximilian-hahn/exploreCOSMOS
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13132" title="Abstract">arXiv:2402.13132</a> [<a href="/pdf/2402.13132" title="Download PDF">pdf</a>, <a href="/format/2402.13132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electric Field Evaluation of Reconfigurable Intelligent Surface in  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cui%2C+Z">Zhuangzhuang Cui</a>, 
<a href="/search/eess?searchtype=author&query=Minucci%2C+F">Franco Minucci</a>, 
<a href="/search/eess?searchtype=author&query=Hersyandika%2C+R">Rizqi Hersyandika</a>, 
<a href="/search/eess?searchtype=author&query=Alonso%2C+R+M">Rodney Martinez Alonso</a>, 
<a href="/search/eess?searchtype=author&query=Guevara%2C+A+P">Andrea P. Guevara</a>, 
<a href="/search/eess?searchtype=author&query=Sallouha%2C+H">Hazem Sallouha</a>, 
<a href="/search/eess?searchtype=author&query=Pollin%2C+S">Sofie Pollin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures, 1 table, to be included in proc. IEEE DySPAN 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) used as infrastructure in wireless
networks has been a trend, thanks to its low cost and high flexibility. Working
in many ways including reflective mirrors and phase-shifted surfaces, RIS is
able to enhance the coverage in communications and provide more degrees of
freedom for sensing. However, the key issue lies in how to place RIS in
accordance with the regulations for electromagnetic field (EMF) exposure, which
requires refined evaluations. In this paper, we first investigate the
regulations in terms of E-field. Then, relevant deployment characteristics are
evaluated jointly: the minimum distance from the base station (BS) to the RIS,
and the minimum height of the RIS are given for a given BS power limit and as
function of the number of RIS elements. The ray-tracing simulations verify the
correctness of our analysis. Besides, different frequency ranges (FRs) and
radiation patterns of RIS elements are investigated. The results show that the
EMF exposure risk is negligible when RIS works in the reflective-only (RO)
mode. However, when it works in the beamforming (BO) mode, its placement should
be well specified based on our analytical framework to comply with the
regulations of E-field limit in general public scenarios. Finally, we provide
an E-field measurement methodology and low-cost solutions in terms of general
wireless networks and 5G standalone networks, which pave the way for real-world
evaluation in future work.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13135" title="Abstract">arXiv:2402.13135</a> [<a href="/pdf/2402.13135" title="Download PDF">pdf</a>, <a href="/ps/2402.13135" title="Download PostScript">ps</a>, <a href="/format/2402.13135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Literature Review on Task Allocation and Performance  Management Techniques in Cloud Data Center
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+N">Nidhika Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+N">Navneet Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Saini%2C+K+S">Kamaljit Singh Saini</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Sahil Verma</a>, 
<a href="/search/cs?searchtype=author&query=Alabdulatif%2C+A">Abdulatif Alabdulatif</a>, 
<a href="/search/cs?searchtype=author&query=Khurma%2C+R+A">Ruba Abu Khurma</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Arenas%2C+M">Maribel Garcia-Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">As cloud computing usage grows, cloud data centers play an increasingly
important role. To maximize resource utilization, ensure service quality, and
enhance system performance, it is crucial to allocate tasks and manage
performance effectively. The purpose of this study is to provide an extensive
analysis of task allocation and performance management techniques employed in
cloud data centers. The aim is to systematically categorize and organize
previous research by identifying the cloud computing methodologies, categories,
and gaps. A literature review was conducted, which included the analysis of 463
task allocations and 480 performance management papers. The review revealed
three task allocation research topics and seven performance management methods.
Task allocation research areas are resource allocation, load-Balancing, and
scheduling. Performance management includes monitoring and control, power and
energy management, resource utilization optimization, quality of service
management, fault management, virtual machine management, and network
management. The study proposes new techniques to enhance cloud computing work
allocation and performance management. Short-comings in each approach can guide
future research. The research's findings on cloud data center task allocation
and performance management can assist academics, practitioners, and cloud
service providers in optimizing their systems for dependability,
cost-effectiveness, and scalability. Innovative methodologies can steer future
research to fill gaps in the literature.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13137" title="Abstract">arXiv:2402.13137</a> [<a href="/pdf/2402.13137" title="Download PDF">pdf</a>, <a href="/format/2402.13137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hidden Space of Transformer Language Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alabi%2C+J+O">Jesujoba O. Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Mosbach%2C+M">Marius Mosbach</a>, 
<a href="/search/cs?searchtype=author&query=Eyal%2C+M">Matan Eyal</a>, 
<a href="/search/cs?searchtype=author&query=Klakow%2C+D">Dietrich Klakow</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We analyze the operation of transformer language adapters, which are small
modules trained on top of a frozen language model to adapt its predictions to
new target languages. We show that adapted predictions mostly evolve in the
source language the model was trained on, while the target language becomes
pronounced only in the very last layers of the model. Moreover, the adaptation
process is gradual and distributed across layers, where it is possible to skip
small groups of adapters without decreasing adaptation performance. Last, we
show that adapters operate on top of the model's frozen representation space
while largely preserving its structure, rather than on an 'isolated' subspace.
Our findings provide a deeper view into the adaptation process of language
models to new languages, showcasing the constraints imposed on it by the
underlying model and introduces practical implications to enhance its
efficiency.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13139" title="Abstract">arXiv:2402.13139</a> [<a href="/pdf/2402.13139" title="Download PDF">pdf</a>, <a href="/format/2402.13139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Dynamic Edge-Colouring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christiansen%2C+A+B+G">Aleksander B. G. Christiansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given a dynamic graph $G$ with $n$ vertices and $m$ edges subject to
insertion an deletions of edges, we show how to maintain a
$(1+\varepsilon)\Delta$-edge-colouring of $G$ without the use of randomisation.
<br />More specifically, we show a deterministic dynamic algorithm with an
amortised update time of $2^{\tilde{O}_{\log \varepsilon^{-1}}(\sqrt{\log n})}$
using $(1+\varepsilon)\Delta$ colours. If $\varepsilon^{-1} \in
2^{O(\log^{0.49} n)}$, then our update time is sub-polynomial in $n$.
<br />While there exists randomised algorithms maintaining colourings with the same
number of colours [Christiansen STOC'23, Duan, He, Zhang SODA'19, Bhattacarya,
Costa, Panski, Solomon SODA'24] in polylogarithmic and even constant update
time, this is the first deterministic algorithm to go below the greedy
threshold of $2\Delta-1$ colours for all input graphs.
<br />On the way to our main result, we show how to dynamically maintain a shallow
hierarchy of degree-splitters with both recourse and update time in $n^{o(1)}$.
We believe that this algorithm might be of independent interest.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13143" title="Abstract">arXiv:2402.13143</a> [<a href="/pdf/2402.13143" title="Download PDF">pdf</a>, <a href="/ps/2402.13143" title="Download PostScript">ps</a>, <a href="/format/2402.13143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Mapping Protocol -- UX Design role in software development  process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orme%C3%B1o%2C+E">Emilio Orme&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Pinciroli%2C+F">Fernando Pinciroli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2209.12004">arXiv:2209.12004</a>, <a href="/abs/1702.02653">arXiv:1702.02653</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">A systematic mapping protocol is a method for conducting a literature review
in a rigorous and transparent way. It aims to provide an overview of the
current state of research on a specific topic, identify gaps and opportunities,
and guide future work. In this document, we present a systematic mapping
protocol for investigating the role of the UX designer in the software
development process. We define the research questions, scope, sources, search
strategy, selection criteria, data extraction, and analysis methods that we
will use to conduct the mapping study. Our goal is to understand how the UX
designers collaborate with other stakeholders, what methods and tools they use,
what challenges they face, and what outcomes they achieve in different contexts
and domains.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13144" title="Abstract">arXiv:2402.13144</a> [<a href="/pdf/2402.13144" title="Download PDF">pdf</a>, <a href="/format/2402.13144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaopan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yukun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+Z">Zelin Zang</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We introduce a novel approach for parameter generation, named neural network diffusion (\textbf{p-diff}, p stands for parameter), which employs a standard latent diffusion model to synthesize a new set of parameters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models have achieved remarkable success in image and video
generation. In this work, we demonstrate that diffusion models can also
\textit{generate high-performing neural network parameters}. Our approach is
simple, utilizing an autoencoder and a standard latent diffusion model. The
autoencoder extracts latent representations of a subset of the trained network
parameters. A diffusion model is then trained to synthesize these latent
parameter representations from random noise. It then generates new
representations that are passed through the autoencoder's decoder, whose
outputs are ready to use as new subsets of network parameters. Across various
architectures and datasets, our diffusion process consistently generates models
of comparable or improved performance over trained networks, with minimal
additional cost. Notably, we empirically find that the generated models perform
differently with the trained networks. Our results encourage more exploration
on the versatile use of diffusion models.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13145" title="Abstract">arXiv:2402.13145</a> [<a href="/pdf/2402.13145" title="Download PDF">pdf</a>, <a href="/format/2402.13145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for  Boosting Metaphor Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yujie Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xinrong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Metaphor is a prominent linguistic device in human language and literature,
as they add color, imagery, and emphasis to enhance effective communication.
This paper introduces a large-scale high quality annotated Chinese Metaphor
Corpus, which comprises around 28K sentences drawn from a diverse range of
Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the
accuracy and consistency of our annotations, we introduce a comprehensive set
of guidelines. These guidelines address the facets of metaphor annotation,
including identifying tenors, vehicles, and grounds to handling the
complexities of similes, personifications, juxtapositions, and hyperboles.
Breaking tradition, our approach to metaphor generation emphasizes grounds and
their distinct features rather than the conventional combination of tenors and
vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are
able to generate metaphors that resonate more with real-world intuition. We
test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using
our annotated corpus. These models are able to generate creative and fluent
metaphor sentences more frequently induced by selected samples from our
dataset, demonstrating the value of our corpus for Chinese metaphor research.
The code is available in the
https://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13146" title="Abstract">arXiv:2402.13146</a> [<a href="/pdf/2402.13146" title="Download PDF">pdf</a>, <a href="/format/2402.13146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for  Video-Grounded Dialog
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdessaied%2C+A">Adnen Abdessaied</a>, 
<a href="/search/cs?searchtype=author&query=von+Hochmeister%2C+M">Manuel von Hochmeister</a>, 
<a href="/search/cs?searchtype=author&query=Bulling%2C+A">Andreas Bulling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present the Object Language Video Transformer (OLViT) - a novel model for
video dialog operating over a multi-modal attention-based dialog state tracker.
Existing video dialog models struggle with questions requiring both spatial and
temporal localization within videos, long-term temporal reasoning, and accurate
object tracking across multiple dialog turns. OLViT addresses these challenges
by maintaining a global dialog state based on the output of an Object State
Tracker (OST) and a Language State Tracker (LST): while the OST attends to the
most important objects within the video, the LST keeps track of the most
important linguistic co-references to previous dialog turns. In stark contrast
to previous works, our approach is generic by nature and is therefore capable
of learning continuous multi-modal dialog state representations of the most
relevant objects and rounds. As a result, they can be seamlessly integrated
into Large Language Models (LLMs) and offer high flexibility in dealing with
different datasets and tasks. Evaluations on the challenging DVD (response
classification) and SIMMC 2.1 (response generation) datasets show that OLViT
achieves new state-of-the-art performance across both datasets.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13147" title="Abstract">arXiv:2402.13147</a> [<a href="/pdf/2402.13147" title="Download PDF">pdf</a>, <a href="/format/2402.13147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+H">Huy Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+T">Tien Mai</a>, 
<a href="/search/cs?searchtype=author&query=Varakantham%2C+P">Pradeep Varakantham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider offline imitation learning (IL), which aims to mimic the expert's
behavior from its demonstration without further interaction with the
environment. One of the main challenges in offline IL is dealing with the
limited support of expert demonstrations that cover only a small fraction of
the state-action spaces. In this work, we consider offline IL, where expert
demonstrations are limited but complemented by a larger set of sub-optimal
demonstrations of lower expertise levels. Most of the existing offline IL
methods developed for this setting are based on behavior cloning or
distribution matching, where the aim is to match the occupancy distribution of
the imitation policy with that of the expert policy. Such an approach often
suffers from over-fitting, as expert demonstrations are limited to accurately
represent any occupancy distribution. On the other hand, since sub-optimal sets
are much larger, there is a high chance that the imitation policy is trained
towards sub-optimal policies. In this paper, to address these issues, we
propose a new approach based on inverse soft-Q learning, where a regularization
term is added to the training objective, with the aim of aligning the learned
rewards with a pre-assigned reward function that allocates higher weights to
state-action pairs from expert demonstrations, and lower weights to those from
lower expertise levels. On standard benchmarks, our inverse soft-Q learning
significantly outperforms other offline IL baselines by a large margin.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13148" title="Abstract">arXiv:2402.13148</a> [<a href="/pdf/2402.13148" title="Download PDF">pdf</a>, <a href="/format/2402.13148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Jailbreak Prompts via In-Context Adversarial Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yufei Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Haomin Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Taicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kehan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhenwen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hongyan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate remarkable capabilities across
diverse applications. However, concerns regarding their security, particularly
the vulnerability to jailbreak attacks, persist. Drawing inspiration from
adversarial training in deep learning and LLM agent learning processes, we
introduce the In-Context Adversarial Game (ICAG) for defending against
jailbreaks without the need for fine-tuning. ICAG leverages agent learning to
conduct an adversarial game, aiming to dynamically extend knowledge to defend
against jailbreaks. Unlike traditional methods that rely on static datasets,
ICAG employs an iterative process to enhance both the defense and attack
agents. This continuous improvement process strengthens defenses against newly
generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy,
where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success
rates across various attack scenarios. Moreover, ICAG demonstrates remarkable
transferability to other LLMs, indicating its potential as a versatile defense
mechanism.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13149" title="Abstract">arXiv:2402.13149</a> [<a href="/pdf/2402.13149" title="Download PDF">pdf</a>, <a href="/ps/2402.13149" title="Download PostScript">ps</a>, <a href="/format/2402.13149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choosing a Suitable Requirement Prioritization Method: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhenawi%2C+E">Esraa Alhenawi</a>, 
<a href="/search/cs?searchtype=author&query=Awawdeh%2C+S">Shatha Awawdeh</a>, 
<a href="/search/cs?searchtype=author&query=Khurma%2C+R+A">Ruba Abu Khurma</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Arenas%2C+M">Maribel Garc&#xed;a-Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Hudaib%2C+A">Amjad Hudaib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software requirements prioritization plays a crucial role in software
development. It can be viewed as the process of ordering requirements by
determining which requirements must be done first and which can be done later.
Powerful requirements prioritization techniques are of paramount importance to
finish the implementation on time and within budget. Many factors affect
requirement prioritization such as stakeholder expectations, complexity,
dependency, scalability, risk, and cost. Therefore, finding the proper order of
requirements is a challenging process. Hence, different types of requirements
prioritization techniques have been developed to support this task. In this
survey, we propose a novel classification that can classify the prioritization
techniques under two major classes: relative and exact prioritization
techniques class, where each class is divided into two subclasses. We depend in
our classification on the way the value of ranking is given to the requirement,
either explicitly as a specific value in the case of the exact prioritization
techniques class, or implicitly in the case of the Relative prioritization
technique class. An overview of fifteen different requirements prioritization
techniques are presented and organized according to the proposed classification
criteria's. Moreover, we make a comparison between methods that are related to
the same subclass to analyze their strengths and weaknesses. Based on the
comparison results, the properties for each proposed subclass of techniques are
identified. Depending on these properties, we present some recommendations to
help project managers in the process of selecting the most suitable technique
to prioritize requirements based on their project characteristics (number of
requirements, time, cost, and accuracy).
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13151" title="Abstract">arXiv:2402.13151</a> [<a href="/pdf/2402.13151" title="Download PDF">pdf</a>, <a href="/format/2402.13151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost-Tight Bounds on Preserving Cuts in Classes of Submodular  Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Sanjeev Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Putterman%2C+A+L">Aaron L. Putterman</a>, 
<a href="/search/cs?searchtype=author&query=Sudan%2C+M">Madhu Sudan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Recently, a number of variants of the notion of cut-preserving hypergraph
sparsification have been studied in the literature. These variants include
directed hypergraph sparsification, submodular hypergraph sparsification,
general notions of approximation including spectral approximations, and more
general notions like sketching that can answer cut queries using more general
data structures than just sparsifiers. In this work, we provide reductions
between these different variants of hypergraph sparsification and establish new
upper and lower bounds on the space complexity of preserving their cuts. At a
high level, our results use the same general principle, namely, by showing that
cuts in one class of hypergraphs can be simulated by cuts in a simpler class of
hypergraphs, we can leverage sparsification results for the simpler class of
hypergraphs.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13152" title="Abstract">arXiv:2402.13152</a> [<a href="/pdf/2402.13152" title="Download PDF">pdf</a>, <a href="/format/2402.13152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech  Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acosta-Triana%2C+J">Jos&#xe9;-M. Acosta-Triana</a>, 
<a href="/search/cs?searchtype=author&query=Gimeno-G%C3%B3mez%2C+D">David Gimeno-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Hinarejos%2C+C">Carlos-D. Mart&#xed;nez-Hinarejos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">More than 7,000 known languages are spoken around the world. However, due to
the lack of annotated resources, only a small fraction of them are currently
covered by speech technologies. Albeit self-supervised speech representations,
recent massive speech corpora collections, as well as the organization of
challenges, have alleviated this inequality, most studies are mainly
benchmarked on English. This situation is aggravated when tasks involving both
acoustic and visual speech modalities are addressed. In order to promote
research on low-resource languages for audio-visual speech technologies, we
present AnnoTheia, a semi-automatic annotation toolkit that detects when a
person speaks on the scene and the corresponding transcription. In addition, to
show the complete process of preparing AnnoTheia for a language of interest, we
also describe the adaptation of a pre-trained model for active speaker
detection to Spanish, using a database not initially conceived for this type of
task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on
GitHub.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13153" title="Abstract">arXiv:2402.13153</a> [<a href="/pdf/2402.13153" title="Download PDF">pdf</a>, <a href="/format/2402.13153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustered Planarity Variants for Level Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fink%2C+S+D">Simon D. Fink</a>, 
<a href="/search/cs?searchtype=author&query=Pfretzschner%2C+M">Matthias Pfretzschner</a>, 
<a href="/search/cs?searchtype=author&query=Rutter%2C+I">Ignaz Rutter</a>, 
<a href="/search/cs?searchtype=author&query=Sieper%2C+M+D">Marie Diana Sieper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We consider variants of the clustered planarity problem for level-planar
drawings. So far, only convex clusters have been studied in this setting. We
introduce two new variants that both insist on a level-planar drawing of the
input graph but relax the requirements on the shape of the clusters. In
unrestricted Clustered Level Planarity (uCLP) we only require that they are
bounded by simple closed curves that enclose exactly the vertices of the
cluster and cross each edge of the graph at most once. The problem y-monotone
Clustered Level Planarity (y-CLP) requires that additionally it must be
possible to augment each cluster with edges that do not cross the cluster
boundaries so that it becomes connected while the graph remains level-planar,
thereby mimicking a classic characterization of clustered planarity in the
level-planar setting.
<br />We give a polynomial-time algorithm for uCLP if the input graph is
biconnected and has a single source. By contrast, we show that y-CLP is hard
under the same restrictions and it remains NP-hard even if the number of levels
is bounded by a constant and there is only a single non-trivial cluster.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13156" title="Abstract">arXiv:2402.13156</a> [<a href="/pdf/2402.13156" title="Download PDF">pdf</a>, <a href="/ps/2402.13156" title="Download PostScript">ps</a>, <a href="/format/2402.13156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret-Minimizing Contracts: Agency Under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+M">Martino Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Marchesi%2C+A">Alberto Marchesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the fundamental problem of designing contracts in principal-agent
problems under uncertainty. Previous works mostly addressed Bayesian settings
in which principal's uncertainty is modeled as a probability distribution over
agent's types. In this paper, we study a setting in which the principal has no
distributional information about agent's type. In particular, in our setting,
the principal only knows some uncertainty set defining possible agent's action
costs. Thus, the principal takes a robust (adversarial) approach by trying to
design contracts which minimize the (additive) regret: the maximum difference
between what the principal could have obtained had them known agent's costs and
what they actually get under the selected contract.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13159" title="Abstract">arXiv:2402.13159</a> [<a href="/pdf/2402.13159" title="Download PDF">pdf</a>, <a href="/format/2402.13159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barking dogs: A Fr&#xe9;chet distance variant for detour detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Klute%2C+F">Fabian Klute</a>, 
<a href="/search/cs?searchtype=author&query=Parada%2C+I">Irene Parada</a>, 
<a href="/search/cs?searchtype=author&query=Schnider%2C+P">Patrick Schnider</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Imagine you are a dog behind a fence $Q$ and a hiker is passing by at
constant speed along the hiking path $P$. In order to fulfil your duties as a
watchdog, you desire to bark as long as possible at the human. However, your
barks can only be heard in a fixed radius $\rho$ and, as a dog, you have
bounded speed $s$. Can you optimize your route along the fence $Q$ in order to
maximize the barking time with radius $\rho$, assuming you can run backwards
and forward at speed at most $s$?
<br />We define the barking distance from a polyline $P$ on $n$ vertices to a
polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking
radius if you run optimally along $Q$. This asymmetric similarity measure
between two curves can be used to detect outliers in $Q$ compared to $P$ that
other established measures like the Fr\'echet distance and Dynamic Time Warping
fail to capture at times. We consider this measure in three different settings.
In the discrete setting, the traversals of $P$ and $Q$ are both discrete. For
this case we show that the barking distance from $P$ to $Q$ can be computed in
$O(nm\log s)$ time. In the semi-discrete setting, the traversal of $Q$ is
continuous while the one of $P$ is again discrete. Here, we show how to compute
the barking distance in time $O(nm\log (nm))$. Finally, in the continuous
setting in which both traversals are continuous, we show that the problem can
be solved in polynomial time. For all the settings we show that, assuming SETH,
no truly subquadratic algorithm can exist.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13169" title="Abstract">arXiv:2402.13169</a> [<a href="/pdf/2402.13169" title="Download PDF">pdf</a>, <a href="/format/2402.13169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Verification for Blockchain-based Insurance Claims Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neupane%2C+R+L">Roshan Lal Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Bonnah%2C+E">Ernest Bonnah</a>, 
<a href="/search/cs?searchtype=author&query=Bhusal%2C+B">Bishnu Bhusal</a>, 
<a href="/search/cs?searchtype=author&query=Neupane%2C+K">Kiran Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+K+A">Khaza Anuarul Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Calyam%2C+P">Prasad Calyam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Insurance claims processing involves multi-domain entities and multi-source
data, along with a number of human-agent interactions. Use of Blockchain
technology-based platform can significantly improve scalability and response
time for processing of claims which are otherwise manually-intensive and
time-consuming. However, the chaincodes involved within the processes that
issue claims, approve or deny them as required, need to be formally verified to
ensure secure and reliable processing of transactions in Blockchain. In this
paper, we use a formal modeling approach to verify various processes and their
underlying chaincodes relating to different stages in insurance claims
processing viz., issuance, approval, denial, and flagging for fraud
investigation by using linear temporal logic (LTL). We simulate the formalism
on the chaincodes and analyze the breach of chaincodes via model checking.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13170" title="Abstract">arXiv:2402.13170</a> [<a href="/pdf/2402.13170" title="Download PDF">pdf</a>, <a href="/ps/2402.13170" title="Download PostScript">ps</a>, <a href="/format/2402.13170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Space Bounds for Subset Sum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belova%2C+T">Tatiana Belova</a>, 
<a href="/search/cs?searchtype=author&query=Chukhin%2C+N">Nikolai Chukhin</a>, 
<a href="/search/cs?searchtype=author&query=Kulikov%2C+A+S">Alexander S. Kulikov</a>, 
<a href="/search/cs?searchtype=author&query=Mihajlin%2C+I">Ivan Mihajlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">More than 40 years ago, Schroeppel and Shamir presented an algorithm that
solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and
space $O^*(2^{0.25n})$. The time upper bound remains unbeaten, but the space
upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough
paper by Nederlof and W\k{e}grzycki (STOC 2021). Their algorithm is a clever
combination of a number of previously known techniques with a new reduction and
a new algorithm for the Orthogonal Vectors problem.
<br />In this paper, we give two new algorithms for Subset Sum. We start by
presenting an Arthur--Merlin algorithm: upon receiving the verifier's
randomness, the prover sends an $n/4$-bit long proof to the verifier who checks
it in (deterministic) time and space $O^*(2^{n/4})$. The simplicity of this
algorithm has a number of interesting consequences: it can be parallelized
easily; also, by enumerating all possible proofs, one recovers upper bounds on
time and space for Subset Sum proved by Schroeppel and Shamir in 1979. As it is
the case with the previously known algorithms for Subset Sum, our algorithm
follows from an algorithm for $4$-SUM: we prove that, using verifier's coin
tosses, the prover can prepare a $\log_2 n$-bit long proof verifiable in time
$\tilde{O}(n)$. Another interesting consequence of this result is the following
fine-grained lower bound: assuming that $4$-SUM cannot be solved in time
$O(n^{2-\varepsilon})$ for all $\varepsilon&gt;0$, Circuit SAT cannot be solved in
time $O(g2^{(1-\varepsilon)n})$, for all $\varepsilon&gt;0$.
<br />Then, we improve the space bound by Nederlof and W\k{e}grzycki to
$O^*(2^{0.246n})$ and also simplify their algorithm and its analysis. We
achieve this space bound by further filtering sets of subsets using a random
prime number. This allows us to reduce an instance of Subset Sum to a larger
number of instances of smaller size.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13171" title="Abstract">arXiv:2402.13171</a> [<a href="/pdf/2402.13171" title="Download PDF">pdf</a>, <a href="/format/2402.13171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> waLBerla-wind: a lattice-Boltzmann-based high-performance flow solver  for wind energy applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schottenhamml%2C+H">Helen Schottenhamml</a>, 
<a href="/search/cs?searchtype=author&query=Anciaux-Sedrakian%2C+A">Ani Anciaux-Sedrakian</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+F">Fr&#xe9;d&#xe9;ric Blondel</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6stler%2C+H">Harald K&#xf6;stler</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCde%2C+U">Ulrich R&#xfc;de</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">This article presents the development of a new wind turbine simulation
software to study wake flow physics. To this end, the design and development of
waLBerla-wind, a new simulator based on the lattice-Boltzmann method that is
known for its excellent performance and scaling properties, will be presented.
Here it will be used for large eddy simulations (LES) coupled with actuator
wind turbine models. Due to its modular software design, waLBerla-wind is
flexible and extensible with regard to turbine configurations. Additionally it
is performance portable across different hardware architectures, another
critical design goal. The new solver is validated by presenting force
distributions and velocity profiles and comparing them with experimental data
and a vortex solver. Furthermore, waLBerla-wind's performance is
\revision{compared to a theoretical peak performance}, and analysed with weak
and strong scaling benchmarks on CPU and GPU systems. This analysis
demonstrates the suitability for large-scale applications and future
cost-effective full wind farm simulations.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13172" title="Abstract">arXiv:2402.13172</a> [<a href="/pdf/2402.13172" title="Download PDF">pdf</a>, <a href="/format/2402.13172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Kinematics Estimation from Video with a Biomechanical Model and  Synthetic Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhi-Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+B">Bofan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+J+C">Judith Cueto Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Kruk%2C+E">Eline van der Kruk</a>, 
<a href="/search/cs?searchtype=author&query=Seth%2C+A">Ajay Seth</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xucong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate 3D kinematics estimation of human body is crucial in various
applications for human health and mobility, such as rehabilitation, injury
prevention, and diagnosis, as it helps to understand the biomechanical loading
experienced during movement. Conventional marker-based motion capture is
expensive in terms of financial investment, time, and the expertise required.
Moreover, due to the scarcity of datasets with accurate annotations, existing
markerless motion capture methods suffer from challenges including unreliable
2D keypoint detection, limited anatomic accuracy, and low generalization
capability. In this work, we propose a novel biomechanics-aware network that
directly outputs 3D kinematics from two input views with consideration of
biomechanical prior and spatio-temporal information. To train the model, we
create synthetic dataset ODAH with accurate kinematics annotations generated by
aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal
model. Our extensive experiments demonstrate that the proposed approach, only
trained on synthetic data, outperforms previous state-of-the-art methods when
evaluated across multiple datasets, revealing a promising direction for
enhancing video-based human motion capture.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13178" title="Abstract">arXiv:2402.13178</a> [<a href="/pdf/2402.13178" title="Download PDF">pdf</a>, <a href="/format/2402.13178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Retrieval-Augmented Generation for Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guangzhi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Homepage: <a href="https://teddy-xionggz.github.io/benchmark-medical-rag/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the "lost-in-the-middle" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13179" title="Abstract">arXiv:2402.13179</a> [<a href="/pdf/2402.13179" title="Download PDF">pdf</a>, <a href="/format/2402.13179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> homotopy.io: a proof assistant for finitely-presented globular  $n$-categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corbyn%2C+N">Nathan Corbyn</a>, 
<a href="/search/cs?searchtype=author&query=Heidemann%2C+L">Lukas Heidemann</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+N">Nick Hu</a>, 
<a href="/search/cs?searchtype=author&query=Sarti%2C+C">Chiara Sarti</a>, 
<a href="/search/cs?searchtype=author&query=Tataru%2C+C">Calin Tataru</a>, 
<a href="/search/cs?searchtype=author&query=Vicary%2C+J">Jamie Vicary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT)

</div>
<p class="mathjax">We present the proof assistant homotopy.io for working with
finitely-presented semistrict higher categories. The tool runs in the browser
with a point-and-click interface, allowing direct manipulation of proof objects
via a graphical representation. We describe the user interface and explain how
the tool can be used in practice. We also describe the essential subsystems of
the tool, including collapse, contraction, expansion, typechecking, and layout,
as well as key implementation details including data structure encoding,
memoisation, and rendering. These technical innovations have been essential for
achieving good performance in a resource-constrained setting.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13181" title="Abstract">arXiv:2402.13181</a> [<a href="/pdf/2402.13181" title="Download PDF">pdf</a>, <a href="/format/2402.13181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DINOBot: Robot Manipulation via Retrieval and Alignment with Vision  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Palo%2C+N">Norman Di Palo</a>, 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose DINOBot, a novel imitation learning framework for robot
manipulation, which leverages the image-level and pixel-level capabilities of
features extracted from Vision Transformers trained with DINO. When interacting
with a novel object, DINOBot first uses these features to retrieve the most
visually similar object experienced during human demonstrations, and then uses
this object to align its end-effector with the novel object to enable effective
interaction. Through a series of real-world experiments on everyday tasks, we
show that exploiting both the image-level and pixel-level properties of vision
foundation models enables unprecedented learning efficiency and generalisation.
Videos and code are available at https://www.robot-learning.uk/dinobot.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13182" title="Abstract">arXiv:2402.13182</a> [<a href="/pdf/2402.13182" title="Download PDF">pdf</a>, <a href="/format/2402.13182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order-Optimal Regret in Distributed Kernel Bandits using Uniform  Sampling with Shared Randomness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavlovic%2C+N">Nikola Pavlovic</a>, 
<a href="/search/cs?searchtype=author&query=Salgia%2C+S">Sudeep Salgia</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider distributed kernel bandits where $N$ agents aim to
collaboratively maximize an unknown reward function that lies in a reproducing
kernel Hilbert space. Each agent sequentially queries the function to obtain
noisy observations at the query points. Agents can share information through a
central server, with the objective of minimizing regret that is accumulating
over time $T$ and aggregating over agents. We develop the first algorithm that
achieves the optimal regret order (as defined by centralized learning) with a
communication cost that is sublinear in both $N$ and $T$. The key features of
the proposed algorithm are the uniform exploration at the local agents and
shared randomness with the central server. Working together with the sparse
approximation of the GP model, these two key components make it possible to
preserve the learning rate of the centralized setting at a diminishing rate of
communication.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13183" title="Abstract">arXiv:2402.13183</a> [<a href="/pdf/2402.13183" title="Download PDF">pdf</a>, <a href="/format/2402.13183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Model Predictive Control for nonlinear discrete-time systems  using iterative time-varying constraint tightening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leister%2C+D+D">Daniel D. Leister</a> (1), 
<a href="/search/eess?searchtype=author&query=Koeln%2C+J+P">Justin P. Koeln</a> (1) ((1) The University of Texas at Dallas)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures. Submitted to the International Journal of Robust and Nonlinear Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Robust Model Predictive Control (MPC) for nonlinear systems is a problem that
poses significant challenges as highlighted by the diversity of approaches
proposed in the last decades. Often compromises with respect to computational
load, conservatism, generality, or implementation complexity have to be made,
and finding an approach that provides the right balance is still a challenge to
the research community. This work provides a contribution by proposing a novel
shrinking-horizon robust MPC formulation for nonlinear discrete-time systems.
By explicitly accounting for how disturbances and linearization errors are
propagated through the nonlinear dynamics, a constraint tightening-based
formulation is obtained, with guarantees of robust constraint satisfaction. The
proposed controller relies on iteratively solving a Nonlinear Program (NLP) to
simultaneously optimize system operation and the required constraint
tightening. Numerical experiments show the effectiveness of the proposed
controller with three different choices of NLP solvers as well as significantly
improved computational speed, better scalability, and generally reduced
conservatism when compared to an existing technique from the literature.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13184" title="Abstract">arXiv:2402.13184</a> [<a href="/pdf/2402.13184" title="Download PDF">pdf</a>, <a href="/format/2402.13184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What if LLMs Have Different World Views: Simulating Alien Civilizations  with LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhaoqian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kai Mei</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we introduce "CosmoAgent," an innovative artificial
intelligence framework utilizing Large Language Models (LLMs) to simulate
complex interactions between human and extraterrestrial civilizations, with a
special emphasis on Stephen Hawking's cautionary advice about not sending radio
signals haphazardly into the universe. The goal is to assess the feasibility of
peaceful coexistence while considering potential risks that could threaten
well-intentioned civilizations. Employing mathematical models and state
transition matrices, our approach quantitatively evaluates the development
trajectories of civilizations, offering insights into future decision-making at
critical points of growth and saturation. Furthermore, the paper acknowledges
the vast diversity in potential living conditions across the universe, which
could foster unique cosmologies, ethical codes, and worldviews among various
civilizations. Recognizing the Earth-centric bias inherent in current LLM
designs, we propose the novel concept of using LLMs with diverse ethical
paradigms and simulating interactions between entities with distinct moral
principles. This innovative research provides a new way to understand complex
inter-civilizational dynamics, expanding our perspective while pioneering novel
strategies for conflict resolution, crucial for preventing interstellar
conflicts. We have also released the code and datasets to enable further
academic investigation into this interesting area of research. The code is
available at https://github.com/agiresearch/AlienAgent.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13185" title="Abstract">arXiv:2402.13185</a> [<a href="/pdf/2402.13185" title="Download PDF">pdf</a>, <a href="/format/2402.13185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jianhongbai.github.io/UniEdit/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in text-guided video editing have showcased promising results
in appearance editing (e.g., stylization). However, video motion editing in the
temporal dimension (e.g., from eating to waving), which distinguishes video
editing from image editing, is underexplored. In this work, we present UniEdit,
a tuning-free framework that supports both video motion and appearance editing
by harnessing the power of a pre-trained text-to-video generator within an
inversion-then-generation framework. To realize motion editing while preserving
source video content, based on the insights that temporal and spatial
self-attention layers encode inter-frame and intra-frame dependency
respectively, we introduce auxiliary motion-reference and reconstruction
branches to produce text-guided motion and source features respectively. The
obtained features are then injected into the main editing path via temporal and
spatial self-attention layers. Extensive experiments demonstrate that UniEdit
covers video motion editing and various appearance editing scenarios, and
surpasses the state-of-the-art methods. Our code will be publicly available.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13187" title="Abstract">arXiv:2402.13187</a> [<a href="/pdf/2402.13187" title="Download PDF">pdf</a>, <a href="/format/2402.13187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Calibration in Subquadratic Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lunjia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Kevin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chutong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the recent literature on machine learning and decision making, calibration
has emerged as a desirable and widely-studied statistical property of the
outputs of binary prediction models. However, the algorithmic aspects of
measuring model calibration have remained relatively less well-explored.
Motivated by [BGHN23], which proposed a rigorous framework for measuring
distances to calibration, we initiate the algorithmic study of calibration
through the lens of property testing. We define the problem of calibration
testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on
(predictions, binary outcomes), our goal is to distinguish between the case
where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$
is $\varepsilon$-far from calibration.
<br />We design an algorithm based on approximate linear programming, which solves
calibration testing information-theoretically optimally (up to constant
factors) in time $O(n^{1.5} \log(n))$. This improves upon state-of-the-art
black-box linear program solvers requiring $\Omega(n^\omega)$ time, where
$\omega &gt; 2$ is the exponent of matrix multiplication. We also develop
algorithms for tolerant variants of our testing problem, and give sample
complexity lower bounds for alternative calibration distances to the one
considered in this work. Finally, we present preliminary experiments showing
that the testing problem we define faithfully captures standard notions of
calibration, and that our algorithms scale to accommodate moderate sample
sizes.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13188" title="Abstract">arXiv:2402.13188</a> [<a href="/pdf/2402.13188" title="Download PDF">pdf</a>, <a href="/format/2402.13188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Calibration and Multi-Hop Modeling for Temporal Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Di Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Many models that leverage knowledge graphs (KGs) have recently demonstrated
remarkable success in question answering (QA) tasks. In the real world, many
facts contained in KGs are time-constrained thus temporal KGQA has received
increasing attention. Despite the fruitful efforts of previous models in
temporal KGQA, they still have several limitations. (I) They adopt pre-trained
language models (PLMs) to obtain question representations, while PLMs tend to
focus on entity information and ignore entity transfer caused by temporal
constraints, and finally fail to learn specific temporal representations of
entities. (II) They neither emphasize the graph structure between entities nor
explicitly model the multi-hop relationship in the graph, which will make it
difficult to solve complex multi-hop question answering. To alleviate this
problem, we propose a novel Question Calibration and Multi-Hop Modeling
(QC-MHM) approach. Specifically, We first calibrate the question representation
by fusing the question and the time-constrained concepts in KG. Then, we
construct the GNN layer to complete multi-hop message passing. Finally, the
question representation is combined with the embedding output by the GNN to
generate the final prediction. Empirical results verify that the proposed model
achieves better performance than the state-of-the-art models in the benchmark
dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions
dataset's complex questions are absolutely improved by 5.1% and 1.2% compared
to the best-performing baseline. Moreover, QC-MHM can generate interpretable
and trustworthy predictions.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13191" title="Abstract">arXiv:2402.13191</a> [<a href="/pdf/2402.13191" title="Download PDF">pdf</a>, <a href="/format/2402.13191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Blockchain technology within an Information Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salzano%2C+F">Francesco Salzano</a>, 
<a href="/search/cs?searchtype=author&query=Marchesi%2C+L">Lodovica Marchesi</a>, 
<a href="/search/cs?searchtype=author&query=Pareschi%2C+R">Remo Pareschi</a>, 
<a href="/search/cs?searchtype=author&query=Tonelli%2C+R">Roberto Tonelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Context: Blockchain-based Information Ecosystems (BBIEs) are a type of
information ecosystem in which blockchain technology is used to provide a trust
mechanism among parties and to manage shared business logic, breaking the
traditional scheme of Information Ecosystems dominated by a leading company and
leveraging the decentralization of data management, information flow, and
business logic. Objective: In this paper, we propose architecture and technical
aspects concerning the creation of a BBIE, underlining the advantages supplied
and the logic decomposition among the business and storage components. Method:
The requirements are derived from the current needs of the collaborative
business and the data collected by surveying practitioners. To get these needs
we followed the Grounded Theory research approach. We validate our
architectural schema against a case study dealing with the management of a wine
supply chain, also involving different companies and supervision authorities.
Results: The proposed solution integrates blockchain-based applications with
the existing information system as a module of the ecosystem, leveraging on the
low costs, scalability, and high-level security because of the restricted
access to the network. Conclusion: We must go a long way in deepening and
refining the possibilities offered by technology in supporting innovative
multi-organizational business models. BBIEs can contribute substantially to
paving the way in such a direction.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13195" title="Abstract">arXiv:2402.13195</a> [<a href="/pdf/2402.13195" title="Download PDF">pdf</a>, <a href="/format/2402.13195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Flight Demonstration of a Quadrotor for Urban Mapping and  Target Tracking Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hague%2C+C">Collin Hague</a>, 
<a href="/search/cs?searchtype=author&query=Kakavitsas%2C+N">Nick Kakavitsas</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jincheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Beam%2C+C">Chris Beam</a>, 
<a href="/search/cs?searchtype=author&query=Willis%2C+A">Andrew Willis</a>, 
<a href="/search/cs?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 10 figures, To be presented at IEEE SoutheastCon 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper describes the hardware design and flight demonstration of a small
quadrotor with imaging sensors for urban mapping, hazard avoidance, and target
tracking research. The vehicle is equipped with five cameras, including two
pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a
two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running
the Robot Operating System software is used for data collection. An autonomous
tracking behavior was implemented to coordinate the motion of the quadrotor and
gimbaled camera to track a moving GPS coordinate. The data collection system
was demonstrated through a flight test that tracked a moving GPS-tagged vehicle
through a series of roads and parking lots. A map of the environment was
reconstructed from the collected images using the Direct Sparse Odometry (DSO)
algorithm. The performance of the quadrotor was also characterized by acoustic
noise, communication range, battery voltage in hover, and maximum speed tests.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13196" title="Abstract">arXiv:2402.13196</a> [<a href="/pdf/2402.13196" title="Download PDF">pdf</a>, <a href="/format/2402.13196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Kernel Tests of Conditional Independence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pogodin%2C+R">Roman Pogodin</a>, 
<a href="/search/cs?searchtype=author&query=Schrab%2C+A">Antonin Schrab</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yazhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Sutherland%2C+D+J">Danica J. Sutherland</a>, 
<a href="/search/cs?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We describe a data-efficient, kernel-based approach to statistical testing of
conditional independence. A major challenge of conditional independence
testing, absent in tests of unconditional independence, is to obtain the
correct test level (the specified upper bound on the rate of false positives),
while still attaining competitive test power. Excess false positives arise due
to bias in the test statistic, which is obtained using nonparametric kernel
ridge regression. We propose three methods for bias control to correct the test
level, based on data splitting, auxiliary data, and (where possible) simpler
function classes. We show these combined strategies are effective both for
synthetic and real-world data.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13201" title="Abstract">arXiv:2402.13201</a> [<a href="/pdf/2402.13201" title="Download PDF">pdf</a>, <a href="/format/2402.13201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tiny Reinforcement Learning for Quadruped Locomotion using Decision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akg%C3%BCn%2C+O+E">Orhan Eren Akg&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+N">N&#xe9;stor Cuevas</a>, 
<a href="/search/cs?searchtype=author&query=Farias%2C+M">Matheus Farias</a>, 
<a href="/search/cs?searchtype=author&query=Garces%2C+D">Daniel Garces</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Resource-constrained robotic platforms are particularly useful for tasks that
require low-cost hardware alternatives due to the risk of losing the robot,
like in search-and-rescue applications, or the need for a large number of
devices, like in swarm robotics. For this reason, it is crucial to find
mechanisms for adapting reinforcement learning techniques to the constraints
imposed by lower computational power and smaller memory capacities of these
ultra low-cost robotic platforms. We try to address this need by proposing a
method for making imitation learning deployable onto resource-constrained
robotic platforms. Here we cast the imitation learning problem as a conditional
sequence modeling task and we train a decision transformer using expert
demonstrations augmented with a custom reward. Then, we compress the resulting
generative model using software optimization schemes, including quantization
and pruning. We test our method in simulation using Isaac Gym, a realistic
physics simulation environment designed for reinforcement learning. We
empirically demonstrate that our method achieves natural looking gaits for
Bittle, a resource-constrained quadruped robot. We also run multiple
simulations to show the effects of pruning and quantization on the performance
of the model. Our results show that quantization (down to 4 bits) and pruning
reduce model size by around 30\% while maintaining a competitive reward, making
the model deployable in a resource-constrained system.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13204" title="Abstract">arXiv:2402.13204</a> [<a href="/pdf/2402.13204" title="Download PDF">pdf</a>, <a href="/format/2402.13204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural  Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouzidi%2C+H">Halima Bouzidi</a>, 
<a href="/search/cs?searchtype=author&query=Niar%2C+S">Smail Niar</a>, 
<a href="/search/cs?searchtype=author&query=Ouarnoughi%2C+H">Hamza Ouarnoughi</a>, 
<a href="/search/cs?searchtype=author&query=Talbi%2C+E">El-Ghazali Talbi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in Artificial Intelligence (AI), driven by Neural
Networks (NN), demand innovative neural architecture designs, particularly
within the constrained environments of Internet of Things (IoT) systems, to
balance performance and efficiency. HW-aware Neural Architecture Search
(HW-aware NAS) emerges as an attractive strategy to automate the design of NN
using multi-objective optimization approaches, such as evolutionary algorithms.
However, the intricate relationship between NN design parameters and HW-aware
NAS optimization objectives remains an underexplored research area, overlooking
opportunities to effectively leverage this knowledge to guide the search
process accordingly. Furthermore, the large amount of evaluation data produced
during the search holds untapped potential for refining the optimization
strategy and improving the approximation of the Pareto front. Addressing these
issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware
NAS. Our method leverages adaptive evolutionary operators guided by the learned
importance of NN design parameters. Specifically, through tree-based surrogate
models and a Reinforcement Learning agent, we aspire to gather knowledge on
'How' and 'When' to evolve NN architectures. Comprehensive evaluations across
various NAS search spaces and hardware devices on the ImageNet-1k dataset have
shown the merit of SONATA with up to 0.25% improvement in accuracy and up to
2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto
dominance over the native NSGA-II, further stipulating the importance of
self-adaptive evolution operators in HW-aware NAS.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13208" title="Abstract">arXiv:2402.13208</a> [<a href="/pdf/2402.13208" title="Download PDF">pdf</a>, <a href="/format/2402.13208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Hyenas deal with Human Speech? Speech Recognition and Translation  with ConfHyena
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Papi%2C+S">Sara Papi</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The attention mechanism, a cornerstone of state-of-the-art neural models,
faces computational hurdles in processing long sequences due to its quadratic
complexity. Consequently, research efforts in the last few years focused on
finding more efficient alternatives. Among them, Hyena (Poli et al., 2023)
stands out for achieving competitive results in both language modeling and
image classification, while offering sub-quadratic memory and computational
complexity. Building on these promising results, we propose ConfHyena, a
Conformer whose encoder self-attentions are replaced with an adaptation of
Hyena for speech processing, where the long input sequences cause high
computational costs. Through experiments in automatic speech recognition (for
English) and translation (from English into 8 target languages), we show that
our best ConfHyena model significantly reduces the training time by 27%, at the
cost of minimal quality degradation (~1%), which, in most cases, is not
statistically significant.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13210" title="Abstract">arXiv:2402.13210</a> [<a href="/pdf/2402.13210" title="Download PDF">pdf</a>, <a href="/format/2402.13210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Reward Models for LLM Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+X">Adam X. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Robeyns%2C+M">Maxime Robeyns</a>, 
<a href="/search/cs?searchtype=author&query=Coste%2C+T">Thomas Coste</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bou-Ammar%2C+H">Haitham Bou-Ammar</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To ensure that large language model (LLM) responses are helpful and
non-toxic, we usually fine-tune a reward model on human preference data. We
then select policy responses with high rewards (best-of-n sampling) or further
optimize the policy to produce responses with high rewards (reinforcement
learning from human feedback). However, this process is vulnerable to reward
overoptimization or hacking, in which the responses selected have high rewards
due to errors in the reward model rather than a genuine preference. This is
especially problematic as the prompt or response diverges from the training
data. It should be possible to mitigate these issues by training a Bayesian
reward model, which signals higher uncertainty further from the training data
distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA
(Yang et al., 2024) and found that the resulting uncertainty estimates can
successfully mitigate reward overoptimization in best-of-n sampling.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13211" title="Abstract">arXiv:2402.13211</a> [<a href="/pdf/2402.13211" title="Download PDF">pdf</a>, <a href="/format/2402.13211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models be Good Emotional Supporter? Mitigating  Preference Bias on Emotional Support Conversation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongjin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sunghwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taeyoon Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungjun Moon</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunsouk Cho</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotional Support Conversation (ESC) is a task aimed at alleviating
individuals' emotional distress through daily conversation. Given its inherent
complexity and non-intuitive nature, ESConv dataset incorporates support
strategies to facilitate the generation of appropriate responses. Recently,
despite the remarkable conversational ability of large language models (LLMs),
previous studies have suggested that they often struggle with providing useful
emotional support. Hence, this work initially analyzes the results of LLMs on
ESConv, revealing challenges in selecting the correct strategy and a notable
preference for a specific strategy. Motivated by these, we explore the impact
of the inherent preference in LLMs on providing emotional support, and
consequently, we observe that exhibiting high preference for specific
strategies hinders effective emotional support, aggravating its robustness in
predicting the appropriate strategy. Moreover, we conduct a methodological
study to offer insights into the necessary approaches for LLMs to serve as
proficient emotional supporters. Our findings emphasize that (1) low preference
for specific strategies hinders the progress of emotional support, (2) external
assistance helps reduce preference bias, and (3) LLMs alone cannot become good
emotional supporters. These insights suggest promising avenues for future
research to enhance the emotional intelligence of LLMs.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13212" title="Abstract">arXiv:2402.13212</a> [<a href="/pdf/2402.13212" title="Download PDF">pdf</a>, <a href="/format/2402.13212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Self-Consistency Improves Language Model Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+A">Archiki Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, the first three authors contributed equally; Code: <a href="https://github.com/HanNight/soft_self_consistency">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generations from large language models (LLMs) can be improved by sampling and
scoring multiple solutions to select a final answer. Current "sample and
select" methods such as self-consistency (SC) rely on majority voting to score
answers. However, when tasks have many distinct and valid answers, selection by
voting requires a large number of samples. This makes SC prohibitively
expensive for interactive tasks that involve generating multiple actions
(answers) sequentially. After establishing that majority voting fails to
provide consistent gains on such tasks, we demonstrate how to increase success
rates by softening the scoring criterion. We introduce Soft Self-Consistency
(Soft-SC), which replaces SC's discontinuous scoring with a continuous score
computed from model likelihoods, allowing for selection even when actions are
sparsely distributed. Soft-SC improves both performance and efficiency on
long-horizon interactive tasks, requiring half as many samples as SC for
comparable or better performance. For a fixed number of samples, Soft-SC leads
to a 1.3% increase over SC in absolute success rate on writing bash programs, a
6.6% increase on online shopping (WebShop), and a 4.7% increase for an
interactive household game (ALFWorld). Finally, we show that Soft-SC can be
applied to both open-source and black-box models.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13213" title="Abstract">arXiv:2402.13213</a> [<a href="/pdf/2402.13213" title="Download PDF">pdf</a>, <a href="/format/2402.13213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Softmax Probabilities (Mostly) Predict Large Language Model Correctness  on Multiple-Choice Q&amp;A
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plaut%2C+B">Benjamin Plaut</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Trinh%2C+T">Tu Trinh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although large language models (LLMs) perform impressively on many tasks,
overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A
tasks, wrong answers would be associated with smaller maximum softmax
probabilities (MSPs) compared to correct answers. We comprehensively evaluate
this hypothesis on ten open-source LLMs and five datasets, and find strong
evidence for our hypothesis among models which perform well on the original Q&amp;A
task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from
the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances.
Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging
these findings, we propose a multiple-choice Q&amp;A task with an option to abstain
and show that performance can be improved by selectively abstaining based on
the MSP of the initial model response. We also run the same experiments with
pre-softmax logits instead of softmax probabilities and find similar (but not
identical) results.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13217" title="Abstract">arXiv:2402.13217</a> [<a href="/pdf/2402.13217" title="Download PDF">pdf</a>, <a href="/format/2402.13217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VideoPrism: A Foundational Visual Encoder for Video Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Long Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gundavarapu%2C+N+B">Nitesh B. Gundavarapu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liangzhe Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J+J">Jennifer J. Sun</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+L">Luke Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+R">Rui Qian</a>, 
<a href="/search/cs?searchtype=author&query=Weyand%2C+T">Tobias Weyand</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hornung%2C+R">Rachel Hornung</a>, 
<a href="/search/cs?searchtype=author&query=Schroff%2C+F">Florian Schroff</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+D+A">David A. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huisheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+H">Hartwig Adam</a>, 
<a href="/search/cs?searchtype=author&query=Sirotenko%2C+M">Mikhail Sirotenko</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+B">Boqing Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce VideoPrism, a general-purpose video encoder that tackles diverse
video understanding tasks with a single frozen model. We pretrain VideoPrism on
a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M
video clips with noisy parallel text (e.g., ASR transcripts). The pretraining
approach improves upon masked autoencoding by global-local distillation of
semantic video embeddings and a token shuffling scheme, enabling VideoPrism to
focus primarily on the video modality while leveraging the invaluable text
associated with videos. We extensively test VideoPrism on four broad groups of
video understanding tasks, from web video question answering to CV for science,
achieving state-of-the-art performance on 30 out of 33 video understanding
benchmarks.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13219" title="Abstract">arXiv:2402.13219</a> [<a href="/pdf/2402.13219" title="Download PDF">pdf</a>, <a href="/format/2402.13219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Operator States and the Impact of AI-Enhanced Decision Support  in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning  Framework for Intervention Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A+N">Ammar N. Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Amazu%2C+C+W">Chidera W. Amazu</a>, 
<a href="/search/cs?searchtype=author&query=Mietkiewicz%2C+J">Joseph Mietkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Briwa%2C+H">Houda Briwa</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+A+A">Andres Alonzo Perez</a>, 
<a href="/search/cs?searchtype=author&query=Baldissone%2C+G">Gabriele Baldissone</a>, 
<a href="/search/cs?searchtype=author&query=Demichela%2C+M">Micaela Demichela</a>, 
<a href="/search/cs?searchtype=author&query=Chasparis%2C+G+G">Georgios G. Chasparis</a>, 
<a href="/search/cs?searchtype=author&query=Kelleher%2C+J+D">John D. Kelleher</a>, 
<a href="/search/cs?searchtype=author&query=Leva%2C+M+C">Maria Chiara Leva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">In complex industrial and chemical process control rooms, effective
decision-making is crucial for safety and effi- ciency. The experiments in this
paper evaluate the impact and applications of an AI-based decision support
system integrated into an improved human-machine interface, using dynamic
influ- ence diagrams, a hidden Markov model, and deep reinforcement learning.
The enhanced support system aims to reduce operator workload, improve
situational awareness, and provide different intervention strategies to the
operator adapted to the current state of both the system and human performance.
Such a system can be particularly useful in cases of information overload when
many alarms and inputs are presented all within the same time window, or for
junior operators during training. A comprehensive cross-data analysis was
conducted, involving 47 participants and a diverse range of data sources such
as smartwatch metrics, eye- tracking data, process logs, and responses from
questionnaires. The results indicate interesting insights regarding the effec-
tiveness of the approach in aiding decision-making, decreasing perceived
workload, and increasing situational awareness for the scenarios considered.
Additionally, the results provide valuable insights to compare differences
between styles of information gathering when using the system by individual
participants. These findings are particularly relevant when predicting the
overall performance of the individual participant and their capacity to
successfully handle a plant upset and the alarms connected to it using process
and human-machine interaction logs in real-time. These predictions enable the
development of more effective intervention strategies.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13220" title="Abstract">arXiv:2402.13220</a> [<a href="/pdf/2402.13220" title="Download PDF">pdf</a>, <a href="/format/2402.13220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on  Deceptive Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yusu Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haotian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yinfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zhe Gan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The remarkable advancements in Multimodal Large Language Models (MLLMs) have
not rendered them immune to challenges, particularly in the context of handling
deceptive information in prompts, thus producing hallucinated responses under
such conditions. To quantitatively assess this vulnerability, we present
MAD-Bench, a carefully curated benchmark that contains 850 test samples divided
into 6 categories, such as non-existent objects, count of objects, spatial
relationship, and visual confusion. We provide a comprehensive analysis of
popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as
LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps
between GPT-4V and other models; and previous robust instruction-tuned models,
such as LRV-Instruction and LLaVA-RLHF, are not effective on this new
benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of
any other model in our experiments ranges from 5% to 35%. We further propose a
remedy that adds an additional paragraph to the deceptive prompts to encourage
models to think twice before answering the question. Surprisingly, this simple
method can even double the accuracy; however, the absolute numbers are still
too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark
to stimulate further research to enhance models' resilience against deceptive
prompts.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13221" title="Abstract">arXiv:2402.13221</a> [<a href="/pdf/2402.13221" title="Download PDF">pdf</a>, <a href="/format/2402.13221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset  for Advancing Graph Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friis-Jensen%2C+U">Ulrik Friis-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Johansen%2C+F+L">Frederik L. Johansen</a>, 
<a href="/search/cs?searchtype=author&query=Anker%2C+A+S">Andy S. Anker</a>, 
<a href="/search/cs?searchtype=author&query=Dam%2C+E+B">Erik B. Dam</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+K+M+%C3%98">Kirsten M. &#xd8;. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Selvan%2C+R">Raghavendra Selvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the Applied Data Science (ADS) Track at KDD'24. 16 pages, 15 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Advances in graph machine learning (ML) have been driven by applications in
chemistry as graphs have remained the most expressive representations of
molecules. While early graph ML methods focused primarily on small organic
molecules, recently, the scope of graph ML has expanded to include inorganic
materials. Modelling the periodicity and symmetry of inorganic crystalline
materials poses unique challenges, which existing graph ML methods are unable
to address. Moving to inorganic nanomaterials increases complexity as the scale
of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of
existing graph ML focuses on characterising molecules and materials by
predicting target properties with graphs as input. However, the most exciting
applications of graph ML will be in their generative capabilities, which is
currently not at par with other domains such as images or text.
<br />We invite the graph ML community to address these open challenges by
presenting two new chemically-informed large-scale inorganic (CHILI)
nanomaterials datasets: A medium-scale dataset (with overall &gt;6M nodes, &gt;49M
edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal
types (CHILI-3K) and a large-scale dataset (with overall &gt;183M nodes, &gt;1.2B
edges) of nanomaterials generated from experimentally determined crystal
structures (CHILI-100K). We define 11 property prediction tasks and 6 structure
prediction tasks, which are of special interest for nanomaterial research. We
benchmark the performance of a wide array of baseline methods and use these
benchmarking results to highlight areas which need future work. To the best of
our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial
datasets of this scale -- both on the individual graph level and of the dataset
as a whole -- and the only nanomaterials datasets with high structural and
elemental diversity.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13222" title="Abstract">arXiv:2402.13222</a> [<a href="/pdf/2402.13222" title="Download PDF">pdf</a>, <a href="/format/2402.13222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoCode: A Dataset for Measuring Code Intelligence from Problem  Definitions in Romanian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosma%2C+A">Adrian Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Iordache%2C+B">Bogdan Iordache</a>, 
<a href="/search/cs?searchtype=author&query=Rosso%2C+P">Paolo Rosso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, large language models (LLMs) have become increasingly powerful and
have become capable of solving a plethora of tasks through proper instructions
in natural language. However, the vast majority of testing suites assume that
the instructions are written in English, the de facto prompting language. Code
intelligence and problem solving still remain a difficult task, even for the
most advanced LLMs. Currently, there are no datasets to measure the
generalization power for code-generation models in a language other than
English. In this work, we present RoCode, a competitive programming dataset,
consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and
Python and comprehensive testing suites for each problem. The purpose of RoCode
is to provide a benchmark for evaluating the code intelligence of language
models trained on Romanian / multilingual text as well as a fine-tuning set for
pretrained Romanian models. Through our results and review of related works, we
argue for the need to develop code models for languages other than English.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13225" title="Abstract">arXiv:2402.13225</a> [<a href="/pdf/2402.13225" title="Download PDF">pdf</a>, <a href="/ps/2402.13225" title="Download PostScript">ps</a>, <a href="/format/2402.13225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale  Clinical Tool Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+D">Donald Wright</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Thomas Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wilbur%2C+W+J">W John Wilbur</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhe He</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+A">Andrew Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Clinical calculators play a vital role in healthcare by offering accurate
evidence-based predictions for various purposes such as prognosis.
Nevertheless, their widespread utilization is frequently hindered by usability
challenges, poor dissemination, and restricted functionality. Augmenting large
language models with extensive collections of clinical calculators presents an
opportunity to overcome these obstacles and improve workflow efficiency, but
the scalability of the manual curation process poses a significant challenge.
In response, we introduce AgentMD, a novel language agent capable of curating
and applying clinical calculators across various clinical contexts. Using the
published literature, AgentMD has automatically curated a collection of 2,164
diverse clinical calculators with executable functions and structured
documentation, collectively named RiskCalcs. Manual evaluations show that
RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At
inference time, AgentMD can automatically select and apply the relevant
RiskCalcs tools given any patient description. On the newly established RiskQA
benchmark, AgentMD significantly outperforms chain-of-thought prompting with
GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to
real-world clinical notes for analyzing both population-level and risk-level
patient characteristics. In summary, our study illustrates the utility of
language agents augmented with clinical calculators for healthcare analytics
and patient care.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13227" title="Abstract">arXiv:2402.13227</a> [<a href="/pdf/2402.13227" title="Download PDF">pdf</a>, <a href="/format/2402.13227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Matching on $3$-Uniform Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borst%2C+S">Sander Borst</a>, 
<a href="/search/cs?searchtype=author&query=Kashaev%2C+D">Danish Kashaev</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+Z+K">Zhuan Khye Koh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The online matching problem was introduced by Karp, Vazirani and Vazirani
(STOC 1990) on bipartite graphs with vertex arrivals. It is well-known that the
optimal competitive ratio is $1-1/e$ for both integral and fractional versions
of the problem. Since then, there has been considerable effort to find optimal
competitive ratios for other related settings. In this work, we go beyond the
graph case and study the online matching problem on $k$-uniform hypergraphs.
For $k=3$, we provide an optimal primal-dual fractional algorithm, which
achieves a competitive ratio of $(e-1)/(e+1)\approx 0.4621$. As our main
technical contribution, we present a carefully constructed adversarial
instance, which shows that this ratio is in fact optimal. It combines ideas
from known hard instances for bipartite graphs under the edge-arrival and
vertex-arrival models. For $k\geq 3$, we give a simple integral algorithm which
performs better than greedy when the online nodes have bounded degree. As a
corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform
hypergraphs when every online node has degree at most 2. This is because the
special case where every online node has degree 1 is equivalent to the
edge-arrival model on graphs, for which an upper bound of 1/2 is known.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13228" title="Abstract">arXiv:2402.13228</a> [<a href="/pdf/2402.13228" title="Download PDF">pdf</a>, <a href="/format/2402.13228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Arka Pal</a>, 
<a href="/search/cs?searchtype=author&query=Karkhanis%2C+D">Deep Karkhanis</a>, 
<a href="/search/cs?searchtype=author&query=Dooley%2C+S">Samuel Dooley</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+M">Manley Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Naidu%2C+S">Siddartha Naidu</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Direct Preference Optimisation (DPO) is effective at significantly improving
the performance of large language models (LLMs) on downstream tasks such as
reasoning, summarisation, and alignment. Using pairs of preferred and
dispreferred data, DPO models the \textit{relative} probability of picking one
response over another. In this work, first we show theoretically that the
standard DPO loss can lead to a \textit{reduction} of the model's likelihood of
the preferred examples, as long as the relative probability between the
preferred and dispreferred classes increases. We then show empirically that
this phenomenon occurs when fine-tuning LLMs on common datasets, especially
datasets in which the edit distance between pairs of completions is low. Using
these insights, we design DPO-Positive (DPOP), a new loss function and training
procedure which avoids this failure mode. Surprisingly, we also find that DPOP
significantly outperforms DPO across a wide variety of datasets and downstream
tasks, including datasets with high edit distances between completions. By
fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which
achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly
2\% better than any other open-source model on the HuggingFace Open LLM
Leaderboard and becomes the first open-source LLM to surpass an average
accuracy of 80\%.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13231" title="Abstract">arXiv:2402.13231</a> [<a href="/pdf/2402.13231" title="Download PDF">pdf</a>, <a href="/format/2402.13231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Cultural Alignment of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlKhamissi%2C+B">Badr AlKhamissi</a>, 
<a href="/search/cs?searchtype=author&query=ElNokrashy%2C+M">Muhammad ElNokrashy</a>, 
<a href="/search/cs?searchtype=author&query=AlKhamissi%2C+M">Mai AlKhamissi</a>, 
<a href="/search/cs?searchtype=author&query=Diab%2C+M">Mona Diab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The intricate relationship between language and culture has long been a
subject of exploration within the realm of linguistic anthropology. Large
Language Models (LLMs), promoted as repositories of collective human knowledge,
raise a pivotal question: do these models genuinely encapsulate the diverse
knowledge adopted by different cultures? Our study reveals that these models
demonstrate greater cultural alignment along two dimensions -- firstly, when
prompted with the dominant language of a specific culture, and secondly, when
pretrained with a refined mixture of languages employed by that culture. We
quantify cultural alignment by simulating sociological surveys, comparing model
responses to those of actual survey participants as references. Specifically,
we replicate a survey conducted in various regions of Egypt and the United
States through prompting LLMs with different pretraining data mixtures in both
Arabic and English with the personas of the real respondents and the survey
questions. Further analysis reveals that misalignment becomes more pronounced
for underrepresented personas and for culturally sensitive topics, such as
those probing social values. Finally, we introduce Anthropological Prompting, a
novel method leveraging anthropological reasoning to enhance cultural
alignment. Our study emphasizes the necessity for a more balanced multilingual
pretraining dataset to better represent the diversity of human experience and
the plurality of different cultures with many implications on the topic of
cross-lingual transfer.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13232" title="Abstract">arXiv:2402.13232</a> [<a href="/pdf/2402.13232" title="Download PDF">pdf</a>, <a href="/format/2402.13232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Touch, Vision, and Language Dataset for Multimodal Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Letian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+G">Gaurav Datta</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Panitch%2C+W+C">William Chung-Ho Panitch</a>, 
<a href="/search/cs?searchtype=author&query=Drake%2C+J">Jaimyn Drake</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz%2C+J">Joseph Ortiz</a>, 
<a href="/search/cs?searchtype=author&query=Mukadam%2C+M">Mustafa Mukadam</a>, 
<a href="/search/cs?searchtype=author&query=Lambeta%2C+M">Mike Lambeta</a>, 
<a href="/search/cs?searchtype=author&query=Calandra%2C+R">Roberto Calandra</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+K">Ken Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Touch is an important sensing modality for humans, but it has not yet been
incorporated into a multimodal generative language model. This is partially due
to the difficulty of obtaining natural language labels for tactile data and the
complexity of aligning tactile readings with both visual observations and
language descriptions. As a step towards bridging that gap, this work
introduces a new dataset of 44K in-the-wild vision-touch pairs, with English
language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V
(90%). We use this dataset to train a vision-language-aligned tactile encoder
for open-vocabulary classification and a touch-vision-language (TVL) model for
text generation using the trained encoder. Results suggest that by
incorporating touch, the TVL model improves (+29% classification accuracy)
touch-vision-language alignment over existing models trained on any pair of
those modalities. Although only a small fraction of the dataset is
human-labeled, the TVL model demonstrates improved visual-tactile understanding
over GPT-4V (+12%) and open-source vision-language models (+32%) on a new
touch-vision understanding benchmark. Code and data:
https://tactile-vlm.github.io.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13233" title="Abstract">arXiv:2402.13233</a> [<a href="/pdf/2402.13233" title="Download PDF">pdf</a>, <a href="/format/2402.13233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMORE: Similarity-based Hyperdimensional Domain Adaptation for  Multi-Sensor Time Series Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Faruque%2C+M+A+A">Mohammad Abdullah Al Faruque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.03295">arXiv:2308.03295</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Many real-world applications of the Internet of Things (IoT) employ machine
learning (ML) algorithms to analyze time series information collected by
interconnected sensors. However, distribution shift, a fundamental challenge in
data-driven ML, arises when a model is deployed on a data distribution
different from the training data and can substantially degrade model
performance. Additionally, increasingly sophisticated deep neural networks
(DNNs) are required to capture intricate spatial and temporal dependencies in
multi-sensor time series data, often exceeding the capabilities of today's edge
devices. In this paper, we propose SMORE, a novel resource-efficient domain
adaptation (DA) algorithm for multi-sensor time series classification,
leveraging the efficient and parallel operations of hyperdimensional computing.
SMORE dynamically customizes test-time models with explicit consideration of
the domain context of each sample to mitigate the negative impacts of domain
shifts. Our evaluation on a variety of multi-sensor time series classification
tasks shows that SMORE achieves on average 1.98% higher accuracy than
state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and
4.63x faster inference.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13234" title="Abstract">arXiv:2402.13234</a> [<a href="/pdf/2402.13234" title="Download PDF">pdf</a>, <a href="/format/2402.13234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Insights: Semantic Search in Jupyter Notebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jinpeng Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Semantic search, a process aimed at delivering highly relevant search results
by comprehending the searcher's intent and the contextual meaning of terms
within a searchable dataspace, plays a pivotal role in information retrieval.
In this paper, we investigate the application of large language models to
enhance semantic search capabilities, specifically tailored for the domain of
Jupyter Notebooks. Our objective is to retrieve generated outputs, such as
figures or tables, associated functions and methods, and other pertinent
information.
<br />We demonstrate a semantic search framework that achieves a comprehensive
semantic understanding of the entire notebook's contents, enabling it to
effectively handle various types of user queries. Key components of this
framework include:
<br />1). A data preprocessor is designed to handle diverse types of cells within
Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative
methodology is devised to address token size limitations that arise with
code-type cells. We implement a finer-grained approach to data input,
transitioning from the cell level to the function level, effectively resolving
these issues.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13237" title="Abstract">arXiv:2402.13237</a> [<a href="/pdf/2402.13237" title="Download PDF">pdf</a>, <a href="/format/2402.13237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Pushdown VASS in One Dimension are Easy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perez%2C+G+A">Guillermo A. Perez</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Shrisha Rao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 tables, 6 figures, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">A pushdown vector addition system with states (PVASS) extends the model of
vector addition systems with a pushdown stack. The algorithmic analysis of
PVASS has applications such as static anal- ysis of recursive programs
manipulating integer variables. Unfor- tunately, reachability analysis, even
for one-dimensional PVASS is not known to be decidable. We relax the model of
one-dimensional PVASS to make the counter updates continuous and show that in
this case reachability, coverability, and boundedness are decidable in
polynomial time. In addition, for the extension of the model with lower-bound
guards on the states, we show that coverability and reachability are in NP, and
boundedness is in coNP.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13241" title="Abstract">arXiv:2402.13241</a> [<a href="/pdf/2402.13241" title="Download PDF">pdf</a>, <a href="/format/2402.13241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Causal Discovery from Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Loka Li</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+I">Ignavier Ng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+G">Gongxu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conventional causal discovery methods rely on centralized data, which is
inconsistent with the decentralized nature of data in many real-world
situations. This discrepancy has motivated the development of federated causal
discovery (FCD) approaches. However, existing FCD methods may be limited by
their potentially restrictive assumptions of identifiable functional causal
models or homogeneous data distributions, narrowing their applicability in
diverse scenarios. In this paper, we propose a novel FCD method attempting to
accommodate arbitrary causal models and heterogeneous data. We first utilize a
surrogate variable corresponding to the client index to account for the data
heterogeneity across different clients. We then develop a federated conditional
independence test (FCIT) for causal skeleton discovery and establish a
federated independent change principle (FICP) to determine causal directions.
These approaches involve constructing summary statistics as a proxy of the raw
data to protect data privacy. Owing to the nonparametric properties, FCIT and
FICP make no assumption about particular functional forms, thereby facilitating
the handling of arbitrary causal models. We conduct extensive experiments on
synthetic and real datasets to show the efficacy of our method. The code is
available at \url{https://github.com/lokali/FedCDH.git}.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13243" title="Abstract">arXiv:2402.13243</a> [<a href="/pdf/2402.13243" title="Download PDF">pdf</a>, <a href="/format/2402.13243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Bencheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://hgao-cv.github.io/VADv2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Learning a human-like driving policy from large-scale driving demonstrations
is promising, but the uncertainty and non-deterministic nature of planning make
it challenging. In this work, to cope with the uncertainty problem, we propose
VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes
multi-view image sequences as input in a streaming manner, transforms sensor
data into environmental token embeddings, outputs the probabilistic
distribution of action, and samples one action to control the vehicle. Only
with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on
the CARLA Town05 benchmark, significantly outperforming all existing methods.
It runs stably in a fully end-to-end manner, even without the rule-based
wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13244" title="Abstract">arXiv:2402.13244</a> [<a href="/pdf/2402.13244" title="Download PDF">pdf</a>, <a href="/format/2402.13244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiangeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+T">Tess Christensen</a>, 
<a href="/search/cs?searchtype=author&query=Gilda%2C+S">Shlok Gilda</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+J">Juliana Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+D">Daniela Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Fact-checking is an important way to combat misinformation on social media,
especially during significant social events such as the COVID-19 pandemic and
the U.S. presidential elections. In this study, we thoroughly evaluated the
performance of Google Fact Check, a search engine specifically for
fact-checking results, by analyzing the results returned from Google Fact Check
regarding 1,000 false claims about COVID-19. We found that Google Fact Check
could not provide sufficient fact-checking information for most false claims,
even though the results provided are relatively reliable and helpful. We also
found that claims getting different fact-checking verdicts tend to contain
different emotional tones, and different sources tend to check claims using
dictionary words to different extents and at different lengths. Claims in
different descriptions are likely to get different fact-checking results. We
aimed to bring up the best practice of fact-checking for the general people
based on our analyses.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13249" title="Abstract">arXiv:2402.13249</a> [<a href="/pdf/2402.13249" title="Download PDF">pdf</a>, <a href="/format/2402.13249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Liyan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shalyminov%2C+I">Igor Shalyminov</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A+W">Amy Wing-mei Wong</a>, 
<a href="/search/cs?searchtype=author&query=Burnsky%2C+J">Jon Burnsky</a>, 
<a href="/search/cs?searchtype=author&query=Vincent%2C+J+W">Jake W. Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu&#x27;an Yang</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Siffi Singh</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Song Feng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hwanjun Song</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lijia Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+S">Saab Mansour</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+K">Kathleen McKeown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Linguistic annotations available at <a href="https://github.com/amazon-science/tofueval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Single document news summarization has seen substantial progress on
faithfulness in recent years, driven by research on the evaluation of factual
consistency, or hallucinations. We ask whether these advances carry over to
other text summarization domains. We propose a new evaluation benchmark on
topic-focused dialogue summarization, generated by LLMs of varying sizes. We
provide binary sentence-level human annotations of the factual consistency of
these summaries along with detailed explanations of factually inconsistent
sentences. Our analysis shows that existing LLMs hallucinate significant
amounts of factual errors in the dialogue domain, regardless of the model's
size. On the other hand, when LLMs, including GPT-4, serve as binary factual
evaluators, they perform poorly and can be outperformed by prevailing
state-of-the-art specialized factuality evaluation metrics. Finally, we
conducted an analysis of hallucination types with a curated error taxonomy. We
find that there are diverse errors and error distributions in model-generated
summaries and that non-LLM based metrics can capture all error types better
than LLM-based evaluators.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13250" title="Abstract">arXiv:2402.13250</a> [<a href="/pdf/2402.13250" title="Download PDF">pdf</a>, <a href="/format/2402.13250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video ReCap: Recursive Captioning of Hour-Long Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Mohaiminul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Ngan Ho</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+T">Tushar Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Torresani%2C+L">Lorenzo Torresani</a>, 
<a href="/search/cs?searchtype=author&query=Bertasius%2C+G">Gedas Bertasius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most video captioning models are designed to process short video clips of few
seconds and output text describing low-level visual concepts (e.g., objects,
scenes, atomic actions). However, most real-world videos last for minutes or
hours and have a complex hierarchical structure spanning different temporal
granularities. We propose Video ReCap, a recursive video captioning model that
can process video inputs of dramatically different lengths (from 1 second to 2
hours) and output video captions at multiple hierarchy levels. The recursive
video-language architecture exploits the synergy between different video
hierarchies and can process hour-long videos efficiently. We utilize a
curriculum learning training scheme to learn the hierarchical structure of
videos, starting from clip-level captions describing atomic actions, then
focusing on segment-level descriptions, and concluding with generating
summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by
augmenting Ego4D with 8,267 manually collected long-range video summaries. Our
recursive model can flexibly generate captions at different hierarchy levels
while also being useful for other complex video understanding tasks, such as
VideoQA on EgoSchema. Data, code, and models are available at:
https://sites.google.com/view/vidrecap
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13251" title="Abstract">arXiv:2402.13251</a> [<a href="/pdf/2402.13251" title="Download PDF">pdf</a>, <a href="/format/2402.13251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlashTex: Fast Relightable Mesh Texturing with LightControlNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kangle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Omernick%2C+T">Timothy Omernick</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+A">Alexander Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tinghui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Agrawala%2C+M">Maneesh Agrawala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://flashtex.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Manually creating textures for 3D meshes is time-consuming, even for expert
visual content creators. We propose a fast approach for automatically texturing
an input 3D mesh based on a user-provided text prompt. Importantly, our
approach disentangles lighting from surface material/reflectance in the
resulting texture so that the mesh can be properly relit and rendered in any
lighting environment. We introduce LightControlNet, a new text-to-image model
based on the ControlNet architecture, which allows the specification of the
desired lighting as a conditioning image to the model. Our text-to-texture
pipeline then constructs the texture in two stages. The first stage produces a
sparse set of visually consistent reference views of the mesh using
LightControlNet. The second stage applies a texture optimization based on Score
Distillation Sampling (SDS) that works with LightControlNet to increase the
texture quality while disentangling surface material from lighting. Our
pipeline is significantly faster than previous text-to-texture methods, while
producing high-quality and relightable textures.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13252" title="Abstract">arXiv:2402.13252</a> [<a href="/pdf/2402.13252" title="Download PDF">pdf</a>, <a href="/format/2402.13252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness for Joint Optimization of Camera Poses and  Decomposed Low-Rank Tensorial Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bo-Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+W">Wei-Chen Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Lun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024. Project page: <a href="https://alex04072000.github.io/Joint-TensoRF/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose an algorithm that allows joint refinement of camera
pose and scene geometry represented by decomposed low-rank tensor, using only
2D images as supervision. First, we conduct a pilot study based on a 1D signal
and relate our findings to 3D scenarios, where the naive joint pose
optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.
Moreover, based on the analysis of the frequency spectrum, we propose to apply
convolutional Gaussian filters on 2D and 3D radiance fields for a
coarse-to-fine training schedule that enables joint camera pose optimization.
Leveraging the decomposition property in decomposed low-rank tensor, our method
achieves an equivalent effect to brute-force 3D convolution with only incurring
little computational overhead. To further improve the robustness and stability
of joint optimization, we also propose techniques of smoothed 2D supervision,
randomly scaled kernel parameters, and edge-guided loss mask. Extensive
quantitative and qualitative evaluations demonstrate that our proposed
framework achieves superior performance in novel view synthesis as well as
rapid convergence for optimization.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13253" title="Abstract">arXiv:2402.13253</a> [<a href="/pdf/2402.13253" title="Download PDF">pdf</a>, <a href="/format/2402.13253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiMediX: Bilingual Medical Mixture of Experts LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pieri%2C+S">Sara Pieri</a>, 
<a href="/search/cs?searchtype=author&query=Mullappilly%2C+S+S">Sahal Shaji Mullappilly</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Anwer%2C+R+M">Rao Muhammad Anwer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we introduce BiMediX, the first bilingual medical mixture of
experts LLM designed for seamless interaction in both English and Arabic. Our
model facilitates a wide range of medical interactions in English and Arabic,
including multi-turn chats to inquire about additional details such as patient
symptoms and medical history, multiple-choice question answering, and
open-ended question answering. We propose a semi-automated English-to-Arabic
translation pipeline with human refinement to ensure high-quality translations.
We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.
Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual
instruction set covering 1.3 Million diverse medical interactions, resulting in
over 632 million healthcare specialized tokens for instruction tuning. Our
BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and
maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art
Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,
computed across multiple medical evaluation benchmarks in English, while
operating at 8-times faster inference. Moreover, our BiMediX outperforms the
generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of
10% on our Arabic medical benchmark and 15% on bilingual evaluations across
multiple datasets. Our project page with source code and trained model is
available at https://github.com/mbzuai-oryx/BiMediX .
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13254" title="Abstract">arXiv:2402.13254</a> [<a href="/pdf/2402.13254" title="Download PDF">pdf</a>, <a href="/format/2402.13254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CounterCurate: Enhancing Physical and Semantic Visio-Linguistic  Compositional Reasoning via Counterfactual Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Mu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tengyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+J">Yong Jae Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, Project Page: <a href="https://countercurate.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13255" title="Abstract">arXiv:2402.13255</a> [<a href="/pdf/2402.13255" title="Download PDF">pdf</a>, <a href="/format/2402.13255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tosi%2C+F">Fabio Tosi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Ziren Gong</a>, 
<a href="/search/cs?searchtype=author&query=Sandstr%C3%B6m%2C+E">Erik Sandstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Mattoccia%2C+S">Stefano Mattoccia</a>, 
<a href="/search/cs?searchtype=author&query=Oswald%2C+M+R">Martin R. Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Poggi%2C+M">Matteo Poggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed, 21 Feb 24</h3>
<dl>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10649" title="Abstract">arXiv:2402.10649</a> (cross-list from math.NA) [<a href="/pdf/2402.10649" title="Download PDF">pdf</a>, <a href="/format/2402.10649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite Neural Network Simulation for Solving the 2D Schrodinger  Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parand%2C+K">Kourosh Parand</a>, 
<a href="/search/math?searchtype=author&query=Pakniyat%2C+A">Aida Pakniyat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">The Schrodinger equation is a mathematical equation describing the wave
function's behavior in a quantum-mechanical system. It is a partial
differential equation that provides valuable insights into the fundamental
principles of quantum mechanics. In this paper, the aim was to solve the
Schrodinger equation with sufficient accuracy by using a mixture of neural
networks with the collocation method base Hermite functions. Initially, the
Hermite functions roots were employed as collocation points, enhancing the
efficiency of the solution. The Schrodinger equation is defined in an infinite
domain, the use of Hermite functions as activation functions resulted in
excellent precision. Finally, the proposed method was simulated using MATLAB's
Simulink tool. The results were then compared with those obtained using
Physics-informed neural networks and the presented method.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11170" title="Abstract">arXiv:2402.11170</a> (cross-list from econ.GN) [<a href="/pdf/2402.11170" title="Download PDF">pdf</a>, <a href="/format/2402.11170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Reward Dynamics and Decentralization in Ethereum 2.0: An  Advanced Data Engineering Workflow and Comprehensive Datasets for  Proof-of-Stake Incentives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/econ?searchtype=author&query=Li%2C+S">Shengnan Li</a>, 
<a href="/search/econ?searchtype=author&query=Kraner%2C+B">Benjamin Kraner</a>, 
<a href="/search/econ?searchtype=author&query=Zhang%2C+L">Luyao Zhang</a>, 
<a href="/search/econ?searchtype=author&query=Tessone%2C+C+J">Claudio J. Tessone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY); Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Ethereum 2.0, as the preeminent smart contract blockchain platform,
guarantees the precise execution of applications without third-party
intervention. At its core, this system leverages the Proof-of-Stake (PoS)
consensus mechanism, which utilizes a stochastic process to select validators
for block proposal and validation, consequently rewarding them for their
contributions. However, the implementation of blockchain technology often
diverges from its central tenet of decentralized consensus, presenting
significant analytical challenges. Our study collects consensus reward data
from the Ethereum Beacon chain and conducts a comprehensive analysis of reward
distribution and evolution, categorizing them into attestation, proposer and
sync committee rewards. To evaluate the degree of decentralization in PoS
Ethereum, we apply several inequality indices, including the Shannon entropy,
the Gini Index, the Nakamoto Coefficient, and the Herfindahl-Hirschman Index
(HHI). Our comprehensive dataset is publicly available on Harvard Dataverse,
and our analytical methodologies are accessible via GitHub, promoting
open-access research. Additionally, we provide insights on utilizing our data
for future investigations focused on assessing, augmenting, and refining the
decentralization, security, and efficiency of blockchain systems.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12391" title="Abstract">arXiv:2402.12391</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.12391" title="Download PDF">pdf</a>, <a href="/format/2402.12391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Team of AI-made Scientists for Scientific Discovery from Gene  Expression Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+H">Haoyang Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Y">Yijiang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Jian%2C+J">Jinglin Jian</a>, 
<a href="/search/q-bio?searchtype=author&query=Cheng%2C+Y">Yuxuan Cheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jianrong Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+S">Shuyi Guo</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+J">Jinglei Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Mianchen Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Miantong Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning has emerged as a powerful tool for scientific discovery,
enabling researchers to extract meaningful insights from complex datasets. For
instance, it has facilitated the identification of disease-predictive genes
from gene expression data, significantly advancing healthcare. However, the
traditional process for analyzing such datasets demands substantial human
effort and expertise for the data selection, processing, and analysis. To
address this challenge, we introduce a novel framework, a Team of AI-made
Scientists (TAIS), designed to streamline the scientific discovery pipeline.
TAIS comprises simulated roles, including a project manager, data engineer, and
domain expert, each represented by a Large Language Model (LLM). These roles
collaborate to replicate the tasks typically performed by data scientists, with
a specific focus on identifying disease-predictive genes. Furthermore, we have
curated a benchmark dataset to assess TAIS's effectiveness in gene
identification, demonstrating our system's potential to significantly enhance
the efficiency and scope of scientific exploration. Our findings represent a
solid step towards automating scientific discovery through large language
models.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12392" title="Abstract">arXiv:2402.12392</a> (cross-list from stat.AP) [<a href="/pdf/2402.12392" title="Download PDF">pdf</a>, <a href="/format/2402.12392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Regression Mixture Model to understand the effect of the Covid-19  pandemic on Public Transport Ridership
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Moreau%2C+H">Hugues Moreau</a>, 
<a href="/search/stat?searchtype=author&query=C%C3%B4me%2C+%C3%89">&#xc9;tienne C&#xf4;me</a>, 
<a href="/search/stat?searchtype=author&query=Sam%C3%A9%2C+A">Allou Sam&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Oukhellou%2C+L">Latifa Oukhellou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Covid-19 pandemic drastically changed urban mobility, both during the
height of the pandemic with government lockdowns, but also in the longer term
with the adoption of working-from-home policies. To understand its effects on
rail public transport ridership, we propose a dedicated Regression Mixture
Model able to perform both the clustering of public transport stations and the
segmentation of time periods, while ignoring variations due to additional
variables such as the official lockdowns or non-working days. Each cluster is
thus defined by a series of segments in which the effect of the exogenous
variables is constant. As each segment within a cluster has its own regression
coefficients to model the impact of the covariates, we analyze how these
coefficients evolve to understand the changes in the cluster. We present the
regression mixture model and the parameter estimation using the EM algorithm,
before demonstrating the benefits of the model on both simulated and real data.
Thanks to a five-year dataset of the ridership in the Paris public transport
system, we analyze the impact of the pandemic, not only in terms of the number
of travelers but also on the weekly commute. We further analyze the specific
changes that the pandemic caused inside each cluster.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12396" title="Abstract">arXiv:2402.12396</a> (cross-list from astro-ph.HE) [<a href="/pdf/2402.12396" title="Download PDF">pdf</a>, <a href="/ps/2402.12396" title="Download PostScript">ps</a>, <a href="/format/2402.12396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward using GANs in astrophysical Monte-Carlo simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Isaac%2C+A">Ahab Isaac</a>, 
<a href="/search/astro-ph?searchtype=author&query=Armour%2C+W">Wesley Armour</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ad%C3%A1mek%2C+K">Karel Ad&#xe1;mek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of ADASS XXXIII (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Astrophysical Phenomena (astro-ph.HE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Accurate modelling of spectra produced by X-ray sources requires the use of
Monte-Carlo simulations. These simulations need to evaluate physical processes,
such as those occurring in accretion processes around compact objects by
sampling a number of different probability distributions. This is
computationally time-consuming and could be sped up if replaced by neural
networks. We demonstrate, on an example of the Maxwell-J\"uttner distribution
that describes the speed of relativistic electrons, that the generative
adversarial network (GAN) is capable of statistically replicating the
distribution. The average value of the Kolmogorov-Smirnov test is 0.5 for
samples generated by the neural network, showing that the generated
distribution cannot be distinguished from the true distribution.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12397" title="Abstract">arXiv:2402.12397</a> (cross-list from stat.ML) [<a href="/pdf/2402.12397" title="Download PDF">pdf</a>, <a href="/format/2402.12397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-class Temporal Logic Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+D">Danyang Li</a>, 
<a href="/search/stat?searchtype=author&query=Tron%2C+R">Roberto Tron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Time-series data can represent the behaviors of autonomous systems, such as
drones and self-driving cars. The problem of binary and multi-class
classification has received a lot of attention in this field. Neural networks
represent a popular approach to classifying data; However, they lack
interpretability, which poses a significant challenge in extracting meaningful
information from them. Signal Temporal Logic (STL) is a formalism to describe
the properties of timed behaviors. We propose a method that combines all of the
above: neural networks that represent STL specifications for multi-class
classification of time-series data. We offer two key contributions: 1) We
introduce a notion of margin for multi-class classification, and 2) we
introduce the use of STL-based attributes for enhancing the interpretability of
the results. We evaluate our method on two datasets and compare with
state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12400" title="Abstract">arXiv:2402.12400</a> (cross-list from stat.AP) [<a href="/pdf/2402.12400" title="Download PDF">pdf</a>, <a href="/format/2402.12400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the age-conditioned average treatment effects curves: An  application for assessing load-management strategies in the NBA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nakamura-Sakai%2C+S">Shinpei Nakamura-Sakai</a>, 
<a href="/search/stat?searchtype=author&query=Forastiere%2C+L">Laura Forastiere</a>, 
<a href="/search/stat?searchtype=author&query=Macdonald%2C+B">Brian Macdonald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the realm of competitive sports, understanding the performance dynamics of
athletes, represented by the age curve (showing progression, peak, and
decline), is vital. Our research introduces a novel framework for quantifying
age-specific treatment effects, enhancing the granularity of performance
trajectory analysis. Firstly, we propose a methodology for estimating the age
curve using game-level data, diverging from traditional season-level data
approaches, and tackling its inherent complexities with a meta-learner
framework that leverages advanced machine learning models. This approach
uncovers intricate non-linear patterns missed by existing methods. Secondly,
our framework enables the identification of causal effects, allowing for a
detailed examination of age curves under various conditions. By defining the
Age-Conditioned Treatment Effect (ACTE), we facilitate the exploration of
causal relationships regarding treatment impacts at specific ages. Finally,
applying this methodology to study the effects of rest days on performance
metrics, particularly across different ages, offers valuable insights into load
management strategies' effectiveness. Our findings underscore the importance of
tailored rest periods, highlighting their positive impact on athlete
performance and suggesting a reevaluation of current management practices for
optimizing athlete performance.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12405" title="Abstract">arXiv:2402.12405</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.12405" title="Download PDF">pdf</a>, <a href="/format/2402.12405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> scInterpreter: Training Large Language Models to Interpret scRNA-seq  Data for Cell Type Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+C">Cong Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Xiao%2C+M">Meng Xiao</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Feng%2C+G">Guihai Feng</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, submitted to FCS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the inherent limitations of existing Large Language Models in
directly reading and interpreting single-cell omics data, they demonstrate
significant potential and flexibility as the Foundation Model. This research
focuses on how to train and adapt the Large Language Model with the capability
to interpret and distinguish cell types in single-cell RNA sequencing data. Our
preliminary research results indicate that these foundational models excel in
accurately categorizing known cell types, demonstrating the potential of the
Large Language Models as effective tools for uncovering new biological
insights.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12407" title="Abstract">arXiv:2402.12407</a> (cross-list from eess.IV) [<a href="/pdf/2402.12407" title="Download PDF">pdf</a>, <a href="/format/2402.12407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating local laplacian filters on FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khandelwal%2C+S">Shashwat Khandelwal</a>, 
<a href="/search/eess?searchtype=author&query=Choudhury%2C+Z">Ziaul Choudhury</a>, 
<a href="/search/eess?searchtype=author&query=Shrivastava%2C+S">Shashwat Shrivastava</a>, 
<a href="/search/eess?searchtype=author&query=Purini%2C+S">Suresh Purini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 2 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 10.1109/FPL50879.2020.00028
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Signal Processing (eess.SP)

</div>
<p class="mathjax">Images when processed using various enhancement techniques often lead to edge
degradation and other unwanted artifacts such as halos. These artifacts pose a
major problem for photographic applications where they can denude the quality
of an image. There is a plethora of edge-aware techniques proposed in the field
of image processing. However, these require the application of complex
optimization or post-processing methods. Local Laplacian Filtering is an
edge-aware image processing technique that involves the construction of simple
Gaussian and Laplacian pyramids. This technique can be successfully applied for
detail smoothing, detail enhancement, tone mapping and inverse tone mapping of
an image while keeping it artifact-free. The problem though with this approach
is that it is computationally expensive. Hence, parallelization schemes using
multi-core CPUs and GPUs have been proposed. As is well known, they are not
power-efficient, and a well-designed hardware architecture on an FPGA can do
better on the performance per watt metric. In this paper, we propose a hardware
accelerator, which exploits fully the available parallelism in the Local
Laplacian Filtering algorithm, while minimizing the utilization of on-chip FPGA
resources. On Virtex-7 FPGA, we obtain a 7.5x speed-up to process a 1 MB image
when compared to an optimized baseline CPU implementation. To the best of our
knowledge, we are not aware of any other hardware accelerators proposed in the
research literature for the Local Laplacian Filtering problem.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12435" title="Abstract">arXiv:2402.12435</a> (cross-list from astro-ph.GA) [<a href="/pdf/2402.12435" title="Download PDF">pdf</a>, <a href="/format/2402.12435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emulating the interstellar medium chemistry with neural operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Branca%2C+L">Lorenzo Branca</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pallottini%2C+A">Andrea Pallottini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, Accepted for publication in A&amp;A
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Astrophysics of Galaxies (astro-ph.GA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Galaxy formation and evolution critically depend on understanding the complex
photo-chemical processes that govern the evolution and thermodynamics of the
InterStellar Medium (ISM). Computationally, solving chemistry is among the most
heavy tasks in cosmological and astrophysical simulations. The evolution of
such non-equilibrium photo-chemical network relies on implicit, precise,
computationally costly, ordinary differential equations (ODE) solvers. Here, we
aim at substituting such procedural solvers with fast, pre-trained, emulators
based on neural operators. We emulate a non-equilibrium chemical network up to
H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism,
i.e. by splitting the ODE solver operator that maps the initial conditions and
time evolution into a tensor product of two neural networks. We use
$\texttt{KROME}$ to generate a training set spanning $-2\leq
\log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(20) \leq\log(T/\mathrm{K}) \leq 5.5$,
$-6 \leq \log(n_i/n) &lt; 0$, and by adopting an incident radiation field
$\textbf{F}$ sampled in 10 energy bins with a continuity prior. We separately
train the solver for $T$ and each $n_i$ for $\simeq 4.34\,\rm GPUhrs$. Compared
with the reference solutions obtained by $\texttt{KROME}$ for single zone
models, the typical precision obtained is of order $10^{-2}$, i.e. the $10
\times$ better with a training that is $40 \times$ less costly with respect to
previous emulators which however considered only a fixed $\mathbf{F}$. The
present model achieves a speed-up of a factor of $128 \times$ with respect to
stiff ODE solvers. Our neural emulator represents a significant leap forward in
the modeling of ISM chemistry, offering a good balance of precision,
versatility, and computational efficiency.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12448" title="Abstract">arXiv:2402.12448</a> (cross-list from astro-ph.EP) [<a href="/pdf/2402.12448" title="Download PDF">pdf</a>, <a href="/format/2402.12448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DBNets: A publicly available deep learning tool to measure the masses of  young planets in dusty protoplanetary discs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Ruzza%2C+A">Alessandro Ruzza</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lodato%2C+G">Giuseppe Lodato</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rosotti%2C+G+P">Giovanni Pietro Rosotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">Current methods to characterize embedded planets in protoplanetary disc
observations are severely limited either in their ability to fully account for
the observed complex physics or in their computational and time costs. To
address this shortcoming, we developed DBNets: a deep learning tool, based on
convolutional neural networks, that analyses substructures observed in the dust
continuum emission of protoplanetary discs to quickly infer the mass of
allegedly embedded planets. We focussed on developing a method to reliably
quantify not only the planet mass, but also the associated uncertainty
introduced by our modelling and adopted techniques. Our tests gave promising
results achieving an 87% reduction of the log Mp mean squared error with
respect to an analytical formula fitted on the same data (DBNets metrics: lmse
0.016, r2-score 97%). With the goal of providing the final user of DBNets with
all the tools needed to interpret their measurements and decide on their
significance, we extensively tested our tool on out-of-distribution data. We
found that DBNets can identify inputs strongly outside its training scope
returning an uncertainty above a specific threshold and we thus provided a
rejection criterion that helps determine the significance of the results
obtained. Additionally, we outlined some limitations of our tool: it can be
reliably applied only on discs observed with inclinations below approximately
60{\deg}, in the optically thin regime, with a resolution 8 times better than
the gap radial location and with a signal-to-noise ratio higher than
approximately ten. Finally, we applied DBNets to 33 actual observations of
protoplanetary discs measuring the mass of 48 proposed planets and comparing
our results with the available literature. We confirmed that most of the
observed gaps imply planets in the sub-Jupiter regime. DBNets is publicly
available at dbnets.fisica.unimi.it.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12533" title="Abstract">arXiv:2402.12533</a> (cross-list from math.AP) [<a href="/pdf/2402.12533" title="Download PDF">pdf</a>, <a href="/ps/2402.12533" title="Download PostScript">ps</a>, <a href="/format/2402.12533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exterior Nonlocal Variational Inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Antil%2C+H">Harbir Antil</a>, 
<a href="/search/math?searchtype=author&query=Horton%2C+M+O">Madeline O. Horton</a>, 
<a href="/search/math?searchtype=author&query=Warma%2C+M">Mahamadi Warma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper introduces a new class of variational inequalities where the
obstacle is placed in the exterior domain that is disjoint from the observation
domain. This is carried out with the help of nonlocal fractional operators. The
need for such novel variational inequalities stems from the fact that the
classical approach only allows placing the obstacle either inside the
observation domain or on the boundary. A complete analysis of the continuous
problem is provided. Additionally, perturbation arguments to approximate the
problem are discussed.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12559" title="Abstract">arXiv:2402.12559</a> (cross-list from math.CO) [<a href="/pdf/2402.12559" title="Download PDF">pdf</a>, <a href="/ps/2402.12559" title="Download PostScript">ps</a>, <a href="/format/2402.12559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lettericity of graphs: an FPT algorithm and a bound on the size of  obstructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alecu%2C+B">Bogdan Alecu</a>, 
<a href="/search/math?searchtype=author&query=Kant%C3%A9%2C+M+M">Mamadou Moustapha Kant&#xe9;</a>, 
<a href="/search/math?searchtype=author&query=Lozin%2C+V">Vadim Lozin</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16pages + 3pages appendix. Submitted to a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Lettericity is a graph parameter responsible for many attractive structural
properties. In particular, graphs of bounded lettericity have bounded linear
clique-width and they are well-quasi-ordered by induced subgraphs. The latter
property implies that any hereditary class of graphs of bounded lettericity can
be described by finitely many forbidden induced subgraphs. This, in turn,
implies, in a non-constructive way, polynomial-time recognition of such
classes. However, no constructive algorithms and no specific bounds on the size
of forbidden graphs are available up to date. In the present paper, we develop
an algorithm that recognizes $n$-vertex graphs of lettericity at most $k$ in
time $f(k)n^3$ and show that any minimal graph of lettericity more than $k$ has
at most $2^{O(k^2\log k)}$ vertices.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12564" title="Abstract">arXiv:2402.12564</a> (cross-list from math.CO) [<a href="/pdf/2402.12564" title="Download PDF">pdf</a>, <a href="/format/2402.12564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coloring problems on arrangements of pseudolines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Roch%2C+S">Sandro Roch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended abstract of this article was submitted to EuroCG2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Arrangements of pseudolines are a widely studied generalization of line
arrangements. They are defined as a finite family of infinite curves in the
Euclidean plane, any two of which intersect at exactly one point. One can state
various related coloring problems depending on the number $n$ of pseudolines.
In this article, we show that $n$ colors are sufficient for coloring the
crossings avoiding twice the same color on the boundary of any cell, or,
alternatively, avoiding twice the same color along any pseudoline. We also
study the problem of coloring the pseudolines avoiding monochromatic crossings.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12565" title="Abstract">arXiv:2402.12565</a> (cross-list from eess.SP) [<a href="/pdf/2402.12565" title="Download PDF">pdf</a>, <a href="/format/2402.12565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Detection and Identification Scheme For Reconfigurable  Intelligent Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khaleel%2C+A">Aymen Khaleel</a>, 
<a href="/search/eess?searchtype=author&query=Vural%2C+R">Recep Vural</a>, 
<a href="/search/eess?searchtype=author&query=Ilter%2C+M+C">Mehmet Cagri Ilter</a>, 
<a href="/search/eess?searchtype=author&query=Gerami%2C+M">Majid Gerami</a>, 
<a href="/search/eess?searchtype=author&query=Basar%2C+E">Ertugrul Basar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures, submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS)-empowered communication is one of
the promising physical layer enabling technologies for the sixth generation
(6G) wireless networks due to their unprecedented capabilities in shaping the
wireless communication environment. RISs are modeled as passive objects that
can not transmit or receive wireless signals. While the passiveness of these
surfaces is a key advantage in terms of power consumption and implementation
complexity, it limits their capability to interact with the other active
components in the network. Specifically, unlike conventional base stations
(BSs), which actively identify themselves to user equipment (UEs) by
periodically sending pilot signals, RISs need to be detected from the UE side.
This paper proposes a novel RIS identification (RIS- ID) scheme, enabling UEs
to detect and uniquely identify RISs in their surrounding environment.
Furthermore, to assess the proposed RIS-ID scheme, we propose two performance
metrics: the false and miss detection probabilities. These probabilities are
analytically derived and verified through computer simulations, revealing the
effectiveness of the proposed RIS-ID scheme under different operating
scenarios.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12589" title="Abstract">arXiv:2402.12589</a> (cross-list from math.ST) [<a href="/pdf/2402.12589" title="Download PDF">pdf</a>, <a href="/format/2402.12589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Fourier Coefficients of High-Dimensional Random Geometric Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bangachev%2C+K">Kiril Bangachev</a>, 
<a href="/search/math?searchtype=author&query=Bresler%2C+G">Guy Bresler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> STOC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
<p class="mathjax">The random geometric graph $\mathsf{RGG}(n,\mathbb{S}^{d-1}, p)$ is formed by
sampling $n$ i.i.d. vectors $\{V_i\}_{i = 1}^n$ uniformly on $\mathbb{S}^{d-1}$
and placing an edge between pairs of vertices $i$ and $j$ for which $\langle
V_i,V_j\rangle \ge \tau^p_d,$ where $\tau^p_d$ is such that the expected
density is $p.$ We study the low-degree Fourier coefficients of the
distribution $\mathsf{RGG}(n,\mathbb{S}^{d-1}, p)$ and its Gaussian analogue.
<br />Our main conceptual contribution is a novel two-step strategy for bounding
Fourier coefficients which we believe is more widely applicable to studying
latent space distributions. First, we localize the dependence among edges to
few fragile edges. Second, we partition the space of latent vector
configurations $(\mathsf{RGG}(n,\mathbb{S}^{d-1}, p))^{\otimes n}$ based on the
set of fragile edges and on each subset of configurations, we define a noise
operator acting independently on edges not incident (in an appropriate sense)
to fragile edges.
<br />We apply the resulting bounds to: 1) Settle the low-degree polynomial
complexity of distinguishing spherical and Gaussian random geometric graphs
from Erdos-Renyi both in the case of observing a complete set of edges and in
the non-adaptively chosen mask $\mathcal{M}$ model recently introduced by
[MVW24]; 2) Exhibit a statistical-computational gap for distinguishing
$\mathsf{RGG}$ and the planted coloring model [KVWX23] in a regime when
$\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on
the second eigenvalue of random geometric graphs.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12595" title="Abstract">arXiv:2402.12595</a> (cross-list from eess.SP) [<a href="/pdf/2402.12595" title="Download PDF">pdf</a>, <a href="/ps/2402.12595" title="Download PostScript">ps</a>, <a href="/format/2402.12595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Truncated Polynomial Expansion-Based Detection in Massive MIMO: A  Model-Driven Deep Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Izadinasab%2C+K">Kazem Izadinasab</a>, 
<a href="/search/eess?searchtype=author&query=Shaban%2C+A+W">Ahmed Wagdy Shaban</a>, 
<a href="/search/eess?searchtype=author&query=Damen%2C+O">Oussama Damen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose a deep learning (DL)-based approach for efficiently
computing the inverse of Hermitian matrices using truncated polynomial
expansion (TPE). Our model-driven approach involves optimizing the coefficients
of the TPE during an offline training procedure for a given number of TPE
terms. We apply this method to signal detection in uplink massive
multiple-input multiple-output (MIMO) systems, where the matrix inverse
operation required by linear detectors, such as zero-forcing (ZF) and minimum
mean square error (MMSE), is approximated using TPE. Our simulation results
demonstrate that the proposed learned TPE-based method outperforms the
conventional TPE method with optimal coefficients in terms of asymptotic
convergence speed and reduces the computational complexity of the online
detection stage, albeit at the expense of the offline training stage. However,
the limited number of trainable parameters leads to a swift offline training
process.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12630" title="Abstract">arXiv:2402.12630</a> (cross-list from stat.ML) [<a href="/pdf/2402.12630" title="Download PDF">pdf</a>, <a href="/format/2402.12630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAST: An Optimization Framework for Fast Additive Segmentation in  Transparent ML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+B">Brian Liu</a>, 
<a href="/search/stat?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present FAST, an optimization framework for fast additive segmentation.
FAST segments piecewise constant shape functions for each feature in a dataset
to produce transparent additive models. The framework leverages a novel
optimization procedure to fit these models $\sim$2 orders of magnitude faster
than existing state-of-the-art methods, such as explainable boosting machines
\citep{nori2019interpretml}. We also develop new feature selection algorithms
in the FAST framework to fit parsimonious models that perform well. Through
experiments and case studies, we show that FAST improves the computational
efficiency and interpretability of additive models.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12631" title="Abstract">arXiv:2402.12631</a> (cross-list from quant-ph) [<a href="/pdf/2402.12631" title="Download PDF">pdf</a>, <a href="/format/2402.12631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for  3-Regular MAXCUT and Higher-Round Scaling Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Tate%2C+R">Reuben Tate</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eidenbenz%2C+S">Stephan Eidenbenz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Emerging Technologies (cs.ET); Combinatorics (math.CO); Optimization and Control (math.OC)

</div>
<p class="mathjax">We generalize Farhi et al.'s 0.6924-approximation result technique of the
Max-Cut Quantum Approximate Optimization Algorithm (QAOA) on 3-regular graphs
to obtain provable lower bounds on the approximation ratio for warm-started
QAOA. Given an initialization angle $\theta$, we consider warm-starts where the
initial state is a product state where each qubit position is angle $\theta$
away from either the north or south pole of the Bloch sphere; of the two
possible qubit positions the position of each qubit is decided by some
classically obtained cut encoded as a bitstring $b$. We illustrate through
plots how the properties of $b$ and the initialization angle $\theta$ influence
the bound on the approximation ratios of warm-started QAOA. We consider various
classical algorithms (and the cuts they produce which we use to generate the
warm-start). Our results strongly suggest that there does not exist any choice
of initialization angle that yields a (worst-case) approximation ratio that
simultaneously beats standard QAOA and the classical algorithm used to create
the warm-start.
<br />Additionally, we show that at $\theta=60^\circ$, warm-started QAOA is able to
(effectively) recover the cut used to generate the warm-start, thus suggesting
that in practice, this value could be a promising starting angle to explore
alternate solutions in a heuristic fashion. Finally, for any combinatorial
optimization problem with integer-valued objective values, we provide bounds on
the required circuit depth needed for warm-started QAOA to achieve some change
in approximation ratio; more specifically, we show that for small $\theta$, the
bound is roughly proportional to $1/\theta$.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12668" title="Abstract">arXiv:2402.12668</a> (cross-list from stat.ML) [<a href="/pdf/2402.12668" title="Download PDF">pdf</a>, <a href="/format/2402.12668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomization Can Reduce Both Bias and Variance: A Case Study in Random  Forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+B">Brian Liu</a>, 
<a href="/search/stat?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the often overlooked phenomenon, first noted in
\cite{breiman2001random}, that random forests appear to reduce bias compared to
bagging. Motivated by an interesting paper by \cite{mentch2020randomization},
where the authors argue that random forests reduce effective degrees of freedom
and only outperform bagging ensembles in low signal-to-noise ratio (SNR)
settings, we explore how random forests can uncover patterns in the data missed
by bagging. We empirically demonstrate that in the presence of such patterns,
random forests reduce bias along with variance and increasingly outperform
bagging ensembles when SNR is high. Our observations offer insights into the
real-world success of random forests across a range of SNRs and enhance our
understanding of the difference between random forests and bagging ensembles
with respect to the randomization injected into each split. Our investigations
also yield practical insights into the importance of tuning $mtry$ in random
forests.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12696" title="Abstract">arXiv:2402.12696</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.12696" title="Download PDF">pdf</a>, <a href="/format/2402.12696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fully-integrated lattice Boltzmann method for fluid-structure  interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sun%2C+Y">Yue Sun</a>, 
<a href="/search/physics?searchtype=author&query=Rycroft%2C+C+H">Chris H. Rycroft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present a fully-integrated lattice Boltzmann (LB) method for
fluid--structure interaction (FSI) simulations that efficiently models
deformable solids in complex suspensions and active systems. Our Eulerian
method (LBRMT) couples finite-strain solids to the LB fluid on the same fixed
computational grid with the reference map technique (RMT). An integral part of
the LBRMT is a new LB boundary condition for moving deformable interfaces
across different densities. With this fully Eulerian solid--fluid coupling, the
LBRMT is well-suited for parallelization and simulating multi-body contact
without remeshing or extra meshes. We validate its accuracy via a benchmark of
a deformable solid in a lid-driven cavity, then showcase its versatility
through examples of soft solids rotating and settling. With simulations of
complex suspensions mixing, we highlight potentials of the LBRMT for studying
collective behavior in soft matter and biofluid dynamics.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12701" title="Abstract">arXiv:2402.12701</a> (cross-list from eess.IV) [<a href="/pdf/2402.12701" title="Download PDF">pdf</a>, <a href="/format/2402.12701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> wmh_seg: Transformer based U-Net for Robust and Automatic White Matter  Hyperintensity Segmentation across 1.5T, 3T and 7T
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jinghang Li</a>, 
<a href="/search/eess?searchtype=author&query=Santini%2C+T">Tales Santini</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yuanzhe Huang</a>, 
<a href="/search/eess?searchtype=author&query=Mettenburg%2C+J+M">Joseph M. Mettenburg</a>, 
<a href="/search/eess?searchtype=author&query=Ibrahima%2C+T+S">Tamer S. Ibrahima</a>, 
<a href="/search/eess?searchtype=author&query=Aizensteina%2C+H+J">Howard J. Aizensteina</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+M">Minjie Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">White matter hyperintensity (WMH) remains the top imaging biomarker for
neurodegenerative diseases. Robust and accurate segmentation of WMH holds
paramount significance for neuroimaging studies. The growing shift from 3T to
7T MRI necessitates robust tools for harmonized segmentation across field
strengths and artifacts. Recent deep learning models exhibit promise in WMH
segmentation but still face challenges, including diverse training data
representation and limited analysis of MRI artifacts' impact. To address these,
we introduce wmh_seg, a novel deep learning model leveraging a
transformer-based encoder from SegFormer. wmh_seg is trained on an unmatched
dataset, including 1.5T, 3T, and 7T FLAIR images from various sources,
alongside with artificially added MR artifacts. Our approach bridges gaps in
training diversity and artifact analysis. Our model demonstrated stable
performance across magnetic field strengths, scanner manufacturers, and common
MR imaging artifacts. Despite the unique inhomogeneity artifacts on ultra-high
field MR images, our model still offers robust and stable segmentation on 7T
FLAIR images. Our model, to date, is the first that offers quality white matter
lesion segmentation on 7T FLAIR images.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12704" title="Abstract">arXiv:2402.12704</a> (cross-list from quant-ph) [<a href="/pdf/2402.12704" title="Download PDF">pdf</a>, <a href="/format/2402.12704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Embedding with Transformer for High-dimensional Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+H">Hao-Yuan Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chang%2C+Y">Yen-Jui Chang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liao%2C+S">Shih-Wei Liao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chang%2C+C">Ching-Ray Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum embedding with transformers is a novel and promising architecture for
quantum machine learning to deliver exceptional capability on near-term devices
or simulators. The research incorporated a vision transformer (ViT) to advance
quantum significantly embedding ability and results for a single qubit
classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a
challenging high-dimensional dataset. The study showcases and analyzes
empirical evidence that our transformer-based architecture is a highly
versatile and practical approach to modern quantum machine learning problems.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12710" title="Abstract">arXiv:2402.12710</a> (cross-list from stat.ME) [<a href="/pdf/2402.12710" title="Download PDF">pdf</a>, <a href="/format/2402.12710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Active Learning in Causal Inference with Interference: A  Novel Approach in Online Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhu%2C+H">Hongtao Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Sizhe Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+Y">Yang Su</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+Z">Zhenyu Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+N">Nan Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the domain of causal inference research, the prevalent potential outcomes
framework, notably the Rubin Causal Model (RCM), often overlooks individual
interference and assumes independent treatment effects. This assumption,
however, is frequently misaligned with the intricate realities of real-world
scenarios, where interference is not merely a possibility but a common
occurrence. Our research endeavors to address this discrepancy by focusing on
the estimation of direct and spillover treatment effects under two assumptions:
(1) network-based interference, where treatments on neighbors within connected
networks affect one's outcomes, and (2) non-random treatment assignments
influenced by confounders. To improve the efficiency of estimating potentially
complex effects functions, we introduce an novel active learning approach:
Active Learning in Causal Inference with Interference (ACI). This approach uses
Gaussian process to flexibly model the direct and spillover treatment effects
as a function of a continuous measure of neighbors' treatment assignment. The
ACI framework sequentially identifies the experimental settings that demand
further data. It further optimizes the treatment assignments under the network
interference structure using genetic algorithms to achieve efficient learning
outcome. By applying our method to simulation data and a Tencent game dataset,
we demonstrate its feasibility in achieving accurate effects estimations with
reduced data requirements. This ACI approach marks a significant advancement in
the realm of data efficiency for causal inference, offering a robust and
efficient alternative to traditional methodologies, particularly in scenarios
characterized by complex interference patterns.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12735" title="Abstract">arXiv:2402.12735</a> (cross-list from eess.IV) [<a href="/pdf/2402.12735" title="Download PDF">pdf</a>, <a href="/format/2402.12735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising OCT Images Using Steered Mixture of Experts with Multi-Model  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=%C3%96zkan%2C+A">Ayta&#xe7; &#xd6;zkan</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Stoykova%2C+E">Elena Stoykova</a> (2), 
<a href="/search/eess?searchtype=author&query=Sikora%2C+T">Thomas Sikora</a> (1), 
<a href="/search/eess?searchtype=author&query=Madjarova%2C+V">Violeta Madjarova</a> (2) ((1) Communication Systems Group, Technical University of Berlin, Germany, (2) Institute of Optical Materials and Technologies, Bulgarian Academy of Science, Sofia, Bulgaria)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission contains 10 pages and 4 figures. It was presented at the 2024 SPIE Photonics West, held in San Francisco. The paper details advancements in photonics applications related to healthcare and includes supplementary material with additional datasets for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In Optical Coherence Tomography (OCT), speckle noise significantly hampers
image quality, affecting diagnostic accuracy. Current methods, including
traditional filtering and deep learning techniques, have limitations in noise
reduction and detail preservation. Addressing these challenges, this study
introduces a novel denoising algorithm, Block-Matching Steered-Mixture of
Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE). This method
combines block-matched implementation of the SMoE algorithm with an enhanced
autoencoder architecture, offering efficient speckle noise reduction while
retaining critical image details. Our method stands out by providing improved
edge definition and reduced processing time. Comparative analysis with existing
denoising techniques demonstrates the superior performance of BM-SMoE-AE in
maintaining image integrity and enhancing OCT image usability for medical
diagnostics.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12744" title="Abstract">arXiv:2402.12744</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.12744" title="Download PDF">pdf</a>, <a href="/format/2402.12744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Models for Vibrational Entropy Based on a Spatial  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Torabi%2C+T">Tina Torabi</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yangshuai Wang</a>, 
<a href="/search/physics?searchtype=author&query=Ortner%2C+C">Christoph Ortner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The temperature-dependent behavior of defect densities within a crystalline
structure is intricately linked to the phenomenon of vibrational entropy.
Traditional methods for evaluating vibrational entropy are computationally
intensive, limiting their practical utility. We show that total entropy can be
decomposed into atomic site contributions and rigorously estimate the locality
of site entropy. This analysis suggests that vibrational entropy can be
effectively predicted using a surrogate model for site entropy. We employ
machine learning to develop such a surrogate models employing the Atomic
Cluster Expansion model. We supplement our rigorous analysis with an empirical
convergence study. In addition we demonstrate the performance of our method for
predicting vibrational formation entropy and attempt frequency of the
transition rates, on point defects such as vacancies and interstitials.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12745" title="Abstract">arXiv:2402.12745</a> (cross-list from quant-ph) [<a href="/pdf/2402.12745" title="Download PDF">pdf</a>, <a href="/ps/2402.12745" title="Download PostScript">ps</a>, <a href="/format/2402.12745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+C">Chenyi Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+T">Tongyang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 1 figure, To appear in The Twelfth International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">The problem of minimizing the maximum of $N$ convex, Lipschitz functions
plays significant roles in optimization and machine learning. It has a series
of results, with the most recent one requiring $O(N\epsilon^{-2/3} +
\epsilon^{-8/3})$ queries to a first-order oracle to compute an
$\epsilon$-suboptimal point. On the other hand, quantum algorithms for
optimization are rapidly advancing with speedups shown on many important
optimization problems. In this paper, we conduct a systematic study for quantum
algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz
functions. On one hand, we develop quantum algorithms with an improved
complexity bound of $\tilde{O}(\sqrt{N}\epsilon^{-5/3} + \epsilon^{-8/3})$. On
the other hand, we prove that quantum algorithms must take
$\tilde{\Omega}(\sqrt{N}\epsilon^{-2/3})$ queries to a first order quantum
oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic
factors.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12746" title="Abstract">arXiv:2402.12746</a> (cross-list from eess.AS) [<a href="/pdf/2402.12746" title="Download PDF">pdf</a>, <a href="/ps/2402.12746" title="Download PostScript">ps</a>, <a href="/format/2402.12746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plugin Speech Enhancement: A Universal Speech Enhancement Framework  Inspired by Dynamic Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yanan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+Z">Zihao Cui</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yingying Gao</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+J">Junlan Feng</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+C">Chao Deng</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shilei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The expectation to deploy a universal neural network for speech enhancement,
with the aim of improving noise robustness across diverse speech processing
tasks, faces challenges due to the existing lack of awareness within static
speech enhancement frameworks regarding the expected speech in downstream
modules. These limitations impede the effectiveness of static speech
enhancement approaches in achieving optimal performance for a range of speech
processing tasks, thereby challenging the notion of universal applicability.
The fundamental issue in achieving universal speech enhancement lies in
effectively informing the speech enhancement module about the features of
downstream modules. In this study, we present a novel weighting prediction
approach, which explicitly learns the task relationships from downstream
training information to address the core challenge of universal speech
enhancement. We found the role of deciding whether to employ data augmentation
techniques as crucial downstream training information. This decision
significantly impacts the expected speech and the performance of the speech
enhancement module. Moreover, we introduce a novel speech enhancement network,
the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural
network that includes the speech enhancement module, gate module, and weight
prediction module. Experimental results demonstrate that the proposed Plugin-SE
approach is competitive or superior to other joint training methods across
various downstream tasks.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12762" title="Abstract">arXiv:2402.12762</a> (cross-list from stat.ML) [<a href="/pdf/2402.12762" title="Download PDF">pdf</a>, <a href="/ps/2402.12762" title="Download PostScript">ps</a>, <a href="/format/2402.12762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning under Singularity: An Information Criterion improving WBIC and  sBIC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+L">Lirui Liu</a>, 
<a href="/search/stat?searchtype=author&query=Suzuki%2C+J">Joe Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a novel Information Criterion (IC), termed Learning under
Singularity (LS), designed to enhance the functionality of the Widely
Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian
Information Criterion (sBIC). LS is effective without regularity constraints
and demonstrates stability. Watanabe defined a statistical model or a learning
machine as regular if the mapping from a parameter to a probability
distribution is one-to-one and its Fisher information matrix is positive
definite. In contrast, models not meeting these conditions are termed singular.
Over the past decade, several information criteria for singular cases have been
proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios
but faces challenges with large sample sizes and redundant estimation of known
learning coefficients. Conversely, sBIC is limited in its broader application
due to its dependence on maximum likelihood estimates. LS addresses these
limitations by enhancing the utility of both WBIC and sBIC. It incorporates the
empirical loss from the Widely Applicable Information Criterion (WAIC) to
represent the goodness of fit to the statistical model, along with a penalty
term similar to that of sBIC. This approach offers a flexible and robust method
for model selection, free from regularity constraints.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12828" title="Abstract">arXiv:2402.12828</a> (cross-list from stat.ML) [<a href="/pdf/2402.12828" title="Download PDF">pdf</a>, <a href="/format/2402.12828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGD with Clipping is Secretly Estimating the Median Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schaipp%2C+F">Fabian Schaipp</a>, 
<a href="/search/stat?searchtype=author&query=Garrigos%2C+G">Guillaume Garrigos</a>, 
<a href="/search/stat?searchtype=author&query=Simsekli%2C+U">Umut Simsekli</a>, 
<a href="/search/stat?searchtype=author&query=Gower%2C+R">Robert Gower</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">There are several applications of stochastic optimization where one can
benefit from a robust estimate of the gradient. For example, domains such as
distributed learning with corrupted nodes, the presence of large outliers in
the training data, learning under privacy constraints, or even heavy-tailed
noise due to the dynamics of the algorithm itself. Here we study SGD with
robust gradient estimators based on estimating the median. We first consider
computing the median gradient across samples, and show that the resulting
method can converge even under heavy-tailed, state-dependent noise. We then
derive iterative methods based on the stochastic proximal point method for
computing the geometric median and generalizations thereof. Finally we propose
an algorithm estimating the median gradient across iterations, and find that
several well known methods - in particular different forms of clipping - are
particular cases of this framework.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12853" title="Abstract">arXiv:2402.12853</a> (cross-list from physics.class-ph) [<a href="/pdf/2402.12853" title="Download PDF">pdf</a>, <a href="/format/2402.12853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mechanical analogue of Faraday&#x27;s law for waves in chiral elastic media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Allison%2C+F+J+P">Finn J.P. Allison</a>, 
<a href="/search/physics?searchtype=author&query=Selsil%2C+O">Ozgur Selsil</a>, 
<a href="/search/physics?searchtype=author&query=Haslinger%2C+S+G">Stewart G. Haslinger</a>, 
<a href="/search/physics?searchtype=author&query=Movchan%2C+A+B">Alexander B. Movchan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Total Words:2412 Headers:6 Math Inline:90 Math Display:10
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Physics (physics.class-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Faraday's law on electromagnetic induction, one of the most fundamental laws
of nature, indicates that a change of magnetic field through a coil wire
induces a current in the wire. Electromagnetic induction has many paramount
technological applications today, and provides the link between electric and
magnetic fields, which is crucial to explain the existence of electromagnetic
waves. In our quest to replicate Faraday's law for mechanical systems, we
design an infinite mass-spring "helix-like structure", which consists of a
helix and a central line, and implement Bloch-Floquet conditions to obtain
travelling wave solutions to the proposed problem. The structure's geometrical
chirality is considered in conjunction with a dynamic chirality, introduced by
placing gyroscopes along its central line. It is shown that the interplay
between these two chiralities acts as a mechanical analogue of Faraday's law,
breaking the symmetry of the associated dispersion diagram.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12885" title="Abstract">arXiv:2402.12885</a> (cross-list from stat.ML) [<a href="/pdf/2402.12885" title="Download PDF">pdf</a>, <a href="/ps/2402.12885" title="Download PostScript">ps</a>, <a href="/format/2402.12885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bound on the Maximal Marginal Degrees of Freedom
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dommel%2C+P">Paul Dommel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Common kernel ridge regression is expensive in memory allocation and
computation time. This paper addresses low rank approximations and surrogates
for kernel ridge regression, which bridge these difficulties. The fundamental
contribution of the paper is a lower bound on the rank of the low dimensional
approximation, which is required such that the prediction power remains
reliable. The bound relates the effective dimension with the largest
statistical leverage score. We characterize the effective dimension and its
growth behavior with respect to the regularization parameter by involving the
regularity of the kernel. This growth is demonstrated to be asymptotically
logarithmic for suitably chosen kernels, justifying low-rank approximations as
the Nystr\"om method.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12971" title="Abstract">arXiv:2402.12971</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.12971" title="Download PDF">pdf</a>, <a href="/format/2402.12971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Temporal Unrolling Supports Neural Physics Simulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=List%2C+B">Bjoern List</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+L">Li-Wei Chen</a>, 
<a href="/search/physics?searchtype=author&query=Bali%2C+K">Kartik Bali</a>, 
<a href="/search/physics?searchtype=author&query=Thuerey%2C+N">Nils Thuerey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://ge.in.tum.de/publications/how-temporal-unrolling-supports-neural-physics-simulators/">this https URL</a> , Github Page: <a href="https://github.com/tum-pbs/unrolling">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unrolling training trajectories over time strongly influences the inference
accuracy of neural network-augmented physics simulators. We analyze these
effects by studying three variants of training neural networks on discrete
ground truth trajectories. In addition to commonly used one-step setups and
fully differentiable unrolling, we include a third, less widely used variant:
unrolling without temporal gradients. Comparing networks trained with these
three modalities makes it possible to disentangle the two dominant effects of
unrolling, training distribution shift and long-term gradients. We present a
detailed study across physical systems, network sizes, network architectures,
training setups, and test scenarios. It provides an empirical basis for our
main findings: A non-differentiable but unrolled training setup supported by a
numerical solver can yield 4.5-fold improvements over a fully differentiable
prediction setup that does not utilize this solver. We also quantify a
difference in the accuracy of models trained in a fully differentiable setup
compared to their non-differentiable counterparts. While differentiable setups
perform best, the accuracy of unrolling without temporal gradients comes
comparatively close. Furthermore, we empirically show that these behaviors are
invariant to changes in the underlying physical system, the network
architecture and size, and the numerical scheme. These results motivate
integrating non-differentiable numerical simulators into training setups even
if full differentiability is unavailable. We also observe that the convergence
rate of common neural architectures is low compared to numerical algorithms.
This encourages the use of hybrid approaches combining neural and numerical
algorithms to utilize the benefits of both.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13001" title="Abstract">arXiv:2402.13001</a> (cross-list from quant-ph) [<a href="/pdf/2402.13001" title="Download PDF">pdf</a>, <a href="/ps/2402.13001" title="Download PostScript">ps</a>, <a href="/format/2402.13001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unifying primary framework for quantum graph neural networks from  quantum graph states
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Daskin%2C+A">Ammar Daskin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> short version 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph states are used to represent mathematical graphs as quantum states on
quantum computers. They can be formulated through stabilizer codes or directly
quantum gates and quantum states. In this paper we show that a quantum graph
neural network model can be understood and realized based on graph states. We
show that they can be used either as a parameterized quantum circuits to
represent neural networks or as an underlying structure to construct graph
neural networks on quantum computers.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13005" title="Abstract">arXiv:2402.13005</a> (cross-list from eess.SP) [<a href="/pdf/2402.13005" title="Download PDF">pdf</a>, <a href="/format/2402.13005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SzCORE: A Seizure Community Open-source Research Evaluation framework  for the validation of EEG-based automated seizure detection algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dan%2C+J">Jonathan Dan</a>, 
<a href="/search/eess?searchtype=author&query=Pale%2C+U">Una Pale</a>, 
<a href="/search/eess?searchtype=author&query=Amirshahi%2C+A">Alireza Amirshahi</a>, 
<a href="/search/eess?searchtype=author&query=Cappelletti%2C+W">William Cappelletti</a>, 
<a href="/search/eess?searchtype=author&query=Ingolfsson%2C+T+M">Thorir Mar Ingolfsson</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiaying Wang</a>, 
<a href="/search/eess?searchtype=author&query=Cossettini%2C+A">Andrea Cossettini</a>, 
<a href="/search/eess?searchtype=author&query=Bernini%2C+A">Adriano Bernini</a>, 
<a href="/search/eess?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/eess?searchtype=author&query=Beniczky%2C+S">S&#xe1;ndor Beniczky</a>, 
<a href="/search/eess?searchtype=author&query=Atienza%2C+D">David Atienza</a>, 
<a href="/search/eess?searchtype=author&query=Ryvlin%2C+P">Philippe Ryvlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The need for high-quality automated seizure detection algorithms based on
electroencephalography (EEG) becomes ever more pressing with the increasing use
of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods
of these algorithms influences the reported results and makes comprehensive
evaluation and comparison challenging. This heterogeneity concerns in
particular the choice of datasets, evaluation methodologies, and performance
metrics. In this paper, we propose a unified framework designed to establish
standardization in the validation of EEG-based seizure detection algorithms.
Based on existing guidelines and recommendations, the framework introduces a
set of recommendations and standards related to datasets, file formats, EEG
data input content, seizure annotation input and output, cross-validation
strategies, and performance metrics. We also propose the 10-20 seizure
detection benchmark, a machine-learning benchmark based on public datasets
converted to a standardized format. This benchmark defines the machine-learning
task as well as reporting metrics. We illustrate the use of the benchmark by
evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure
Community Open-source Research Evaluation) framework and benchmark are made
publicly available along with an open-source software library to facilitate
research use, while enabling rigorous evaluation of the clinical significance
of the algorithms, fostering a collective effort to more optimally detect
seizures to improve the lives of people with epilepsy.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13011" title="Abstract">arXiv:2402.13011</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.13011" title="Download PDF">pdf</a>, <a href="/format/2402.13011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An evolutionary game with reputation-based imitation-mutation dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Feng%2C+K">Kehuan Feng</a>, 
<a href="/search/physics?searchtype=author&query=Han%2C+S">Songlin Han</a>, 
<a href="/search/physics?searchtype=author&query=Feng%2C+M">Minyu Feng</a>, 
<a href="/search/physics?searchtype=author&query=Szolnoki%2C+A">Attila Szolnoki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, to be published in Applied Mathematics and Computation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Computer Science and Game Theory (cs.GT); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Reputation plays a crucial role in social interactions by affecting the
fitness of individuals during an evolutionary process. Previous works have
extensively studied the result of imitation dynamics without focusing on
potential irrational choices in strategy updates. We now fill this gap and
explore the consequence of such kind of randomness, or one may interpret it as
an autonomous thinking. In particular, we study how this extended dynamics
alters the evolution of cooperation when individual reputation is directly
linked to collected payoff, hence providing a general fitness function. For a
broadly valid conclusion, our spatial populations cover different types of
interaction topologies, including lattices, small-world and scale-free graphs.
By means of intensive simulations we can detect substantial increase in
cooperation level that shows a reasonable stability in the presence of a
notable strategy mutation.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13018" title="Abstract">arXiv:2402.13018</a> (cross-list from eess.AS) [<a href="/pdf/2402.13018" title="Download PDF">pdf</a>, <a href="/format/2402.13018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMO-SUPERB: An In-depth Look at Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Haibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chou%2C+H">Huang-Cheng Chou</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Goncalves%2C+L">Lucas Goncalves</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+J">Jiawei Du</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+J+R">Jyh-Shing Roger Jang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+C">Chi-Chun Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-Yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> webpage: <a href="https://emosuperb.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Speech emotion recognition (SER) is a pivotal technology for human-computer
interaction systems. However, 80.77% of SER papers yield results that cannot be
reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal
PERformance Benchmark, which aims to enhance open-source initiatives for SER.
EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art
speech self-supervised learning models (SSLMs) for exhaustive evaluation across
six open-source SER datasets. EMO-SUPERB streamlines result sharing via an
online leaderboard, fostering collaboration within a community-driven benchmark
and thereby enhancing the development of SER. On average, 2.58% of annotations
are annotated using natural language. SER relies on classification models and
is unable to process natural languages, leading to the discarding of these
valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural
language annotations, and subsequently re-label the data. By utilizing labels
generated by ChatGPT, we consistently achieve an average relative gain of 3.08%
across all settings.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13071" title="Abstract">arXiv:2402.13071</a> (cross-list from eess.AS) [<a href="/pdf/2402.13071" title="Download PDF">pdf</a>, <a href="/format/2402.13071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codec-SUPERB: An In-Depth Analysis of Sound Codec Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Haibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+H">Ho-Lam Chung</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yi-Cheng Lin</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yuan-Kuei Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xuanjun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Pai%2C+Y">Yu-Chi Pai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsiu-Hsuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github: <a href="https://github.com/voidful/Codec-SUPERB">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The sound codec's dual roles in minimizing data transmission latency and
serving as tokenizers underscore its critical importance. Recent years have
witnessed significant developments in codec models. The ideal sound codec
should preserve content, paralinguistics, speakers, and audio information.
However, the question of which codec achieves optimal sound information
preservation remains unanswered, as in different papers, models are evaluated
on their selected experimental settings. This study introduces Codec-SUPERB, an
acronym for Codec sound processing Universal PERformance Benchmark. It is an
ecosystem designed to assess codec models across representative sound
applications and signal-level metrics rooted in sound domain
knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,
promoting collaboration within a community-driven benchmark database, thereby
stimulating new development cycles for codecs. Furthermore, we undertake an
in-depth analysis to offer insights into codec models from both application and
signal perspectives, diverging from previous codec papers mainly concentrating
on signal-level comparisons. Finally, we will release codes, the leaderboard,
and data to accelerate progress within the community.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13079" title="Abstract">arXiv:2402.13079</a> (cross-list from stat.ML) [<a href="/pdf/2402.13079" title="Download PDF">pdf</a>, <a href="/ps/2402.13079" title="Download PostScript">ps</a>, <a href="/format/2402.13079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mode Estimation with Partial Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Arnal%2C+C">Charles Arnal</a>, 
<a href="/search/stat?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/stat?searchtype=author&query=Perchet%2C+V">Vianney Perchet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Retrieval (cs.IR); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">The combination of lightly supervised pre-training and online fine-tuning has
played a key role in recent AI developments. These new learning pipelines call
for new theoretical frameworks. In this paper, we formalize core aspects of
weakly supervised and active learning with a simple problem: the estimation of
the mode of a distribution using partial feedback. We show how entropy coding
allows for optimal information acquisition from partial feedback, develop
coarse sufficient statistics for mode identification, and adapt bandit
algorithms to our new setting. Finally, we combine those contributions into a
statistically and computationally efficient solution to our problem.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13106" title="Abstract">arXiv:2402.13106</a> (cross-list from stat.ML) [<a href="/pdf/2402.13106" title="Download PDF">pdf</a>, <a href="/format/2402.13106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Generalization Bounds for Deep Compound Gaussian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lyons%2C+C">Carter Lyons</a>, 
<a href="/search/stat?searchtype=author&query=Raj%2C+R+G">Raghu G. Raj</a>, 
<a href="/search/stat?searchtype=author&query=Cheney%2C+M">Margaret Cheney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Algorithm unfolding or unrolling is the technique of constructing a deep
neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide
better interpretability and superior empirical performance over standard DNNs
in signal estimation tasks. An important theoretical question, which has only
recently received attention, is the development of generalization error bounds
for unrolled DNNs. These bounds deliver theoretical and practical insights into
the performance of a DNN on empirical datasets that are distinct from, but
sampled from, the probability density generating the DNN training data. In this
paper, we develop novel generalization error bounds for a class of unrolled
DNNs that are informed by a compound Gaussian prior. These compound Gaussian
networks have been shown to outperform comparative standard and unfolded deep
neural networks in compressive sensing and tomographic imaging problems. The
generalization error bound is formulated by bounding the Rademacher complexity
of the class of compound Gaussian network estimates with Dudley's integral.
Under realistic conditions, we show that, at worst, the generalization error
scales $\mathcal{O}(n\sqrt{\ln(n)})$ in the signal dimension and
$\mathcal{O}(($Network Size$)^{3/2})$ in network size.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13107" title="Abstract">arXiv:2402.13107</a> (cross-list from math.CO) [<a href="/pdf/2402.13107" title="Download PDF">pdf</a>, <a href="/format/2402.13107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Lower Bound on the Number of Pseudoline Arrangements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%BChnast%2C+F+C">Fernando Cort&#xe9;s K&#xfc;hnast</a>, 
<a href="/search/math?searchtype=author&query=Felsner%2C+S">Stefan Felsner</a>, 
<a href="/search/math?searchtype=author&query=Scheucher%2C+M">Manfred Scheucher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript was accepted at SoCG'24 and will be merged with Justin Dallant's manuscript "Improved Lower Bound on the Number of Pseudoline Arrangements" for the proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Arrangements of pseudolines are classic objects in discrete and computational
geometry. They have been studied with increasing intensity since their
introduction almost 100 years ago. The study of the number $B_n$ of
non-isomorphic simple arrangements of $n$ pseudolines goes back to Goodman and
Pollack, Knuth, and others. It is known that $B_n$ is in the order of
$2^{\Theta(n^2)}$ and finding asymptotic bounds on $b_n =
\frac{\log_2(B_n)}{n^2}$ remains a challenging task. In 2011, Felsner and Valtr
showed that $0.1887 \leq b_n \le 0.6571$ for sufficiently large $n$. The upper
bound remains untouched but in 2020 Dumitrescu and Mandal improved the lower
bound constant to $0.2083$. Their approach utilizes the known values of $B_n$
for up to $n=12$.
<br />We tackle the lower bound with a dynamic programming scheme. Our new bound is
$b_n \geq 0.2526$ for sufficiently large $n$. The result is based on a delicate
interplay of theoretical ideas and computer assistance.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13110" title="Abstract">arXiv:2402.13110</a> (cross-list from eess.SP) [<a href="/pdf/2402.13110" title="Download PDF">pdf</a>, <a href="/format/2402.13110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for  In-Air Acoustic Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laurijssen%2C+D">Dennis Laurijssen</a>, 
<a href="/search/eess?searchtype=author&query=Daems%2C+W">Walter Daems</a>, 
<a href="/search/eess?searchtype=author&query=Steckel%2C+J">Jan Steckel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Airborne 3D imaging using ultrasound is a promising sensing modality for
robotic applications in harsh environments. Over the last decade, several
high-performance systems have been proposed in the literature. Most of these
sensors use a reduced aperture microphone array, leading to artifacts in the
resulting acoustic images. This paper presents a novel in-air ultrasound sensor
that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,
in combination with a distributed embedded hardware design to perform the data
acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)
beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able
to create both 2D and 3D ultrasound images of the full-frontal hemisphere with
high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper
describes both the hardware infrastructure needed to obtain such highly
detailed acoustical images, as well as the signal processing chain needed to
convert the raw acoustic data into said images. Utilizing this novel
high-resolution ultrasound imaging sensor, we wish to investigate the limits of
both passive and active airborne ultrasound sensing by utilizing this virtually
artifact-free imaging modality.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13136" title="Abstract">arXiv:2402.13136</a> (cross-list from quant-ph) [<a href="/pdf/2402.13136" title="Download PDF">pdf</a>, <a href="/format/2402.13136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxing Trust Assumptions on Quantum Key Distribution Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Vyas%2C+N">Nilesh Vyas</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mendes%2C+P">Paulo Mendes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Quantum security over long distances with un- trusted relays is largely
unfounded and is still an open ques- tion for active research. Nevertheless,
quantum networks based on trusted relays are being built across the globe.
However, standard QKD network architecture implores a complete trust
requirement on QKD relays, which is too demanding and limits the use cases for
QKD networks. In this work, we explore the possibility to securely relay a
secret in a QKD network by relaxing the trust assumptions (if not completely)
on the relay. We characterize QKD relays with different trust levels, namely,
Full Access Trust (FAT), Partial Access Trust (PAT), and No Access Trust (NAT).
As the name suggests, each level defines the degree with which a relay is
required to be trusted with the secret provided by the key management system
for end- to-end communication. We then review and propose multiple
constructions of the QKD key management system based on the different trust
levels. Main contribution of the paper is realized by evaluating key management
systems with no access trust level. In principle, we review key management with
centralized topology and propose a new decentralized key management system.
These different topologies provide various advantages based on the QKD network
requirements, allowing an operational flexibility in the architecture. We
believe this work presents a new perspective to the open problem of providing a
confiding and a practical solution for future long range secure communications
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13192" title="Abstract">arXiv:2402.13192</a> (cross-list from math.PR) [<a href="/pdf/2402.13192" title="Download PDF">pdf</a>, <a href="/format/2402.13192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial Queues with Nearest Neighbour Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kumar%2C+B+R+V">B. R. Vinay Kumar</a>, 
<a href="/search/math?searchtype=author&query=Leskel%C3%A4%2C+L">Lasse Leskel&#xe4;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A part of this work was accepted to the conference International Teletraffic Congress (ITC 35) held between 3--5 October 2023 in Turin, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Performance (cs.PF)

</div>
<p class="mathjax">In this work we study multi-server queues on a Euclidean space. Consider $N$
servers that are distributed uniformly in $[0,1]^d$. Customers (users) arrive
at the servers according to independent Poisson processes of intensity
$\lambda$. However, they probabilistically decide whether to join the queue
they arrived at, or move to one of the nearest neighbours. The strategy
followed by the customers affects the load on the servers in the long run. In
this paper, we are interested in characterizing the fraction of servers that
bear a larger load as compared to when the users do not follow any strategy,
i.e., they join the queue they arrive at. These are called overloaded servers.
In the one-dimensional case ($d=1$), we evaluate the expected fraction of
overloaded servers for any finite $N$ when the users follow probabilistic
nearest neighbour shift strategies. Additionally, for servers distributed in a
$d$-dimensional space we provide expressions for the fraction of overloaded
servers in the system as the total number of servers $N\rightarrow \infty$.
Numerical experiments are provided to support our claims. Typical applications
of our results include electric vehicles queueing at charging stations, and
queues in airports or supermarkets.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13194" title="Abstract">arXiv:2402.13194</a> (cross-list from quant-ph) [<a href="/pdf/2402.13194" title="Download PDF">pdf</a>, <a href="/format/2402.13194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Wiretap Channel Coding Assisted by Noisy Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cai%2C+M">Minglai Cai</a>, 
<a href="/search/quant-ph?searchtype=author&query=Winter%2C+A">Andreas Winter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We consider the private classical capacity of a quantum wiretap channel,
where the users (sender Alice, receiver Bob, and eavesdropper Eve) have access
to the resource of a shared quantum state, additionally to their channel inputs
and outputs. An extreme case is maximal entanglement or a secret key between
Alice and Bob, both of which would allow for onetime padding the message. But
here both the wiretap channel and the shared state are general. In the other
extreme case that the state is trivial, we recover the wiretap channel and its
private capacity [N. Cai, A. Winter and R. W. Yeung, Probl. Inform. Transm.
40(4):318-336, 2004]. We show how to use the given resource state to build a
code for secret classical communication. Our main result is a lower bound on
the assisted private capacity, which asymptotically meets the multi-letter
converse and which encompasses all sorts of previous results as special cases.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13199" title="Abstract">arXiv:2402.13199</a> (cross-list from eess.AS) [<a href="/pdf/2402.13199" title="Download PDF">pdf</a>, <a href="/format/2402.13199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target Speech Extraction with Pre-trained Self-supervised Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+J">Junyi Peng</a>, 
<a href="/search/eess?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/eess?searchtype=author&query=Ochiai%2C+T">Tsubasa Ochiai</a>, 
<a href="/search/eess?searchtype=author&query=Plchot%2C+O">Oldrich Plchot</a>, 
<a href="/search/eess?searchtype=author&query=Araki%2C+S">Shoko Araki</a>, 
<a href="/search/eess?searchtype=author&query=Cernocky%2C+J">Jan Cernocky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Pre-trained self-supervised learning (SSL) models have achieved remarkable
success in various speech tasks. However, their potential in target speech
extraction (TSE) has not been fully exploited. TSE aims to extract the speech
of a target speaker in a mixture guided by enrollment utterances. We exploit
pre-trained SSL models for two purposes within a TSE framework, i.e., to
process the input mixture and to derive speaker embeddings from the enrollment.
In this paper, we focus on how to effectively use SSL models for TSE. We first
introduce a novel TSE downstream task following the SUPERB principles. This
simple experiment shows the potential of SSL models for TSE, but extraction
performance remains far behind the state-of-the-art. We then extend a powerful
TSE architecture by incorporating two SSL-based modules: an Adaptive Input
Enhancer (AIE) and a speaker encoder. Specifically, the proposed AIE utilizes
intermediate representations from the CNN encoder by adjusting the time
resolution of CNN encoder and transformer blocks through progressive
upsampling, capturing both fine-grained and hierarchical features. Our method
outperforms current TSE systems achieving a SI-SDR improvement of 14.0 dB on
LibriMix. Moreover, we can further improve performance by 0.7 dB by fine-tuning
the whole model including the SSL model parameters.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13200" title="Abstract">arXiv:2402.13200</a> (cross-list from eess.AS) [<a href="/pdf/2402.13200" title="Download PDF">pdf</a>, <a href="/format/2402.13200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Self-supervised Learning Models with Target Speech Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+J">Junyi Peng</a>, 
<a href="/search/eess?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/eess?searchtype=author&query=Ochiai%2C+T">Tsubasa Ochiai</a>, 
<a href="/search/eess?searchtype=author&query=Plchot%2C+O">Oldrich Plchot</a>, 
<a href="/search/eess?searchtype=author&query=Ashihara%2C+T">Takanori Ashihara</a>, 
<a href="/search/eess?searchtype=author&query=Araki%2C+S">Shoko Araki</a>, 
<a href="/search/eess?searchtype=author&query=Cernocky%2C+J">Jan Cernocky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024, Self-supervision in Audio, Speech, and Beyond (SASB) workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Large-scale pre-trained self-supervised learning (SSL) models have shown
remarkable advancements in speech-related tasks. However, the utilization of
these models in complex multi-talker scenarios, such as extracting a target
speaker in a mixture, is yet to be fully evaluated. In this paper, we introduce
target speech extraction (TSE) as a novel downstream task to evaluate the
feature extraction capabilities of pre-trained SSL models. TSE uniquely
requires both speaker identification and speech separation, distinguishing it
from other tasks in the Speech processing Universal PERformance Benchmark
(SUPERB) evaluation. Specifically, we propose a TSE downstream model composed
of two lightweight task-oriented modules based on the same frozen SSL model.
One module functions as a speaker encoder to obtain target speaker information
from an enrollment speech, while the other estimates the target speaker's mask
to extract its speech from the mixture. Experimental results on the Libri2mix
datasets reveal the relevance of the TSE downstream task to probe SSL models,
as its performance cannot be simply deduced from other related tasks such as
speaker verification and separation.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13224" title="Abstract">arXiv:2402.13224</a> (cross-list from math.OC) [<a href="/pdf/2402.13224" title="Download PDF">pdf</a>, <a href="/format/2402.13224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Large Electric Vehicle Charging Stations via User Behavior  Modeling and Stochastic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Puech%2C+A">Alban Puech</a>, 
<a href="/search/math?searchtype=author&query=Rigaut%2C+T">Tristan Rigaut</a>, 
<a href="/search/math?searchtype=author&query=Templier%2C+W">William Templier</a>, 
<a href="/search/math?searchtype=author&query=Tournoud%2C+M">Maud Tournoud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces an Electric Vehicle Charging Station (EVCS) model that
incorporates real-world constraints, such as slot power limitations, contract
threshold overruns penalties, or early disconnections of electric vehicles
(EVs). We propose a formulation of the problem of EVCS control under
uncertainty, and implement two Multi-Stage Stochastic Programming approaches
that leverage user-provided information, namely, Model Predictive Control and
Two-Stage Stochastic Programming. The model addresses uncertainties in charging
session start and end times, as well as in energy demand. A user's behavior
model based on a sojourn-time-dependent stochastic process enhances cost
reduction while maintaining customer satisfaction. The benefits of the two
proposed methods are showcased against two baselines over a 22-day simulation
using a real-world dataset. The two-stage approach proves robust against early
disconnections, considering a more significant number of uncertainty scenarios
for optimization. The algorithm prioritizing user satisfaction over electricity
cost achieves a 20% and 36% improvement in two user satisfaction metrics
compared to an industry-standard baseline. Additionally, the algorithm striking
the best balance between cost and user satisfaction exhibits a mere 3% relative
cost increase compared to the theoretically optimal baseline - for which the
nonanticipativity constraint is relaxed - while attaining 94% and 84% of the
user satisfaction performance in the two used satisfaction metrics.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13226" title="Abstract">arXiv:2402.13226</a> (cross-list from eess.IV) [<a href="/pdf/2402.13226" title="Download PDF">pdf</a>, <a href="/format/2402.13226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRF Solves Undersampled MRI Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jang%2C+T+J">Tae Jun Jang</a>, 
<a href="/search/eess?searchtype=author&query=Hyun%2C+C+M">Chang Min Hyun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Signal Processing (eess.SP)

</div>
<p class="mathjax">This article presents a novel undersampled magnetic resonance imaging (MRI)
technique that leverages the concept of Neural Radiance Field (NeRF). With
radial undersampling, the corresponding imaging problem can be reformulated
into an image modeling task from sparse-view rendered data; therefore, a high
dimensional MR image is obtainable from undersampled $k$-space data by taking
advantage of implicit neural representation. A multi-layer perceptron, which is
designed to output an image intensity from a spatial coordinate, learns the MR
physics-driven rendering relation between given measurement data and desired
image. Effective undersampling strategies for high-quality neural
representation are investigated. The proposed method serves two benefits: (i)
The learning is based fully on single undersampled $k$-space data, not a bunch
of measured data and target image sets. It can be used potentially for
diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively
rare or limited against diversity of clinical images while undersampled
reconstruction is highly demanded. (ii) A reconstructed MR image is a
scan-specific representation highly adaptive to the given $k$-space
measurement. Numerous experiments validate the feasibility and capability of
the proposed approach.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13236" title="Abstract">arXiv:2402.13236</a> (cross-list from eess.AS) [<a href="/pdf/2402.13236" title="Download PDF">pdf</a>, <a href="/format/2402.13236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards audio language modeling - an overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Haibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xuanjun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yi-Cheng Lin</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+H">Ho-Lam Chung</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Neural audio codecs are initially introduced to compress audio data into
compact codes to reduce transmission latency. Researchers recently discovered
the potential of codecs as suitable tokenizers for converting continuous audio
into discrete codes, which can be employed to develop audio language models
(LMs). Numerous high-performance neural audio codecs and codec-based LMs have
been developed. The paper aims to provide a thorough and systematic overview of
the neural audio codec models and codec-based LMs.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13246" title="Abstract">arXiv:2402.13246</a> (cross-list from math.AG) [<a href="/pdf/2402.13246" title="Download PDF">pdf</a>, <a href="/format/2402.13246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game theory of undirected graphical models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Portakal%2C+I">Irem Portakal</a>, 
<a href="/search/math?searchtype=author&query=Sendra-Arranz%2C+J">Javier Sendra-Arranz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We model an $n$-player game $X$ in normal form via undirected discrete
graphical models where the discrete random variables represent the players and
their state spaces are the set of pure strategies. There exists an edge between
the vertices of the graphical model $G$ whenever there is a dependency between
the associated players. We study the Spohn conditional independence (CI)
variety $\mathcal{V}_{X,\mathcal{C}}$, which is the intersection of the
independence model $\mathcal{M}_{\text{global}(G)}$ with the Spohn variety of
the game $X$. We prove a conjecture by the first author and Sturmfels that
$\mathcal{V}_{X,\mathcal{C}}$ is of codimension $n$ in
$\mathcal{M}_{\mathcal{C}}$ for a generic game $X$ with binary choices. In the
case where the undirected graph is a disjoint union of cliques, we analyze
certain algebro-geometric features of Spohn CI varieties and prove affine
universality theorems.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed, 21 Feb 24</h3>
<dl>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1808.10024" title="Abstract">arXiv:1808.10024</a> (replaced) [<a href="/pdf/1808.10024" title="Download PDF">pdf</a>, <a href="/format/1808.10024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hard Non-Monotonic Attention for Character-Level Transduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shijie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+P">Pamela Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EMNLP 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1905.06319" title="Abstract">arXiv:1905.06319</a> (replaced) [<a href="/pdf/1905.06319" title="Download PDF">pdf</a>, <a href="/format/1905.06319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Hard Monotonic Attention for Character-Level Transduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shijie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.11643" title="Abstract">arXiv:2007.11643</a> (replaced) [<a href="/pdf/2007.11643" title="Download PDF">pdf</a>, <a href="/format/2007.11643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-Efficient Graph Kernelizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kammer%2C+F">Frank Kammer</a>, 
<a href="/search/cs?searchtype=author&query=Sajenko%2C+A">Andrej Sajenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.01947" title="Abstract">arXiv:2009.01947</a> (replaced) [<a href="/pdf/2009.01947" title="Download PDF">pdf</a>, <a href="/format/2009.01947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical and Parallelizable Algorithms for Non-Monotone Submodular  Maximization with Size Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kuhnle%2C+A">Alan Kuhnle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Artificial Intelligence Research 79 (2024) 599-638
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.11880" title="Abstract">arXiv:2010.11880</a> (replaced) [<a href="/pdf/2010.11880" title="Download PDF">pdf</a>, <a href="/format/2010.11880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Approximate CoSimRanks via Random Projections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Renchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiaokui Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> full version of the paper titled "Fast Approximate All Pairwise CoSimRanks via Random Projection" published at WISE 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.03396" title="Abstract">arXiv:2011.03396</a> (replaced) [<a href="/pdf/2011.03396" title="Download PDF">pdf</a>, <a href="/format/2011.03396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes Security: A Not So Average Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzikokolakis%2C+K">Konstantinos Chatzikokolakis</a>, 
<a href="/search/cs?searchtype=author&query=Cherubin%2C+G">Giovanni Cherubin</a>, 
<a href="/search/cs?searchtype=author&query=Palamidessi%2C+C">Catuscia Palamidessi</a>, 
<a href="/search/cs?searchtype=author&query=Troncoso%2C+C">Carmela Troncoso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.03865" title="Abstract">arXiv:2011.03865</a> (replaced) [<a href="/pdf/2011.03865" title="Download PDF">pdf</a>, <a href="/format/2011.03865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorial Bernoulli Factories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niazadeh%2C+R">Rad Niazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Leme%2C+R+P">Renato Paes Leme</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jon Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary conference in Proceeding of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC'21)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Niazadeh, R., Paes Leme, R. and Schneider, J., 2023. Combinatorial
  Bernoulli factories. Bernoulli, 29(2), pp.1246-1274
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Computer Science and Game Theory (cs.GT); Combinatorics (math.CO); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03907" title="Abstract">arXiv:2106.03907</a> (replaced) [<a href="/pdf/2106.03907" title="Download PDF">pdf</a>, <a href="/format/2106.03907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Proxy Causal Learning and its Application to Confounded Bandit  Policy Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kanagawa%2C+H">Heishiro Kanagawa</a>, 
<a href="/search/cs?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2010.07154">arXiv:2010.07154</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.03445" title="Abstract">arXiv:2109.03445</a> (replaced) [<a href="/pdf/2109.03445" title="Download PDF">pdf</a>, <a href="/ps/2109.03445" title="Download PostScript">ps</a>, <a href="/format/2109.03445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Batch Asynchronous Stochastic Approximation With  Applications to Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Karandikar%2C+R+L">Rajeeva L. Karandikar</a>, 
<a href="/search/stat?searchtype=author&query=Vidyasagar%2C+M">M. Vidyasagar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.09299" title="Abstract">arXiv:2109.09299</a> (replaced) [<a href="/pdf/2109.09299" title="Download PDF">pdf</a>, <a href="/format/2109.09299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Dense Keypoints Using Unlabeled Multiview Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhixuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haozheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Long Sha</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+S">Sujoy Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H+S">Hyun Soo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08785" title="Abstract">arXiv:2111.08785</a> (replaced) [<a href="/pdf/2111.08785" title="Download PDF">pdf</a>, <a href="/ps/2111.08785" title="Download PostScript">ps</a>, <a href="/format/2111.08785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting AutoAttack Perturbations in the Frequency Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+P">Peter Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Harder%2C+P">Paula Harder</a>, 
<a href="/search/cs?searchtype=author&query=Strassel%2C+D">Dominik Strassel</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+M">Margret Keuper</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+J">Janis Keuper</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at ICML 2021 workshop for robustness
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.01601" title="Abstract">arXiv:2112.01601</a> (replaced) [<a href="/pdf/2112.01601" title="Download PDF">pdf</a>, <a href="/format/2112.01601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is RobustBench/AutoAttack a suitable Benchmark for Adversarial  Robustness?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+P">Peter Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Strassel%2C+D">Dominik Strassel</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+M">Margret Keuper</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+J">Janis Keuper</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI-22 AdvML Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11720" title="Abstract">arXiv:2201.11720</a> (replaced) [<a href="/pdf/2201.11720" title="Download PDF">pdf</a>, <a href="/format/2201.11720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplicial Convolutional Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/eess?searchtype=author&query=Isufi%2C+E">Elvin Isufi</a>, 
<a href="/search/eess?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/eess?searchtype=author&query=Leus%2C+G">Geert Leus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Algebraic Topology (math.AT); Spectral Theory (math.SP)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.00156" title="Abstract">arXiv:2203.00156</a> (replaced) [<a href="/pdf/2203.00156" title="Download PDF">pdf</a>, <a href="/format/2203.00156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preemptive Motion Planning for Human-to-Robot Indirect Placement  Handovers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">Mohammad Khalid Jawed</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+J">Jungseock Joo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Robotics and Automation (ICRA 2022). Supplementary videos: <a href="https://pmp-human-to-robot.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10309" title="Abstract">arXiv:2205.10309</a> (replaced) [<a href="/pdf/2205.10309" title="Download PDF">pdf</a>, <a href="/format/2205.10309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fully Implicit Method for Robust Frictional Contact Handling in  Elastic Rods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+D">Dezhong Tong</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+J">Jungseock Joo</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">M. Khalid Jawed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extreme Mechanics Letters (EML 2023). First two authors have equal contribution. A video summarizing this work is available on YouTube: <a href="https://youtu.be/g0rlCFfWJ8U">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12944" title="Abstract">arXiv:2205.12944</a> (replaced) [<a href="/pdf/2205.12944" title="Download PDF">pdf</a>, <a href="/format/2205.12944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning in Mean Field Games: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lauri%C3%A8re%2C+M">Mathieu Lauri&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Perrin%2C+S">Sarah Perrin</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rolat%2C+J">Julien P&#xe9;rolat</a>, 
<a href="/search/cs?searchtype=author&query=Girgin%2C+S">Sertan Girgin</a>, 
<a href="/search/cs?searchtype=author&query=Muller%2C+P">Paul Muller</a>, 
<a href="/search/cs?searchtype=author&query=%C3%89lie%2C+R">Romuald &#xc9;lie</a>, 
<a href="/search/cs?searchtype=author&query=Geist%2C+M">Matthieu Geist</a>, 
<a href="/search/cs?searchtype=author&query=Pietquin%2C+O">Olivier Pietquin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00708" title="Abstract">arXiv:2206.00708</a> (replaced) [<a href="/pdf/2206.00708" title="Download PDF">pdf</a>, <a href="/format/2206.00708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Boundary Element Method for Acoustic Transmission with Nonconforming  Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=van+%27t+Wout%2C+E">Elwin van &#x27;t Wout</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02911" title="Abstract">arXiv:2206.02911</a> (replaced) [<a href="/pdf/2206.02911" title="Download PDF">pdf</a>, <a href="/format/2206.02911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Boundary Value and Optimal Control Problems on Graphs: A Neural  and Numerical Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garrousian%2C+M">Mehdi Garrousian</a>, 
<a href="/search/cs?searchtype=author&query=Nouranizadeh%2C+A">Amirhossein Nouranizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09514" title="Abstract">arXiv:2206.09514</a> (replaced) [<a href="/pdf/2206.09514" title="Download PDF">pdf</a>, <a href="/format/2206.09514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ethics in AI through the Practitioner&#x27;s View: A Grounded Theory  Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pant%2C+A">Aastha Pant</a>, 
<a href="/search/cs?searchtype=author&query=Hoda%2C+R">Rashina Hoda</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Turhan%2C+B">Burak Turhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.10477" title="Abstract">arXiv:2206.10477</a> (replaced) [<a href="/pdf/2206.10477" title="Download PDF">pdf</a>, <a href="/format/2206.10477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survival Kernets: Scalable and Interpretable Deep Kernel Survival  Analysis with an Accuracy Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+H">George H. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Journal of Machine Learning Research; compared to the previous arXiv version, this draft includes some minor clarifications/edits
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07654" title="Abstract">arXiv:2207.07654</a> (replaced) [<a href="/pdf/2207.07654" title="Download PDF">pdf</a>, <a href="/format/2207.07654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning inducing points and uncertainty on molecular data by scalable  variational Gaussian processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Tsitsvero%2C+M">Mikhail Tsitsvero</a>, 
<a href="/search/physics?searchtype=author&query=Jin%2C+M">Mingoo Jin</a>, 
<a href="/search/physics?searchtype=author&query=Lyalin%2C+A">Andrey Lyalin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08061" title="Abstract">arXiv:2207.08061</a> (replaced) [<a href="/pdf/2207.08061" title="Download PDF">pdf</a>, <a href="/format/2207.08061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Computation in Leaderless and Multi-Leader Disconnected  Anonymous Dynamic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Luna%2C+G+A">Giuseppe A. Di Luna</a>, 
<a href="/search/cs?searchtype=author&query=Viglietta%2C+G">Giovanni Viglietta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 1 figure. arXiv admin note: substantial text overlap with <a href="/abs/2204.02128">arXiv:2204.02128</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14016" title="Abstract">arXiv:2207.14016</a> (replaced) [<a href="/pdf/2207.14016" title="Download PDF">pdf</a>, <a href="/format/2207.14016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascades towards noise-induced transitions on networks revealed using  information flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Elteren%2C+C">Casper van Elteren</a>, 
<a href="/search/cs?searchtype=author&query=Quax%2C+R">Rick Quax</a>, 
<a href="/search/cs?searchtype=author&query=Sloot%2C+P">Peter Sloot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Should contain color
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05360" title="Abstract">arXiv:2208.05360</a> (replaced) [<a href="/pdf/2208.05360" title="Download PDF">pdf</a>, <a href="/format/2208.05360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BehaVerify: Verifying Temporal Logic Specifications for Behavior Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serbinowska%2C+S+S">Serena S. Serbinowska</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+T+T">Taylor T. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02767" title="Abstract">arXiv:2209.02767</a> (replaced) [<a href="/pdf/2209.02767" title="Download PDF">pdf</a>, <a href="/format/2209.02767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separators in Continuous Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blondin%2C+M">Michael Blondin</a>, 
<a href="/search/cs?searchtype=author&query=Esparza%2C+J">Javier Esparza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extension of the FoSSaCS'22 conference version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05550" title="Abstract">arXiv:2209.05550</a> (replaced) [<a href="/pdf/2209.05550" title="Download PDF">pdf</a>, <a href="/ps/2209.05550" title="Download PostScript">ps</a>, <a href="/format/2209.05550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Framework for Online Social Media Auditing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huleihel%2C+W">Wasim Huleihel</a>, 
<a href="/search/cs?searchtype=author&query=Refael%2C+Y">Yehonathan Refael</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08167" title="Abstract">arXiv:2209.08167</a> (replaced) [<a href="/pdf/2209.08167" title="Download PDF">pdf</a>, <a href="/format/2209.08167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cherrat%2C+E+A">El Amine Cherrat</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kerenidis%2C+I">Iordanis Kerenidis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mathur%2C+N">Natansh Mathur</a>, 
<a href="/search/quant-ph?searchtype=author&query=Landman%2C+J">Jonas Landman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Strahm%2C+M">Martin Strahm</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Y+Y">Yun Yvonna Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13664" title="Abstract">arXiv:2210.13664</a> (replaced) [<a href="/pdf/2210.13664" title="Download PDF">pdf</a>, <a href="/format/2210.13664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher  Mixture Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conti%2C+J">Jean-R&#xe9;my Conti</a>, 
<a href="/search/cs?searchtype=author&query=Noiry%2C+N">Nathan Noiry</a>, 
<a href="/search/cs?searchtype=author&query=Despiegel%2C+V">Vincent Despiegel</a>, 
<a href="/search/cs?searchtype=author&query=Gentric%2C+S">St&#xe9;phane Gentric</a>, 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9men%C3%A7on%2C+S">St&#xe9;phan Cl&#xe9;men&#xe7;on</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:4344-4369, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08854" title="Abstract">arXiv:2211.08854</a> (replaced) [<a href="/pdf/2211.08854" title="Download PDF">pdf</a>, <a href="/format/2211.08854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Filters for Signal Processing and Machine Learning on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Isufi%2C+E">Elvin Isufi</a>, 
<a href="/search/eess?searchtype=author&query=Gama%2C+F">Fernando Gama</a>, 
<a href="/search/eess?searchtype=author&query=Shuman%2C+D+I">David I. Shuman</a>, 
<a href="/search/eess?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12891" title="Abstract">arXiv:2211.12891</a> (replaced) [<a href="/pdf/2211.12891" title="Download PDF">pdf</a>, <a href="/ps/2211.12891" title="Download PostScript">ps</a>, <a href="/format/2211.12891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Sensing and Communication: Joint Pilot and Transmission  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+M">Meng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jamalipour%2C+A">Abbas Jamalipour</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Celimuge Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This papar answers the optimal space code-time design for supporting ISAC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14221" title="Abstract">arXiv:2211.14221</a> (replaced) [<a href="/pdf/2211.14221" title="Download PDF">pdf</a>, <a href="/format/2211.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Large Causal Structures from Inverse Covariance Matrix via  Sparse Matrix Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Uemura%2C+K">Kento Uemura</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+A">Akito Fujii</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shuang Chang</a>, 
<a href="/search/cs?searchtype=author&query=Koyanagi%2C+Y">Yusuke Koyanagi</a>, 
<a href="/search/cs?searchtype=author&query=Maruhashi%2C+K">Koji Maruhashi</a>, 
<a href="/search/cs?searchtype=author&query=Sebag%2C+M">Mich&#xe8;le Sebag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03553" title="Abstract">arXiv:2212.03553</a> (replaced) [<a href="/pdf/2212.03553" title="Download PDF">pdf</a>, <a href="/format/2212.03553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressiveness of SHACL Features and Extensions for Full Equality and  Disjointness Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogaerts%2C+B">Bart Bogaerts</a>, 
<a href="/search/cs?searchtype=author&query=Jakubowski%2C+M">Maxime Jakubowski</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Bussche%2C+J">Jan Van den Bussche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06251" title="Abstract">arXiv:2212.06251</a> (replaced) [<a href="/pdf/2212.06251" title="Download PDF">pdf</a>, <a href="/format/2212.06251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoregressive Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacchiocchi%2C+F">Francesco Bacchiocchi</a>, 
<a href="/search/cs?searchtype=author&query=Genalti%2C+G">Gianmarco Genalti</a>, 
<a href="/search/cs?searchtype=author&query=Maran%2C+D">Davide Maran</a>, 
<a href="/search/cs?searchtype=author&query=Mussi%2C+M">Marco Mussi</a>, 
<a href="/search/cs?searchtype=author&query=Restelli%2C+M">Marcello Restelli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06776" title="Abstract">arXiv:2212.06776</a> (replaced) [<a href="/pdf/2212.06776" title="Download PDF">pdf</a>, <a href="/format/2212.06776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+P">Peter Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+M">Margret Keuper</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+J">Janis Keuper</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at VISAPP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10049" title="Abstract">arXiv:2212.10049</a> (replaced) [<a href="/pdf/2212.10049" title="Download PDF">pdf</a>, <a href="/format/2212.10049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OBMO: One Bounding Box Multiple Objects for Monocular 3D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chenxi Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haidong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binbin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01968" title="Abstract">arXiv:2301.01968</a> (replaced) [<a href="/pdf/2301.01968" title="Download PDF">pdf</a>, <a href="/format/2301.01968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Neural Force Manifolds for Sim2Real Robotic Symmetrical Paper  Folding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+D">Dezhong Tong</a>, 
<a href="/search/cs?searchtype=author&query=Terzopoulos%2C+D">Demetri Terzopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+J">Jungseock Joo</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">M. Khalid Jawed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Automation Science and Engineering (T-ASE 2024). First two authors have equal contribution. Supplementary video is available on YouTube: <a href="https://youtu.be/k0nexYGy-P4">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02181" title="Abstract">arXiv:2302.02181</a> (replaced) [<a href="/pdf/2302.02181" title="Download PDF">pdf</a>, <a href="/format/2302.02181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Stitching and Visualization How GAN Generators can Invert Networks  in Real-Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herdt%2C+R">Rudolf Herdt</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+M">Maximilian Schmidt</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Baguer%2C+D+O">Daniel Otero Baguer</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Arrastia%2C+J+L">Jean Le&#x27;Clerc Arrastia</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Maass%2C+P">Peter Maass</a> (1 and 2) ((1) University of Bremen, (2) aisencia)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08766" title="Abstract">arXiv:2302.08766</a> (replaced) [<a href="/pdf/2302.08766" title="Download PDF">pdf</a>, <a href="/format/2302.08766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dagr%C3%A9ou%2C+M">Mathieu Dagr&#xe9;ou</a>, 
<a href="/search/stat?searchtype=author&query=Moreau%2C+T">Thomas Moreau</a>, 
<a href="/search/stat?searchtype=author&query=Vaiter%2C+S">Samuel Vaiter</a>, 
<a href="/search/stat?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09444" title="Abstract">arXiv:2302.09444</a> (replaced) [<a href="/pdf/2302.09444" title="Download PDF">pdf</a>, <a href="/format/2302.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mBEST: Realtime Deformable Linear Object Detection Through Minimal  Bending Energy Skeleton Pixel Traversals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+D">Dezhong Tong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Brian Park</a>, 
<a href="/search/cs?searchtype=author&query=Terzopoulos%2C+D">Demetri Terzopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+J">Jungseock Joo</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">Mohammad Khalid Jawed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Robotics and Automation Letters (RA-L 2023). YouTube video: <a href="https://youtu.be/q84I9i0DOK4">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01074" title="Abstract">arXiv:2303.01074</a> (replaced) [<a href="/pdf/2303.01074" title="Download PDF">pdf</a>, <a href="/format/2303.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning not to Regret
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sychrovsk%C3%BD%2C+D">David Sychrovsk&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0ustr%2C+M">Michal &#x160;ustr</a>, 
<a href="/search/cs?searchtype=author&query=Davoodi%2C+E">Elnaz Davoodi</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+M">Martin Schmid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02898" title="Abstract">arXiv:2303.02898</a> (replaced) [<a href="/pdf/2303.02898" title="Download PDF">pdf</a>, <a href="/format/2303.02898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics  at Single-Precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zheng%2C+C">Candi Zheng</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+W">Wang Yang</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+S">Shiyi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04795" title="Abstract">arXiv:2303.04795</a> (replaced) [<a href="/pdf/2303.04795" title="Download PDF">pdf</a>, <a href="/format/2303.04795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilized profunctors and stable species of structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiore%2C+M">Marcelo Fiore</a>, 
<a href="/search/cs?searchtype=author&query=Galal%2C+Z">Zeinab Galal</a>, 
<a href="/search/cs?searchtype=author&query=Paquet%2C+H">Hugo Paquet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FSCD 2022 special issue of Logical Methods in Computer Science, minor changes (LMCS formatting)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05789" title="Abstract">arXiv:2303.05789</a> (replaced) [<a href="/pdf/2303.05789" title="Download PDF">pdf</a>, <a href="/format/2303.05789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnoMalNet: Outlier Detection based Malaria Cell Image Classification  Method Leveraging Deep Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huq%2C+A">Aminul Huq</a>, 
<a href="/search/eess?searchtype=author&query=Reza%2C+M+T">Md Tanzim Reza</a>, 
<a href="/search/eess?searchtype=author&query=Hossain%2C+S">Shahriar Hossain</a>, 
<a href="/search/eess?searchtype=author&query=Dipto%2C+S+M">Shakib Mahmud Dipto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and Published at International Journal of Reconfigurable and Embedded Systems (IJRES)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Reconfigurable and Embedded Systems
  (IJRES), Vol 13, No 1: March 2024, P. 171-178
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07923" title="Abstract">arXiv:2303.07923</a> (replaced) [<a href="/pdf/2303.07923" title="Download PDF">pdf</a>, <a href="/format/2303.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPT Constant-Approximations for Capacitated Clustering to Minimize the  Sum of Cluster Radii
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bandyapadhyay%2C+S">Sayan Bandyapadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Lochet%2C+W">William Lochet</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+S">Saket Saurabh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version: fix an error in the proof of Lemma 2.5
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08485" title="Abstract">arXiv:2303.08485</a> (replaced) [<a href="/pdf/2303.08485" title="Download PDF">pdf</a>, <a href="/format/2303.08485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Fairness be Automated? Guidelines and Opportunities for  Fairness-aware AutoML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weerts%2C+H">Hilde Weerts</a>, 
<a href="/search/cs?searchtype=author&query=Pfisterer%2C+F">Florian Pfisterer</a>, 
<a href="/search/cs?searchtype=author&query=Feurer%2C+M">Matthias Feurer</a>, 
<a href="/search/cs?searchtype=author&query=Eggensperger%2C+K">Katharina Eggensperger</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+E">Edward Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Awad%2C+N">Noor Awad</a>, 
<a href="/search/cs?searchtype=author&query=Vanschoren%2C+J">Joaquin Vanschoren</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Artificial Intelligence Research 79 (2024) 639-677
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09205" title="Abstract">arXiv:2303.09205</a> (replaced) [<a href="/pdf/2303.09205" title="Download PDF">pdf</a>, <a href="/format/2303.09205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing bias in online selection with limited budget of comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benomar%2C+Z">Ziyad Benomar</a>, 
<a href="/search/cs?searchtype=author&query=Chzhen%2C+E">Evgenii Chzhen</a>, 
<a href="/search/cs?searchtype=author&query=Schreuder%2C+N">Nicolas Schreuder</a>, 
<a href="/search/cs?searchtype=author&query=Perchet%2C+V">Vianney Perchet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15187" title="Abstract">arXiv:2303.15187</a> (replaced) [<a href="/pdf/2303.15187" title="Download PDF">pdf</a>, <a href="/format/2303.15187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avatarm: an Avatar With Manipulation Capabilities for the Physical  Metaverse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villani%2C+A">Alberto Villani</a>, 
<a href="/search/cs?searchtype=author&query=Cortigiani%2C+G">Giovanni Cortigiani</a>, 
<a href="/search/cs?searchtype=author&query=Brogi%2C+B">Bernardo Brogi</a>, 
<a href="/search/cs?searchtype=author&query=D%27Aurizio%2C+N">Nicole D&#x27;Aurizio</a>, 
<a href="/search/cs?searchtype=author&query=Baldi%2C+T+L">Tommaso Lisini Baldi</a>, 
<a href="/search/cs?searchtype=author&query=Prattichizzo%2C+D">Domenico Prattichizzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02525" title="Abstract">arXiv:2304.02525</a> (replaced) [<a href="/pdf/2304.02525" title="Download PDF">pdf</a>, <a href="/format/2304.02525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supporting Energy-Based Learning With An Ising Machine Substrate: A Case  Study on RBM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vengalam%2C+U+K+R">Uday Kumar Reddy Vengalam</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+T">Tong Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Michael Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02541" title="Abstract">arXiv:2304.02541</a> (replaced) [<a href="/pdf/2304.02541" title="Download PDF">pdf</a>, <a href="/format/2304.02541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kalvin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chenxuan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Carlson%2C+N">Nathaniel Carlson</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+N">Nathaniel Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D">David Mortensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02688" title="Abstract">arXiv:2304.02688</a> (replaced) [<a href="/pdf/2304.02688" title="Download PDF">pdf</a>, <a href="/format/2304.02688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Further: Flatness at the Rescue of Early Stopping for Adversarial  Example Transferability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gubri%2C+M">Martin Gubri</a>, 
<a href="/search/cs?searchtype=author&query=Cordy%2C+M">Maxime Cordy</a>, 
<a href="/search/cs?searchtype=author&query=Traon%2C+Y+L">Yves Le Traon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 2: originally submitted in April 2023 and revised in February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03335" title="Abstract">arXiv:2304.03335</a> (replaced) [<a href="/pdf/2304.03335" title="Download PDF">pdf</a>, <a href="/format/2304.03335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hardware-Aware Static Optimization of Hyperdimensional Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> Pu (Luke)Yi, 
<a href="/search/cs?searchtype=author&query=Achour%2C+S">Sara Achour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05218" title="Abstract">arXiv:2304.05218</a> (replaced) [<a href="/pdf/2304.05218" title="Download PDF">pdf</a>, <a href="/ps/2304.05218" title="Download PostScript">ps</a>, <a href="/format/2304.05218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Neural Radiance Fields with Depth-aware Optimization for Novel  View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Beiji Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06857" title="Abstract">arXiv:2304.06857</a> (replaced) [<a href="/pdf/2304.06857" title="Download PDF">pdf</a>, <a href="/format/2304.06857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Self-Supervised Learning for Remote Sensing with Elevation  Data: A Case Study with Scarce And High Level Semantic Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1o-Idarraga%2C+O+A">Omar A. Casta&#xf1;o-Idarraga</a>, 
<a href="/search/cs?searchtype=author&query=Ramos-Poll%C3%A1n%2C+R">Raul Ramos-Poll&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Kalaitzis%2C+F">Freddie Kalaitzis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11671" title="Abstract">arXiv:2304.11671</a> (replaced) [<a href="/pdf/2304.11671" title="Download PDF">pdf</a>, <a href="/format/2304.11671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Battery Capacity Knee-Onset Identification and Early Prediction Using  Degradation Curvature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Huang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Altaf%2C+F">Faisal Altaf</a>, 
<a href="/search/eess?searchtype=author&query=Wik%2C+T">Torsten Wik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03582" title="Abstract">arXiv:2305.03582</a> (replaced) [<a href="/pdf/2305.03582" title="Download PDF">pdf</a>, <a href="/format/2305.03582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multimodal dynamical variational autoencoder for audiovisual speech  representation learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadok%2C+S">Samir Sadok</a>, 
<a href="/search/cs?searchtype=author&query=Leglaive%2C+S">Simon Leglaive</a>, 
<a href="/search/cs?searchtype=author&query=Girin%2C+L">Laurent Girin</a>, 
<a href="/search/cs?searchtype=author&query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A9guier%2C+R">Renaud S&#xe9;guier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 figures, <a href="https://samsad35.github.io/site-mdvae/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05013" title="Abstract">arXiv:2305.05013</a> (replaced) [<a href="/pdf/2305.05013" title="Download PDF">pdf</a>, <a href="/ps/2305.05013" title="Download PostScript">ps</a>, <a href="/format/2305.05013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Diagonal Reconfigurable Intelligent Surfaces Utilizing Graph  Theory: Modeling, Architecture Design, and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nerini%2C+M">Matteo Nerini</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shanpu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06382" title="Abstract">arXiv:2305.06382</a> (replaced) [<a href="/pdf/2305.06382" title="Download PDF">pdf</a>, <a href="/format/2305.06382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ercan%2C+B">Burak Ercan</a>, 
<a href="/search/cs?searchtype=author&query=Eker%2C+O">Onur Eker</a>, 
<a href="/search/cs?searchtype=author&query=Saglam%2C+C">Canberk Saglam</a>, 
<a href="/search/cs?searchtype=author&query=Erdem%2C+A">Aykut Erdem</a>, 
<a href="/search/cs?searchtype=author&query=Erdem%2C+E">Erkut Erdem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures. Accepted by IEEE Transactions on Image Processing. The project page can be found at <a href="https://ercanburak.github.io/HyperE2VID.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07439" title="Abstract">arXiv:2305.07439</a> (replaced) [<a href="/pdf/2305.07439" title="Download PDF">pdf</a>, <a href="/ps/2305.07439" title="Download PostScript">ps</a>, <a href="/format/2305.07439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimension Results for Extremal-Generic Polynomial Systems over Complete  Toric Varieties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bender%2C+M">Mat&#xed;as Bender</a>, 
<a href="/search/cs?searchtype=author&query=Spaenlehauer%2C+P">Pierre-Jean Spaenlehauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Journal of Algebra
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08014" title="Abstract">arXiv:2305.08014</a> (replaced) [<a href="/pdf/2305.08014" title="Download PDF">pdf</a>, <a href="/ps/2305.08014" title="Download PostScript">ps</a>, <a href="/format/2305.08014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by  Leveraging Lightweight All-ConvNet and Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+R">Md. Rabiul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Massicotte%2C+D">Daniel Massicotte</a>, 
<a href="/search/cs?searchtype=author&query=Massicotte%2C+P+Y">Philippe Y. Massicotte</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei-Ping Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09181" title="Abstract">arXiv:2305.09181</a> (replaced) [<a href="/pdf/2305.09181" title="Download PDF">pdf</a>, <a href="/format/2305.09181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Rectified Version) Push-LSVRG-UP: Distributed Stochastic Optimization  over Unbalanced Directed Networks with Uncoordinated Triggered Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jinhui Hu</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+G">Guo Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+H">Huaqing Li</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+Z">Zixiang Shen</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+W">Weidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 30 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING, VOL. 10, NO.
  2, 2023, PP. 934-950
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10790" title="Abstract">arXiv:2305.10790</a> (replaced) [<a href="/pdf/2305.10790" title="Download PDF">pdf</a>, <a href="/format/2305.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listen, Think, and Understand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gong%2C+Y">Yuan Gong</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+H">Hongyin Luo</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/eess?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/eess?searchtype=author&query=Glass%2C+J">James Glass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024. Code, dataset, and models are available at <a href="https://github.com/YuanGongND/ltu.">this https URL</a> The interactive demo is at <a href="https://huggingface.co/spaces/yuangongfdu/ltu">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10978" title="Abstract">arXiv:2305.10978</a> (replaced) [<a href="/pdf/2305.10978" title="Download PDF">pdf</a>, <a href="/format/2305.10978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Client Selection for Federated Policy Optimization with Environment  Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhijie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S+H">S.H. Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11463" title="Abstract">arXiv:2305.11463</a> (replaced) [<a href="/pdf/2305.11463" title="Download PDF">pdf</a>, <a href="/format/2305.11463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Sliced MMD Flows with Riesz Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+C">Christian Wald</a>, 
<a href="/search/cs?searchtype=author&query=Altekr%C3%BCger%2C+F">Fabian Altekr&#xfc;ger</a>, 
<a href="/search/cs?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12990" title="Abstract">arXiv:2305.12990</a> (replaced) [<a href="/pdf/2305.12990" title="Download PDF">pdf</a>, <a href="/format/2305.12990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentence Representations via Gaussian Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoda%2C+S">Shohei Yoda</a>, 
<a href="/search/cs?searchtype=author&query=Tsukagoshi%2C+H">Hayato Tsukagoshi</a>, 
<a href="/search/cs?searchtype=author&query=Sasano%2C+R">Ryohei Sasano</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Koichi Takeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14456" title="Abstract">arXiv:2305.14456</a> (replaced) [<a href="/pdf/2305.14456" title="Download PDF">pdf</a>, <a href="/format/2305.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Having Beer after Prayer? Measuring Cultural Bias in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+M+J">Michael J. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14836" title="Abstract">arXiv:2305.14836</a> (replaced) [<a href="/pdf/2305.14836" title="Download PDF">pdf</a>, <a href="/format/2305.14836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for  Autonomous Driving Scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+T">Tianwen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingjing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+L">Linhai Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15405" title="Abstract">arXiv:2305.15405</a> (replaced) [<a href="/pdf/2305.15405" title="Download PDF">pdf</a>, <a href="/format/2305.15405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Textless Low-Resource Speech-to-Speech Translation With Unit Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diwan%2C+A">Anuj Diwan</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Anirudh Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Harwath%2C+D">David Harwath</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16189" title="Abstract">arXiv:2305.16189</a> (replaced) [<a href="/pdf/2305.16189" title="Download PDF">pdf</a>, <a href="/format/2305.16189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Martian time-series unraveled: A multi-scale nested approach with  factorial variational autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siahkoohi%2C+A">Ali Siahkoohi</a>, 
<a href="/search/cs?searchtype=author&query=Morel%2C+R">Rudy Morel</a>, 
<a href="/search/cs?searchtype=author&query=Balestriero%2C+R">Randall Balestriero</a>, 
<a href="/search/cs?searchtype=author&query=Allys%2C+E">Erwan Allys</a>, 
<a href="/search/cs?searchtype=author&query=Sainton%2C+G">Gr&#xe9;gory Sainton</a>, 
<a href="/search/cs?searchtype=author&query=Kawamura%2C+T">Taichi Kawamura</a>, 
<a href="/search/cs?searchtype=author&query=de+Hoop%2C+M+V">Maarten V. de Hoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17282" title="Abstract">arXiv:2305.17282</a> (replaced) [<a href="/pdf/2305.17282" title="Download PDF">pdf</a>, <a href="/format/2305.17282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal consistency of the $k$-NN rule in metric spaces and Nagata  dimension. II
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumari%2C+S">Sushma Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Pestov%2C+V+G">Vladimir G. Pestov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Latex 2e, 28 pages, 1 figure. The second revision as requested by the anonymous ESAIM:PS reviewer. The proof of Proposition 1.2 has been redone, Fig. 1 added, and a number of smaller changes and improvements made
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17342" title="Abstract">arXiv:2305.17342</a> (replaced) [<a href="/pdf/2305.17342" title="Download PDF">pdf</a>, <a href="/format/2305.17342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Adversarial Policies: A Generalized Attack Formulation and  Provable Defense in RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19971" title="Abstract">arXiv:2305.19971</a> (replaced) [<a href="/pdf/2305.19971" title="Download PDF">pdf</a>, <a href="/format/2305.19971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning in the Presence of Adversarial Client Unavailability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lili Su</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+M">Ming Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Pengkun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01990" title="Abstract">arXiv:2306.01990</a> (replaced) [<a href="/pdf/2306.01990" title="Download PDF">pdf</a>, <a href="/format/2306.01990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivizing Exploration with Linear Contexts and Combinatorial Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sellke%2C+M">Mark Sellke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning (ICML) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04802" title="Abstract">arXiv:2306.04802</a> (replaced) [<a href="/pdf/2306.04802" title="Download PDF">pdf</a>, <a href="/format/2306.04802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Knowledge Graphs for Healthcare: Resources, Applications,  and Promises
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hejie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaying Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenjing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shaojun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+X">Xuan Kan</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+J">Joyce Ho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Carl Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05108" title="Abstract">arXiv:2306.05108</a> (replaced) [<a href="/pdf/2306.05108" title="Download PDF">pdf</a>, <a href="/format/2306.05108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Graph: A Unified Graph Representation with Datasets and  Benchmarks for Complex Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zehui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingzhu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Stan%2C+G">Guy-Bart Stan</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06770" title="Abstract">arXiv:2306.06770</a> (replaced) [<a href="/pdf/2306.06770" title="Download PDF">pdf</a>, <a href="/format/2306.06770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Knowledge Extraction from LLMs for Task Learning through Agent  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirk%2C+J+R">James R. Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+R+E">Robert E. Wray</a>, 
<a href="/search/cs?searchtype=author&query=Lindes%2C+P">Peter Lindes</a>, 
<a href="/search/cs?searchtype=author&query=Laird%2C+J+E">John E. Laird</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, 3 tables, bibliography, appendix (34 pages total). Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06945" title="Abstract">arXiv:2306.06945</a> (replaced) [<a href="/pdf/2306.06945" title="Download PDF">pdf</a>, <a href="/format/2306.06945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater Acoustic Target Recognition based on Smoothness-inducing  Regularization and Spectrogram-based Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Ji Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenchao Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering, 2023, 281: 114926
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07588" title="Abstract">arXiv:2306.07588</a> (replaced) [<a href="/pdf/2306.07588" title="Download PDF">pdf</a>, <a href="/format/2306.07588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending adjacency matrices to 3D with triangles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+R">Rusheng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Purchase%2C+H+C">Helen C. Purchase</a>, 
<a href="/search/cs?searchtype=author&query=Dwyer%2C+T">Tim Dwyer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in PacificVis 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09690" title="Abstract">arXiv:2306.09690</a> (replaced) [<a href="/pdf/2306.09690" title="Download PDF">pdf</a>, <a href="/format/2306.09690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Physiological and Psychological Responses in Virtual  Reality and Flat Screen Gaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsal%2C+R">Ritik Vatsal</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shrivatsa Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Thareja%2C+R">Rushil Thareja</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+M">Mrinmoy Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+O">Ojaswa Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+J">Jainendra Shukla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted in the IEEE Transactions on Affective Computing. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13891" title="Abstract">arXiv:2306.13891</a> (replaced) [<a href="/pdf/2306.13891" title="Download PDF">pdf</a>, <a href="/format/2306.13891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the Causal Effect of Early ArXiving on Paper Acceptance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wadden%2C+D">David Wadden</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at CLeaR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14011" title="Abstract">arXiv:2306.14011</a> (replaced) [<a href="/pdf/2306.14011" title="Download PDF">pdf</a>, <a href="/format/2306.14011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-driven Autotuning of Graphics Processing Unit  Accelerated Computational Fluid Dynamics for Enhanced Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Weicheng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+C+J">Christohper John Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14882" title="Abstract">arXiv:2306.14882</a> (replaced) [<a href="/pdf/2306.14882" title="Download PDF">pdf</a>, <a href="/format/2306.14882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citadel: Enclaves with Microarchitectural Isolation and Secure Shared  Memory on a Speculative Out-of-Order Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drean%2C+J">Jules Drean</a>, 
<a href="/search/cs?searchtype=author&query=Gomez-Garcia%2C+M">Miguel Gomez-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Jepsen%2C+F">Fisher Jepsen</a>, 
<a href="/search/cs?searchtype=author&query=Bourgeat%2C+T">Thomas Bourgeat</a>, 
<a href="/search/cs?searchtype=author&query=Devadas%2C+S">Srinivas Devadas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17782" title="Abstract">arXiv:2306.17782</a> (replaced) [<a href="/pdf/2306.17782" title="Download PDF">pdf</a>, <a href="/ps/2306.17782" title="Download PostScript">ps</a>, <a href="/format/2306.17782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Federated Low Rank Matrix Recovery via Alternating GD and  Minimization: A Simple Proof
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaswani%2C+N">Namrata Vaswani</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Information Theory 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03186" title="Abstract">arXiv:2307.03186</a> (replaced) [<a href="/pdf/2307.03186" title="Download PDF">pdf</a>, <a href="/format/2307.03186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TGRL: An Algorithm for Teacher Guided Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shenfeld%2C+I">Idan Shenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhang-Wei Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tamar%2C+A">Aviv Tamar</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07047" title="Abstract">arXiv:2307.07047</a> (replaced) [<a href="/pdf/2307.07047" title="Download PDF">pdf</a>, <a href="/format/2307.07047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Collaborative Human-LM Dialogue Generation Help Information  Extraction from Human Dialogues?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+B">Bo-Ru Lu</a>, 
<a href="/search/cs?searchtype=author&query=Haduong%2C+N">Nikita Haduong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chia-Hsuan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Koester%2C+P">Paul Koester</a>, 
<a href="/search/cs?searchtype=author&query=Utke%2C+J">Jean Utke</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorf%2C+M">Mari Ostendorf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07090" title="Abstract">arXiv:2307.07090</a> (replaced) [<a href="/pdf/2307.07090" title="Download PDF">pdf</a>, <a href="/format/2307.07090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choice Models and Permutation Invariance: Demand Estimation in  Differentiated Products Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Singh%2C+A">Amandeep Singh</a>, 
<a href="/search/econ?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/econ?searchtype=author&query=Yoganarasimhan%2C+H">Hema Yoganarasimhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11106" title="Abstract">arXiv:2307.11106</a> (replaced) [<a href="/pdf/2307.11106" title="Download PDF">pdf</a>, <a href="/format/2307.11106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The importance of feature preprocessing for differentially private  linear optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Ziteng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+A+T">Ananda Theertha Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A+K">Aditya Krishna Menon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11225" title="Abstract">arXiv:2307.11225</a> (replaced) [<a href="/pdf/2307.11225" title="Download PDF">pdf</a>, <a href="/format/2307.11225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small But Unwieldy: A Lower Bound on Adjacency Labels for Small Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonnet%2C+%C3%89">&#xc9;douard Bonnet</a>, 
<a href="/search/math?searchtype=author&query=Duron%2C+J">Julien Duron</a>, 
<a href="/search/math?searchtype=author&query=Sylvester%2C+J">John Sylvester</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>, 
<a href="/search/math?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 1 figure, shortened abstract, corrected graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12449" title="Abstract">arXiv:2307.12449</a> (replaced) [<a href="/pdf/2307.12449" title="Download PDF">pdf</a>, <a href="/format/2307.12449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyPP: Dynamic Parameter Prediction to Accelerate Convergence of  Variational Quantum Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+S">Satwik Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+D">Debarshi Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12497" title="Abstract">arXiv:2307.12497</a> (replaced) [<a href="/pdf/2307.12497" title="Download PDF">pdf</a>, <a href="/format/2307.12497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Integer Lattices as Ideals into Polynomial Rings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yihang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yanbin Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16200" title="Abstract">arXiv:2307.16200</a> (replaced) [<a href="/pdf/2307.16200" title="Download PDF">pdf</a>, <a href="/format/2307.16200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zefa Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Ziyi Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research, <a href="https://link.springer.com/article/10.1007/s11633-023-1461-5">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01152" title="Abstract">arXiv:2308.01152</a> (replaced) [<a href="/pdf/2308.01152" title="Download PDF">pdf</a>, <a href="/ps/2308.01152" title="Download PostScript">ps</a>, <a href="/format/2308.01152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skolem Meets Bateman-Horn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luca%2C+F">Florian Luca</a>, 
<a href="/search/cs?searchtype=author&query=Maynard%2C+J">James Maynard</a>, 
<a href="/search/cs?searchtype=author&query=Noubissie%2C+A">Armand Noubissie</a>, 
<a href="/search/cs?searchtype=author&query=Ouaknine%2C+J">Jo&#xeb;l Ouaknine</a>, 
<a href="/search/cs?searchtype=author&query=Worrell%2C+J">James Worrell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01421" title="Abstract">arXiv:2308.01421</a> (replaced) [<a href="/pdf/2308.01421" title="Download PDF">pdf</a>, <a href="/format/2308.01421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularization, early-stopping and dreaming: a Hopfield-like setup to  address generalization and overfitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agliari%2C+E">Elena Agliari</a>, 
<a href="/search/cs?searchtype=author&query=Alemanno%2C+F">Francesco Alemanno</a>, 
<a href="/search/cs?searchtype=author&query=Aquaro%2C+M">Miriam Aquaro</a>, 
<a href="/search/cs?searchtype=author&query=Fachechi%2C+A">Alberto Fachechi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 10 figures, 4 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01733" title="Abstract">arXiv:2308.01733</a> (replaced) [<a href="/pdf/2308.01733" title="Download PDF">pdf</a>, <a href="/format/2308.01733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimisation-based domain-decomposition reduced order model for  parameter-dependent non-stationary fluid dynamics problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Prusak%2C+I">Ivan Prusak</a>, 
<a href="/search/math?searchtype=author&query=Torlo%2C+D">Davide Torlo</a>, 
<a href="/search/math?searchtype=author&query=Nonino%2C+M">Monica Nonino</a>, 
<a href="/search/math?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2211.14528">arXiv:2211.14528</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04585" title="Abstract">arXiv:2308.04585</a> (replaced) [<a href="/pdf/2308.04585" title="Download PDF">pdf</a>, <a href="/ps/2308.04585" title="Download PostScript">ps</a>, <a href="/format/2308.04585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel Single Proxy Control for Deterministic Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+L">Liyuan Xu</a>, 
<a href="/search/stat?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08742" title="Abstract">arXiv:2308.08742</a> (replaced) [<a href="/pdf/2308.08742" title="Download PDF">pdf</a>, <a href="/format/2308.08742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMET: Precise Model Editing in a Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shasha Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shezheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jie Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09842" title="Abstract">arXiv:2308.09842</a> (replaced) [<a href="/pdf/2308.09842" title="Download PDF">pdf</a>, <a href="/format/2308.09842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enumerating Safe Regions in Deep Neural Networks with Provable  Probabilistic Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marzari%2C+L">Luca Marzari</a>, 
<a href="/search/cs?searchtype=author&query=Corsi%2C+D">Davide Corsi</a>, 
<a href="/search/cs?searchtype=author&query=Marchesini%2C+E">Enrico Marchesini</a>, 
<a href="/search/cs?searchtype=author&query=Farinelli%2C+A">Alessandro Farinelli</a>, 
<a href="/search/cs?searchtype=author&query=Cicalese%2C+F">Ferdinando Cicalese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 38th Annual AAAI Conference on Artificial Intelligence 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11978" title="Abstract">arXiv:2308.11978</a> (replaced) [<a href="/pdf/2308.11978" title="Download PDF">pdf</a>, <a href="/format/2308.11978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will More Expressive Graph Neural Networks do Better on Generative  Tasks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xiandong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2nd Learning on Graphs Conference (LoG 2023). 26 pages, 5 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12032" title="Abstract">arXiv:2308.12032</a> (replaced) [<a href="/pdf/2308.12032" title="Download PDF">pdf</a>, <a href="/format/2308.12032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Quantity to Quality: Boosting LLM Performance with Self-Guided Data  Selection for Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14714" title="Abstract">arXiv:2308.14714</a> (replaced) [<a href="/pdf/2308.14714" title="Download PDF">pdf</a>, <a href="/format/2308.14714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic Surveillance Stackelberg Game: Co-Optimizing Defense  Placement and Patrol Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=John%2C+Y">Yohan John</a>, 
<a href="/search/eess?searchtype=author&query=Diaz-Garcia%2C+G">Gilberto Diaz-Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+X">Xiaoming Duan</a>, 
<a href="/search/eess?searchtype=author&query=Marden%2C+J+R">Jason R. Marden</a>, 
<a href="/search/eess?searchtype=author&query=Bullo%2C+F">Francesco Bullo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, submitted as a technical note to the IEEE Transactions on Automatic Control. Replaced to fix inaccuracies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15949" title="Abstract">arXiv:2308.15949</a> (replaced) [<a href="/pdf/2308.15949" title="Download PDF">pdf</a>, <a href="/format/2308.15949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latency-aware Unified Dynamic Networks for Efficient Image Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yizeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zeyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yifan Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaofei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16071" title="Abstract">arXiv:2308.16071</a> (replaced) [<a href="/pdf/2308.16071" title="Download PDF">pdf</a>, <a href="/format/2308.16071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Image Synthesis via Class-Adaptive Cross-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fontanini%2C+T">Tomaso Fontanini</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+C">Claudio Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Lisanti%2C+G">Giuseppe Lisanti</a>, 
<a href="/search/cs?searchtype=author&query=Bertozzi%2C+M">Massimo Bertozzi</a>, 
<a href="/search/cs?searchtype=author&query=Prati%2C+A">Andrea Prati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and models available at <a href="https://github.com/TFonta/CA2SIS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01781" title="Abstract">arXiv:2309.01781</a> (replaced) [<a href="/pdf/2309.01781" title="Download PDF">pdf</a>, <a href="/format/2309.01781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-concordant Smoothing for Large-Scale Convex Composite Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Adeoye%2C+A+D">Adeyemi D. Adeoye</a>, 
<a href="/search/math?searchtype=author&query=Bemporad%2C+A">Alberto Bemporad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04761" title="Abstract">arXiv:2309.04761</a> (replaced) [<a href="/pdf/2309.04761" title="Download PDF">pdf</a>, <a href="/format/2309.04761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Deep Learning Techniques in Educational Data  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuanguo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zongyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07251" title="Abstract">arXiv:2309.07251</a> (replaced) [<a href="/pdf/2309.07251" title="Download PDF">pdf</a>, <a href="/format/2309.07251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Contextual Gender Bias Suppression for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oba%2C+D">Daisuke Oba</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Findings - Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08081" title="Abstract">arXiv:2309.08081</a> (replaced) [<a href="/pdf/2309.08081" title="Download PDF">pdf</a>, <a href="/ps/2309.08081" title="Download PostScript">ps</a>, <a href="/format/2309.08081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the Assmus--Mattson theorem for some ternary codes (a resume)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bannai%2C+E">Eiichi Bannai</a>, 
<a href="/search/math?searchtype=author&query=Miezaki%2C+T">Tsuyoshi Miezaki</a>, 
<a href="/search/math?searchtype=author&query=Nakasora%2C+H">Hiroyuki Nakasora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, this is a resume of "A note on the Assmus--Mattson theorem for some ternary codes"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09882" title="Abstract">arXiv:2309.09882</a> (replaced) [<a href="/pdf/2309.09882" title="Download PDF">pdf</a>, <a href="/format/2309.09882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Boustrophedon Paths That Enable Optimization Via Gradient  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manzini%2C+T">Thomas Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+R">Robin Murphy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09968" title="Abstract">arXiv:2309.09968</a> (replaced) [<a href="/pdf/2309.09968" title="Download PDF">pdf</a>, <a href="/format/2309.09968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating and Imputing Tabular Data via Diffusion and Flow-based  Gradient-Boosted Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jolicoeur-Martineau%2C+A">Alexia Jolicoeur-Martineau</a>, 
<a href="/search/cs?searchtype=author&query=Fatras%2C+K">Kilian Fatras</a>, 
<a href="/search/cs?searchtype=author&query=Kachman%2C+T">Tal Kachman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/SamsungSAILMontreal/ForestDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10433" title="Abstract">arXiv:2309.10433</a> (replaced) [<a href="/pdf/2309.10433" title="Download PDF">pdf</a>, <a href="/format/2309.10433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Writer-Defined AI Personas for On-Demand Feedback Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benharrak%2C+K">Karim Benharrak</a>, 
<a href="/search/cs?searchtype=author&query=Zindulka%2C+T">Tim Zindulka</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+F">Florian Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Heuer%2C+H">Hendrik Heuer</a>, 
<a href="/search/cs?searchtype=author&query=Buschek%2C+D">Daniel Buschek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10926" title="Abstract">arXiv:2309.10926</a> (replaced) [<a href="/pdf/2309.10926" title="Download PDF">pdf</a>, <a href="/format/2309.10926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Autoregressive Streaming ASR With Label Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Siddhant Arora</a>, 
<a href="/search/cs?searchtype=author&query=Saon%2C+G">George Saon</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Kingsbury%2C+B">Brian Kingsbury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11087" title="Abstract">arXiv:2309.11087</a> (replaced) [<a href="/pdf/2309.11087" title="Download PDF">pdf</a>, <a href="/format/2309.11087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embed-Search-Align: DNA Sequence Alignment using Transformer Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Holur%2C+P">Pavan Holur</a>, 
<a href="/search/q-bio?searchtype=author&query=Enevoldsen%2C+K+C">K. C. Enevoldsen</a>, 
<a href="/search/q-bio?searchtype=author&query=Mboning%2C+L">Lajoyce Mboning</a>, 
<a href="/search/q-bio?searchtype=author&query=Georgiou%2C+T">Thalia Georgiou</a>, 
<a href="/search/q-bio?searchtype=author&query=Bouchard%2C+L">Louis-S. Bouchard</a>, 
<a href="/search/q-bio?searchtype=author&query=Pellegrini%2C+M">Matteo Pellegrini</a>, 
<a href="/search/q-bio?searchtype=author&query=Roychowdhury%2C+V">Vwani Roychowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, Tables 7, Figures 5
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11134" title="Abstract">arXiv:2309.11134</a> (replaced) [<a href="/pdf/2309.11134" title="Download PDF">pdf</a>, <a href="/format/2309.11134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNSS/Multi-Sensor Fusion Using Continuous-Time Factor Graph Optimization  for Robust Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chih-Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vallery%2C+H">Heike Vallery</a>, 
<a href="/search/cs?searchtype=author&query=Barfoot%2C+T+D">Timothy D. Barfoot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Robotics (2024-02-19)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11972" title="Abstract">arXiv:2309.11972</a> (replaced) [<a href="/pdf/2309.11972" title="Download PDF">pdf</a>, <a href="/format/2309.11972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalize Synchronization Mechanism: Specification, Properties, Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+C">Chih-Wei Chien</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chi-Yeh Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 figure. To be submitted to conferences in 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13207" title="Abstract">arXiv:2309.13207</a> (replaced) [<a href="/pdf/2309.13207" title="Download PDF">pdf</a>, <a href="/format/2309.13207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation  for Earth System Science Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schreck%2C+J+S">John S. Schreck</a>, 
<a href="/search/cs?searchtype=author&query=Gagne%2C+D+J">David John Gagne II</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+C">Charlie Becker</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+W+E">William E. Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Elmore%2C+K">Kim Elmore</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Da Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gantos%2C+G">Gabrielle Gantos</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Eliot Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kimpara%2C+D">Dhamma Kimpara</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+T">Thomas Martin</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+M+J">Maria J. Molina</a>, 
<a href="/search/cs?searchtype=author&query=Pryzbylo%2C+V+M">Vanessa M. Pryzbylo</a>, 
<a href="/search/cs?searchtype=author&query=Radford%2C+J">Jacob Radford</a>, 
<a href="/search/cs?searchtype=author&query=Saavedra%2C+B">Belen Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Willson%2C+J">Justin Willson</a>, 
<a href="/search/cs?searchtype=author&query=Wirz%2C+C">Christopher Wirz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13506" title="Abstract">arXiv:2309.13506</a> (replaced) [<a href="/pdf/2309.13506" title="Download PDF">pdf</a>, <a href="/format/2309.13506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Usability of Differential Privacy Tools with Data  Practitioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngong%2C+I+C">Ivoline C. Ngong</a>, 
<a href="/search/cs?searchtype=author&query=Stenger%2C+B">Brad Stenger</a>, 
<a href="/search/cs?searchtype=author&query=Near%2C+J+P">Joseph P. Near</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yuanyuan Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17261" title="Abstract">arXiv:2309.17261</a> (replaced) [<a href="/pdf/2309.17261" title="Download PDF">pdf</a>, <a href="/format/2309.17261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware  Diffusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yukang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haonan Han</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chaoqun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zunnan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yachao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17415" title="Abstract">arXiv:2309.17415</a> (replaced) [<a href="/pdf/2309.17415" title="Download PDF">pdf</a>, <a href="/format/2309.17415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intuitive or Dependent? Investigating LLMs&#x27; Behavior Style to  Conflicting Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+J">Jiahao Ying</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Kai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yidong He</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Long Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongbin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00267" title="Abstract">arXiv:2310.00267</a> (replaced) [<a href="/pdf/2310.00267" title="Download PDF">pdf</a>, <a href="/format/2310.00267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Physics of Preference: Unravelling Imprecision of Human Preferences  through Magnetisation Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Maksymov%2C+I+S">Ivan S. Maksymov</a>, 
<a href="/search/physics?searchtype=author&query=Pogrebna%2C+G">Ganna Pogrebna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Other Condensed Matter (cond-mat.other); Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00967" title="Abstract">arXiv:2310.00967</a> (replaced) [<a href="/pdf/2310.00967" title="Download PDF">pdf</a>, <a href="/format/2310.00967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and  Accelerating Distributed DNN Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+D">Daegun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sangyoon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30th IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC 2023). Code: <a href="https://github.com/kljp/micro">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01201" title="Abstract">arXiv:2310.01201</a> (replaced) [<a href="/pdf/2310.01201" title="Download PDF">pdf</a>, <a href="/format/2310.01201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sebia%2C+H">Hana Sebia</a>, 
<a href="/search/cs?searchtype=author&query=Guyet%2C+T">Thomas Guyet</a>, 
<a href="/search/cs?searchtype=author&query=Audureau%2C+E">Etienne Audureau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01376" title="Abstract">arXiv:2310.01376</a> (replaced) [<a href="/pdf/2310.01376" title="Download PDF">pdf</a>, <a href="/format/2310.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Distribution-Agnostic Generalized Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaomeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03013" title="Abstract">arXiv:2310.03013</a> (replaced) [<a href="/pdf/2310.03013" title="Download PDF">pdf</a>, <a href="/format/2310.03013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemiReward: A General Reward Model for Semi-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weiyang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Camera Ready. Preprint V2 (25 pages) with the source code at <a href="https://github.com/Westlake-AI/SemiReward">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03447" title="Abstract">arXiv:2310.03447</a> (replaced) [<a href="/pdf/2310.03447" title="Download PDF">pdf</a>, <a href="/format/2310.03447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLAIM: AIM-based Synthetic Data Generation in the Federated Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maddock%2C+S">Samuel Maddock</a>, 
<a href="/search/cs?searchtype=author&query=Cormode%2C+G">Graham Cormode</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03755" title="Abstract">arXiv:2310.03755</a> (replaced) [<a href="/pdf/2310.03755" title="Download PDF">pdf</a>, <a href="/format/2310.03755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics Informed Neural Network Code for 2D Transient Problems  (PINN-2DT) Compatible with Google Colab
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maczuga%2C+P">Pawe&#x142; Maczuga</a>, 
<a href="/search/cs?searchtype=author&query=Sikora%2C+M">Maciej Sikora</a>, 
<a href="/search/cs?searchtype=author&query=Skocze%C5%84%2C+M">Maciej Skocze&#x144;</a>, 
<a href="/search/cs?searchtype=author&query=Ro%C5%BCnawski%2C+P">Przemys&#x142;aw Ro&#x17c;nawski</a>, 
<a href="/search/cs?searchtype=author&query=T%C5%82uszcz%2C+F">Filip T&#x142;uszcz</a>, 
<a href="/search/cs?searchtype=author&query=Szubert%2C+M">Marcin Szubert</a>, 
<a href="/search/cs?searchtype=author&query=%C5%81o%C5%9B%2C+M">Marcin &#x141;o&#x15b;</a>, 
<a href="/search/cs?searchtype=author&query=Dzwinel%2C+W">Witold Dzwinel</a>, 
<a href="/search/cs?searchtype=author&query=Pingali%2C+K">Keshav Pingali</a>, 
<a href="/search/cs?searchtype=author&query=Paszy%C5%84ski%2C+M">Maciej Paszy&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG); Mathematical Software (cs.MS); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04270" title="Abstract">arXiv:2310.04270</a> (replaced) [<a href="/pdf/2310.04270" title="Download PDF">pdf</a>, <a href="/format/2310.04270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Large Language Models on Benchmark  Biomedical Text Processing Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahan%2C+I">Israt Jahan</a>, 
<a href="/search/cs?searchtype=author&query=Laskar%2C+M+T+R">Md Tahmid Rahman Laskar</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimmy Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Computers in Biology and Medicine journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04918" title="Abstract">arXiv:2310.04918</a> (replaced) [<a href="/pdf/2310.04918" title="Download PDF">pdf</a>, <a href="/format/2310.04918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+L">Lei You</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H+V">Hei Victor Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05105" title="Abstract">arXiv:2310.05105</a> (replaced) [<a href="/pdf/2310.05105" title="Download PDF">pdf</a>, <a href="/format/2310.05105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Graph Neural Networks Learn: Lessons from Training Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ongoing work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05668" title="Abstract">arXiv:2310.05668</a> (replaced) [<a href="/pdf/2310.05668" title="Download PDF">pdf</a>, <a href="/format/2310.05668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuiguang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Web Conference 2024 (WWW 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06650" title="Abstract">arXiv:2310.06650</a> (replaced) [<a href="/pdf/2310.06650" title="Download PDF">pdf</a>, <a href="/format/2310.06650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Parallelized, Adam-Based Solver for Reserve and Security Constrained  AC Unit Commitment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chevalier%2C+S">Samuel Chevalier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06692" title="Abstract">arXiv:2310.06692</a> (replaced) [<a href="/pdf/2310.06692" title="Download PDF">pdf</a>, <a href="/format/2310.06692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Anni Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07138" title="Abstract">arXiv:2310.07138</a> (replaced) [<a href="/pdf/2310.07138" title="Download PDF">pdf</a>, <a href="/format/2310.07138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Task Routing for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Byeongjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+S">Sangmin Woo</a>, 
<a href="/search/cs?searchtype=author&query=Go%2C+H">Hyojun Go</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin-Young Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changick Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07974" title="Abstract">arXiv:2310.07974</a> (replaced) [<a href="/pdf/2310.07974" title="Download PDF">pdf</a>, <a href="/format/2310.07974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causality-based Cost Allocation for Peer-to-Peer Energy Trading in  Distribution System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+H+J">Hyun Joong Kim</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Y+H">Yong Hyun Song</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jip Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08331" title="Abstract">arXiv:2310.08331</a> (replaced) [<a href="/pdf/2310.08331" title="Download PDF">pdf</a>, <a href="/format/2310.08331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dealing with uncertainty: balancing exploration and exploitation in deep  recurrent reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zangirolami%2C+V">Valentina Zangirolami</a>, 
<a href="/search/stat?searchtype=author&query=Borrotti%2C+M">Matteo Borrotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, added renferences, corrected typos, revised argument in section 3, results unchanged, redifinition of the article structure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08513" title="Abstract">arXiv:2310.08513</a> (replaced) [<a href="/pdf/2310.08513" title="Download PDF">pdf</a>, <a href="/format/2310.08513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How connectivity structure shapes rich and lazy learning in neural  circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y+H">Yuhan Helena Liu</a>, 
<a href="/search/cs?searchtype=author&query=Baratin%2C+A">Aristide Baratin</a>, 
<a href="/search/cs?searchtype=author&query=Cornford%2C+J">Jonathan Cornford</a>, 
<a href="/search/cs?searchtype=author&query=Mihalas%2C+S">Stefan Mihalas</a>, 
<a href="/search/cs?searchtype=author&query=Shea-Brown%2C+E">Eric Shea-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08587" title="Abstract">arXiv:2310.08587</a> (replaced) [<a href="/pdf/2310.08587" title="Download PDF">pdf</a>, <a href="/format/2310.08587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudo-Generalized Dynamic View Synthesis from a Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaoming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Colburn%2C+A">Alex Colburn</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fangchang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Bautista%2C+M+A">Miguel Angel Bautista</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J+M">Joshua M. Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A+G">Alexander G. Schwing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; Originally titled as "Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?"; Project page: <a href="https://xiaoming-zhao.github.io/projects/pgdvs">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08949" title="Abstract">arXiv:2310.08949</a> (replaced) [<a href="/pdf/2310.08949" title="Download PDF">pdf</a>, <a href="/format/2310.08949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EasyGen: Easing Multimodal Generation with a Bidirectional Conditional  Diffusion Model and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qijiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao-Ming Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09234" title="Abstract">arXiv:2310.09234</a> (replaced) [<a href="/pdf/2310.09234" title="Download PDF">pdf</a>, <a href="/format/2310.09234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClickPrompt: CTR Models are Strong Prompt Generators for Adapting  Language Models to CTR Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hangyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yunjia Xi</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yanru Qu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kangning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09754" title="Abstract">arXiv:2310.09754</a> (replaced) [<a href="/pdf/2310.09754" title="Download PDF">pdf</a>, <a href="/format/2310.09754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huanhuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weizhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liuji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10570" title="Abstract">arXiv:2310.10570</a> (replaced) [<a href="/pdf/2310.10570" title="Download PDF">pdf</a>, <a href="/format/2310.10570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Context Utilization in Summarization with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravaut%2C+M">Mathieu Ravaut</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13459" title="Abstract">arXiv:2310.13459</a> (replaced) [<a href="/pdf/2310.13459" title="Download PDF">pdf</a>, <a href="/format/2310.13459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Nonconvex-Nonconcave Training via Linear Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pethick%2C+T">Thomas Pethick</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wanyun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14022" title="Abstract">arXiv:2310.14022</a> (replaced) [<a href="/pdf/2310.14022" title="Download PDF">pdf</a>, <a href="/format/2310.14022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trocq: Proof Transfer for Free, With or Without Univalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+C">Cyril Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Crance%2C+E">Enzo Crance</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">Assia Mahboubi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14807" title="Abstract">arXiv:2310.14807</a> (replaced) [<a href="/pdf/2310.14807" title="Download PDF">pdf</a>, <a href="/ps/2310.14807" title="Download PostScript">ps</a>, <a href="/format/2310.14807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Chaitin&#x27;s Heuristic Principle and Halting Probability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salehi%2C+S">Saeed Salehi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages (two parts)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Information Theory (cs.IT); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15098" title="Abstract">arXiv:2310.15098</a> (replaced) [<a href="/pdf/2310.15098" title="Download PDF">pdf</a>, <a href="/format/2310.15098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acquiring Weak Annotations for Tumor Localization in Temporal and  Volumetric Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+Y">Yu-Cheng Chou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Deng-Ping Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mach. Intell. Res. (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15627" title="Abstract">arXiv:2310.15627</a> (replaced) [<a href="/pdf/2310.15627" title="Download PDF">pdf</a>, <a href="/format/2310.15627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Directed Acyclic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Thompson%2C+R">Ryan Thompson</a>, 
<a href="/search/stat?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>, 
<a href="/search/stat?searchtype=author&query=Kohn%2C+R">Robert Kohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 27th International Conference on Artificial Intelligence and Statistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15724" title="Abstract">arXiv:2310.15724</a> (replaced) [<a href="/pdf/2310.15724" title="Download PDF">pdf</a>, <a href="/format/2310.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variator: Accelerating Pre-trained Models with Plug-and-Play Compression  Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuqi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenbin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17504" title="Abstract">arXiv:2310.17504</a> (replaced) [<a href="/pdf/2310.17504" title="Download PDF">pdf</a>, <a href="/format/2310.17504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Pillars improving Vision Foundation Model Distillation for Lidar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puy%2C+G">Gilles Puy</a>, 
<a href="/search/cs?searchtype=author&query=Gidaris%2C+S">Spyros Gidaris</a>, 
<a href="/search/cs?searchtype=author&query=Boulch%2C+A">Alexandre Boulch</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%A9oni%2C+O">Oriane Sim&#xe9;oni</a>, 
<a href="/search/cs?searchtype=author&query=Sautier%2C+C">Corentin Sautier</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Bursuc%2C+A">Andrei Bursuc</a>, 
<a href="/search/cs?searchtype=author&query=Marlet%2C+R">Renaud Marlet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at <a href="https://github.com/valeoai/ScaLR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18212" title="Abstract">arXiv:2310.18212</a> (replaced) [<a href="/pdf/2310.18212" title="Download PDF">pdf</a>, <a href="/format/2310.18212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of Algorithms for Causal Structure Learning to Hyperparameter  Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Machlanski%2C+D">Damian Machlanski</a>, 
<a href="/search/cs?searchtype=author&query=Samothrakis%2C+S">Spyridon Samothrakis</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+P">Paul Clarke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 3rd Conference on Causal Learning and Reasoning (CLeaR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18940" title="Abstract">arXiv:2310.18940</a> (replaced) [<a href="/pdf/2310.18940" title="Download PDF">pdf</a>, <a href="/format/2310.18940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents with Reinforcement Learning for Strategic Play in the  Werewolf Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zelai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19192" title="Abstract">arXiv:2310.19192</a> (replaced) [<a href="/pdf/2310.19192" title="Download PDF">pdf</a>, <a href="/format/2310.19192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of Grid-like Representations by Training Recurrent Networks  with Conformal Normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+D">Dehong Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Gao%2C+R">Ruiqi Gao</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+W">Wen-Hao Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wei%2C+X">Xue-Xin Wei</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19531" title="Abstract">arXiv:2310.19531</a> (replaced) [<a href="/pdf/2310.19531" title="Download PDF">pdf</a>, <a href="/format/2310.19531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties  in Generative Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenpeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xue Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+G">Guiguang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19906" title="Abstract">arXiv:2310.19906</a> (replaced) [<a href="/pdf/2310.19906" title="Download PDF">pdf</a>, <a href="/format/2310.19906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Prototype-based Graph Information Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Sangwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20187" title="Abstract">arXiv:2310.20187</a> (replaced) [<a href="/pdf/2310.20187" title="Download PDF">pdf</a>, <a href="/format/2310.20187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Pre-Training for Precipitation Post-Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Sojung An</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jiyeon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Na%2C+I">Inchae Na</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+W">Wooyeon Park</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Sujeong You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 1 table, accepted to NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning at [this http URL](<a href="https://www.climatechange.ai/papers/neurips2023/18">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20522" title="Abstract">arXiv:2310.20522</a> (replaced) [<a href="/pdf/2310.20522" title="Download PDF">pdf</a>, <a href="/format/2310.20522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight bounds on adjacency labels for monotone graph classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonnet%2C+%C3%89">&#xc9;douard Bonnet</a>, 
<a href="/search/math?searchtype=author&query=Duron%2C+J">Julien Duron</a>, 
<a href="/search/math?searchtype=author&query=Sylvester%2C+J">John Sylvester</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>, 
<a href="/search/math?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New result added (monotone small classes have bounded degeneracy - thus an implicit representation). 22 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00967" title="Abstract">arXiv:2311.00967</a> (replaced) [<a href="/pdf/2311.00967" title="Download PDF">pdf</a>, <a href="/format/2311.00967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Interpreter for Robot Task Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shirai%2C+K">Keisuke Shirai</a>, 
<a href="/search/cs?searchtype=author&query=Beltran-Hernandez%2C+C+C">Cristian C. Beltran-Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Hamaya%2C+M">Masashi Hamaya</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+A">Atsushi Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+S">Shohei Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Kawaharazuka%2C+K">Kento Kawaharazuka</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kazutoshi Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Ushiku%2C+Y">Yoshitaka Ushiku</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+S">Shinsuke Mori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01139" title="Abstract">arXiv:2311.01139</a> (replaced) [<a href="/pdf/2311.01139" title="Download PDF">pdf</a>, <a href="/format/2311.01139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Add and Thin: Diffusion for Temporal Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%BCdke%2C+D">David L&#xfc;dke</a>, 
<a href="/search/cs?searchtype=author&query=Bilo%C5%A1%2C+M">Marin Bilo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Shchur%2C+O">Oleksandr Shchur</a>, 
<a href="/search/cs?searchtype=author&query=Lienen%2C+M">Marten Lienen</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05661" title="Abstract">arXiv:2311.05661</a> (replaced) [<a href="/pdf/2311.05661" title="Download PDF">pdf</a>, <a href="/format/2311.05661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Engineering a Prompt Engineer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinyuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Axmed%2C+M">Maxamed Axmed</a>, 
<a href="/search/cs?searchtype=author&query=Pryzant%2C+R">Reid Pryzant</a>, 
<a href="/search/cs?searchtype=author&query=Khani%2C+F">Fereshte Khani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06555" title="Abstract">arXiv:2311.06555</a> (replaced) [<a href="/pdf/2311.06555" title="Download PDF">pdf</a>, <a href="/format/2311.06555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language  Models for Document-Level Event Argument Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hanzhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+J">Junlang Qian</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zijian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zixiao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kezhi Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08570" title="Abstract">arXiv:2311.08570</a> (replaced) [<a href="/pdf/2311.08570" title="Download PDF">pdf</a>, <a href="/ps/2311.08570" title="Download PostScript">ps</a>, <a href="/format/2311.08570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxation strength for multilinear optimization: McCormick strikes back
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schutte%2C+E">Emily Schutte</a>, 
<a href="/search/math?searchtype=author&query=Walter%2C+M">Matthias Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09538" title="Abstract">arXiv:2311.09538</a> (replaced) [<a href="/pdf/2311.09538" title="Download PDF">pdf</a>, <a href="/format/2311.09538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Privacy Risks in Online Self-Disclosures with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+Y">Yao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Krsek%2C+I">Isadora Krsek</a>, 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Kabra%2C+A">Anubha Kabra</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LLMs, Privacy, HCI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09766" title="Abstract">arXiv:2311.09766</a> (replaced) [<a href="/pdf/2311.09766" title="Download PDF">pdf</a>, <a href="/format/2311.09766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09868" title="Abstract">arXiv:2311.09868</a> (replaced) [<a href="/pdf/2311.09868" title="Download PDF">pdf</a>, <a href="/format/2311.09868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INTERVENOR: Prompting the Coding Ability of Large Language Models with  the Interactive Chain of Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+G">Ganqu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 19 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10537" title="Abstract">arXiv:2311.10537</a> (replaced) [<a href="/pdf/2311.10537" title="Download PDF">pdf</a>, <a href="/format/2311.10537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedAgents: Large Language Models as Collaborators for Zero-shot Medical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Anni Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12631" title="Abstract">arXiv:2311.12631</a> (replaced) [<a href="/pdf/2311.12631" title="Download PDF">pdf</a>, <a href="/format/2311.12631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via  Blender-Oriented GPT Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jiaxi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Mingfu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiancheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianzhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yafei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12882" title="Abstract">arXiv:2311.12882</a> (replaced) [<a href="/pdf/2311.12882" title="Download PDF">pdf</a>, <a href="/ps/2311.12882" title="Download PostScript">ps</a>, <a href="/format/2311.12882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of Current Applications of Large Language Models in Various  Medical Specialities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mumtaz%2C+U">Ummara Mumtaz</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Awais Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Mumtaz%2C+S">Summaya Mumtaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages and one figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13199" title="Abstract">arXiv:2311.13199</a> (replaced) [<a href="/pdf/2311.13199" title="Download PDF">pdf</a>, <a href="/format/2311.13199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Synthetic Supervising and Multi-view Consistency  Self-supervising based Animal 3D Reconstruction by Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Zijian Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lihang Ying</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Li Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13845" title="Abstract">arXiv:2311.13845</a> (replaced) [<a href="/pdf/2311.13845" title="Download PDF">pdf</a>, <a href="/format/2311.13845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Touring sampling with pushforward maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/cs?searchtype=author&query=Arnal%2C+C">Charles Arnal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICASSP, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14688" title="Abstract">arXiv:2311.14688</a> (replaced) [<a href="/pdf/2311.14688" title="Download PDF">pdf</a>, <a href="/format/2311.14688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Procedural Fairness Through Decoupling Objectionable Data Generating  Components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zeyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jialu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Spirtes%2C+P">Peter Spirtes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 12th International Conference on Learning Representations
  (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16476" title="Abstract">arXiv:2311.16476</a> (replaced) [<a href="/pdf/2311.16476" title="Download PDF">pdf</a>, <a href="/format/2311.16476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LANS: A Layout-Aware Neural Solver for Plane Geometry Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhong-Zhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming-Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cheng-Lin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17281" title="Abstract">arXiv:2311.17281</a> (replaced) [<a href="/pdf/2311.17281" title="Download PDF">pdf</a>, <a href="/ps/2311.17281" title="Download PostScript">ps</a>, <a href="/format/2311.17281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower Bounds on Adaptive Sensing for Matrix Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacham%2C+P">Praneeth Kacham</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P Woodruff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed minor typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18083" title="Abstract">arXiv:2311.18083</a> (replaced) [<a href="/pdf/2311.18083" title="Download PDF">pdf</a>, <a href="/format/2311.18083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Co-Training: Two Views are Better than One
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rothenberger%2C+J+C">Jay C. Rothenberger</a>, 
<a href="/search/cs?searchtype=author&query=Diochnos%2C+D+I">Dimitrios I. Diochnos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 14 figures, 10 tables, for implementation see <a href="https://github.com/JayRothenberger/Meta-Co-Training">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18126" title="Abstract">arXiv:2311.18126</a> (replaced) [<a href="/pdf/2311.18126" title="Download PDF">pdf</a>, <a href="/format/2311.18126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisMech: A Discrete Differential Geometry-based Physical Simulator for  Soft Robots and Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+R">Ran Jing</a>, 
<a href="/search/cs?searchtype=author&query=Sabelhaus%2C+A">Andrew Sabelhaus</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">Mohammad Khalid Jawed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Robotics and Automation Letters (RA-L 2024). Youtube video: <a href="https://www.youtube.com/watch?v=0jE9h5GpOek">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00394" title="Abstract">arXiv:2312.00394</a> (replaced) [<a href="/pdf/2312.00394" title="Download PDF">pdf</a>, <a href="/ps/2312.00394" title="Download PostScript">ps</a>, <a href="/format/2312.00394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Coding Capacity of Reverse-Complement and Palindromic  Duplication-Correcting Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yohananov%2C+L">Lev Yohananov</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+M">Moshe Schwartz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01817" title="Abstract">arXiv:2312.01817</a> (replaced) [<a href="/pdf/2312.01817" title="Download PDF">pdf</a>, <a href="/format/2312.01817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simulation method for the wetting dynamics of liquid droplets on  deformable membranes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Mokbel%2C+M">Marcel Mokbel</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mokbel%2C+D">Dominic Mokbel</a>, 
<a href="/search/cond-mat?searchtype=author&query=Liese%2C+S">Susanne Liese</a>, 
<a href="/search/cond-mat?searchtype=author&query=Weber%2C+C+A">Christoph A. Weber</a>, 
<a href="/search/cond-mat?searchtype=author&query=Aland%2C+S">Sebastian Aland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Computational Engineering, Finance, and Science (cs.CE); Biological Physics (physics.bio-ph); Computational Physics (physics.comp-ph); Subcellular Processes (q-bio.SC)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02073" title="Abstract">arXiv:2312.02073</a> (replaced) [<a href="/pdf/2312.02073" title="Download PDF">pdf</a>, <a href="/format/2312.02073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Glitch in the Matrix? Locating and Detecting Language Model Grounding  with Fakepedia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monea%2C+G">Giovanni Monea</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+V">Vishrav Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Eisner%2C+J">Jason Eisner</a>, 
<a href="/search/cs?searchtype=author&query=K%C4%B1c%C4%B1man%2C+E">Emre K&#x131;c&#x131;man</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Patra%2C+B">Barun Patra</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04974" title="Abstract">arXiv:2312.04974</a> (replaced) [<a href="/pdf/2312.04974" title="Download PDF">pdf</a>, <a href="/format/2312.04974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shifting Climates: Climate Change Communication from YouTube to TikTok
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pera%2C+A">Arianna Pera</a>, 
<a href="/search/cs?searchtype=author&query=Aiello%2C+L+M">Luca Maria Aiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WebSci 2024. 6 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06365" title="Abstract">arXiv:2312.06365</a> (replaced) [<a href="/pdf/2312.06365" title="Download PDF">pdf</a>, <a href="/ps/2312.06365" title="Download PostScript">ps</a>, <a href="/format/2312.06365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simulation-based Approach to Kinematics Analysis of a Quadruped Robot  and Prototype Leg Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahriar%2C+A">Abid Shahriar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 Figures. v3: Minor Update
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06658" title="Abstract">arXiv:2312.06658</a> (replaced) [<a href="/pdf/2312.06658" title="Download PDF">pdf</a>, <a href="/format/2312.06658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean estimation in the add-remove model of differential privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulesza%2C+A">Alex Kulesza</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+A+T">Ananda Theertha Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06717" title="Abstract">arXiv:2312.06717</a> (replaced) [<a href="/pdf/2312.06717" title="Download PDF">pdf</a>, <a href="/format/2312.06717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Issues in Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+P">Peter Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> February 2023 Update
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07331" title="Abstract">arXiv:2312.07331</a> (replaced) [<a href="/pdf/2312.07331" title="Download PDF">pdf</a>, <a href="/format/2312.07331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupled Confusion Correction: Learning from Crowds with Sparse  Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hansong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shikun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenggang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shiming Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08128" title="Abstract">arXiv:2312.08128</a> (replaced) [<a href="/pdf/2312.08128" title="Download PDF">pdf</a>, <a href="/format/2312.08128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clockwork Diffusion: Efficient Generation With Model-Step Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habibian%2C+A">Amirhossein Habibian</a>, 
<a href="/search/cs?searchtype=author&query=Ghodrati%2C+A">Amir Ghodrati</a>, 
<a href="/search/cs?searchtype=author&query=Fathima%2C+N">Noor Fathima</a>, 
<a href="/search/cs?searchtype=author&query=Sautiere%2C+G">Guillaume Sautiere</a>, 
<a href="/search/cs?searchtype=author&query=Garrepalli%2C+R">Risheek Garrepalli</a>, 
<a href="/search/cs?searchtype=author&query=Porikli%2C+F">Fatih Porikli</a>, 
<a href="/search/cs?searchtype=author&query=Petersen%2C+J">Jens Petersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08223" title="Abstract">arXiv:2312.08223</a> (replaced) [<a href="/pdf/2312.08223" title="Download PDF">pdf</a>, <a href="/format/2312.08223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-wise Graph Contrastive Learning for Image Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+C">Chanyong Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+G">Gihyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08617" title="Abstract">arXiv:2312.08617</a> (replaced) [<a href="/pdf/2312.08617" title="Download PDF">pdf</a>, <a href="/format/2312.08617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our  Open-Source Dataset and Lightweight Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wenji Fang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhiyao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08976" title="Abstract">arXiv:2312.08976</a> (replaced) [<a href="/pdf/2312.08976" title="Download PDF">pdf</a>, <a href="/format/2312.08976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Retrieval-Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapkin%2C+A">Anton Shapkin</a>, 
<a href="/search/cs?searchtype=author&query=Litvinov%2C+D">Denis Litvinov</a>, 
<a href="/search/cs?searchtype=author&query=Zharov%2C+Y">Yaroslav Zharov</a>, 
<a href="/search/cs?searchtype=author&query=Bogomolov%2C+E">Egor Bogomolov</a>, 
<a href="/search/cs?searchtype=author&query=Galimzyanov%2C+T">Timur Galimzyanov</a>, 
<a href="/search/cs?searchtype=author&query=Bryksin%2C+T">Timofey Bryksin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09007" title="Abstract">arXiv:2312.09007</a> (replaced) [<a href="/pdf/2312.09007" title="Download PDF">pdf</a>, <a href="/format/2312.09007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hongwei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liew%2C+S+C">Soung Chang Liew</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10993" title="Abstract">arXiv:2312.10993</a> (replaced) [<a href="/e-print/2312.10993" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realistic Human Motion Generation with Cross-Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zeping Ren</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoli Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Some conclusion is incorrect. We're going to do some additional experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11389" title="Abstract">arXiv:2312.11389</a> (replaced) [<a href="/pdf/2312.11389" title="Download PDF">pdf</a>, <a href="/format/2312.11389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Estimation of Under Frequency Load Shedding after Outages in  Small Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rajabdorri%2C+M">Mohammad Rajabdorri</a>, 
<a href="/search/eess?searchtype=author&query=Sigrist%2C+L">Lukas Sigrist</a>, 
<a href="/search/eess?searchtype=author&query=Lobato%2C+E">Enrique Lobato</a>, 
<a href="/search/eess?searchtype=author&query=Troffaes%2C+M+C+M">Matthias C. M. Troffaes</a>, 
<a href="/search/eess?searchtype=author&query=Kazemtabrizi%2C+B">Behzad Kazemtabrizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11456" title="Abstract">arXiv:2312.11456</a> (replaced) [<a href="/pdf/2312.11456" title="Download PDF">pdf</a>, <a href="/format/2312.11456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Preference Learning from Human Feedback: Bridging Theory and  Practice for RLHF under KL-Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hanze Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages; mathematical foundation and practical algorithms of RLHF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12137" title="Abstract">arXiv:2312.12137</a> (replaced) [<a href="/pdf/2312.12137" title="Download PDF">pdf</a>, <a href="/format/2312.12137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Arm Identification with Fixed Budget: A Large Deviation Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Po-An Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tzeng%2C+R">Ruo-Chun Tzeng</a>, 
<a href="/search/cs?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We made small for implementing SH algorithm, now it has been corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12347" title="Abstract">arXiv:2312.12347</a> (replaced) [<a href="/pdf/2312.12347" title="Download PDF">pdf</a>, <a href="/format/2312.12347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised  Temporal Action Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Feixiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zheheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huiyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15006" title="Abstract">arXiv:2312.15006</a> (replaced) [<a href="/pdf/2312.15006" title="Download PDF">pdf</a>, <a href="/format/2312.15006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Impact of Prompting Methods on ChatGPT&#x27;s Mathematical  Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+C">Chloe Wong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hanwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Aguenza%2C+J">Juan Aguenza</a>, 
<a href="/search/cs?searchtype=author&query=Bhujangari%2C+S">Sai Bhujangari</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+B">Benthan Vu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xun Lei</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+A">Amisha Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Fluss%2C+M">Manny Fluss</a>, 
<a href="/search/cs?searchtype=author&query=Phuong%2C+E">Eric Phuong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Raja Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Vats%2C+V">Vanshika Vats</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">James Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15995" title="Abstract">arXiv:2312.15995</a> (replaced) [<a href="/pdf/2312.15995" title="Download PDF">pdf</a>, <a href="/ps/2312.15995" title="Download PostScript">ps</a>, <a href="/format/2312.15995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization in Kernel Regression Under Realistic Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barzilai%2C+D">Daniel Barzilai</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+O">Ohad Shamir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16414" title="Abstract">arXiv:2312.16414</a> (replaced) [<a href="/pdf/2312.16414" title="Download PDF">pdf</a>, <a href="/format/2312.16414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bellman Optimal Stepsize Straightening of Flow-Matching Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B">Bao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B">Binh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+A">Viet Anh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04157" title="Abstract">arXiv:2401.04157</a> (replaced) [<a href="/pdf/2401.04157" title="Download PDF">pdf</a>, <a href="/format/2401.04157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RePLan: Robotic Replanning with Perception and Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skreta%2C+M">Marta Skreta</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J+L">Jia Lin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Darvish%2C+K">Kourosh Darvish</a>, 
<a href="/search/cs?searchtype=author&query=Aspuru-Guzik%2C+A">Al&#xe1;n Aspuru-Guzik</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Animesh Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04620" title="Abstract">arXiv:2401.04620</a> (replaced) [<a href="/pdf/2401.04620" title="Download PDF">pdf</a>, <a href="/format/2401.04620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent Alignment in Evolving Social Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shimin Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04812" title="Abstract">arXiv:2401.04812</a> (replaced) [<a href="/pdf/2401.04812" title="Download PDF">pdf</a>, <a href="/format/2401.04812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-and-Bound for Non-Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yaoguang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhizhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicun Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at AAAI 2024. Code is available at <a href="https://github.com/aaucsd/MCIR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04831" title="Abstract">arXiv:2401.04831</a> (replaced) [<a href="/pdf/2401.04831" title="Download PDF">pdf</a>, <a href="/format/2401.04831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Low-Altitude Navigation in Steep Terrain with Fixed-Wing Aerial  Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+J">Jaeyoung Lim</a>, 
<a href="/search/cs?searchtype=author&query=Achermann%2C+F">Florian Achermann</a>, 
<a href="/search/cs?searchtype=author&query=Girod%2C+R">Rik Girod</a>, 
<a href="/search/cs?searchtype=author&query=Lawrance%2C+N">Nicholas Lawrance</a>, 
<a href="/search/cs?searchtype=author&query=Siegwart%2C+R">Roland Siegwart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Robotics and Automation Letters (RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06659" title="Abstract">arXiv:2401.06659</a> (replaced) [<a href="/pdf/2401.06659" title="Download PDF">pdf</a>, <a href="/format/2401.06659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual  World Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06853" title="Abstract">arXiv:2401.06853</a> (replaced) [<a href="/pdf/2401.06853" title="Download PDF">pdf</a>, <a href="/format/2401.06853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Can Learn Temporal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+S">Siheng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Payani%2C+A">Ali Payani</a>, 
<a href="/search/cs?searchtype=author&query=Kompella%2C+R">Ramana Kompella</a>, 
<a href="/search/cs?searchtype=author&query=Fekri%2C+F">Faramarz Fekri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07128" title="Abstract">arXiv:2401.07128</a> (replaced) [<a href="/pdf/2401.07128" title="Download PDF">pdf</a>, <a href="/format/2401.07128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EHRAgent: Code Empowers Large Language Models for Few-shot Complex  Tabular Reasoning on Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanda Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+J">Joyce Ho</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Carl Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M+D">May D. Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07453" title="Abstract">arXiv:2401.07453</a> (replaced) [<a href="/pdf/2401.07453" title="Download PDF">pdf</a>, <a href="/format/2401.07453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Editing at Scale leads to Gradual and Catastrophic Forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Akshat Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Anurag Rao</a>, 
<a href="/search/cs?searchtype=author&query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added experiments with GPT-J and appendix for all samples and ablations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07598" title="Abstract">arXiv:2401.07598</a> (replaced) [<a href="/pdf/2401.07598" title="Download PDF">pdf</a>, <a href="/format/2401.07598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+D">Divyanshu Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Ashutosh Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Watts%2C+I">Ishaan Watts</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 23 figures, 45 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07851" title="Abstract">arXiv:2401.07851</a> (replaced) [<a href="/pdf/2401.07851" title="Download PDF">pdf</a>, <a href="/format/2401.07851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Efficiency in Large Language Model Inference: A Comprehensive  Survey of Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Heming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08189" title="Abstract">arXiv:2401.08189</a> (replaced) [<a href="/pdf/2401.08189" title="Download PDF">pdf</a>, <a href="/format/2401.08189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRewrite: Prompt Rewriting with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weize Kong</a>, 
<a href="/search/cs?searchtype=author&query=Hombaiah%2C+S+A">Spurthi Amba Hombaiah</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09198" title="Abstract">arXiv:2401.09198</a> (replaced) [<a href="/pdf/2401.09198" title="Download PDF">pdf</a>, <a href="/format/2401.09198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space and Time Continuous Physics Simulation From Partial Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steeven%2C+J">Janny Steeven</a>, 
<a href="/search/cs?searchtype=author&query=Madiha%2C+N">Nadri Madiha</a>, 
<a href="/search/cs?searchtype=author&query=Julie%2C+D">Digne Julie</a>, 
<a href="/search/cs?searchtype=author&query=Christian%2C+W">Wolf Christian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://continuous-pde.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09491" title="Abstract">arXiv:2401.09491</a> (replaced) [<a href="/pdf/2401.09491" title="Download PDF">pdf</a>, <a href="/ps/2401.09491" title="Download PostScript">ps</a>, <a href="/format/2401.09491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory, Space, and Planning: Multiscale Predictive Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published as a chapter in an edited volume by Oxford University Press (Editors: Sara Aronowitz and Lynn Nadel)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10638" title="Abstract">arXiv:2401.10638</a> (replaced) [<a href="/pdf/2401.10638" title="Download PDF">pdf</a>, <a href="/ps/2401.10638" title="Download PostScript">ps</a>, <a href="/format/2401.10638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurately Computing Expected Visiting Times and Stationary  Distributions in Markov Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mertens%2C+H">Hannah Mertens</a>, 
<a href="/search/cs?searchtype=author&query=Katoen%2C+J">Joost-Pieter Katoen</a>, 
<a href="/search/cs?searchtype=author&query=Quatmann%2C+T">Tim Quatmann</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+T">Tobias Winkler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12585" title="Abstract">arXiv:2401.12585</a> (replaced) [<a href="/pdf/2401.12585" title="Download PDF">pdf</a>, <a href="/format/2401.12585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLANG: New Concept Comprehension of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Baolong Bi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12594" title="Abstract">arXiv:2401.12594</a> (replaced) [<a href="/pdf/2401.12594" title="Download PDF">pdf</a>, <a href="/format/2401.12594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification  and Learning Analytics to Train Cybersecurity Competencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nespoli%2C+P">Pantaleone Nespoli</a>, 
<a href="/search/cs?searchtype=author&query=Albaladejo-Gonz%C3%A1lez%2C+M">Mariano Albaladejo-Gonz&#xe1;lez</a>, 
<a href="/search/cs?searchtype=author&query=Valera%2C+J+A+P">Jos&#xe9; Antonio Pastor Valera</a>, 
<a href="/search/cs?searchtype=author&query=Ruip%C3%A9rez-Valiente%2C+J+A">Jos&#xe9; A. Ruip&#xe9;rez-Valiente</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Alfaro%2C+J">Joaquin Garcia-Alfaro</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A1rmol%2C+F+G">F&#xe9;lix G&#xf3;mez M&#xe1;rmol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13129" title="Abstract">arXiv:2401.13129</a> (replaced) [<a href="/pdf/2401.13129" title="Download PDF">pdf</a>, <a href="/format/2401.13129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seed-Guided Fine-Grained Entity Typing in Science and Engineering  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanzhen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Popa%2C+L">Lucian Popa</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz%2C+L">Larisa Shwartz</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">ChengXiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages; Accepted to AAAI 2024 (Code: <a href="https://github.com/yuzhimanhua/SEType">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13227" title="Abstract">arXiv:2401.13227</a> (replaced) [<a href="/pdf/2401.13227" title="Download PDF">pdf</a>, <a href="/format/2401.13227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LPNL: Scalable Link Prediction with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Baolong Bi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13601" title="Abstract">arXiv:2401.13601</a> (replaced) [<a href="/pdf/2401.13601" title="Download PDF">pdf</a>, <a href="/format/2401.13601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-LLMs: Recent Advances in MultiModal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Duzhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yahan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxing Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14606" title="Abstract">arXiv:2401.14606</a> (replaced) [<a href="/pdf/2401.14606" title="Download PDF">pdf</a>, <a href="/format/2401.14606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenging Low Homophily in Social Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guandong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by The Web Conference (WWW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16348" title="Abstract">arXiv:2401.16348</a> (replaced) [<a href="/pdf/2401.16348" title="Download PDF">pdf</a>, <a href="/format/2401.16348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the TENOR of Labeling: Re-evaluating Topic Models for Content  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Andrew Mao</a>, 
<a href="/search/cs?searchtype=author&query=Stephens%2C+D">Daniel Stephens</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+P">Pranav Goel</a>, 
<a href="/search/cs?searchtype=author&query=Walpole%2C+E">Emily Walpole</a>, 
<a href="/search/cs?searchtype=author&query=Dima%2C+A">Alden Dima</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+J">Juan Fung</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 tables, 6 figures, Accepted to EACL Main Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16553" title="Abstract">arXiv:2401.16553</a> (replaced) [<a href="/pdf/2401.16553" title="Download PDF">pdf</a>, <a href="/format/2401.16553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelectLLM: Can LLMs Select Important Instructions to Annotate?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parkar%2C+R+S">Ritik Sachin Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaehyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+I">Jong Inn Park</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author: Jong Inn Park | PI: Dongyeop Kang
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16643" title="Abstract">arXiv:2401.16643</a> (replaced) [<a href="/pdf/2401.16643" title="Download PDF">pdf</a>, <a href="/format/2401.16643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game of Coding: Beyond Trusted Majorities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nodehi%2C+H+A">Hanzaleh Akbari Nodehi</a>, 
<a href="/search/cs?searchtype=author&query=Cadambe%2C+V+R">Viveck R. Cadambe</a>, 
<a href="/search/cs?searchtype=author&query=Maddah-Ali%2C+M+A">Mohammad Ali Maddah-Ali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16668" title="Abstract">arXiv:2401.16668</a> (replaced) [<a href="/pdf/2401.16668" title="Download PDF">pdf</a>, <a href="/format/2401.16668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InteractOut: Leveraging Interaction Proxies as Input Manipulation  Strategies for Reducing Smartphone Overuse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hongxiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Anhong Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01607" title="Abstract">arXiv:2402.01607</a> (replaced) [<a href="/pdf/2402.01607" title="Download PDF">pdf</a>, <a href="/format/2402.01607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Counterfactuals With Necessary Backtracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+G">Guang-Yuan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01697" title="Abstract">arXiv:2402.01697</a> (replaced) [<a href="/pdf/2402.01697" title="Download PDF">pdf</a>, <a href="/format/2402.01697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhizhuo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+E">Ehsan-Ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lik-Hang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024; Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01930" title="Abstract">arXiv:2402.01930</a> (replaced) [<a href="/pdf/2402.01930" title="Download PDF">pdf</a>, <a href="/format/2402.01930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Optimism Bias in Incomplete Cooperative Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%9Aradn%C3%ADk%2C+F">Filip &#xda;radn&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Sychrovsk%C3%BD%2C+D">David Sychrovsk&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cern%C3%BD%2C+J">Jakub &#x10c;ern&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cern%C3%BD%2C+M">Martin &#x10c;ern&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02977" title="Abstract">arXiv:2402.02977</a> (replaced) [<a href="/pdf/2402.02977" title="Download PDF">pdf</a>, <a href="/format/2402.02977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Flow Models: Flowing in Your Style
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Kien Do</a>, 
<a href="/search/cs?searchtype=author&query=Kieu%2C+D">Duc Kieu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Toan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thin Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03173" title="Abstract">arXiv:2402.03173</a> (replaced) [<a href="/pdf/2402.03173" title="Download PDF">pdf</a>, <a href="/format/2402.03173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MULTI: Multimodal Understanding Leaderboard with Text and Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yichuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hailin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yingzi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Situo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liangtai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, 10 tables. Details and access are available at: <a href="https://OpenDFM.github.io/MULTI-Benchmark/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03190" title="Abstract">arXiv:2402.03190</a> (replaced) [<a href="/pdf/2402.03190" title="Download PDF">pdf</a>, <a href="/format/2402.03190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Hallucination Detection for Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03843" title="Abstract">arXiv:2402.03843</a> (replaced) [<a href="/pdf/2402.03843" title="Download PDF">pdf</a>, <a href="/format/2402.03843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new method for optical steel rope non-destructive damage detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yunqing Bao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04333" title="Abstract">arXiv:2402.04333</a> (replaced) [<a href="/pdf/2402.04333" title="Download PDF">pdf</a>, <a href="/format/2402.04333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LESS: Selecting Influential Data for Targeted Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Malladi%2C+S">Sadhika Malladi</a>, 
<a href="/search/cs?searchtype=author&query=Gururangan%2C+S">Suchin Gururangan</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data are available at <a href="https://github.com/princeton-nlp/LESS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05388" title="Abstract">arXiv:2402.05388</a> (replaced) [<a href="/pdf/2402.05388" title="Download PDF">pdf</a>, <a href="/format/2402.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Form-From: A Design Space of Social Media Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+X">Amy X. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+M+S">Michael S. Bernstein</a>, 
<a href="/search/cs?searchtype=author&query=Karger%2C+D+R">David R. Karger</a>, 
<a href="/search/cs?searchtype=author&query=Ackerman%2C+M+S">Mark S. Ackerman</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 167 (April
  2024), 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05919" title="Abstract">arXiv:2402.05919</a> (replaced) [<a href="/pdf/2402.05919" title="Download PDF">pdf</a>, <a href="/format/2402.05919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Control for Geometry-Conditioned PBR Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vainer%2C+S">Shimon Vainer</a>, 
<a href="/search/cs?searchtype=author&query=Boss%2C+M">Mark Boss</a>, 
<a href="/search/cs?searchtype=author&query=Parger%2C+M">Mathias Parger</a>, 
<a href="/search/cs?searchtype=author&query=Kutsy%2C+K">Konstantin Kutsy</a>, 
<a href="/search/cs?searchtype=author&query=De+Nigris%2C+D">Dante De Nigris</a>, 
<a href="/search/cs?searchtype=author&query=Rowles%2C+C">Ciara Rowles</a>, 
<a href="/search/cs?searchtype=author&query=Perony%2C+N">Nicolas Perony</a>, 
<a href="/search/cs?searchtype=author&query=Donn%C3%A9%2C+S">Simon Donn&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures; Project page: <a href="https://unity-research.github.io/holo-gen/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05926" title="Abstract">arXiv:2402.05926</a> (replaced) [<a href="/pdf/2402.05926" title="Download PDF">pdf</a>, <a href="/format/2402.05926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of Zeroth-Order Federated Tuning for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhenqing Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05937" title="Abstract">arXiv:2402.05937</a> (replaced) [<a href="/pdf/2402.05937" title="Download PDF">pdf</a>, <a href="/format/2402.05937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstaGen: Enhancing Object Detection by Training on Synthetic Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chengjian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yujie Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+Z">Zequn Jie</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05956" title="Abstract">arXiv:2402.05956</a> (replaced) [<a href="/pdf/2402.05956" title="Download PDF">pdf</a>, <a href="/format/2402.05956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale transformers with Adaptive Pathways for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yunyao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yang Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chenjuan Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06196" title="Abstract">arXiv:2402.06196</a> (replaced) [<a href="/pdf/2402.06196" title="Download PDF">pdf</a>, <a href="/format/2402.06196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minaee%2C+S">Shervin Minaee</a>, 
<a href="/search/cs?searchtype=author&query=Mikolov%2C+T">Tomas Mikolov</a>, 
<a href="/search/cs?searchtype=author&query=Nikzad%2C+N">Narjes Nikzad</a>, 
<a href="/search/cs?searchtype=author&query=Chenaghlu%2C+M">Meysam Chenaghlu</a>, 
<a href="/search/cs?searchtype=author&query=Socher%2C+R">Richard Socher</a>, 
<a href="/search/cs?searchtype=author&query=Amatriain%2C+X">Xavier Amatriain</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2401.14423">arXiv:2401.14423</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06989" title="Abstract">arXiv:2402.06989</a> (replaced) [<a href="/pdf/2402.06989" title="Download PDF">pdf</a>, <a href="/ps/2402.06989" title="Download PostScript">ps</a>, <a href="/format/2402.06989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing for Work with Intelligent Entities: A Review of Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+J+E">James E. McCarthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07140" title="Abstract">arXiv:2402.07140</a> (replaced) [<a href="/pdf/2402.07140" title="Download PDF">pdf</a>, <a href="/format/2402.07140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Descriptive Order Improves Reasoning with Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuyao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wenjie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07197" title="Abstract">arXiv:2402.07197</a> (replaced) [<a href="/pdf/2402.07197" title="Download PDF">pdf</a>, <a href="/format/2402.07197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphTranslator: Aligning Graph Model to Large Language Model for  Open-ended Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Shen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Yanhu Mo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoxiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07220" title="Abstract">arXiv:2402.07220</a> (replaced) [<a href="/pdf/2402.07220" title="Download PDF">pdf</a>, <a href="/format/2402.07220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KVQ: Kwai Video Quality Assessment for Short-form Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lu%2C+Y">Yiting Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/eess?searchtype=author&query=Pei%2C+Y">Yajing Pei</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+Q">Qizhi Xie</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+Y">Yunpeng Qu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+M">Ming Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+C">Chao Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07422" title="Abstract">arXiv:2402.07422</a> (replaced) [<a href="/pdf/2402.07422" title="Download PDF">pdf</a>, <a href="/ps/2402.07422" title="Download PostScript">ps</a>, <a href="/format/2402.07422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> News Recommendation with Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chufeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weisheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Journal of Industrial Engineering and Applied Science
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Industrial Engineering and Applied Science 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07429" title="Abstract">arXiv:2402.07429</a> (replaced) [<a href="/pdf/2402.07429" title="Download PDF">pdf</a>, <a href="/ps/2402.07429" title="Download PostScript">ps</a>, <a href="/format/2402.07429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Particle Filter SLAM for Vehicle Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chufeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiqiang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, Journal of Industrial Engineering and Applied Science
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Industrial Engineering and Applied Science 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07485" title="Abstract">arXiv:2402.07485</a> (replaced) [<a href="/pdf/2402.07485" title="Download PDF">pdf</a>, <a href="/format/2402.07485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yifei Xin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhesong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bilei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07845" title="Abstract">arXiv:2402.07845</a> (replaced) [<a href="/pdf/2402.07845" title="Download PDF">pdf</a>, <a href="/ps/2402.07845" title="Download PostScript">ps</a>, <a href="/format/2402.07845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Optimisation of GNNs for Node Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leeney%2C+W">William Leeney</a>, 
<a href="/search/cs?searchtype=author&query=McConville%2C+R">Ryan McConville</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07859" title="Abstract">arXiv:2402.07859</a> (replaced) [<a href="/pdf/2402.07859" title="Download PDF">pdf</a>, <a href="/format/2402.07859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lissard: Long and Simple Sequential Reasoning Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bueno%2C+M">Mirelle Bueno</a>, 
<a href="/search/cs?searchtype=author&query=Lotufo%2C+R">Roberto Lotufo</a>, 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+R">Rodrigo Nogueira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08303" title="Abstract">arXiv:2402.08303</a> (replaced) [<a href="/e-print/2402.08303" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatCell: Facilitating Single-Cell Analysis with Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xinle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Penghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaohui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> I have decided to temporarily withdraw this draft as I am in the process of making further revisions to improve its content. Code: <a href="https://github.com/zjunlp/ChatCell">this https URL</a> Dataset: <a href="https://huggingface.co/datasets/zjunlp/ChatCell-Instructions">this https URL</a> Demo: <a href="https://chat.openai.com/g/g-vUwj222gQ-chatcell">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08437" title="Abstract">arXiv:2402.08437</a> (replaced) [<a href="/pdf/2402.08437" title="Download PDF">pdf</a>, <a href="/format/2402.08437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Camera Calibration through Geometric Constraints from Rotation and  Projection Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waleed%2C+M">Muhammad Waleed</a>, 
<a href="/search/cs?searchtype=author&query=Rauf%2C+A">Abdul Rauf</a>, 
<a href="/search/cs?searchtype=author&query=Taj%2C+M">Murtaza Taj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08491" title="Abstract">arXiv:2402.08491</a> (replaced) [<a href="/pdf/2402.08491" title="Download PDF">pdf</a>, <a href="/format/2402.08491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Controlled Traversing of the Attractor  Landscape of Boolean Models in the Context of Cellular Reprogramming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mizera%2C+A">Andrzej Mizera</a>, 
<a href="/search/cs?searchtype=author&query=Zarzycki%2C+J">Jakub Zarzycki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Molecular Networks (q-bio.MN); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08496" title="Abstract">arXiv:2402.08496</a> (replaced) [<a href="/pdf/2402.08496" title="Download PDF">pdf</a>, <a href="/format/2402.08496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Review of Data-to-Text NLG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osuji%2C+C+C">Chinonso Cynthia Osuji</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+T+C">Thiago Castro Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+B">Brian Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08617" title="Abstract">arXiv:2402.08617</a> (replaced) [<a href="/pdf/2402.08617" title="Download PDF">pdf</a>, <a href="/format/2402.08617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Quantum Power Flow: Unveiling the Limits of Practical  Quantum Advantage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pareek%2C+P">Parikshit Pareek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jayakumar%2C+A">Abhijith Jayakumar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Coffrin%2C+C">Carleton Coffrin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Misra%2C+S">Sidhant Misra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09191" title="Abstract">arXiv:2402.09191</a> (replaced) [<a href="/pdf/2402.09191" title="Download PDF">pdf</a>, <a href="/format/2402.09191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber Deception Reactive: TCP Stealth Redirection to On-Demand Honeypots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopez%2C+P+B">Pedro Beltran Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Nespoli%2C+P">Pantaleone Nespoli</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+M+G">Manuel Gil Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Performance (cs.PF); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09444" title="Abstract">arXiv:2402.09444</a> (replaced) [<a href="/pdf/2402.09444" title="Download PDF">pdf</a>, <a href="/format/2402.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Action Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zeng%2C+L">Ling-An Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+W">Wei-Shi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Image Processing 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09662" title="Abstract">arXiv:2402.09662</a> (replaced) [<a href="/pdf/2402.09662" title="Download PDF">pdf</a>, <a href="/format/2402.09662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mubarrat%2C+S+T">Syed T. Mubarrat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA 2024). 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09723" title="Abstract">arXiv:2402.09723</a> (replaced) [<a href="/pdf/2402.09723" title="Download PDF">pdf</a>, <a href="/format/2402.09723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Arm Identification for Prompt Learning under a Limited Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shi%2C+C">Chengshuai Shi</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+K">Kun Yang</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+C">Cong Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09772" title="Abstract">arXiv:2402.09772</a> (replaced) [<a href="/pdf/2402.09772" title="Download PDF">pdf</a>, <a href="/format/2402.09772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Experimental Design for Partially Observable Pure Birth  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eshragh%2C+A">Ali Eshragh</a>, 
<a href="/search/math?searchtype=author&query=Skerritt%2C+M+P">Matthew P. Skerritt</a>, 
<a href="/search/math?searchtype=author&query=Salvy%2C+B">Bruno Salvy</a>, 
<a href="/search/math?searchtype=author&query=McCallum%2C+T">Thomas McCallum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10011" title="Abstract">arXiv:2402.10011</a> (replaced) [<a href="/pdf/2402.10011" title="Download PDF">pdf</a>, <a href="/format/2402.10011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Group Equivariant Simplicial Message Passing Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Eijkelboom%2C+F">Floor Eijkelboom</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10184" title="Abstract">arXiv:2402.10184</a> (replaced) [<a href="/pdf/2402.10184" title="Download PDF">pdf</a>, <a href="/format/2402.10184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Josef Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10204" title="Abstract">arXiv:2402.10204</a> (replaced) [<a href="/pdf/2402.10204" title="Download PDF">pdf</a>, <a href="/format/2402.10204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radio-astronomical Image Reconstruction with Conditional Denoising  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Drozdova%2C+M">Mariia Drozdova</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kinakh%2C+V">Vitaliy Kinakh</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bait%2C+O">Omkar Bait</a>, 
<a href="/search/astro-ph?searchtype=author&query=Taran%2C+O">Olga Taran</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lastufka%2C+E">Erica Lastufka</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dessauges-Zavadsky%2C+M">Miroslava Dessauges-Zavadsky</a>, 
<a href="/search/astro-ph?searchtype=author&query=Holotyak%2C+T">Taras Holotyak</a>, 
<a href="/search/astro-ph?searchtype=author&query=Schaerer%2C+D">Daniel Schaerer</a>, 
<a href="/search/astro-ph?searchtype=author&query=Voloshynovskiy%2C+S">Slava Voloshynovskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In production in Astronomy&amp;Astrophyics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10259" title="Abstract">arXiv:2402.10259</a> (replaced) [<a href="/pdf/2402.10259" title="Download PDF">pdf</a>, <a href="/format/2402.10259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object  with Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sikuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiemin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Ruofan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lingxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://gaussianobject.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10307" title="Abstract">arXiv:2402.10307</a> (replaced) [<a href="/e-print/2402.10307" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Radio to Overcome Critical Link Budgets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R+R">Ralf R. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is not correct. The paper calculates the beamforming gain of transmit beamforming for N antennas as N, but it should be N^2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10311" title="Abstract">arXiv:2402.10311</a> (replaced) [<a href="/pdf/2402.10311" title="Download PDF">pdf</a>, <a href="/format/2402.10311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The optimal placement of the head in the noun phrase. The case of  demonstrative, numeral, adjective and noun
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrer-i-Cancho%2C+R">Ramon Ferrer-i-Cancho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10318" title="Abstract">arXiv:2402.10318</a> (replaced) [<a href="/e-print/2402.10318" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Antenna Towards Inband Shift Keying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R+R">Ralf R. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is incorrect. It calculates the beamforming gain of transmit beamforming for N antennas as N, but it should be N^2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10329" title="Abstract">arXiv:2402.10329</a> (replaced) [<a href="/pdf/2402.10329" title="Download PDF">pdf</a>, <a href="/format/2402.10329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Manipulation Interface: In-The-Wild Robot Teaching Without  In-The-Wild Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Cheng Chi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenjia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chuer Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cousineau%2C+E">Eric Cousineau</a>, 
<a href="/search/cs?searchtype=author&query=Burchfiel%2C+B">Benjamin Burchfiel</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Siyuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://umi-gripper.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10445" title="Abstract">arXiv:2402.10445</a> (replaced) [<a href="/pdf/2402.10445" title="Download PDF">pdf</a>, <a href="/format/2402.10445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Learning with Different Labeling Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Mingda Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 included additional discussion on related work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10669" title="Abstract">arXiv:2402.10669</a> (replaced) [<a href="/pdf/2402.10669" title="Download PDF">pdf</a>, <a href="/format/2402.10669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humans or LLMs as the Judge? A Study on Judgement Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+H">Guiming Hardy Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shunian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziche Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10870" title="Abstract">arXiv:2402.10870</a> (replaced) [<a href="/pdf/2402.10870" title="Download PDF">pdf</a>, <a href="/format/2402.10870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best of Three Worlds: Adaptive Experimentation for Digital Marketing in  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiez%2C+T">Tanner Fiez</a>, 
<a href="/search/cs?searchtype=author&query=Nassif%2C+H">Houssam Nassif</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Arick Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gamez%2C+S">Sergio Gamez</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Web Conference (WWW), Singapore, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10930" title="Abstract">arXiv:2402.10930</a> (replaced) [<a href="/pdf/2402.10930" title="Download PDF">pdf</a>, <a href="/format/2402.10930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanchen Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yifei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+D">Derek Chow</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zichen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+K">Kauna Lei</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bangfei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sylvester%2C+D">Dennis Sylvester</a>, 
<a href="/search/cs?searchtype=author&query=Kielian%2C+G">Gregory Kielian</a>, 
<a href="/search/cs?searchtype=author&query=Saligane%2C+M">Mehdi Saligane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10949" title="Abstract">arXiv:2402.10949</a> (replaced) [<a href="/pdf/2402.10949" title="Download PDF">pdf</a>, <a href="/ps/2402.10949" title="Download PostScript">ps</a>, <a href="/format/2402.10949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Unreasonable Effectiveness of Eccentric Automatic Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Battle%2C+R">Rick Battle</a>, 
<a href="/search/cs?searchtype=author&query=Gollapudi%2C+T">Teja Gollapudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10964" title="Abstract">arXiv:2402.10964</a> (replaced) [<a href="/pdf/2402.10964" title="Download PDF">pdf</a>, <a href="/format/2402.10964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal feature rescaling in machine learning based on neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vitr%C3%B2%2C+F+M">Federico Maria Vitr&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Leonesio%2C+M">Marco Leonesio</a>, 
<a href="/search/cs?searchtype=author&query=Fagiano%2C+L">Lorenzo Fagiano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11101" title="Abstract">arXiv:2402.11101</a> (replaced) [<a href="/pdf/2402.11101" title="Download PDF">pdf</a>, <a href="/ps/2402.11101" title="Download PostScript">ps</a>, <a href="/format/2402.11101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-based material parameters extraction from perovskite experiments  via Bayesian optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Zhan%2C+H">Hualin Zhan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ahmad%2C+V">Viqar Ahmad</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mayon%2C+A">Azul Mayon</a>, 
<a href="/search/cond-mat?searchtype=author&query=Tabi%2C+G">Grace Tabi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bui%2C+A+D">Anh Dinh Bui</a>, 
<a href="/search/cond-mat?searchtype=author&query=Li%2C+Z">Zhuofeng Li</a>, 
<a href="/search/cond-mat?searchtype=author&query=Walter%2C+D">Daniel Walter</a>, 
<a href="/search/cond-mat?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>, 
<a href="/search/cond-mat?searchtype=author&query=Weber%2C+K">Klaus Weber</a>, 
<a href="/search/cond-mat?searchtype=author&query=White%2C+T">Thomas White</a>, 
<a href="/search/cond-mat?searchtype=author&query=Catchpole%2C+K">Kylie Catchpole</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11105" title="Abstract">arXiv:2402.11105</a> (replaced) [<a href="/pdf/2402.11105" title="Download PDF">pdf</a>, <a href="/format/2402.11105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magic Mirror on the Wall, How to Benchmark Quantum Error Correction  Codes, Overall ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chatterjee%2C+A">Avimita Chatterjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11125" title="Abstract">arXiv:2402.11125</a> (replaced) [<a href="/pdf/2402.11125" title="Download PDF">pdf</a>, <a href="/format/2402.11125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> See Spot Guide: Accessible Interfaces for an Assistive Quadruped Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hata%2C+R">Rayna Hata</a>, 
<a href="/search/cs?searchtype=author&query=Trikasemsak%2C+N">Narit Trikasemsak</a>, 
<a href="/search/cs?searchtype=author&query=Giudice%2C+A">Andrea Giudice</a>, 
<a href="/search/cs?searchtype=author&query=Doore%2C+S+A">Stacy A. Doore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of the paper presented as "Rayna Hata and Stacy A. Doore. Assistive Agile Robot for Non-visual Navigation. IEEE International Conference on Intelligent Robots and Systems, Detroit, MI. October 1-5, 2023"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11127" title="Abstract">arXiv:2402.11127</a> (replaced) [<a href="/pdf/2402.11127" title="Download PDF">pdf</a>, <a href="/format/2402.11127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-Embroidery: A Study of Weaving Quantum Error Correction into the  Fabric of Quantum Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chatterjee%2C+A">Avimita Chatterjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+D">Debarshi Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11205" title="Abstract">arXiv:2402.11205</a> (replaced) [<a href="/pdf/2402.11205" title="Download PDF">pdf</a>, <a href="/format/2402.11205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Quantum Circuit for Block Encoding a Pairing Hamiltonian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nucl-th?searchtype=author&query=Liu%2C+D">Diyi Liu</a>, 
<a href="/search/nucl-th?searchtype=author&query=Du%2C+W">Weijie Du</a>, 
<a href="/search/nucl-th?searchtype=author&query=Lin%2C+L">Lin Lin</a>, 
<a href="/search/nucl-th?searchtype=author&query=Vary%2C+J+P">James P.Vary</a>, 
<a href="/search/nucl-th?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Nuclear Theory (nucl-th)</span>; Numerical Analysis (math.NA); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11222" title="Abstract">arXiv:2402.11222</a> (replaced) [<a href="/pdf/2402.11222" title="Download PDF">pdf</a>, <a href="/ps/2402.11222" title="Download PostScript">ps</a>, <a href="/format/2402.11222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Treewidth versus clique number. IV. Tree-independence number of graphs  excluding an induced star
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dallard%2C+C">Cl&#xe9;ment Dallard</a>, 
<a href="/search/math?searchtype=author&query=Krnc%2C+M">Matja&#x17e; Krnc</a>, 
<a href="/search/math?searchtype=author&query=Kwon%2C+O">O-joung Kwon</a>, 
<a href="/search/math?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Munaro%2C+A">Andrea Munaro</a>, 
<a href="/search/math?searchtype=author&query=%C5%A0torgel%2C+K">Kenny &#x160;torgel</a>, 
<a href="/search/math?searchtype=author&query=Wiederrecht%2C+S">Sebastian Wiederrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11248" title="Abstract">arXiv:2402.11248</a> (replaced) [<a href="/pdf/2402.11248" title="Download PDF">pdf</a>, <a href="/format/2402.11248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLLaVO: Crayon Large Language and Vision mOdel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byung-Kwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Beomchan Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C+W">Chae Won Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11401" title="Abstract">arXiv:2402.11401</a> (replaced) [<a href="/pdf/2402.11401" title="Download PDF">pdf</a>, <a href="/format/2402.11401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphKD: Exploring Knowledge Distillation Towards Document Object  Detection with Structured Graph Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Ayan Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11454" title="Abstract">arXiv:2402.11454</a> (replaced) [<a href="/pdf/2402.11454" title="Download PDF">pdf</a>, <a href="/format/2402.11454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Internally-Disconnected Communities in Leiden and Louvain  Community Detection Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+S">Subhajit Sahu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures, 1 table. arXiv admin note: text overlap with <a href="/abs/2312.13936">arXiv:2312.13936</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11502" title="Abstract">arXiv:2402.11502</a> (replaced) [<a href="/pdf/2402.11502" title="Download PDF">pdf</a>, <a href="/format/2402.11502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAD: Generative End-to-End Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenzhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ruiqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xianda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at: <a href="https://github.com/wzzheng/GenAD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11512" title="Abstract">arXiv:2402.11512</a> (replaced) [<a href="/pdf/2402.11512" title="Download PDF">pdf</a>, <a href="/format/2402.11512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Prejudice to Parity: A New Approach to Debiasing Large Language  Model Word Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakshit%2C+A">Aishik Rakshit</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Smriti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Keshari%2C+S">Shuvam Keshari</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+G">Arijit Ghosh Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11752" title="Abstract">arXiv:2402.11752</a> (replaced) [<a href="/pdf/2402.11752" title="Download PDF">pdf</a>, <a href="/ps/2402.11752" title="Download PostScript">ps</a>, <a href="/format/2402.11752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models  via Reparameterisation and Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagner%2C+D">Dominik Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Khajwal%2C+B">Basim Khajwal</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+C+-+L">C.-H. Luke Ong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11756" title="Abstract">arXiv:2402.11756</a> (replaced) [<a href="/pdf/2402.11756" title="Download PDF">pdf</a>, <a href="/format/2402.11756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in  Generative LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakman%2C+Y+F">Yavuz Faruk Bakman</a>, 
<a href="/search/cs?searchtype=author&query=Yaldiz%2C+D+N">Duygu Nur Yaldiz</a>, 
<a href="/search/cs?searchtype=author&query=Buyukates%2C+B">Baturalp Buyukates</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chenyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11770" title="Abstract">arXiv:2402.11770</a> (replaced) [<a href="/pdf/2402.11770" title="Download PDF">pdf</a>, <a href="/format/2402.11770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Chain-of-Thought Prompting for Few-Shot Generation of  Content-Grounded QA Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Ganhotra%2C+J">Jatin Ganhotra</a>, 
<a href="/search/cs?searchtype=author&query=Astudillo%2C+R+F">Ram&#xf3;n Fernandez Astudillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11792" title="Abstract">arXiv:2402.11792</a> (replaced) [<a href="/pdf/2402.11792" title="Download PDF">pdf</a>, <a href="/format/2402.11792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xuguang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+T">Tao Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11821" title="Abstract">arXiv:2402.11821</a> (replaced) [<a href="/pdf/2402.11821" title="Download PDF">pdf</a>, <a href="/format/2402.11821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microstructures and Accuracy of Graph Recall by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hejie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11853" title="Abstract">arXiv:2402.11853</a> (replaced) [<a href="/pdf/2402.11853" title="Download PDF">pdf</a>, <a href="/format/2402.11853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car  Social Robot in Real Driving Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanchao Li</a>, 
<a href="/search/cs?searchtype=author&query=Urquhart%2C+L">Lachlan Urquhart</a>, 
<a href="/search/cs?searchtype=author&query=Karatas%2C+N">Nihan Karatas</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shun Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ishiguro%2C+H">Hiroshi Ishiguro</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xun Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM Transactions on Computer-Human Interaction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11883" title="Abstract">arXiv:2402.11883</a> (replaced) [<a href="/pdf/2402.11883" title="Download PDF">pdf</a>, <a href="/format/2402.11883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InMD-X: Large Language Models for Internal Medicine Doctors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gwon%2C+H">Hansle Gwon</a> (1), 
<a href="/search/cs?searchtype=author&query=Ahn%2C+I">Imjin Ahn</a> (1), 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyoje Jung</a> (2), 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Byeolhee Kim</a> (2), 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Hak Kim</a> (3), 
<a href="/search/cs?searchtype=author&query=Jun%2C+T+J">Tae Joon Jun</a> (4) ((1) INMED DATA, Seoul, Republic of Korea (2) Department of Information Medicine, Asan Medical Center, Seoul, Republic of Korea (3) Division of Cardiology, Department of Information Medicine, Asan Medical Center, University of Ulsan College of Medicine, Seoul, Republic of Korea (4) Big Data Research Center, Asan Institute for Life Sciences, Asan Medical Center, Seoul, Republic of Korea)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11895" title="Abstract">arXiv:2402.11895</a> (replaced) [<a href="/pdf/2402.11895" title="Download PDF">pdf</a>, <a href="/format/2402.11895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging or Breaking: Impact of Intergroup Interactions on Religious  Polarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+R">Rochana Chaturvedi</a>, 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+S">Sugat Chaturvedi</a>, 
<a href="/search/cs?searchtype=author&query=Zheleva%2C+E">Elena Zheleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11922" title="Abstract">arXiv:2402.11922</a> (replaced) [<a href="/pdf/2402.11922" title="Download PDF">pdf</a>, <a href="/format/2402.11922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenyang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11926" title="Abstract">arXiv:2402.11926</a> (replaced) [<a href="/pdf/2402.11926" title="Download PDF">pdf</a>, <a href="/format/2402.11926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with  error based time stepping for hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Babbar%2C+A">Arpit Babbar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekar%2C+P">Praveen Chandrashekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 15 figures (with sub-figures), 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11940" title="Abstract">arXiv:2402.11940</a> (replaced) [<a href="/pdf/2402.11940" title="Download PDF">pdf</a>, <a href="/format/2402.11940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+M">Mingze Ni</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yifei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12030" title="Abstract">arXiv:2402.12030</a> (replaced) [<a href="/pdf/2402.12030" title="Download PDF">pdf</a>, <a href="/format/2402.12030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Cross-Tokenizer Distillation: the Universal Logit Distillation  Loss for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boizard%2C+N">Nicolas Boizard</a>, 
<a href="/search/cs?searchtype=author&query=Haddad%2C+K+E">Kevin El Haddad</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12065" title="Abstract">arXiv:2402.12065</a> (replaced) [<a href="/pdf/2402.12065" title="Download PDF">pdf</a>, <a href="/format/2402.12065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WKVQuant: Quantizing Weight and Key/Value Cache for Large Language  Models Gains More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuxuan Yue</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Duanmu%2C+H">Haojie Duanmu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sifan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianlong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Frist work to exclusively quantize weight and Key/Value cache for large language models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12130" title="Abstract">arXiv:2402.12130</a> (replaced) [<a href="/pdf/2402.12130" title="Download PDF">pdf</a>, <a href="/ps/2402.12130" title="Download PostScript">ps</a>, <a href="/format/2402.12130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dudek%2C+P">Piotr Dudek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An essay in contribution to the Festschrift for Professor Steve Furber, Manchester, 12 January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12140" title="Abstract">arXiv:2402.12140</a> (replaced) [<a href="/pdf/2402.12140" title="Download PDF">pdf</a>, <a href="/format/2402.12140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many-Stage Optimal Stabilized Runge-Kutta Methods for Hyperbolic Partial  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doehring%2C+D">Daniel Doehring</a>, 
<a href="/search/math?searchtype=author&query=Gassner%2C+G+J">Gregor J. Gassner</a>, 
<a href="/search/math?searchtype=author&query=Torrilhon%2C+M">Manuel Torrilhon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Classical Analysis and ODEs (math.CA)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12161" title="Abstract">arXiv:2402.12161</a> (replaced) [<a href="/pdf/2402.12161" title="Download PDF">pdf</a>, <a href="/format/2402.12161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Endowing Pre-trained Graph Models with Provable Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12188" title="Abstract">arXiv:2402.12188</a> (replaced) [<a href="/pdf/2402.12188" title="Download PDF">pdf</a>, <a href="/format/2402.12188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure of activity in multiregion recurrent neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Clark%2C+D+G">David G. Clark</a>, 
<a href="/search/q-bio?searchtype=author&query=Beiran%2C+M">Manuel Beiran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures; updated author info
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12208" title="Abstract">arXiv:2402.12208</a> (replaced) [<a href="/pdf/2402.12208" title="Download PDF">pdf</a>, <a href="/format/2402.12208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Codec: Reducing the Gaps Between Discrete Codec Representation  and Speech Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ji%2C+S">Shengpeng Ji</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+M">Minghui Fang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Rongjie Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zuo%2C+J">Jialung Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shulei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12243" title="Abstract">arXiv:2402.12243</a> (replaced) [<a href="/pdf/2402.12243" title="Download PDF">pdf</a>, <a href="/format/2402.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of Noise in Text-to-SQL: An Examination of the  BIRD-Bench Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wretblad%2C+N">Niklas Wretblad</a>, 
<a href="/search/cs?searchtype=author&query=Riseby%2C+F+G">Fredrik Gordh Riseby</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+R">Rahul Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+A">Amin Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Holmstr%C3%B6m%2C+O">Oskar Holmstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12372" title="Abstract">arXiv:2402.12372</a> (replaced) [<a href="/pdf/2402.12372" title="Download PDF">pdf</a>, <a href="/format/2402.12372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HunFlair2 in a cross-corpus evaluation of biomedical named entity  recognition and normalization tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A4nger%2C+M">Mario S&#xe4;nger</a>, 
<a href="/search/cs?searchtype=author&query=Garda%2C+S">Samuele Garda</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+D">Xing David Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weber-Genzel%2C+L">Leon Weber-Genzel</a>, 
<a href="/search/cs?searchtype=author&query=Droop%2C+P">Pia Droop</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+B">Benedikt Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Akbik%2C+A">Alan Akbik</a>, 
<a href="/search/cs?searchtype=author&query=Leser%2C+U">Ulf Leser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item405">Cross-lists</a></li>
<li><a href="#item456">Replacements</a></li>
</ul>
<small>[ total of 748 entries:  <b>1-748</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
