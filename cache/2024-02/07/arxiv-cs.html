<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon  5 Feb 24  to  Tue  6 Feb 24, announced Wed,  7 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item437">Cross-lists</a></li>
<li><a href="#item502">Replacements</a></li>
</ul>
<small>[ total of 800 entries:  <b>1-800</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed,  7 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03314" title="Abstract">arXiv:2402.03314</a> [<a href="/pdf/2402.03314" title="Download PDF">pdf</a>, <a href="/format/2402.03314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Results on a Mixed Finite Element Approach for a Model  Convection-Diffusion Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bacuta%2C+C">Constantin Bacuta</a>, 
<a href="/search/math?searchtype=author&query=Hayes%2C+D">Daniel Hayes</a>, 
<a href="/search/math?searchtype=author&query=O%27Grady%2C+T">Tyler O&#x27;Grady</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 figures. arXiv admin note: substantial text overlap with <a href="/abs/2302.07809">arXiv:2302.07809</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider a model convection-diffusion problem and present our recent
numerical and analysis results regarding mixed finite element formulation and
discretization in the singular perturbed case when the convection term
dominates the problem. Using the concepts of optimal norm and saddle point
reformulation, we found new error estimates for the case of uniform meshes. We
compare the standard linear Galerkin discretization to a saddle point least
square discretization that uses quadratic test functions, and explain the
non-physical oscillations of the discrete solutions. We also relate a known
upwinding Petrov Galerkin method and the stream-line diffusion discretization
method, by emphasizing the resulting linear systems and by comparing
appropriate error norms. The results can be extended to the multidimensional
case in order to find efficient approximations for more general singular
perturbed problems including convection dominated models
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03315" title="Abstract">arXiv:2402.03315</a> [<a href="/pdf/2402.03315" title="Download PDF">pdf</a>, <a href="/format/2402.03315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTHDet: Rotate Table Area and Head Detection in images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenxing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Minglei Tong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Traditional models focus on horizontal table detection but struggle in
rotating contexts, limiting progress in table recognition. This paper
introduces a new task: detecting table regions and localizing head-tail parts
in rotation scenarios. We propose corresponding datasets, evaluation metrics,
and methods. Our novel method, 'Adaptively Bounded Rotation,' addresses dataset
scarcity in detecting rotated tables and their head-tail parts. We produced
'TRR360D,' a dataset incorporating semantic information of table head and tail,
based on 'ICDAR2019MTD.' A new metric, 'R360 AP,' measures precision in
detecting rotated regions and localizing head-tail parts. Our baseline, the
high-speed and accurate 'RTMDet-S,' is chosen after extensive review and
testing. We introduce 'RTHDet,' enhancing the baseline with a 'r360' rotated
rectangle angle representation and an 'Angle Loss' branch, improving head-tail
localization. By applying transfer learning and adaptive boundary rotation
augmentation, RTHDet's AP50 (T&lt;90) improved from 23.7% to 88.7% compared to the
baseline. This demonstrates RTHDet's effectiveness in detecting rotating table
regions and accurately localizing head and tail parts.RTHDet is integrated into
the widely-used open-source MMRotate toolkit:
https://github.com/open-mmlab/mmrotate/tree/dev-1.x/projects/RR360.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03317" title="Abstract">arXiv:2402.03317</a> [<a href="/pdf/2402.03317" title="Download PDF">pdf</a>, <a href="/format/2402.03317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular  Value Penalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xixu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Runkai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+C+H">Cheuk Hang Leung</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision Transformers (ViTs) have gained prominence as a preferred choice for a
wide range of computer vision tasks due to their exceptional performance.
However, their widespread adoption has raised concerns about security in the
face of malicious attacks. Most existing methods rely on empirical adjustments
during the training process, lacking a clear theoretical foundation. In this
study, we address this gap by introducing SpecFormer, specifically designed to
enhance ViTs' resilience against adversarial attacks, with support from
carefully derived theoretical guarantees. We establish local Lipschitz bounds
for the self-attention layer and introduce a novel approach, Maximum Singular
Value Penalization (MSVP), to attain precise control over these bounds. We
seamlessly integrate MSVP into ViTs' attention layers, using the power
iteration method for enhanced computational efficiency. The modified model,
SpecFormer, effectively reduces the spectral norms of attention weight
matrices, thereby enhancing network local Lipschitzness. This, in turn, leads
to improved training efficiency and robustness. Extensive experiments on CIFAR
and ImageNet datasets confirm SpecFormer's superior performance in defending
against adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03319" title="Abstract">arXiv:2402.03319</a> [<a href="/pdf/2402.03319" title="Download PDF">pdf</a>, <a href="/format/2402.03319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Reservoir Computing Enabled by Solitary Waves and  Biologically-Inspired Nonlinear Transformation of Input Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maksymov%2C+I+S">Ivan S. Maksymov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Supplementary Video can be found here: <a href="https://youtu.be/Zwu3KEo8f00">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Chaotic Dynamics (nlin.CD); Pattern Formation and Solitons (nlin.PS); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Reservoir computing (RC) systems can efficiently forecast chaotic time series
using nonlinear dynamical properties of an artificial neural network of random
connections. The versatility of RC systems has motivated further research on
both hardware counterparts of traditional RC algorithms and more efficient
RC-like schemes. Inspired by the nonlinear processes in a living biological
brain and using solitary waves excited on the surface of a flowing liquid film,
in this paper we experimentally validate a physical RC system that substitutes
the effect of randomness for a nonlinear transformation of input data. Carrying
out all operations using a microcontroller with a minimal computational power,
we demonstrate that the so-designed RC system serves as a technically simple
hardware counterpart to the `next-generation' improvement of the traditional RC
algorithm.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03323" title="Abstract">arXiv:2402.03323</a> [<a href="/pdf/2402.03323" title="Download PDF">pdf</a>, <a href="/ps/2402.03323" title="Download PostScript">ps</a>, <a href="/format/2402.03323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Portable medical devices creation technology by using the Bluetooth  module
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadukin%2C+A+O">A.O. Dadukin</a>, 
<a href="/search/cs?searchtype=author&query=Pchelintseva%2C+N+I">N.I. Pchelintseva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Russian language, Published in "Digital journal: science, machinery, and education" Reference: <a href="https://nto-journal.ru/authors/354/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">The article is devoted Bluetooth wireless personal area networks
specification, which provides standard for exchanging data over short
distances. It is shown how the technology has evolved and its application in
the design of devices. Health Device Profile considered in details, which the
main feature is the work of a medical orientation devices.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03324" title="Abstract">arXiv:2402.03324</a> [<a href="/pdf/2402.03324" title="Download PDF">pdf</a>, <a href="/ps/2402.03324" title="Download PostScript">ps</a>, <a href="/format/2402.03324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Requirements for a Career in Information Security: A Comprehensive  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkongolo%2C+M">Mike Nkongolo</a>, 
<a href="/search/cs?searchtype=author&query=Mennega%2C+N">Nita Mennega</a>, 
<a href="/search/cs?searchtype=author&query=van+Zyl%2C+I">Izaan van Zyl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This research paper adopts a methodology by conducting a thorough literature
review to uncover the essential prerequisites for achieving a prosperous career
in the field of Information Security (IS). The primary objective is to increase
public awareness regarding the diverse opportunities available in the
Information Security (IS) field. The initial search involved scouring four
prominent academic databases using the specific keywords "cybersecurity" and
"skills," resulting in the identification of a substantial corpus of 1,520
articles. After applying rigorous screening criteria, a refined set of 31
relevant papers was selected for further analysis. Thematic analysis was
conducted on these studies to identify and delineate the crucial knowledge and
skills that an IS professional should possess. The research findings emphasize
the significant time investment required for individuals to acquire the
necessary technical proficiency in the cybersecurity domain. Furthermore, the
study recognizes the existence of gender-related obstacles for women pursuing
cybersecurity careers due to the field's unique requirements. It suggests that
females can potentially overcome these barriers by initially entering the
profession at lower levels and subsequently advancing based on individual
circumstances.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03325" title="Abstract">arXiv:2402.03325</a> [<a href="/pdf/2402.03325" title="Download PDF">pdf</a>, <a href="/format/2402.03325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connect Later: Improving Fine-tuning for Robustness with Targeted  Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Helen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Models trained on a labeled source domain (e.g., labeled images from wildlife
camera traps) often generalize poorly when deployed on an out-of-distribution
(OOD) target domain (e.g., images from new camera trap locations). In the
domain adaptation setting where unlabeled target data is available,
self-supervised pretraining (e.g., masked autoencoding or contrastive learning)
is a promising method to mitigate this performance drop. Pretraining improves
OOD error when the generic data augmentations used (e.g., masking or cropping)
connect the source and target domains, which may be far apart in the input
space. In this paper, we show on real-world tasks that standard fine-tuning
after pretraining does not consistently improve OOD error over simply training
from scratch on labeled source data. To better leverage pretraining for
distribution shifts, we propose Connect Later: after pretraining with generic
augmentations, fine-tune with targeted augmentations designed with knowledge of
the distribution shift. Pretraining learns good representations within the
source and target domains, while targeted augmentations connect the domains
better during fine-tuning. Connect Later improves average OOD error over
standard fine-tuning and supervised learning with targeted augmentations on 4
real-world datasets: Connect Later achieves the state-of-the-art on
astronomical time-series classification (AstroClassification) by 2.5%, wildlife
species identification (iWildCam-WILDS) with ResNet-50 by 0.9%, and tumor
identification (Camelyon17-WILDS) with DenseNet121 by 1.1%; as well as best
performance on a new dataset for astronomical time-series redshift prediction
(Redshifts) by 0.03 RMSE (11% relative). Code and datasets are available at
https://github.com/helenqu/connect-later.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03326" title="Abstract">arXiv:2402.03326</a> [<a href="/pdf/2402.03326" title="Download PDF">pdf</a>, <a href="/format/2402.03326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Slot Structured World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collu%2C+J">Jonathan Collu</a>, 
<a href="/search/cs?searchtype=author&query=Majellaro%2C+R">Riccardo Majellaro</a>, 
<a href="/search/cs?searchtype=author&query=Plaat%2C+A">Aske Plaat</a>, 
<a href="/search/cs?searchtype=author&query=Moerland%2C+T+M">Thomas M. Moerland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability to perceive and reason about individual objects and their
interactions is a goal to be achieved for building intelligent artificial
systems. State-of-the-art approaches use a feedforward encoder to extract
object embeddings and a latent graph neural network to model the interaction
between these object embeddings. However, the feedforward encoder can not
extract {\it object-centric} representations, nor can it disentangle multiple
objects with similar appearance. To solve these issues, we introduce {\it Slot
Structured World Models} (SSWM), a class of world models that combines an {\it
object-centric} encoder (based on Slot Attention) with a latent graph-based
dynamics model. We evaluate our method in the Spriteworld benchmark with simple
rules of physical interaction, where Slot Structured World Models consistently
outperform baselines on a range of (multi-step) prediction tasks with
action-conditional object interactions. All code to reproduce paper experiments
is available from
\url{https://github.com/JonathanCollu/Slot-Structured-World-Models}.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03327" title="Abstract">arXiv:2402.03327</a> [<a href="/pdf/2402.03327" title="Download PDF">pdf</a>, <a href="/format/2402.03327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dingning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yuenan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhenfei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yongshun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we introduce Uni3D-LLM, a unified framework that leverages a
Large Language Model (LLM) to integrate tasks of 3D perception, generation, and
editing within point cloud scenes. This framework empowers users to
effortlessly generate and modify objects at specified locations within a scene,
guided by the versatility of natural language descriptions. Uni3D-LLM harnesses
the expressive power of natural language to allow for precise command over the
generation and editing of 3D objects, thereby significantly enhancing
operational flexibility and controllability. By mapping point cloud into the
unified representation space, Uni3D-LLM achieves cross-application
functionality, enabling the seamless execution of a wide array of tasks,
ranging from the accurate instantiation of 3D objects to the diverse
requirements of interactive design. Through a comprehensive suite of rigorous
experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and
editing of point cloud has been validated. Additionally, we have assessed the
impact of integrating a point cloud perception module on the generation and
editing processes, confirming the substantial potential of our approach for
practical applications.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03328" title="Abstract">arXiv:2402.03328</a> [<a href="/pdf/2402.03328" title="Download PDF">pdf</a>, <a href="/format/2402.03328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale Generative AI Models Lack Visual Number Sense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Testolin%2C+A">Alberto Testolin</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+K">Kuinan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zorzi%2C+M">Marco Zorzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Humans can readily judge the number of objects in a visual scene, even
without counting, and such a skill has been documented in a variety of animal
species and in babies prior to language development and formal schooling.
Numerical judgments are error-free for small sets, while for larger collections
responses become approximate, with variability increasing proportionally to the
target number. This response pattern is observed for items of all kinds,
despite variation in object features (such as color or shape), suggesting that
our visual number sense relies on abstract representations of numerosity. Here,
we investigated whether generative Artificial Intelligence (AI) models based on
large-scale transformer architectures can reliably name the number of objects
in simple visual stimuli or generate images containing a target number of items
in the 1-10 range. Surprisingly, none of the foundation models considered
performed in a human-like way: They all made striking errors even with small
numbers, the response variability often did not increase in a systematic way,
and the pattern of errors varied with object category. Our findings demonstrate
that advanced AI systems still lack a basic ability that supports an intuitive
understanding of numbers, which in humans is foundational for numeracy and
mathematical development.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03329" title="Abstract">arXiv:2402.03329</a> [<a href="/pdf/2402.03329" title="Download PDF">pdf</a>, <a href="/format/2402.03329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Salient Patch Selection for Data-Efficient Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhaohui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+P">Paul Weng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To improve the sample efficiency of vision-based deep reinforcement learning
(RL), we propose a novel method, called SPIRL, to automatically extract
important patches from input images. Following Masked Auto-Encoders, SPIRL is
based on Vision Transformer models pre-trained in a self-supervised fashion to
reconstruct images from randomly-sampled patches. These pre-trained models can
then be exploited to detect and select salient patches, defined as hard to
reconstruct from neighboring patches. In RL, the SPIRL agent processes selected
salient patches via an attention module. We empirically validate SPIRL on Atari
games to test its data-efficiency against relevant state-of-the-art methods,
including some traditional model-based methods and keypoint-based models. In
addition, we analyze our model's interpretability capabilities.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03332" title="Abstract">arXiv:2402.03332</a> [<a href="/pdf/2402.03332" title="Download PDF">pdf</a>, <a href="/format/2402.03332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyclic Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zihe Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper answers a fundamental question in artificial neural network (ANN)
design: We do not need to build ANNs layer-by-layer sequentially to guarantee
the Directed Acyclic Graph (DAG) property. Drawing inspiration from biological
intelligence (BI), where neurons form a complex, graph-structured network, we
introduce the groundbreaking Cyclic Neural Networks (Cyclic NNs). It emulates
the flexible and dynamic graph nature of biological neural systems, allowing
neuron connections in any graph-like structure, including cycles. This offers
greater adaptability compared to the DAG structure of current ANNs. We further
develop the Graph Over Multi-layer Perceptron, which is the first detailed
model based on this new design paradigm. Experimental validation of the Cyclic
NN's advantages on widely tested datasets in most generalized cases,
demonstrating its superiority over current BP training methods through the use
of a forward-forward (FF) training algorithm. This research illustrates a
totally new ANN design paradigm, which is a significant departure from current
ANN designs, potentially leading to more biologically plausible AI systems.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03337" title="Abstract">arXiv:2402.03337</a> [<a href="/pdf/2402.03337" title="Download PDF">pdf</a>, <a href="/format/2402.03337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement-learning robotic sailboats: simulator and preliminary  results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasconcellos%2C+E+C">Eduardo Charles Vasconcellos</a> (UFF), 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+R+M">Ronald M Sampaio</a>, 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+A+P+D">Andr&#xe9; P D Ara&#xfa;jo</a> (UFF), 
<a href="/search/cs?searchtype=author&query=Clua%2C+E+W+G">Esteban Walter Gonzales Clua</a>, 
<a href="/search/cs?searchtype=author&query=Preux%2C+P">Philippe Preux</a> (SEQUEL, GRAppA - LIFL), 
<a href="/search/cs?searchtype=author&query=Guerra%2C+R">Raphael Guerra</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+L+M+G">Luiz M G Gon&#xe7;alves</a> (UFRN), 
<a href="/search/cs?searchtype=author&query=Mart%C3%AD%2C+L">Luis Mart&#xed;</a>, 
<a href="/search/cs?searchtype=author&query=Lira%2C+H">Hernan Lira</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Pi%2C+N">Nayat Sanchez-Pi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Workshop on Robot Learning Workshop: Pretraining,
  Fine-Tuning, and Generalization with Large Scale Models, Dec 2023, New
  Orelans, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work focuses on the main challenges and problems in developing a virtual
oceanic environment reproducing real experiments using Unmanned Surface
Vehicles (USV) digital twins. We introduce the key features for building
virtual worlds, considering using Reinforcement Learning (RL) agents for
autonomous navigation and control. With this in mind, the main problems concern
the definition of the simulation equations (physics and mathematics), their
effective implementation, and how to include strategies for simulated control
and perception (sensors) to be used with RL. We present the modeling,
implementation steps, and challenges required to create a functional digital
twin based on a real robotic sailing vessel. The application is immediate for
developing navigation algorithms based on RL to be applied on real boats.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03339" title="Abstract">arXiv:2402.03339</a> [<a href="/pdf/2402.03339" title="Download PDF">pdf</a>, <a href="/format/2402.03339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interplay of Semantic Communication and Knowledge Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bingyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rongpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhifeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honggang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Contributing to a Wiley book, copyright might be transferred without further notice; And the paper "Knowledge Enhanced Semantic Communication Receiver" (available at <a href="/abs/2302.07727">arXiv:2302.07727</a>) constitutes a segment of this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the swiftly advancing realm of communication technologies, Semantic
Communication (SemCom), which emphasizes knowledge understanding and
processing, has emerged as a hot topic. By integrating artificial intelligence
technologies, SemCom facilitates a profound understanding, analysis and
transmission of communication content. In this chapter, we clarify the means of
knowledge learning in SemCom with a particular focus on the utilization of
Knowledge Graphs (KGs). Specifically, we first review existing efforts that
combine SemCom with knowledge learning. Subsequently, we introduce a
KG-enhanced SemCom system, wherein the receiver is carefully calibrated to
leverage knowledge from its static knowledge base for ameliorating the decoding
performance. Contingent upon this framework, we further explore potential
approaches that can empower the system to operate in evolving knowledge base
more effectively. Furthermore, we investigate the possibility of integration
with Large Language Models (LLMs) for data augmentation, offering additional
perspective into the potential implementation means of SemCom. Extensive
numerical results demonstrate that the proposed framework yields superior
performance on top of the KG-enhanced decoding and manifests its versatility
under different scenarios.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03342" title="Abstract">arXiv:2402.03342</a> [<a href="/pdf/2402.03342" title="Download PDF">pdf</a>, <a href="/format/2402.03342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MADRL-based UAVs Trajectory Design with Anti-Collision Mechanism in  Vehicular Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spampinato%2C+L">Leonardo Spampinato</a>, 
<a href="/search/cs?searchtype=author&query=Testi%2C+E">Enrico Testi</a>, 
<a href="/search/cs?searchtype=author&query=Buratti%2C+C">Chiara Buratti</a>, 
<a href="/search/cs?searchtype=author&query=Marini%2C+R">Riccardo Marini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In upcoming 6G networks, unmanned aerial vehicles (UAVs) are expected to play
a fundamental role by acting as mobile base stations, particularly for
demanding vehicle-to-everything (V2X) applications. In this scenario, one of
the most challenging problems is the design of trajectories for multiple UAVs,
cooperatively serving the same area. Such joint trajectory design can be
performed using multi-agent deep reinforcement learning (MADRL) algorithms, but
ensuring collision-free paths among UAVs becomes a critical challenge.
Traditional methods involve imposing high penalties during training to
discourage unsafe conditions, but these can be proven to be ineffective,
whereas binary masks can be used to restrict unsafe actions, but naively
applying them to all agents can lead to suboptimal solutions and
inefficiencies. To address these issues, we propose a rank-based binary masking
approach. Higher-ranked UAVs move optimally, while lower-ranked UAVs use this
information to define improved binary masks, reducing the number of unsafe
actions. This approach allows to obtain a good trade-off between exploration
and exploitation, resulting in enhanced training performance, while maintaining
safety constraints.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03347" title="Abstract">arXiv:2402.03347</a> [<a href="/pdf/2402.03347" title="Download PDF">pdf</a>, <a href="/ps/2402.03347" title="Download PostScript">ps</a>, <a href="/format/2402.03347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning With Densenet201 Architecture Model For Potato Leaf  Disease Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charisma%2C+R+A">Rifqi Alfinnur Charisma</a>, 
<a href="/search/cs?searchtype=author&query=Adhinata%2C+F+D">Faisal Dharma Adhinata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Potato plants are plants that are beneficial to humans. Like other plants in
general, potato plants also have diseases; if this disease is not treated
immediately, there will be a significant decrease in food production.
Therefore, it is necessary to detect diseases quickly and precisely so that
disease control can be carried out effectively and efficiently. Classification
of potato leaf disease can be done directly. Still, the symptoms cannot always
explain the type of disease that attacks potato leaves because there are many
types of diseases with symptoms that look the same. Humans also have
deficiencies in determining the results of identification of potato leaf
disease, so sometimes the results of identification between individuals can be
different. Therefore, the use of Deep Learning for the classification process
of potato leaf disease is expected to shorten the time and have a high
classification accuracy. This study uses a deep learning method with the
DenseNet201 architecture. The choice to use the DenseNet201 algorithm in this
study is because the model can identify important features of potato leaves and
recognize early signs of emerging diseases. This study aimed to evaluate the
effectiveness of the transfer learning method with the DenseNet201 architecture
in increasing the classification accuracy of potato leaf disease compared to
traditional classification methods. This study uses two types of scenarios,
namely, comparing the number of dropouts and comparing the three optimizers.
This test produces the best model using dropout 0.1 and Adam optimizer with an
accuracy of 99.5% for training, 95.2% for validation, and 96% for the confusion
matrix. In this study, using data testing, as many as 40 images were tested
into the model that has been built. The test results on this model resulted in
a new accuracy for classifying potato leaf disease, namely 92.5%.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03348" title="Abstract">arXiv:2402.03348</a> [<a href="/pdf/2402.03348" title="Download PDF">pdf</a>, <a href="/format/2402.03348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Respect the model: Fine-grained and Robust Explanation with Sharing  Ratio Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Sangyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yearim Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+N">Nojun Kwak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The truthfulness of existing explanation methods in authentically elucidating
the underlying model's decision-making process has been questioned. Existing
methods have deviated from faithfully representing the model, thus susceptible
to adversarial attacks. To address this, we propose a novel eXplainable AI
(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects
the model's inference process, resulting in significantly enhanced robustness
in our explanations. Different from the conventional emphasis on the neuronal
level, we adopt a vector perspective to consider the intricate nonlinear
interactions between filters. We also introduce an interesting observation
termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the
importance of inactive neurons and redefine relevance encapsulating all
relevant information including both active and inactive neurons. Our method,
SRD, allows for the recursive decomposition of a Pointwise Feature Vector
(PFV), providing a high-resolution Effective Receptive Field (ERF) at any
layer.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03355" title="Abstract">arXiv:2402.03355</a> [<a href="/pdf/2402.03355" title="Download PDF">pdf</a>, <a href="/ps/2402.03355" title="Download PostScript">ps</a>, <a href="/format/2402.03355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Techniques to Detect Crime Leaders within a Criminal Network: A Survey,  Experimental, and Comparative Evaluations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taha%2C+K">Kamal Taha</a>, 
<a href="/search/cs?searchtype=author&query=Shoufan%2C+A">Abdulhadi Shoufan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This survey paper offers a thorough analysis of techniques and algorithms
used in the identification of crime leaders within criminal networks. For each
technique, the paper examines its effectiveness, limitations, potential for
improvement, and future prospects. The main challenge faced by existing survey
papers focusing on algorithms for identifying crime leaders and predicting
crimes is effectively categorizing these algorithms. To address this
limitation, this paper proposes a new methodological taxonomy that
hierarchically classifies algorithms into more detailed categories and specific
techniques. The paper includes empirical and experimental evaluations to rank
the different techniques. The combination of the methodological taxonomy,
empirical evaluations, and experimental comparisons allows for a nuanced and
comprehensive understanding of the techniques and algorithms for identifying
crime leaders, assisting researchers in making informed decisions. Moreover,
the paper offers valuable insights into the future prospects of techniques for
identifying crime leaders, emphasizing potential advancements and opportunities
for further research. Here's an overview of our empirical analysis findings and
experimental insights, along with the solution we've devised: (1) PageRank and
Eigenvector centrality are reliable for mapping network connections, (2) Katz
Centrality can effectively identify influential criminals through indirect
links, stressing their significance in criminal networks, (3) current models
fail to account for the specific impacts of criminal influence levels, the
importance of socio-economic context, and the dynamic nature of criminal
networks and hierarchies, and (4) we propose enhancements, such as
incorporating temporal dynamics and sentiment analysis to reflect the fluidity
of criminal activities and relationships, which could improve the detection of
key criminals .
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03357" title="Abstract">arXiv:2402.03357</a> [<a href="/pdf/2402.03357" title="Download PDF">pdf</a>, <a href="/format/2402.03357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers  via Self-Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaofei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Ke Deng</a>, 
<a href="/search/cs?searchtype=author&query=Dann%2C+M">Michael Dann</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiuzhen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, full version of this paper is accepted by AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study aims to minimize the influence of fake news on social networks by
deploying debunkers to propagate true news. This is framed as a reinforcement
learning problem, where, at each stage, one user is selected to propagate true
news. A challenging issue is episodic reward where the "net" effect of
selecting individual debunkers cannot be discerned from the interleaving
information propagation on social networks, and only the collective effect from
mitigation efforts can be observed. Existing Self-Imitation Learning (SIL)
methods have shown promise in learning from episodic rewards, but are
ill-suited to the real-world application of fake news mitigation because of
their poor sample efficiency. To learn a more effective debunker selection
policy for fake news mitigation, this study proposes NAGASIL - Negative
sampling and state Augmented Generative Adversarial Self-Imitation Learning,
which consists of two improvements geared towards fake news mitigation:
learning from negative samples, and an augmented state representation to
capture the "real" environment state by integrating the current observed state
with the previous state-action pairs from the same campaign. Experiments on two
social networks show that NAGASIL yields superior performance to standard GASIL
and state-of-the-art fake news mitigation models.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03358" title="Abstract">arXiv:2402.03358</a> [<a href="/pdf/2402.03358" title="Download PDF">pdf</a>, <a href="/format/2402.03358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,  and Condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Mohammad Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Juntong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">Many real-world datasets can be naturally represented as graphs, spanning a
wide range of domains. However, the increasing complexity and size of graph
datasets present significant challenges for analysis and computation. In
response, graph reduction techniques have gained prominence for simplifying
large graphs while preserving essential properties. In this survey, we aim to
provide a comprehensive understanding of graph reduction methods, including
graph sparsification, graph coarsening, and graph condensation. Specifically,
we establish a unified definition for these methods and introduce a
hierarchical taxonomy to categorize the challenges they address. Our survey
then systematically reviews the technical details of these methods and
emphasizes their practical applications across diverse scenarios. Furthermore,
we outline critical research directions to ensure the continued effectiveness
of graph reduction techniques, as well as provide a comprehensive paper list at
https://github.com/ChandlerBang/awesome-graph-reduction. We hope this survey
will bridge literature gaps and propel the advancement of this promising field.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03362" title="Abstract">arXiv:2402.03362</a> [<a href="/pdf/2402.03362" title="Download PDF">pdf</a>, <a href="/format/2402.03362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NanoNER: Named Entity Recognition for nanobiology using experts&#x27;  knowledge and distant supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lentschat%2C+M">Martin Lentschat</a> (SIGMA, GETALP), 
<a href="/search/cs?searchtype=author&query=Labb%C3%A9%2C+C">Cyril Labb&#xe9;</a> (LIG, SIGMA), 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ran Cheng</a> (LIG, SIGMA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Here we present the training and evaluation of NanoNER, a Named Entity
Recognition (NER) model for Nanobiology. NER consists in the identification of
specific entities in spans of unstructured texts and is often a primary task in
Natural Language Processing (NLP) and Information Extraction. The aim of our
model is to recognise entities previously identified by domain experts as
constituting the essential knowledge of the domain. Relying on ontologies,
which provide us with a domain vocabulary and taxonomy, we implemented an
iterative process enabling experts to determine the entities relevant to the
domain at hand. We then delve into the potential of distant supervision
learning in NER, supporting how this method can increase the quantity of
annotated data with minimal additional manpower. On our full corpus of 728
full-text nanobiology articles, containing more than 120k entity occurrences,
NanoNER obtained a F1-score of 0.98 on the recognition of previously known
entities. Our model also demonstrated its ability to discover new entities in
the text, with precision scores ranging from 0.77 to 0.81. Ablation experiments
further confirmed this and allowed us to assess the dependency of our approach
on the external resources. It highlighted the dependency of the approach to the
resource, while also confirming its ability to rediscover up to 30% of the
ablated terms. This paper details the methodology employed, experimental
design, and key findings, providing valuable insights and directions for future
related researches on NER in specialized domain. Furthermore, since our
approach require minimal manpower , we believe that it can be generalized to
other specialized fields.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03365" title="Abstract">arXiv:2402.03365</a> [<a href="/pdf/2402.03365" title="Download PDF">pdf</a>, <a href="/format/2402.03365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholinejad%2C+N">Nemat Gholinejad</a>, 
<a href="/search/cs?searchtype=author&query=Chehreghani%2C+M+H">Mostafa Haghir Chehreghani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In recent years, graph neural networks (GNNs) have become a popular tool to
improve the accuracy and performance of recommender systems. Modern recommender
systems are not only designed to serve the end users, but also to benefit other
participants, such as items and items providers. These participants may have
different or conflicting goals and interests, which raise the need for fairness
and popularity bias considerations. GNN-based recommendation methods also face
the challenges of unfairness and popularity bias and their normalization and
aggregation processes suffer from these challenges. In this paper, we propose a
fair GNN-based recommender system, called HetroFair, to improve items' side
fairness. HetroFair uses two separate components to generate fairness-aware
embeddings: i) fairness-aware attention which incorporates dot product in the
normalization process of GNNs, to decrease the effect of nodes' degrees, and
ii) heterophily feature weighting to assign distinct weights to different
features during the aggregation process. In order to evaluate the effectiveness
of HetroFair, we conduct extensive experiments over six real-world datasets.
Our experimental results reveal that HetroFair not only alleviates the
unfairness and popularity bias on the items' side, but also achieves superior
accuracy on the users' side. Our implementation is publicly available at
https://github.com/NematGH/HetroFair
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03366" title="Abstract">arXiv:2402.03366</a> [<a href="/pdf/2402.03366" title="Download PDF">pdf</a>, <a href="/format/2402.03366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Explainable Recommendation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yicui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chingsheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinrong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+B">Bin Kong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Providing explanations within the recommendation system would boost user
satisfaction and foster trust, especially by elaborating on the reasons for
selecting recommended items tailored to the user. The predominant approach in
this domain revolves around generating text-based explanations, with a notable
emphasis on applying large language models (LLMs). However, refining LLMs for
explainable recommendations proves impractical due to time constraints and
computing resource limitations. As an alternative, the current approach
involves training the prompt rather than the LLM. In this study, we developed a
model that utilizes the ID vectors of user and item inputs as prompts for
GPT-2. We employed a joint training mechanism within a multi-task learning
framework to optimize both the recommendation task and explanation task. This
strategy enables a more effective exploration of users' interests, improving
recommendation effectiveness and user satisfaction. Through the experiments,
our method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor
and Amazon dataset respectively, demonstrates superior performance over four
SOTA methods in terms of explainability evaluation metric. In addition, we
identified that the proposed model is able to ensure stable textual quality on
the three public datasets.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03367" title="Abstract">arXiv:2402.03367</a> [<a href="/pdf/2402.03367" title="Download PDF">pdf</a>, <a href="/format/2402.03367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAG-Fusion: a New Take on Retrieval-Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rackauckas%2C+Z">Zackary Rackauckas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03368" title="Abstract">arXiv:2402.03368</a> [<a href="/pdf/2402.03368" title="Download PDF">pdf</a>, <a href="/ps/2402.03368" title="Download PostScript">ps</a>, <a href="/format/2402.03368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical and Experimental Perspectives on Big Data in Recommendation  Systems: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taha%2C+K">Kamal Taha</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+P+D">Paul D. Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Taha%2C+A">Aya Taha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This survey paper provides a comprehensive analysis of big data algorithms in
recommendation systems, addressing the lack of depth and precision in existing
literature. It proposes a two-pronged approach: a thorough analysis of current
algorithms and a novel, hierarchical taxonomy for precise categorization. The
taxonomy is based on a tri-level hierarchy, starting with the methodology
category and narrowing down to specific techniques. Such a framework allows for
a structured and comprehensive classification of algorithms, assisting
researchers in understanding the interrelationships among diverse algorithms
and techniques. Covering a wide range of algorithms, this taxonomy first
categorizes algorithms into four main analysis types: User and Item
Similarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and
Algorithmic Methods, and Mathematical Modeling Methods, with further
subdivisions into sub-categories and techniques. The paper incorporates both
empirical and experimental evaluations to differentiate between the techniques.
The empirical evaluation ranks the techniques based on four criteria. The
experimental assessments rank the algorithms that belong to the same category,
sub-category, technique, and sub-technique. Also, the paper illuminates the
future prospects of big data techniques in recommendation systems, underscoring
potential advancements and opportunities for further research in this field
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03370" title="Abstract">arXiv:2402.03370</a> [<a href="/pdf/2402.03370" title="Download PDF">pdf</a>, <a href="/format/2402.03370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of tortured phrases in scientific literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martel%2C+E">El&#xe9;na Martel</a> (SIGMA, LIG), 
<a href="/search/cs?searchtype=author&query=Lentschat%2C+M">Martin Lentschat</a> (SIGMA, GETALP), 
<a href="/search/cs?searchtype=author&query=Labb%C3%A9%2C+C">Cyril Labb&#xe9;</a> (LIG, SIGMA )
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2nd Workshop on Information Extraction from
  Scientific Publications, Nov 2023, Bali, Indonesia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Digital Libraries (cs.DL)

</div>
<p class="mathjax">This paper presents various automatic detection methods to extract so called
tortured phrases from scientific papers. These tortured phrases, e.g. flag to
clamor instead of signal to noise, are the results of paraphrasing tools used
to escape plagiarism detection. We built a dataset and evaluated several
strategies to flag previously undocumented tortured phrases. The proposed and
tested methods are based on language models and either on embeddings
similarities or on predictions of masked token. We found that an approach using
token prediction and that propagates the scores to the chunk level gives the
best results. With a recall value of .87 and a precision value of .61, it could
retrieve new tortured phrases to be submitted to domain experts for validation.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03373" title="Abstract">arXiv:2402.03373</a> [<a href="/pdf/2402.03373" title="Download PDF">pdf</a>, <a href="/format/2402.03373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeMalloc: Semantics-Informed Memory Allocator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Meng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Asokan%2C+N">N. Asokan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Use-after-free (UAF) is a critical and prevalent problem in memory unsafe
languages. While many solutions have been proposed, they seem to balance
security, run-time cost, and memory overhead (an impossible trinity) in awkward
ways. In this paper, we show that a balance can be achieved by passing more
semantics about the heap object to the allocator for it to make informed
allocation decisions. More specifically, we propose a new notion of thread-,
context-, and flow-sensitive "type", SemaType, to capture the semantics and
prototype a SemaType-based allocator that aims for the best trade-off amongst
the impossible trinity. In SeMalloc, only heap objects allocated from the same
call site and via the same function call stack can possibly share a virtual
memory address, which effectively stops type-confusion attacks and make UAF
vulnerabilities harder to exploit. Through extensive empirical evaluation, we
show that SeMalloc is realistic: (a) SeMalloc is effective in thwarting all
real-world vulnerabilities we tested; (b) benchmark programs run even slightly
faster with SeMalloc than the default heap allocator, at a memory overhead
ranges from 46% to 247%; and (c) SeMalloc balances security and overhead
strictly better than other closely related works.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03375" title="Abstract">arXiv:2402.03375</a> [<a href="/pdf/2402.03375" title="Download PDF">pdf</a>, <a href="/format/2402.03375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BetterV: Controlled Verilog Generation with Discriminative Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+Z">Zehua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+H">Hui-Ling Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Due to the growing complexity of modern Integrated Circuits (ICs), there is a
need for automated circuit design methods. Recent years have seen rising
research in hardware design language generation to facilitate the design
process. In this work, we propose a Verilog generation framework, BetterV,
which fine-tunes the large language models (LLMs) on processed domain-specific
datasets and incorporates generative discriminators for guidance on particular
design demands. The Verilog modules are collected, filtered and processed from
internet to form a clean and abundant dataset. Instruct-tuning methods are
specially designed to fine-tuned the LLMs to understand the knowledge about
Verilog. Furthermore, data are augmented to enrich the training set and also
used to train a generative discriminator on particular downstream task, which
leads a guidance for the LLMs to optimize the Verilog implementation. BetterV
has the ability to generate syntactically and functionally correct Verilog,
which can outperform GPT-4 on the VerilogEval-machine benchmark. With the help
of task-specific generative discriminator, BetterV can achieve remarkable
improvement on various electronic design automation (EDA) downstream tasks,
including the netlist node reduction for synthesis and verification runtime
reduction with Boolean Satisfiability (SAT) solving.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03376" title="Abstract">arXiv:2402.03376</a> [<a href="/pdf/2402.03376" title="Download PDF">pdf</a>, <a href="/ps/2402.03376" title="Download PostScript">ps</a>, <a href="/format/2402.03376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Conformal LiDAR-Mapping for Structured SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prieto-Fern%C3%A1ndez%2C+N">Natalia Prieto-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Blanco%2C+S">Sergio Fern&#xe1;ndez-Blanco</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Blanco%2C+%C3%81">&#xc1;lvaro Fern&#xe1;ndez-Blanco</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Carro-De-Lorenzo%2C+F">Francisco Carro-De-Lorenzo</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Instrumentation and Measurement, Volume 72,
  June 2023, 8504110
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">One of the main challenges in simultaneous localization and mapping (SLAM) is
real-time processing. High-computational loads linked to data acquisition and
processing complicate this task. This article presents an efficient feature
extraction approach for mapping structured environments. The proposed
methodology, weighted conformal LiDAR-mapping (WCLM), is based on the
extraction of polygonal profiles and propagation of uncertainties from raw
measurement data. This is achieved using conformal M bius transformation. The
algorithm has been validated experimentally using 2-D data obtained from a
low-cost Light Detection and Ranging (LiDAR) range finder. The results obtained
suggest that computational efficiency is significantly improved with reference
to other state-of-the-art SLAM approaches.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03378" title="Abstract">arXiv:2402.03378</a> [<a href="/pdf/2402.03378" title="Download PDF">pdf</a>, <a href="/ps/2402.03378" title="Download PostScript">ps</a>, <a href="/format/2402.03378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point  Process Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pulido%2C+C">Cristian Pulido</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+F">Francisco G&#xf3;mez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 5 figures, 1 Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The Perception of Security (PoS) refers to people's opinions about security
or insecurity in a place or situation. While surveys have traditionally been
the primary means to capture such perceptions, they need to be improved in
their ability to offer real-time monitoring or predictive insights into future
security perceptions. Recent evidence suggests that social network content can
provide complementary insights into quantifying these perceptions. However, the
challenge of accurately predicting these perceptions, with the capacity to
anticipate them, still needs to be explored. This article introduces an
innovative approach to PoS within short time frames using social network data.
Our model incorporates external factors that influence the publication and
reposting of content related to security perceptions. Our results demonstrate
that this proposed model achieves competitive predictive performance and
maintains a high degree of interpretability regarding the factors influencing
security perceptions. This research contributes to understanding how temporal
patterns and external factors impact the anticipation of security perceptions,
providing valuable insights for proactive security planning.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03379" title="Abstract">arXiv:2402.03379</a> [<a href="/pdf/2402.03379" title="Download PDF">pdf</a>, <a href="/format/2402.03379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entire Chain Uplift Modeling with Context-Enhanced Learning for  Intelligent Marketing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Min Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xue Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinhua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yi Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Uplift modeling, vital in online marketing, seeks to accurately measure the
impact of various strategies, such as coupons or discounts, on different users
by predicting the Individual Treatment Effect (ITE). In an e-commerce setting,
user behavior follows a defined sequential chain, including impression, click,
and conversion. Marketing strategies exert varied uplift effects at each stage
within this chain, impacting metrics like click-through and conversion rate.
Despite its utility, existing research has neglected to consider the inter-task
across all stages impacts within a specific treatment and has insufficiently
utilized the treatment information, potentially introducing substantial bias
into subsequent marketing decisions. We identify these two issues as the
chain-bias problem and the treatment-unadaptive problem. This paper introduces
the Entire Chain UPlift method with context-enhanced learning (ECUP), devised
to tackle these issues. ECUP consists of two primary components: 1) the Entire
Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE
throughout the entire chain space, models the various impacts of treatments on
each task, and integrates task prior information to enhance context awareness
across all stages, capturing the impact of treatment on different tasks, and 2)
the Treatment-Enhanced Network, which facilitates fine-grained treatment
modeling through bit-level feature interactions, thereby enabling adaptive
feature adjustment. Extensive experiments on public and industrial datasets
validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan
food delivery platform, serving millions of daily active users, with the
related dataset released for future research.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03380" title="Abstract">arXiv:2402.03380</a> [<a href="/pdf/2402.03380" title="Download PDF">pdf</a>, <a href="/ps/2402.03380" title="Download PostScript">ps</a>, <a href="/format/2402.03380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified K-means with Cluster Assignment -- Application to COVID-19 Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawat%2C+S">Shreyash Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Vijayarajan%2C+V">V. Vijayarajan</a>, 
<a href="/search/cs?searchtype=author&query=Prasath%2C+V+B+S">V. B. Surya Prasath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Text extraction is a highly subjective problem which depends on the dataset
that one is working on and the kind of summarization details that needs to be
extracted out. All the steps ranging from preprocessing of the data, to the
choice of an optimal model for predictions, depends on the problem and the
corpus at hand. In this paper, we describe a text extraction model where the
aim is to extract word specified information relating to the semantics such
that we can get all related and meaningful information about that word in a
succinct format. This model can obtain meaningful results and can augment
ubiquitous search model or a normal clustering or topic modelling algorithms.
By utilizing new technique called two cluster assignment technique with K-means
model, we improved the ontology of the retrieved text. We further apply the
vector average damping technique for flexible movement of clusters. Our
experimental results on a recent corpus of Covid-19 shows that we obtain good
results based on main keywords.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03384" title="Abstract">arXiv:2402.03384</a> [<a href="/pdf/2402.03384" title="Download PDF">pdf</a>, <a href="/ps/2402.03384" title="Download PostScript">ps</a>, <a href="/format/2402.03384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survival and grade of the glioma prediction using transfer learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rubio%2C+S+V">Santiago Valbuena Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Olivera%2C+O+G">Oscar Garc&#xed;a-Olalla Olivera</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Alonso%2C+M">Maria-Inmaculada Gonz&#xe1;lez-Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PeerJ Computer Science, Volume 9, December 2023, ID e1723
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Glioblastoma is a highly malignant brain tumor with a life expectancy of only
3 to 6 months without treatment. Detecting and predicting its survival and
grade accurately are crucial. This study introduces a novel approach using
transfer learning techniques. Various pre-trained networks, including
EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive
optimization to identify the most suitable architecture. Transfer learning was
applied to fine-tune these models on a glioblastoma image dataset, aiming to
achieve two objectives: survival and tumor grade prediction.The experimental
results show 65% accuracy in survival prediction, classifying patients into
short, medium, or long survival categories. Additionally, the prediction of
tumor grade achieved an accuracy of 97%, accurately differentiating low-grade
gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is
attributed to the effectiveness of transfer learning, surpassing the current
state-of-the-art methods. In conclusion, this study presents a promising method
for predicting the survival and grade of glioblastoma. Transfer learning
demonstrates its potential in enhancing prediction models, particularly in
scenarios with limited large datasets. These findings hold promise for
improving diagnostic and treatment approaches for glioblastoma patients.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03385" title="Abstract">arXiv:2402.03385</a> [<a href="/pdf/2402.03385" title="Download PDF">pdf</a>, <a href="/ps/2402.03385" title="Download PostScript">ps</a>, <a href="/format/2402.03385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adolescent relational behaviour and the obesity pandemic: A descriptive  study applying social network analysis and machine learning techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+M+C">Mar&#xed;a Cristina Mart&#xed;nez-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Quiroga-S%C3%A1nchez%2C+E">Enedina Quiroga-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Arias-Ramos%2C+N">Natalia Arias-Ramos</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Plos One, Volume 18, Issue 8, August 2023, e0289553
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Aim: To study the existence of subgroups by exploring the similarities
between the attributes of the nodes of the groups, in relation to diet and
gender and, to analyse the connectivity between groups based on aspects of
similarities between them through SNA and artificial intelligence techniques.
<br />Methods: 235 students from 5 different educational centres participate in
this study between March and December 2015. Data analysis carried out is
divided into two blocks: social network analysis and unsupervised machine
learning techniques. As for the social network analysis, the Girvan-Newman
technique was applied to find the best number of cohesive groups within each of
the friendship networks of the different classes analysed.
<br />Results: After applying Girvan-Newman in the three classes, the best division
into clusters was respectively 2 for classroom A, 7 for classroom B and 6 for
classroom C. There are significant differences between the groups and the
gender and diet variables. After applying K-means using population diet as an
input variable, a K-means clustering of 2 clusters for class A, 3 clusters for
class B and 3 clusters for class C is obtained.
<br />Conclusion: Adolescents form subgroups within their classrooms. Subgroup
cohesion is defined by the fact that nodes share similarities in aspects that
influence obesity, they share attributes related to food quality and gender.
The concept of homophily, related to SNA, justifies our results. Artificial
intelligence techniques together with the application of the Girvan-Newman
provide robustness to the structural analysis of similarities and cohesion
between subgroups.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03386" title="Abstract">arXiv:2402.03386</a> [<a href="/pdf/2402.03386" title="Download PDF">pdf</a>, <a href="/ps/2402.03386" title="Download PostScript">ps</a>, <a href="/format/2402.03386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A generalized decision tree ensemble based on the NeuralNetworks  architecture: Distributed Gradient Boosting Forest (DGBF)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado-Panadero%2C+%C3%81">&#xc1;ngel Delgado-Panadero</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Intelligence, Volume 53, July 2023, pages 22991-23003
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Tree ensemble algorithms as RandomForest and GradientBoosting are currently
the dominant methods for modeling discrete or tabular data, however, they are
unable to perform a hierarchical representation learning from raw data as
NeuralNetworks does thanks to its multi-layered structure, which is a key
feature for DeepLearning problems and modeling unstructured data. This
limitation is due to the fact that tree algorithms can not be trained with
back-propagation because of their mathematical nature. However, in this work,
we demonstrate that the mathematical formulation of bagging and boosting can be
combined together to define a graph-structured-tree-ensemble algorithm with a
distributed representation learning process between trees naturally (without
using back-propagation). We call this novel approach Distributed Gradient
Boosting Forest (DGBF) and we demonstrate that both RandomForest and
GradientBoosting can be expressed as particular graph architectures of DGBT.
Finally, we see that the distributed learning outperforms both RandomForest and
GradientBoosting in 7 out of 9 datasets.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03387" title="Abstract">arXiv:2402.03387</a> [<a href="/pdf/2402.03387" title="Download PDF">pdf</a>, <a href="/format/2402.03387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Order in Autoregressive Graph Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen-Karlik%2C+E">Edo Cohen-Karlik</a>, 
<a href="/search/cs?searchtype=author&query=Rozenberg%2C+E">Eyal Rozenberg</a>, 
<a href="/search/cs?searchtype=author&query=Freedman%2C+D">Daniel Freedman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph generation is a fundamental problem in various domains, including
chemistry and social networks. Recent work has shown that molecular graph
generation using recurrent neural networks (RNNs) is advantageous compared to
traditional generative approaches which require converting continuous latent
representations into graphs. One issue which arises when treating graph
generation as sequential generation is the arbitrary order of the sequence
which results from a particular choice of graph flattening method. In this work
we propose using RNNs, taking into account the non-sequential nature of graphs
by adding an Orderless Regularization (OLR) term that encourages the hidden
state of the recurrent model to be invariant to different valid orderings
present under the training distribution. We demonstrate that sequential graph
generation models benefit from our proposed regularization scheme, especially
when data is scarce. Our findings contribute to the growing body of research on
graph generation and provide a valuable tool for various applications requiring
the synthesis of realistic and diverse graph structures.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03388" title="Abstract">arXiv:2402.03388</a> [<a href="/pdf/2402.03388" title="Download PDF">pdf</a>, <a href="/format/2402.03388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delivery Optimized Discovery in Behavioral User Segmentation under  Budget Constrain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+H">Harshita Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A+R">Atanu R. Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+S">Sunav Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Indela%2C+P+K">Paavan Kumar Indela</a>, 
<a href="/search/cs?searchtype=author&query=Parwatala%2C+V+P">Veda Pranav Parwatala</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Srinjayee Paul</a>, 
<a href="/search/cs?searchtype=author&query=Maiti%2C+A">Aurghya Maiti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Users' behavioral footprints online enable firms to discover behavior-based
user segments (or, segments) and deliver segment specific messages to users.
Following the discovery of segments, delivery of messages to users through
preferred media channels like Facebook and Google can be challenging, as only a
portion of users in a behavior segment find match in a medium, and only a
fraction of those matched actually see the message (exposure). Even high
quality discovery becomes futile when delivery fails. Many sophisticated
algorithms exist for discovering behavioral segments; however, these ignore the
delivery component. The problem is compounded because (i) the discovery is
performed on the behavior data space in firms' data (e.g., user clicks), while
the delivery is predicated on the static data space (e.g., geo, age) as defined
by media; and (ii) firms work under budget constraint. We introduce a
stochastic optimization based algorithm for delivery optimized discovery of
behavioral user segmentation and offer new metrics to address the joint
optimization. We leverage optimization under a budget constraint for delivery
combined with a learning-based component for discovery. Extensive experiments
on a public dataset from Google and a proprietary dataset show the
effectiveness of our approach by simultaneously improving delivery metrics,
reducing budget spend and achieving strong predictive performance in discovery.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03391" title="Abstract">arXiv:2402.03391</a> [<a href="/pdf/2402.03391" title="Download PDF">pdf</a>, <a href="/format/2402.03391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear model predictive control-based guidance law for path following  of unmanned surface vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Manzano%2C+J+M">J. M. Manzano</a>, 
<a href="/search/eess?searchtype=author&query=Salvador%2C+J+R">J. R. Salvador</a>, 
<a href="/search/eess?searchtype=author&query=Limon%2C+D">D. Limon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures. Preprint submitted to Ocean Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work proposes a nonlinear model predictive control-based guidance
strategy for unmanned surface vehicles, focused on path following. The
application of this strategy, in addition to overcome the drawbacks of previous
line-of-sight-based guidance laws, intends to enable the application of
predictive strategies also to the low-level control, responsible for tracking
the references provided by the guidance strategy. The stability and robustness
of the proposed strategy are theoretically discussed. Furthermore, given the
non-negligible computational cost of such nonlinear predictive guidance
strategy, the practical nonlinear model predictive control strategy is also
applied in order to reduce the computational cost to a great extent while
retaining the robust and stable features of the original predictive strategy.
The effectiveness and advantages of both proposed strategies over other
nonlinear guidance laws are illustrated through a complete set of simulations.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03395" title="Abstract">arXiv:2402.03395</a> [<a href="/pdf/2402.03395" title="Download PDF">pdf</a>, <a href="/ps/2402.03395" title="Download PostScript">ps</a>, <a href="/format/2402.03395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel scheme for a PCM-based cold energy storage system. Design,  modelling, and simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Suffo%2C+J+J">J. J. Suffo</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 14 figures. Preprint submitted to Applied Thermal Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies the design and the dynamic modelling of a novel
cold-storage refrigeration system based on phase-change materials (PCM). Cold
production supported by thermal storage systems (TES) is a very appealing field
of research, since it renders possible higher levels of efficiency in cold
production systems, via flexible cold-energy management, combining demand
fulfilment with cost optimization strategies. The paper proposes and compares
two different simulation models for a cold-storage refrigeration system based
on PCM. First, a continuous model, the application of which is limited to
certain conditions, but, given such conditions, it is a precise model, useful
for the design of the TES structural parameters, as well as for comparison and
validation of the other model. The second proposed model is a discrete one,
which, despite of being a discrete approximation of the continuous process, it
is of general validity.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03396" title="Abstract">arXiv:2402.03396</a> [<a href="/pdf/2402.03396" title="Download PDF">pdf</a>, <a href="/format/2402.03396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large  Language Models for Program Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yifeng He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiabo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yuyang Rong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+E">Ethan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The remarkable capability of large language models (LLMs) in generating
high-quality code has drawn increasing attention in the software testing
community. However, existing code LLMs often demonstrate unsatisfactory
capabilities in generating accurate and complete tests since they were trained
on code snippets collected without differentiating between code for testing
purposes and other code. In this paper, we present a large-scale dataset
UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test
Synthesis. Associating tests with the tested functions is crucial for LLMs to
infer the expected behavior and the logic paths to be verified. By leveraging
Language Server Protocol, UniTSyn achieves the challenging goal of collecting
focal-test pairs without per-project execution setups or per-language
heuristics that tend to be fragile and difficult to scale. It contains 2.7
million focal-test pairs across five mainstream programming languages, making
it possible to be utilized for enhancing the test generation ability of LLMs.
The details of UniTSyn can be found in Table 1. Our experiments demonstrate
that, by building an autoregressive model based on UniTSyn, we can achieve
significant benefits in learning and understanding unit test representations,
resulting in improved generation accuracy and code coverage across all
evaluated programming languages. Code and data will be publicly available.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03408" title="Abstract">arXiv:2402.03408</a> [<a href="/pdf/2402.03408" title="Download PDF">pdf</a>, <a href="/format/2402.03408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Effective Invocation Methods of Massive LLM Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bolin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+D">Dianbo Sui</a>, 
<a href="/search/cs?searchtype=author&query=Tum%2C+Z">Zhiying Tum</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiabao Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Language models as a service (LMaaS) enable users to accomplish tasks without
requiring specialized knowledge, simply by paying a service provider. However,
numerous providers offer massive large language model (LLM) services with
variations in latency, performance, and pricing. Consequently, constructing the
cost-saving LLM services invocation strategy with low-latency and
high-performance responses that meet specific task demands becomes a pressing
challenge. This paper provides a comprehensive overview of the LLM services
invocation methods. Technically, we give a formal definition of the problem of
constructing effective invocation strategy in LMaaS and present the LLM
services invocation framework. The framework classifies existing methods into
four different components, including input abstract, semantic cache, solution
design, and output enhancement, which can be freely combined with each other.
Finally, we emphasize the open challenges that have not yet been well addressed
in this task and shed light on future research.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03413" title="Abstract">arXiv:2402.03413</a> [<a href="/pdf/2402.03413" title="Download PDF">pdf</a>, <a href="/format/2402.03413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Video Quality Assessment: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Huiyu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yucheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Perceptual video quality assessment plays a vital role in the field of video
processing due to the existence of quality degradations introduced in various
stages of video signal acquisition, compression, transmission and display. With
the advancement of internet communication and cloud service technology, video
content and traffic are growing exponentially, which further emphasizes the
requirement for accurate and rapid assessment of video quality. Therefore,
numerous subjective and objective video quality assessment studies have been
conducted over the past two decades for both generic videos and specific videos
such as streaming, user-generated content (UGC), 3D, virtual and augmented
reality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey
provides an up-to-date and comprehensive review of these video quality
assessment studies. Specifically, we first review the subjective video quality
assessment methodologies and databases, which are necessary for validating the
performance of video quality metrics. Second, the objective video quality
assessment algorithms for general purposes are surveyed and concluded according
to the methodologies utilized in the quality measures. Third, we overview the
objective video quality assessment measures for specific applications and
emerging topics. Finally, the performances of the state-of-the-art video
quality assessment measures are compared and analyzed. This survey provides a
systematic overview of both classical works and recent progresses in the realm
of video quality assessment, which can help other researchers quickly access
the field and conduct relevant research.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03417" title="Abstract">arXiv:2402.03417</a> [<a href="/pdf/2402.03417" title="Download PDF">pdf</a>, <a href="/format/2402.03417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computer Vision Based Approach for Stalking Detection Using a  CNN-LSTM-MLP Hybrid Fusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M">Murad Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+S">Shahriar Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Faisal%2C+M+B+H">Md. Billal Hossain Faisal</a>, 
<a href="/search/cs?searchtype=author&query=Neloy%2C+M+M+H">Md. Musnad Hossin Neloy</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+T">Md. Tonmoy Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Reza%2C+M+T">Md. Tanzim Reza</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+G+R">Md. Golam Rabiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+Z">Md Zia Uddin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for publication in the PLOS ONE journal, 17 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Criminal and suspicious activity detection has become a popular research
topic in recent years. The rapid growth of computer vision technologies has had
a crucial impact on solving this issue. However, physical stalking detection is
still a less explored area despite the evolution of modern technology.
Nowadays, stalking in public places has become a common occurrence with women
being the most affected. Stalking is a visible action that usually occurs
before any criminal activity begins as the stalker begins to follow, loiter,
and stare at the victim before committing any criminal activity such as
assault, kidnapping, rape, and so on. Therefore, it has become a necessity to
detect stalking as all of these criminal activities can be stopped in the first
place through stalking detection. In this research, we propose a novel deep
learning-based hybrid fusion model to detect potential stalkers from a single
video with a minimal number of frames. We extract multiple relevant features,
such as facial landmarks, head pose estimation, and relative distance, as
numerical values from video frames. This data is fed into a multilayer
perceptron (MLP) to perform a classification task between a stalking and a
non-stalking scenario. Simultaneously, the video frames are fed into a
combination of convolutional and LSTM models to extract the spatio-temporal
features. We use a fusion of these numerical and spatio-temporal features to
build a classifier to detect stalking incidents. Additionally, we introduce a
dataset consisting of stalking and non-stalking videos gathered from various
feature films and television series, which is also used to train the model. The
experimental results show the efficiency and dynamism of our proposed stalker
detection system, achieving 89.58% testing accuracy with a significant
improvement as compared to the state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03435" title="Abstract">arXiv:2402.03435</a> [<a href="/pdf/2402.03435" title="Download PDF">pdf</a>, <a href="/format/2402.03435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Psychological Assessments with Large Language Models: A Privacy-Focused  and Cost-Effective Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanco-Cuaresma%2C+S">Sergi Blanco-Cuaresma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Workshop on Computational Linguistics and Clinical Psychology (CLPsych) at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">This study explores the use of Large Language Models (LLMs) to analyze text
comments from Reddit users, aiming to achieve two primary objectives: firstly,
to pinpoint critical excerpts that support a predefined psychological
assessment of suicidal risk; and secondly, to summarize the material to
substantiate the preassigned suicidal risk level. The work is circumscribed to
the use of "open-source" LLMs that can be run locally, thereby enhancing data
privacy. Furthermore, it prioritizes models with low computational
requirements, making it accessible to both individuals and institutions
operating on limited computing budgets. The implemented strategy only relies on
a carefully crafted prompt and a grammar to guide the LLM's text completion.
Despite its simplicity, the evaluation metrics show outstanding results, making
it a valuable privacy-focused and cost-effective approach. This work is part of
the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared
task.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03445" title="Abstract">arXiv:2402.03445</a> [<a href="/pdf/2402.03445" title="Download PDF">pdf</a>, <a href="/format/2402.03445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion via Image-Based Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anciukevicius%2C+T">Titas Anciukevicius</a>, 
<a href="/search/cs?searchtype=author&query=Manhardt%2C+F">Fabian Manhardt</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Paul Henderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024. Project page: <a href="https://anciukevicius.github.io/generative-image-based-rendering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generating 3D scenes is a challenging open problem, which requires
synthesizing plausible content that is fully consistent in 3D space. While
recent methods such as neural radiance fields excel at view synthesis and 3D
reconstruction, they cannot synthesize plausible details in unobserved regions
since they lack a generative capability. Conversely, existing generative
methods are typically not capable of reconstructing detailed, large-scale
scenes in the wild, as they use limited-capacity 3D scene representations,
require aligned camera poses, or rely on additional regularizers. In this work,
we introduce the first diffusion model able to perform fast, detailed
reconstruction and generation of real-world 3D scenes. To achieve this, we make
three contributions. First, we introduce a new neural scene representation,
IB-planes, that can efficiently and accurately represent large 3D scenes,
dynamically allocating more capacity as needed to capture details visible in
each image. Second, we propose a denoising-diffusion framework to learn a prior
over this novel 3D scene representation, using only 2D images without the need
for any additional supervision signal such as masks or depths. This supports 3D
reconstruction and generation in a unified architecture. Third, we develop a
principled approach to avoid trivial 3D solutions when integrating the
image-based rendering with the diffusion model, by dropping out representations
of some images. We evaluate the model on several challenging datasets of real
and synthetic images, and demonstrate superior results on generation, novel
view synthesis and 3D reconstruction.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03448" title="Abstract">arXiv:2402.03448</a> [<a href="/pdf/2402.03448" title="Download PDF">pdf</a>, <a href="/format/2402.03448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Sporadic Federated Learning: A Unified Methodology with  Generalized Convergence Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zehtabi%2C+S">Shahryar Zehtabi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Parasnis%2C+R">Rohit Parasnis</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinalipour%2C+S">Seyyedali Hosseinalipour</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Decentralized Federated Learning (DFL) has received significant recent
research attention, capturing settings where both model updates and model
aggregations -- the two key FL processes -- are conducted by the clients. In
this work, we propose Decentralized Sporadic Federated Learning
($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of
sporadicity in both of these processes, modeling the impact of different forms
of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$
unifies many of the prominent decentralized optimization methods, e.g.,
distributed gradient descent (DGD), randomized gossip (RG), and decentralized
federated averaging (DFedAvg), under a single modeling framework. We
analytically characterize the convergence behavior of $\texttt{DSpodFL}$,
showing, among other insights, that we can match a geometric convergence rate
to a finite optimality gap under more general assumptions than in existing
works. Through experiments, we demonstrate that $\texttt{DSpodFL}$ achieves
significantly improved training speeds and robustness to variations in system
parameters compared to the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03449" title="Abstract">arXiv:2402.03449</a> [<a href="/pdf/2402.03449" title="Download PDF">pdf</a>, <a href="/format/2402.03449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending RAIM with a Gaussian Mixture of Opportunistic Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Papadimitratos%2C+P">Panos Papadimitratos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">GNSS are indispensable for various applications, but they are vulnerable to
spoofing attacks. The original receiver autonomous integrity monitoring (RAIM)
was not designed for securing GNSS. In this context, RAIM was extended with
wireless signals, termed signals of opportunity (SOPs), or onboard sensors,
typically assumed benign. However, attackers might also manipulate wireless
networks, raising the need for a solution that considers untrustworthy SOPs. To
address this, we extend RAIM by incorporating all opportunistic information,
i.e., measurements from terrestrial infrastructures and onboard sensors,
culminating in one function for robust GNSS spoofing detection. The objective
is to assess the likelihood of GNSS spoofing by analyzing locations derived
from extended RAIM solutions, which include location solutions from GNSS
pseudorange subsets and wireless signal subsets of untrusted networks. Our
method comprises two pivotal components: subset generation and location fusion.
Subsets of ranging information are created and processed through positioning
algorithms, producing temporary locations. Onboard sensors provide speed,
acceleration, and attitude data, aiding in location filtering based on motion
constraints. The filtered locations, modeled with uncertainty, are fused into a
composite likelihood function normalized for GNSS spoofing detection.
Theoretical assessments of GNSS-only and multi-infrastructure scenarios under
uncoordinated and coordinated attacks are conducted. The detection of these
attacks is feasible when the number of benign subsets exceeds a specific
threshold. A real-world dataset from the Kista area is used for experimental
validation. Comparative analysis against baseline methods shows a significant
improvement in detection accuracy achieved by our Gaussian Mixture RAIM
approach. Moreover, we discuss leveraging RAIM results for plausible location
recovery.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03450" title="Abstract">arXiv:2402.03450</a> [<a href="/pdf/2402.03450" title="Download PDF">pdf</a>, <a href="/format/2402.03450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommendation Fairness in Social Networks Over Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+H">Hussain Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Sikdar%2C+S">Sandipan Sikdar</a>, 
<a href="/search/cs?searchtype=author&query=Helic%2C+D">Denis Helic</a>, 
<a href="/search/cs?searchtype=author&query=Strohmaier%2C+M">Markus Strohmaier</a>, 
<a href="/search/cs?searchtype=author&query=Kern%2C+R">Roman Kern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In social recommender systems, it is crucial that the recommendation models
provide equitable visibility for different demographic groups, such as gender
or race. Most existing research has addressed this problem by only studying
individual static snapshots of networks that typically change over time. To
address this gap, we study the evolution of recommendation fairness over time
and its relation to dynamic network properties. We examine three real-world
dynamic networks by evaluating the fairness of six recommendation algorithms
and analyzing the association between fairness and network properties over
time. We further study how interventions on network properties influence
fairness by examining counterfactual scenarios with alternative evolution
outcomes and differing network properties. Our results on empirical datasets
suggest that recommendation fairness improves over time, regardless of the
recommendation method. We also find that two network properties, minority
ratio, and homophily ratio, exhibit stable correlations with fairness over
time. Our counterfactual study further suggests that an extreme homophily ratio
potentially contributes to unfair recommendations even with a balanced minority
ratio. Our work provides insights into the evolution of fairness within dynamic
networks in social science. We believe that our findings will help system
operators and policymakers to better comprehend the implications of temporal
changes and interventions targeting fairness in social networks.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03455" title="Abstract">arXiv:2402.03455</a> [<a href="/pdf/2402.03455" title="Download PDF">pdf</a>, <a href="/format/2402.03455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Median and Small Parsimony Problems on RNA trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marchand%2C+B">Bertrand Marchand</a>, 
<a href="/search/cs?searchtype=author&query=Anselmetti%2C+Y">Yoann Anselmetti</a>, 
<a href="/search/cs?searchtype=author&query=Lafond%2C+M">Manuel Lafond</a>, 
<a href="/search/cs?searchtype=author&query=Ouangraoua%2C+A">A&#xef;da Ouangraoua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Motivation: Non-coding RNAs (ncRNAs) express their functions by adopting
molecular structures. Specifically, RNA secondary structures serve as a
relatively stable intermediate step before tertiary structures, offering a
reliable signature of molecular function. Consequently, within an RNA
functional family, secondary structures are generally more evolutionarily
conserved than sequences. Conversely, homologous RNA families grouped within an
RNA clan share ancestors but typically exhibit structural differences.
Inferring the evolution of RNA structures within RNA families and clans is
crucial for gaining insights into functional adaptations over time and
providing clues about the Ancient RNA World Hypothesis. Results: We introduce
the median problem and the small parsimony problem for ncRNA families, where
secondary structures are represented as leaf-labelled trees. We utilize the
Robinson-Foulds (RF) tree distance, which corresponds to a specific edit
distance between RNA trees, and a new metric called the Internal-Leafset (IL)
distance. While the RF tree distance compares sets of leaves descending from
internal nodes of two RNA trees, the IL distance compares the collection of
leaf-children of internal nodes. The latter is better at capturing differences
in structural elements of RNAs than the RF distance, which is more focused on
base pairs. We also consider a more general tree edit distance that allows the
mapping of base pairs that are not perfectly aligned. We study the theoretical
complexity of the median problem and the small parsimony problem under the
three distance metrics and various biologically-relevant constraints, and we
present polynomial-time maximum parsimony algorithms for solving some versions
of the problems. Our algorithms are applied to ncRNA families from the RFAM
database, illustrating their practical utility
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03456" title="Abstract">arXiv:2402.03456</a> [<a href="/pdf/2402.03456" title="Download PDF">pdf</a>, <a href="/format/2402.03456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Multiview Representation for Self-supervised Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Siyuan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Kai Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+G">Ge Cui</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haoteng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+L">Liang Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures, 2 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Representation learning constitutes a pivotal cornerstone in contemporary
deep learning paradigms, offering a conduit to elucidate distinctive features
within the latent space and interpret the deep models. Nevertheless, the
inherent complexity of anatomical patterns and the random nature of lesion
distribution in medical image segmentation pose significant challenges to the
disentanglement of representations and the understanding of salient features.
Methods guided by the maximization of mutual information, particularly within
the framework of contrastive learning, have demonstrated remarkable success and
superiority in decoupling densely intertwined representations. However, the
effectiveness of contrastive learning highly depends on the quality of the
positive and negative sample pairs, i.e. the unselected average mutual
information among multi-views would obstruct the learning strategy so the
selection of the views is vital. In this work, we introduce a novel approach
predicated on representation distance-based mutual information (MI)
maximization for measuring the significance of different views, aiming at
conducting more efficient contrastive learning and representation
disentanglement. Additionally, we introduce an MI re-ranking strategy for
representation selection, benefiting both the continuous MI estimating and
representation significance distance measuring. Specifically, we harness
multi-view representations extracted from the frequency domain, re-evaluating
their significance based on mutual information across varying frequencies,
thereby facilitating a multifaceted contrastive learning approach to bolster
semantic comprehension. The statistical results under the five metrics
demonstrate that our proposed framework proficiently constrains the MI
maximization-driven representation selection and steers the multi-view
contrastive learning process.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03457" title="Abstract">arXiv:2402.03457</a> [<a href="/pdf/2402.03457" title="Download PDF">pdf</a>, <a href="/format/2402.03457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Interpretable Traffic Destination Prediction using  Explainable Boosting Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousif%2C+Y">Yasin Yousif</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J">J&#xf6;rg M&#xfc;ller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Developing accurate models for traffic trajectory predictions is crucial for
achieving fully autonomous driving. Various deep neural network models have
been employed to address this challenge, but their black-box nature hinders
transparency and debugging capabilities in a deployed system. Glass-box models
offer a solution by providing full interpretability through methods like
\ac{GAM}. In this study, we evaluate an efficient additive model called
\ac{EBM} for traffic prediction on three popular mixed traffic datasets:
\ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models
perform competitively in predicting pedestrian destinations within \ac{SDD} and
\ac{InD} while providing modest predictions for vehicle-dominant Argoverse
dataset. Additionally, our transparent trained models allow us to analyse
feature importance and interactions, as well as provide qualitative examples of
predictions explanation. The full training code will be made public upon
publication.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03464" title="Abstract">arXiv:2402.03464</a> [<a href="/pdf/2402.03464" title="Download PDF">pdf</a>, <a href="/ps/2402.03464" title="Download PostScript">ps</a>, <a href="/format/2402.03464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fuzzy Approach to Record Linkages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+P+K">Pratik K. Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal Paper (9 pages, 6 Figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Record Linkage is the process of identifying and unifying records from
various independent data sources. Existing strategies, which can be either
deterministic or probabilistic, often fail to link records satisfactorily under
uncertainty. This paper describes an indigenously (locally) developed fuzzy
linkage method, based on fuzzy set techniques, which can effectively account
for this uncertainty prevalent in the disparate data sources and address the
shortcomings of the existing approaches. Extensive testing, evaluation and
comparisons have demonstrated the efficacy of this fuzzy approach for record
linkages.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03465" title="Abstract">arXiv:2402.03465</a> [<a href="/pdf/2402.03465" title="Download PDF">pdf</a>, <a href="/format/2402.03465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband  Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uvaydov%2C+D">Daniel Uvaydov</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Milin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C+P">Clifton Paul Robinson</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+S">Salvatore D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Melodia%2C+T">Tommaso Melodia</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Spectrum has become an extremely scarce and congested resource. As a
consequence, spectrum sensing enables the coexistence of different wireless
technologies in shared spectrum bands. Most existing work requires spectrograms
to classify signals. Ultimately, this implies that images need to be
continuously created from I/Q samples, thus creating unacceptable latency for
real-time operations. In addition, spectrogram-based approaches do not achieve
sufficient granularity level as they are based on object detection performed on
pixels and are based on rectangular bounding boxes. For this reason, we propose
a completely novel approach based on semantic spectrum segmentation, where
multiple signals are simultaneously classified and localized in both time and
frequency at the I/Q level. Conversely from the state-of-the-art computer
vision algorithm, we add non-local blocks to combine the spatial features of
signals, and thus achieve better performance. In addition, we propose a novel
data generation approach where a limited set of easy-to-collect real-world
wireless signals are ``stitched together'' to generate large-scale, wideband,
and diverse datasets. Experimental results obtained on multiple testbeds
(including the Arena testbed) using multiple antennas, multiple sampling
frequencies, and multiple radios over the course of 3 days show that our
approach classifies and localizes signals with a mean intersection over union
(IOU) of 96.70% across 5 wireless protocols while performing in real-time with
a latency of 2.6 ms. Moreover, we demonstrate that our approach based on
non-local blocks achieves 7% more accuracy when segmenting the most challenging
signals with respect to the state-of-the-art U-Net algorithm. We will release
our 17 GB dataset and code.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03466" title="Abstract">arXiv:2402.03466</a> [<a href="/pdf/2402.03466" title="Download PDF">pdf</a>, <a href="/format/2402.03466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Encoded Graph Neural Networks for Deformation Prediction under  Contact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saleh%2C+M">Mahdi Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Sommersperger%2C+M">Michael Sommersperger</a>, 
<a href="/search/cs?searchtype=author&query=Navab%2C+N">Nassir Navab</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 2024 IEEE International Conference on Robotics and Automation (ICRA2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG); Robotics (cs.RO)

</div>
<p class="mathjax">In robotics, it's crucial to understand object deformation during tactile
interactions. A precise understanding of deformation can elevate robotic
simulations and have broad implications across different industries. We
introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such
predictions. Similar to robotic grasping and manipulation scenarios, we focus
on modeling the dynamics between a rigid mesh contacting a deformable mesh
under external forces. Our approach represents both the soft body and the rigid
body within graph structures, where nodes hold the physical states of the
meshes. We also incorporate cross-attention mechanisms to capture the interplay
between the objects. By jointly learning geometry and physics, our model
reconstructs consistent and detailed deformations. We've made our code and
dataset public to advance research in robotic simulation and grasping.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03467" title="Abstract">arXiv:2402.03467</a> [<a href="/pdf/2402.03467" title="Download PDF">pdf</a>, <a href="/ps/2402.03467" title="Download PostScript">ps</a>, <a href="/format/2402.03467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gess%2C+B">Benjamin Gess</a>, 
<a href="/search/cs?searchtype=author&query=Kassing%2C+S">Sebastian Kassing</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+N">Nimit Rana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We give quantitative estimates for the rate of convergence of Riemannian
stochastic gradient descent (RSGD) to Riemannian gradient flow and to a
diffusion process, the so-called Riemannian stochastic modified flow (RSMF).
Using tools from stochastic differential geometry we show that, in the small
learning rate regime, RSGD can be approximated by the solution to the RSMF
driven by an infinite-dimensional Wiener process. The RSMF accounts for the
random fluctuations of RSGD and, thereby, increases the order of approximation
compared to the deterministic Riemannian gradient flow. The RSGD is build using
the concept of a retraction map, that is, a cost efficient approximation of the
exponential map, and we prove quantitative bounds for the weak error of the
diffusion approximation under assumptions on the retraction map, the geometry
of the manifold, and the random estimators of the gradient.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03468" title="Abstract">arXiv:2402.03468</a> [<a href="/pdf/2402.03468" title="Download PDF">pdf</a>, <a href="/format/2402.03468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Tensor Completion Powered by Arbitrary Linear Transforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+L">Li Ge</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, a tensor completion problem is studied, which aims to perfectly
recover the tensor from partial observations. Existing theoretical guarantee
requires the involved transform to be orthogonal, which hinders its
applications. In this paper, jumping out of the constraints of isotropy or
self-adjointness, the theoretical guarantee of exact tensor completion with
arbitrary linear transforms is established. To that end, we define a new
tensor-tensor product, which leads us to a new definition of the tensor nuclear
norm. Equipped with these tools, an efficient algorithm based on alternating
direction of multipliers is designed to solve the transformed tensor completion
program and the theoretical bound is obtained. Our model and proof greatly
enhance the flexibility of tensor completion and extensive experiments validate
the superiority of the proposed method.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03469" title="Abstract">arXiv:2402.03469</a> [<a href="/pdf/2402.03469" title="Download PDF">pdf</a>, <a href="/format/2402.03469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference-free Alignment Learning with Regularized Relevance Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Learning from human preference has been considered key to aligning Large
Language Models (LLMs) with human values. However, contrary to popular belief,
our preliminary study reveals that reward models trained on human preference
datasets tend to give higher scores to long off-topic responses than short
on-topic ones. Motivated by this observation, we explore a preference-free
approach utilizing `relevance' as a key objective for alignment. On our first
attempt, we find that the relevance score obtained by a retriever alone is
vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when
we utilize the score as a reward for reinforcement learning. To mitigate it, we
integrate effective inductive biases into the vanilla relevance to regularize
each other, resulting in a mixture of reward functions: Regularized Relevance
Reward ($R^3$). $R^3$ significantly improves performance on preference
benchmarks by providing a robust reward signal. Notably, $R^3$ does not require
any human preference datasets (i.e., preference-free), outperforming
open-source reward models in improving human preference. Our analysis
demonstrates that $R^3$ has advantages in elevating human preference while
minimizing its side effects. Finally, we show the generalizability of $R^3$,
consistently improving instruction-tuned models in various backbones and sizes
without additional dataset cost. Our code is available at
https://github.com/naver-ai/RRR.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03471" title="Abstract">arXiv:2402.03471</a> [<a href="/pdf/2402.03471" title="Download PDF">pdf</a>, <a href="/format/2402.03471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Information of Large Language Model Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghai Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Theory (cs.IT)

</div>
<p class="mathjax">This paper investigates the information encoded in the embeddings of large
language models (LLMs). We conduct simulations to analyze the representation
entropy and discover a power law relationship with model sizes. Building upon
this observation, we propose a theory based on (conditional) entropy to
elucidate the scaling law phenomenon. Furthermore, we delve into the
auto-regressive structure of LLMs and examine the relationship between the last
token and previous context tokens using information theory and regression
techniques. Specifically, we establish a theoretical connection between the
information gain of new tokens and ridge regression. Additionally, we explore
the effectiveness of Lasso regression in selecting meaningful tokens, which
sometimes outperforms the closely related attention weights. Finally, we
conduct controlled experiments, and find that information is distributed across
tokens, rather than being concentrated in specific "meaningful" tokens alone.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03477" title="Abstract">arXiv:2402.03477</a> [<a href="/pdf/2402.03477" title="Download PDF">pdf</a>, <a href="/format/2402.03477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arabic Synonym BERT-based Adversarial Examples for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alshahrani%2C+N">Norah Alshahrani</a>, 
<a href="/search/cs?searchtype=author&query=Alshahrani%2C+S">Saied Alshahrani</a>, 
<a href="/search/cs?searchtype=author&query=Wali%2C+E">Esma Wali</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+J">Jeanna Matthews</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted at The 18th Conference of the European Chapter of the Association for Computational Linguistics (Student Research Workshop), March 17-22, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text classification systems have been proven vulnerable to adversarial text
examples, modified versions of the original text examples that are often
unnoticed by human eyes, yet can force text classification models to alter
their classification. Often, research works quantifying the impact of
adversarial text attacks have been applied only to models trained in English.
In this paper, we introduce the first word-level study of adversarial attacks
in Arabic. Specifically, we use a synonym (word-level) attack using a Masked
Language Modeling (MLM) task with a BERT model in a black-box setting to assess
the robustness of the state-of-the-art text classification models to
adversarial attacks in Arabic. To evaluate the grammatical and semantic
similarities of the newly produced adversarial examples using our synonym
BERT-based attack, we invite four human evaluators to assess and compare the
produced adversarial examples with their original examples. We also study the
transferability of these newly produced Arabic adversarial examples to various
models and investigate the effectiveness of defense mechanisms against these
adversarial examples on the BERT models. We find that fine-tuned BERT models
were more susceptible to our synonym attacks than the other Deep Neural
Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that
fine-tuned BERT models were more susceptible to transferred attacks. We,
lastly, find that fine-tuned BERT models successfully regain at least 2% in
accuracy after applying adversarial training as an initial defense mechanism.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03478" title="Abstract">arXiv:2402.03478</a> [<a href="/pdf/2402.03478" title="Download PDF">pdf</a>, <a href="/format/2402.03478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a  Single Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+M+A">Matthew A. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+M+J">Maria J. Molina</a>, 
<a href="/search/cs?searchtype=author&query=Metzler%2C+C+A">Christopher A. Metzler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Estimating and disentangling epistemic uncertainty (uncertainty that can be
reduced with more training data) and aleatoric uncertainty (uncertainty that is
inherent to the task at hand) is critically important when applying machine
learning (ML) to high-stakes applications such as medical imaging and weather
forecasting. Conditional diffusion models' breakthrough ability to accurately
and efficiently sample from the posterior distribution of a dataset now makes
uncertainty estimation conceptually straightforward: One need only train and
sample from a large ensemble of diffusion models. Unfortunately, training such
an ensemble becomes computationally intractable as the complexity of the model
architecture grows.
<br />In this work we introduce a new approach to ensembling, hyper-diffusion,
which allows one to accurately estimate epistemic and aleatoric uncertainty
with a single model. Unlike existing Monte Carlo dropout based single-model
ensembling methods, hyper-diffusion offers the same prediction accuracy as
multi-model ensembles. We validate our approach on two distinct tasks: x-ray
computed tomography (CT) reconstruction and weather temperature forecasting.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03479" title="Abstract">arXiv:2402.03479</a> [<a href="/pdf/2402.03479" title="Download PDF">pdf</a>, <a href="/format/2402.03479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context  Environment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcin%2C+S">Samuel Garcin</a>, 
<a href="/search/cs?searchtype=author&query=Doran%2C+J">James Doran</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangmin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.03494">arXiv:2310.03494</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Autonomous agents trained using deep reinforcement learning (RL) often lack
the ability to successfully generalise to new environments, even when they
share characteristics with the environments they have encountered during
training. In this work, we investigate how the sampling of individual
environment instances, or levels, affects the zero-shot generalisation (ZSG)
ability of RL agents. We discover that, for deep actor-critic architectures
sharing their base layers, prioritising levels according to their value loss
minimises the mutual information between the agent's internal representation
and the set of training levels in the generated training data. This provides a
novel theoretical justification for the implicit regularisation achieved by
certain adaptive sampling strategies. We then turn our attention to
unsupervised environment design (UED) methods, which have more control over the
data generation mechanism. We find that existing UED methods can significantly
shift the training distribution, which translates to low ZSG performance. To
prevent both overfitting and distributional shift, we introduce in-context
environment design (ICED). ICED generates levels using a variational
autoencoder trained over an initial set of level parameters, reducing
distributional shift, and achieves significant improvements in ZSG over
adaptive level sampling strategies and UED methods.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03480" title="Abstract">arXiv:2402.03480</a> [<a href="/pdf/2402.03480" title="Download PDF">pdf</a>, <a href="/format/2402.03480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A  Survey and Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hudson%2C+N">Nathaniel Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Pauloski%2C+J+G">J. Gregory Pauloski</a>, 
<a href="/search/cs?searchtype=author&query=Baughman%2C+M">Matt Baughman</a>, 
<a href="/search/cs?searchtype=author&query=Kamatar%2C+A">Alok Kamatar</a>, 
<a href="/search/cs?searchtype=author&query=Sakarvadia%2C+M">Mansi Sakarvadia</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+L">Logan Ward</a>, 
<a href="/search/cs?searchtype=author&query=Chard%2C+R">Ryan Chard</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+A">Andr&#xe9; Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Levental%2C+M">Maksim Levental</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Engler%2C+W">Will Engler</a>, 
<a href="/search/cs?searchtype=author&query=Skelly%2C+O+P">Owen Price Skelly</a>, 
<a href="/search/cs?searchtype=author&query=Blaiszik%2C+B">Ben Blaiszik</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+R">Rick Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Chard%2C+K">Kyle Chard</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, accepted for publication in the proceedings of the 10th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Deep learning methods are transforming research, enabling new techniques, and
ultimately leading to new discoveries. As the demand for more capable AI models
continues to grow, we are now entering an era of Trillion Parameter Models
(TPM), or models with more than a trillion parameters -- such as Huawei's
PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and
providers that caters to the specific needs of the scientific community. We
then outline the significant technical challenges and open problems in system
design for serving TPMs to enable scientific research and discovery.
Specifically, we describe the requirements of a comprehensive software stack
and interfaces to support the diverse and flexible requirements of researchers.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03481" title="Abstract">arXiv:2402.03481</a> [<a href="/pdf/2402.03481" title="Download PDF">pdf</a>, <a href="/format/2402.03481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sejoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Ustun%2C+B">Berk Ustun</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 6th FAccTRec Workshop on Responsible Recommendation @ ACM RecSys 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Modern recommender systems may output considerably different recommendations
due to small perturbations in the training data. Changes in the data from a
single user will alter the recommendations as well as the recommendations of
other users. In applications like healthcare, housing, and finance, this
sensitivity can have adverse effects on user experience. We propose a method to
stabilize a given recommender system against such perturbations. This is a
challenging task due to (1) the lack of a ``reference'' rank list that can be
used to anchor the outputs; and (2) the computational challenges in ensuring
the stability of rank lists with respect to all possible perturbations of
training data. Our method, FINEST, overcomes these challenges by obtaining
reference rank lists from a given recommendation model and then fine-tuning the
model under simulated perturbation scenarios with rank-preserving
regularization on sampled items. Our experiments on real-world datasets
demonstrate that FINEST can ensure that recommender models output stable
recommendations under a wide range of different perturbations without
compromising next-item prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03483" title="Abstract">arXiv:2402.03483</a> [<a href="/pdf/2402.03483" title="Download PDF">pdf</a>, <a href="/format/2402.03483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWAG: Storytelling With Action Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+Z">Zeeshan Patel</a>, 
<a href="/search/cs?searchtype=author&query=El-Refai%2C+K">Karim El-Refai</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jonathan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianle Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated long-form story generation typically employs long-context large
language models (LLMs) for one-shot creation, which can produce cohesive but
not necessarily engaging content. We introduce Storytelling With Action
Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach
reduces story writing to a search problem through a two-model feedback loop:
one LLM generates story content, and another auxiliary LLM is used to choose
the next best "action" to steer the story's future direction. Our results show
that SWAG can substantially outperform previous end-to-end story generation
techniques when evaluated by GPT-4 and through human evaluation, and our SWAG
pipeline using only open-source models surpasses GPT-3.5-Turbo.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03484" title="Abstract">arXiv:2402.03484</a> [<a href="/pdf/2402.03484" title="Download PDF">pdf</a>, <a href="/format/2402.03484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing PubMed User Query Logs for Post Hoc Explanations of  Recommended Similar Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+A">Ashley Shin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Anibal%2C+J">James Anibal</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Searching for a related article based on a reference article is an integral
part of scientific research. PubMed, like many academic search engines, has a
"similar articles" feature that recommends articles relevant to the current
article viewed by a user. Explaining recommended items can be of great utility
to users, particularly in the literature search process. With more than a
million biomedical papers being published each year, explaining the recommended
similar articles would facilitate researchers and clinicians in searching for
related articles. Nonetheless, the majority of current literature
recommendation systems lack explanations for their suggestions. We employ a
post hoc approach to explaining recommendations by identifying relevant tokens
in the titles of similar articles. Our major contribution is building PubCLogs
by repurposing 5.6 million pairs of coclicked articles from PubMed's user query
logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title
(HSAT), a transformer-based model designed to select the most relevant parts of
the title of a similar article, based on the title and abstract of a seed
article. HSAT demonstrates strong performance in our empirical evaluations,
achieving an F1 score of 91.72 percent on the PubCLogs test set, considerably
outperforming several baselines including BM25 (70.62), MPNet (67.11), MedCPT
(62.22), GPT-3.5 (46.00), and GPT-4 (64.89). Additional evaluations on a
separate, manually annotated test set further verifies HSAT's performance.
Moreover, participants of our user study indicate a preference for HSAT, due to
its superior balance between conciseness and comprehensiveness. Our study
suggests that repurposing user query logs of academic search engines can be a
promising way to train state-of-the-art models for explaining literature
recommendation.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03486" title="Abstract">arXiv:2402.03486</a> [<a href="/pdf/2402.03486" title="Download PDF">pdf</a>, <a href="/format/2402.03486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early prediction of onset of sepsis in Clinical Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+F">Fahim Mohammad</a>, 
<a href="/search/cs?searchtype=author&query=Arunachalam%2C+L">Lakshmi Arunachalam</a>, 
<a href="/search/cs?searchtype=author&query=Sadhu%2C+S">Samanway Sadhu</a>, 
<a href="/search/cs?searchtype=author&query=Aasman%2C+B">Boudewijn Aasman</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shweta Garg</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Adil Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Colman%2C+S">Silvie Colman</a>, 
<a href="/search/cs?searchtype=author&query=Arunachalam%2C+M">Meena Arunachalam</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sudhir Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Mirhaji%2C+P">Parsa Mirhaji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures and 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">This study proposes the use of Machine Learning models to predict the early
onset of sepsis using deidentified clinical data from Montefiore Medical Center
in Bronx, NY, USA. A supervised learning approach was adopted, wherein an
XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107
features (including the original and derived features). Subsequently, the model
was evaluated on the remaining 20\% of the test data. The model was validated
on prospective data that was entirely unseen during the training phase. To
assess the model's performance at the individual patient level and timeliness
of the prediction, a normalized utility score was employed, a widely recognized
scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis
Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag
Rate were also devised. The model achieved a normalized utility score of 0.494
on test data and 0.378 on prospective data at threshold 0.3. The F1 scores were
80.8\% and 67.1\% respectively for the test data and the prospective data for
the same threshold, highlighting its potential to be integrated into clinical
decision-making processes effectively. These results bear testament to the
model's robust predictive capabilities and its potential to substantially
impact clinical decision-making processes.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03487" title="Abstract">arXiv:2402.03487</a> [<a href="/pdf/2402.03487" title="Download PDF">pdf</a>, <a href="/format/2402.03487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shooting Methods for Fractional Dirichlet-Type Boundary Value Problems  of Order $&#x3b1;\in (1,2)$ With Caputo Derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Diethelm%2C+K">Kai Diethelm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For the numerical solution of Dirichlet-type boundary value problems
associated to nonlinear fractional differential equations of order $\alpha \in
(1,2)$ that use Caputo derivatives, we suggest to employ shooting methods. In
particular, we demonstrate that the so-called proportional secting technique
for selecting the required initial values leads to numerical schemes that
converge to high accuracy in a very small number of shooting iterations, and we
provide an explanation of the analytical background for this favourable
numerical behaviour.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03488" title="Abstract">arXiv:2402.03488</a> [<a href="/pdf/2402.03488" title="Download PDF">pdf</a>, <a href="/format/2402.03488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redex -&gt; Coq: towards a theory of decidability of Redex&#x27;s reduction  semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soldevila%2C+M">Mallku Soldevila</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+R">Rodrigo Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Ziliani%2C+B">Beta Ziliani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">We propose the first steps in the development of a tool to automate the
translation of Redex models into a (hopefully) semantically equivalent model in
Coq, and to provide tactics to help in the certification of fundamental
properties of such models. The work is heavily based on a model of Redex's
semantics developed by Klein et al. By means of a simple generalization of the
matching problem in Redex, we obtain an algorithm suitable for its
mechanization in Coq, for which we prove its soundness properties and its
correspondence with the original solution proposed by Klein et al. In the
process, we also adequate some parts of our mechanization to better prepare it
for the future inclusion of Redex features absent in the present model, like
its Kleene-star operator. Finally, we discuss future avenues of development
that are enabled by this work.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03494" title="Abstract">arXiv:2402.03494</a> [<a href="/pdf/2402.03494" title="Download PDF">pdf</a>, <a href="/format/2402.03494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Text: Improving LLM&#x27;s Decision Making for Robot Navigation via  Vocal Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xingpeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Haoming Meng</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Bera%2C+A">Aniket Bera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This work highlights a critical shortcoming in text-based Large Language
Models (LLMs) used for human-robot interaction, demonstrating that text alone
as a conversation modality falls short in such applications. While LLMs excel
in processing text in these human conversations, they struggle with the nuances
of verbal instructions in scenarios like social navigation, where ambiguity and
uncertainty can erode trust in robotic and other AI systems. We can address
this shortcoming by moving beyond text and additionally focusing on the
paralinguistic features of these audio responses. These features are the
aspects of spoken communication that do not involve the literal wording
(lexical content) but convey meaning and nuance through how something is said.
We present "Beyond Text"; an approach that improves LLM decision-making by
integrating audio transcription along with a subsection of these features,
which focus on the affect and more relevant in human-robot conversations. This
approach not only achieves a 70.26% winning rate, outperforming existing LLMs
by 48.30%, but also enhances robustness against token manipulation adversarial
attacks, highlighted by a 22.44% less decrease ratio than the text-only
language model in winning rate. "Beyond Text" marks an advancement in social
robot navigation and broader Human-Robot interactions, seamlessly integrating
text-based guidance with human-audio-informed language models.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03495" title="Abstract">arXiv:2402.03495</a> [<a href="/pdf/2402.03495" title="Download PDF">pdf</a>, <a href="/format/2402.03495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Stochastic Infinitely Deep Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calvo-Ordonez%2C+S">Sergio Calvo-Ordonez</a>, 
<a href="/search/cs?searchtype=author&query=Meunier%2C+M">Matthieu Meunier</a>, 
<a href="/search/cs?searchtype=author&query=Piatti%2C+F">Francesco Piatti</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuantao Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages including supplementary material, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
<p class="mathjax">In this paper, we present Partially Stochastic Infinitely Deep Bayesian
Neural Networks, a novel family of architectures that integrates partial
stochasticity into the framework of infinitely deep neural networks. Our new
class of architectures is designed to improve the limitations of existing
architectures around computational efficiency at training and inference time.
To do this, we leverage the advantages of partial stochasticity in the
infinite-depth limit which include the benefits of full stochasticity e.g.
robustness, uncertainty quantification, and memory efficiency, whilst improving
their limitations around computational efficiency at training and inference
time. We present a variety of architectural configurations, offering
flexibility in network design including different methods for weight partition.
We also provide mathematical guarantees on the expressivity of our models by
establishing that our network family qualifies as Universal Conditional
Distribution Approximators. Lastly, empirical evaluations across multiple tasks
show that our proposed architectures achieve better downstream task performance
and uncertainty quantification than their counterparts while being
significantly more efficient.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03496" title="Abstract">arXiv:2402.03496</a> [<a href="/pdf/2402.03496" title="Download PDF">pdf</a>, <a href="/format/2402.03496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Remove the Square-Root in Adaptive Gradient Methods? A  Second-Order Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dangel%2C+F">Felix Dangel</a>, 
<a href="/search/cs?searchtype=author&query=Eschenhagen%2C+R">Runa Eschenhagen</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+J">Juhan Bae</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Makhzani%2C+A">Alireza Makhzani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Adaptive gradient optimizers like Adam(W) are the default training algorithms
for many deep learning architectures, such as transformers. Their diagonal
preconditioner is based on the gradient outer product which is incorporated
into the parameter update via a square root. While these methods are often
motivated as approximate second-order methods, the square root represents a
fundamental difference. In this work, we investigate how the behavior of
adaptive methods changes when we remove the root, i.e. strengthen their
second-order motivation. Surprisingly, we find that such square-root-free
adaptive methods close the generalization gap to SGD on convolutional
architectures, while maintaining their root-based counterpart's performance on
transformers. The second-order perspective also has practical benefits for the
development of adaptive methods with non-diagonal preconditioner. In contrast
to root-based counterparts like Shampoo, they do not require numerically
unstable matrix square roots and therefore work well in low precision, which we
demonstrate empirically. This raises important questions regarding the
currently overlooked role of adaptivity for the success of adaptive methods.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03501" title="Abstract">arXiv:2402.03501</a> [<a href="/pdf/2402.03501" title="Download PDF">pdf</a>, <a href="/format/2402.03501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Inpainting-Infused Pipeline for Attire and Background Replacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perche-Mahlow%2C+F+R">Felipe Rodrigues Perche-Mahlow</a>, 
<a href="/search/cs?searchtype=author&query=Felipe-Zanella%2C+A">Andr&#xe9; Felipe-Zanella</a>, 
<a href="/search/cs?searchtype=author&query=Cruz-Casta%C3%B1eda%2C+W+A">William Alberto Cruz-Casta&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Amadeus%2C+M">Marcellus Amadeus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In recent years, groundbreaking advancements in Generative Artificial
Intelligence (GenAI) have triggered a transformative paradigm shift,
significantly influencing various domains. In this work, we specifically
explore an integrated approach, leveraging advanced techniques in GenAI and
computer vision emphasizing image manipulation. The methodology unfolds through
several stages, including depth estimation, the creation of inpaint masks based
on depth information, the generation and replacement of backgrounds utilizing
Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the
subsequent replacement of clothes and application of aesthetic changes through
an inpainting pipeline. Experiments conducted in this study underscore the
methodology's efficacy, highlighting its potential to produce visually
captivating content. The convergence of these advanced techniques allows users
to input photographs of individuals and manipulate them to modify clothing and
background based on specific prompts without manually input inpainting masks,
effectively placing the subjects within the vast landscape of creative
imagination.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03502" title="Abstract">arXiv:2402.03502</a> [<a href="/pdf/2402.03502" title="Download PDF">pdf</a>, <a href="/format/2402.03502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Does Unlabeled Data Provably Help Out-of-Distribution Detection?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xuefeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+I">Ilias Diakonikolas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixuan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Using unlabeled data to regularize the machine learning models has
demonstrated promise for improving safety and reliability in detecting
out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild
data is non-trivial due to the heterogeneity of both in-distribution (ID) and
OOD data. This lack of a clean set of OOD samples poses significant challenges
in learning an optimal OOD classifier. Currently, there is a lack of research
on formally understanding how unlabeled data helps OOD detection. This paper
bridges the gap by introducing a new learning framework SAL (Separate And
Learn) that offers both strong theoretical guarantees and empirical
effectiveness. The framework separates candidate outliers from the unlabeled
data and then trains an OOD classifier using the candidate outliers and the
labeled ID data. Theoretically, we provide rigorous error bounds from the lens
of separability and learnability, formally justifying the two components in our
algorithm. Our theory shows that SAL can separate the candidate outliers with
small error rates, which leads to a generalization guarantee for the learned
OOD classifier. Empirically, SAL achieves state-of-the-art performance on
common benchmarks, reinforcing our theoretical insights. Code is publicly
available at https://github.com/deeplearning-wisc/sal.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03507" title="Abstract">arXiv:2402.03507</a> [<a href="/pdf/2402.03507" title="Download PDF">pdf</a>, <a href="/format/2402.03507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural networks for abstraction and reasoning: Towards broad  generalization in machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bober-Irizar%2C+M">Mikel Bober-Irizar</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Soumya Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages main text, 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">For half a century, artificial intelligence research has attempted to
reproduce the human qualities of abstraction and reasoning - creating computer
systems that can learn new concepts from a minimal set of examples, in settings
where humans find this easy. While specific neural networks are able to solve
an impressive range of problems, broad generalisation to situations outside
their training data has proved elusive.In this work, we look at several novel
approaches for solving the Abstraction &amp; Reasoning Corpus (ARC), a dataset of
abstract visual reasoning tasks introduced to test algorithms on broad
generalization. Despite three international competitions with $100,000 in
prizes, the best algorithms still fail to solve a majority of ARC tasks and
rely on complex hand-crafted rules, without using machine learning at all. We
revisit whether recent advances in neural networks allow progress on this task.
<br />First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.
DreamCoder automatically writes programs in a bespoke domain-specific language
to perform reasoning, using a neural network to mimic human intuition. We
present the Perceptual Abstraction and Reasoning Language (PeARL) language,
which allows DreamCoder to solve ARC tasks, and propose a new recognition model
that allows us to significantly improve on the previous best implementation.We
also propose a new encoding and augmentation scheme that allows large language
models (LLMs) to solve ARC tasks, and find that the largest models can solve
some ARC tasks. LLMs are able to solve a different group of problems to
state-of-the-art solvers, and provide an interesting way to complement other
approaches. We perform an ensemble analysis, combining models to achieve better
results than any system alone. Finally, we publish the arckit Python library to
make future research on ARC easier.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03509" title="Abstract">arXiv:2402.03509</a> [<a href="/pdf/2402.03509" title="Download PDF">pdf</a>, <a href="/format/2402.03509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramprasad%2C+S">Sanjana Ramprasad</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K">Kundan Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C Wallace</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown that large language models (LLMs) are capable of
generating summaries zero-shot (i.e., without explicit supervision) that, under
human assessment, are often comparable or even preferred to manually composed
reference summaries. However, this prior work has focussed almost exclusively
on evaluating news article summarization. How do zero-shot summarizers perform
in other (potentially more specialized) domains? In this work we evaluate
zero-shot generated summaries across specialized domains including biomedical
articles, and legal bills (in addition to standard news benchmarks for
reference). We focus especially on the factuality of outputs. We acquire
annotations from domain experts to identify inconsistencies in summaries and
systematically categorize these errors. We analyze whether the prevalence of a
given domain in the pretraining corpus affects extractiveness and faithfulness
of generated summaries of articles in this domain. We release all collected
annotations to facilitate additional research toward measuring and realizing
factually accurate summarization, beyond news articles. The dataset can be
downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03510" title="Abstract">arXiv:2402.03510</a> [<a href="/pdf/2402.03510" title="Download PDF">pdf</a>, <a href="/format/2402.03510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autopilot System for Depth and Pitch Control in Underwater Vehicles:  Navigating Near-Surface Waves and Disturbances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Petrov%2C+V">Vladimir Petrov</a>, 
<a href="/search/eess?searchtype=author&query=MacLin%2C+G">Gage MacLin</a>, 
<a href="/search/eess?searchtype=author&query=Cichella%2C+V">Venanzio Cichella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Submitted to IFAC conference (for September 15, 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a framework for depth and pitch control of underwater
vehicles in near-surface wave conditions. By effectively managing tail, sail
plane angles and hover tank operations utilizing a Linear Quadratic Regulator
controller and L1 Adaptive Autopilot augmentation, the system ensures balanced
control input distribution and significantly attenuates wave disturbances. This
development in underwater vehicle control systems offers potential for improved
functionality across a range of marine applications. The proposed framework is
demonstrated to be robust in a variety of wave conditions, enabling more
precise navigation and improved safety in operational scenarios. The
effectiveness of this control strategy is validated through extensive
simulations using the Joubert BB2 model.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03513" title="Abstract">arXiv:2402.03513</a> [<a href="/pdf/2402.03513" title="Download PDF">pdf</a>, <a href="/format/2402.03513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Super-Resolution for Optimized Bitrate and Green Online Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menon%2C+V+V">Vignesh V Menon</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+P+T">Prajit T Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Premkumar%2C+A">Amritha Premkumar</a>, 
<a href="/search/cs?searchtype=author&query=Bross%2C+B">Benjamin Bross</a>, 
<a href="/search/cs?searchtype=author&query=Marpe%2C+D">Detlev Marpe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 Picture Coding Symposium (PCS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Conventional per-title encoding schemes strive to optimize encoding
resolutions to deliver the utmost perceptual quality for each bitrate ladder
representation. Nevertheless, maintaining encoding time within an acceptable
threshold is equally imperative in online streaming applications. Furthermore,
modern client devices are equipped with the capability for fast
deep-learning-based video super-resolution (VSR) techniques, enhancing the
perceptual quality of the decoded bitstream. This suggests that opting for
lower resolutions in representations during the encoding process can curtail
the overall energy consumption without substantially compromising perceptual
quality. In this context, this paper introduces a video super-resolution-based
latency-aware optimized bitrate encoding scheme (ViSOR) designed for online
adaptive streaming applications. ViSOR determines the encoding resolution for
each target bitrate, ensuring the highest achievable perceptual quality after
VSR within the bound of a maximum acceptable latency. Random forest-based
prediction models are trained to predict the perceptual quality after VSR and
the encoding time for each resolution using the spatiotemporal features
extracted for each video segment. Experimental results show that ViSOR
targeting fast super-resolution convolutional neural network (FSRCNN) achieves
an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the
same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder
encoding of 4 s segments using the x265 encoder, when the maximum acceptable
latency for each representation is set as two seconds. Considering a just
noticeable difference (JND) of six VMAF points, the average cumulative storage
consumption and encoding energy for each segment is reduced by 79.32 % and
68.21 %, respectively, contributing towards greener streaming.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03517" title="Abstract">arXiv:2402.03517</a> [<a href="/pdf/2402.03517" title="Download PDF">pdf</a>, <a href="/format/2402.03517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially Consistent Air-to-Ground Channel Modeling via Generative  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giuliani%2C+A">Amedeo Giuliani</a>, 
<a href="/search/cs?searchtype=author&query=Nikbakht%2C+R">Rasoul Nikbakht</a>, 
<a href="/search/cs?searchtype=author&query=Geraci%2C+G">Giovanni Geraci</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Seongjoon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Angel Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Wireless Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">This article proposes a generative neural network architecture for spatially
consistent air-to-ground channel modeling. The approach considers the
trajectories of uncrewed aerial vehicles along typical urban paths, capturing
spatial dependencies within received signal strength (RSS) sequences from
multiple cellular base stations (gNBs). Through the incorporation of
conditioning data, the model accurately discriminates between gNBs and drives
the correlation matrix distance between real and generated sequences to minimal
values. This enables evaluating performance and mobility management metrics
with spatially (and by extension temporally) consistent RSS values, rather than
independent snapshots. For some tasks underpinned by these metrics, say
handovers, consistency is essential.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03519" title="Abstract">arXiv:2402.03519</a> [<a href="/pdf/2402.03519" title="Download PDF">pdf</a>, <a href="/format/2402.03519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical  System for Punctuation Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiliang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chia-Tien Chang</a>, 
<a href="/search/cs?searchtype=author&query=Gardiner%2C+S">Shayna Gardiner</a>, 
<a href="/search/cs?searchtype=author&query=Rossouw%2C+D">David Rossouw</a>, 
<a href="/search/cs?searchtype=author&query=Robertson%2C+J">Jonas Robertson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to UnImplicit workshop at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Punctuation restoration is a crucial step after Automatic Speech Recognition
(ASR) systems to enhance transcript readability and facilitate subsequent NLP
tasks. Nevertheless, conventional lexical-based approaches are inadequate for
solving the punctuation restoration task in Spanish, where ambiguity can be
often found between unpunctuated declaratives and questions. In this study, we
propose a novel hybrid acoustic-lexical punctuation restoration system for
Spanish transcription, which consolidates acoustic and lexical signals through
a modular process. Our experiment results show that the proposed system can
effectively improve F1 score of question marks and overall punctuation
restoration on both public and internal Spanish conversational datasets.
Additionally, benchmark comparison against LLMs (Large Language Model)
indicates the superiority of our approach in accuracy, reliability and latency.
Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module
also benefits from our proposed system.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03522" title="Abstract">arXiv:2402.03522</a> [<a href="/pdf/2402.03522" title="Download PDF">pdf</a>, <a href="/format/2402.03522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influencer Identification on Link Predicted Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaposnik%2C+L+P">Laura P. Schaposnik</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Raina Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages + appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">How could one identify a potential influencer, or how would admissions look
like in a University program for influencers? In the realm of social network
analysis, influence maximization and link prediction stand out as pivotal
challenges. Influence maximization focuses on identifying a set of key nodes to
maximize information dissemination, while link prediction aims to foresee
potential connections within the network. Given the complexity of these tasks,
especially in large-scale networks, we propose a novel algorithm, The Social
Sphere Model, tailored for weighted networks. This algorithm uniquely combines
link prediction techniques with influence maximization strategies to
effectively identify future vital nodes. Our approach leverages two distinct
contagion models, offering a promising solution with lower computational
demands. This advancement not only enhances our understanding of network
dynamics but also opens new avenues for efficient network management and
influence strategy development.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03525" title="Abstract">arXiv:2402.03525</a> [<a href="/pdf/2402.03525" title="Download PDF">pdf</a>, <a href="/ps/2402.03525" title="Download PostScript">ps</a>, <a href="/format/2402.03525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Picker Routing Problem in Warehousing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dunn%2C+G">George Dunn</a>, 
<a href="/search/cs?searchtype=author&query=Charkhgard%2C+H">Hadi Charkhgard</a>, 
<a href="/search/cs?searchtype=author&query=Eshragh%2C+A">Ali Eshragh</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoudinazlou%2C+S">Sasan Mahmoudinazlou</a>, 
<a href="/search/cs?searchtype=author&query=Stojanovski%2C+E">Elizabeth Stojanovski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Order Picker Routing is a critical issue in Warehouse Operations Management.
Due to the complexity of the problem and the need for quick solutions,
suboptimal algorithms are frequently employed in practice. However,
Reinforcement Learning offers an appealing alternative to traditional
heuristics, potentially outperforming existing methods in terms of speed and
accuracy. We introduce an attention based neural network for modeling picker
tours, which is trained using Reinforcement Learning. Our method is evaluated
against existing heuristics across a range of problem parameters to demonstrate
its efficacy. A key advantage of our proposed method is its ability to offer an
option to reduce the perceived complexity of routes.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03526" title="Abstract">arXiv:2402.03526</a> [<a href="/pdf/2402.03526" title="Download PDF">pdf</a>, <a href="/format/2402.03526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark  Detection with State Space Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Haifan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+L">Luoyao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yitao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Code is available at <a href="https://github.com/lhaof/nnMamba">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the field of biomedical image analysis, the quest for architectures
capable of effectively capturing long-range dependencies is paramount,
especially when dealing with 3D image segmentation, classification, and
landmark detection. Traditional Convolutional Neural Networks (CNNs) struggle
with locality respective field, and Transformers have a heavy computational
load when applied to high-dimensional medical images. In this paper, we
introduce nnMamba, a novel architecture that integrates the strengths of CNNs
and the advanced long-range modeling capabilities of State Space Sequence
Models (SSMs). nnMamba adds the SSMs to the convolutional residual-block to
extract local features and model complex dependencies. For diffirent tasks, we
build different blocks to learn the features. Extensive experiments demonstrate
nnMamba's superiority over state-of-the-art methods in a suite of challenging
tasks, including 3D image segmentation, classification, and landmark detection.
nnMamba emerges as a robust solution, offering both the local representation
ability of CNNs and the efficient global context processing of SSMs, setting a
new standard for long-range dependency modeling in medical image analysis. Code
is available at https://github.com/lhaof/nnMamba
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03530" title="Abstract">arXiv:2402.03530</a> [<a href="/pdf/2402.03530" title="Download PDF">pdf</a>, <a href="/format/2402.03530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Aaron Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y+S">Yun Seo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Dow%2C+S+P">Steven P. Dow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, accepted at the 29th ACM Conference on Intelligent User Interfaces (IUI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Peer review is a cornerstone of science. Research communities conduct peer
reviews to assess contributions and to improve the overall quality of science
work. Every year, new community members are recruited as peer reviewers for the
first time. How could technology help novices adhere to their community's
practices and standards for peer reviewing? To better understand peer review
practices and challenges, we conducted a formative study with 10 novices and 10
experts. We found that many experts adopt a workflow of annotating,
note-taking, and synthesizing notes into well-justified reviews that align with
community standards. Novices lack timely guidance on how to read and assess
submissions and how to structure paper reviews. To support the peer review
process, we developed ReviewFlow -- an AI-driven workflow that scaffolds
novices with contextual reflections to critique and annotate submissions,
in-situ knowledge support to assess novelty, and notes-to-outline synthesis to
help align peer reviews with community expectations. In a within-subjects
experiment, 16 inexperienced reviewers wrote reviews using ReviewFlow and a
baseline environment with minimal guidance. Participants produced more
comprehensive reviews using ReviewFlow than the baseline, calling out more pros
and cons, but they still struggled to provide actionable suggestions to address
the weaknesses. While participants appreciated the streamlined process support
from ReviewFlow, they also expressed concerns about using AI as part of the
scientific review process. We discuss the implications of using AI to scaffold
peer review process on scientific work and beyond.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03531" title="Abstract">arXiv:2402.03531</a> [<a href="/pdf/2402.03531" title="Download PDF">pdf</a>, <a href="/ps/2402.03531" title="Download PostScript">ps</a>, <a href="/format/2402.03531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness and Privacy Guarantees in Federated Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Solanki%2C+S">Sambhav Solanki</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shweta Jain</a>, 
<a href="/search/cs?searchtype=author&query=Gujar%2C+S">Sujit Gujar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper considers the contextual multi-armed bandit (CMAB) problem with
fairness and privacy guarantees in a federated environment. We consider
merit-based exposure as the desired fair outcome, which provides exposure to
each action in proportion to the reward associated. We model the algorithm's
effectiveness using fairness regret, which captures the difference between fair
optimal policy and the policy output by the algorithm. Applying fair CMAB
algorithm to each agent individually leads to fairness regret linear in the
number of agents. We propose that collaborative -- federated learning can be
more effective and provide the algorithm Fed-FairX-LinUCB that also ensures
differential privacy. The primary challenge in extending the existing privacy
framework is designing the communication protocol for communicating required
information across agents. A naive protocol can either lead to weaker privacy
guarantees or higher regret. We design a novel communication protocol that
allows for (i) Sub-linear theoretical bounds on fairness regret for
Fed-FairX-LinUCB and comparable bounds for the private counterpart,
Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of
privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our
proposed algorithm with extensive simulations-based experiments. We show that
both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness
regret.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03533" title="Abstract">arXiv:2402.03533</a> [<a href="/pdf/2402.03533" title="Download PDF">pdf</a>, <a href="/format/2402.03533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 0.5V, 6.2$&#x3bc;$W, 0.059mm$^{2}$ Sinusoidal Current Generator IC with  0.088% THD for Bio-Impedance Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kwantae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changhyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sungpill Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+H">Hoi-Jun Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 8 figures, 1 table, 2020 IEEE Symposium on VLSI Circuits
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">This paper presents the first sub-10$\mu$W, sub-0.1% total harmonic
distortion (THD) sinusoidal current generator (CG) integrated circuit (IC) that
is capable of 20kHz output for the bio-impedance (Bio-Z) sensing applications.
To benefit from the ultra-low-power nature of near-threshold operation, a 9b
pseudo-sine lookup table (LUT) is 3b $\Delta\Sigma$ modulated in the digital
domain, thus linearity burden of the digital-to-analog converter (DAC) is
avoided and only a 1.29$\mu$W of logic power is consumed, from a 0.5V supply
and a 2.56MHz clock frequency. A half-period (HP) reset is introduced in the
capacitive DAC, leading to around 30dB reduction of in-band noise by avoiding
the sampling of data-dependent glitches and attenuating the kT/C noise and the
non-idealities of reset switches (SW).
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03534" title="Abstract">arXiv:2402.03534</a> [<a href="/pdf/2402.03534" title="Download PDF">pdf</a>, <a href="/ps/2402.03534" title="Download PostScript">ps</a>, <a href="/format/2402.03534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANN-based position and speed sensorless estimation for BLDC motors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gamazo-Real%2C+J">Jose-Carlos Gamazo-Real</a>, 
<a href="/search/eess?searchtype=author&query=Martinez-Martinez%2C+V">Victor Martinez-Martinez</a>, 
<a href="/search/eess?searchtype=author&query=Gomez-Gil%2C+J">Jaime Gomez-Gil</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Measurement, vol. 188, no. 110602, pp. 1-15, 2022, ISSN 0263-2241
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
<p class="mathjax">BLDC motor applications require precise position and speed measurements,
traditionally obtained with sensors. This article presents a method for
estimating those measurements without position sensors using terminal phase
voltages with attenuated spurious, acquired with a FPGA that also operates a
PWM-controlled inverter. Voltages are labelled with electrical and virtual
rotor states using an encoder that provides training and testing data for two
three-layer ANNs with perceptron-based cascade topology. The first ANN
estimates the position from features of voltages with incremental timestamps,
and the second ANN estimates the speed from features of position differentials
considering timestamps in an acquisition window. Sensor-based training and
sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained
absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the
overall position estimation significantly improved conventional and advanced
methods, and the speed estimation slightly improved conventional methods, but
was worse than in advanced ones.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03539" title="Abstract">arXiv:2402.03539</a> [<a href="/pdf/2402.03539" title="Download PDF">pdf</a>, <a href="/format/2402.03539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended Version of: On the Structural Hardness of Answer Set  Programming: Can Structure Efficiently Confine the Power of Disjunctions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hecher%2C+M">Markus Hecher</a>, 
<a href="/search/cs?searchtype=author&query=Kiesel%2C+R">Rafael Kiesel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Answer Set Programming (ASP) is a generic problem modeling and solving
framework with a strong focus on knowledge representation and a rapid growth of
industrial applications. So far, the study of complexity resulted in
characterizing hardness and determining their sources, fine-grained insights in
the form of dichotomy-style results, as well as detailed parameterized
complexity landscapes. Unfortunately, for the well-known parameter treewidth
disjunctive programs require double-exponential runtime under reasonable
complexity assumptions. This quickly becomes out of reach. We deal with the
classification of structural parameters for disjunctive ASP on the program's
rule structure (incidence graph).
<br />First, we provide a polynomial kernel to obtain single-exponential runtime in
terms of vertex cover size, despite subset-minimization being not represented
in the program's structure. Then we turn our attention to strictly better
structural parameters between vertex cover size and treewidth. Here, we provide
double-exponential lower bounds for the most prominent parameters in that
range: treedepth, feedback vertex size, and cliquewidth. Based on this, we
argue that unfortunately our options beyond vertex cover size are limited. Our
results provide an in-depth hardness study, relying on a novel reduction from
normal to disjunctive programs, trading the increase of complexity for an
exponential parameter compression.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03540" title="Abstract">arXiv:2402.03540</a> [<a href="/pdf/2402.03540" title="Download PDF">pdf</a>, <a href="/format/2402.03540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulation Games for Trustworthy Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaghini%2C+M">Mohammad Yaghini</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Patty Liu</a>, 
<a href="/search/cs?searchtype=author&query=Boenisch%2C+F">Franziska Boenisch</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Existing work on trustworthy machine learning (ML) often concentrates on
individual aspects of trust, such as fairness or privacy. Additionally, many
techniques overlook the distinction between those who train ML models and those
responsible for assessing their trustworthiness. To address these issues, we
propose a framework that views trustworthy ML as a multi-objective multi-agent
optimization problem. This naturally lends itself to a game-theoretic
formulation we call regulation games. We illustrate a particular game instance,
the SpecGame in which we model the relationship between an ML model builder and
fairness and privacy regulators. Regulators wish to design penalties that
enforce compliance with their specification, but do not want to discourage
builders from participation. Seeking such socially optimal (i.e., efficient for
all agents) solutions to the game, we introduce ParetoPlay. This novel
equilibrium search algorithm ensures that agents remain on the Pareto frontier
of their objectives and avoids the inefficiencies of other equilibria.
Simulating SpecGame through ParetoPlay can provide policy guidance for ML
Regulation. For instance, we show that for a gender classification application,
regulators can enforce a differential privacy budget that is on average 4.0
lower if they take the initiative to specify their desired guarantee first.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03541" title="Abstract">arXiv:2402.03541</a> [<a href="/pdf/2402.03541" title="Download PDF">pdf</a>, <a href="/format/2402.03541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAMLET: Graph Transformer Neural Operator for Partial Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bryutkin%2C+A">Andrey Bryutkin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiahao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhongying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A">Angelica Aviles-Rivero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present a novel graph transformer framework, HAMLET, designed to address
the challenges in solving partial differential equations (PDEs) using neural
networks. The framework uses graph transformers with modular input encoders to
directly incorporate differential equation information into the solution
process. This modularity enhances parameter correspondence control, making
HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.
Notably, HAMLET scales effectively with increasing data complexity and noise,
showcasing its robustness. HAMLET is not just tailored to a single type of
physical simulation, but can be applied across various domains. Moreover, it
boosts model resilience and performance, especially in scenarios with limited
data. We demonstrate, through extensive experiments, that our framework is
capable of outperforming current techniques for PDEs.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03543" title="Abstract">arXiv:2402.03543</a> [<a href="/pdf/2402.03543" title="Download PDF">pdf</a>, <a href="/ps/2402.03543" title="Download PostScript">ps</a>, <a href="/format/2402.03543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Lawvere Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacci%2C+G">Giorgio Bacci</a>, 
<a href="/search/cs?searchtype=author&query=Mardare%2C+R">Radu Mardare</a>, 
<a href="/search/cs?searchtype=author&query=Panangaden%2C+P">Prakash Panangaden</a>, 
<a href="/search/cs?searchtype=author&query=Plotkin%2C+G">Gordon Plotkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In this paper, we study Polynomial Lawvere logic (PL), a logic on the
quantale of the extended positive reals, developed for reasoning about metric
spaces. PL is appropriate for encoding quantitative reasoning principles, such
as quantitative equational logic. PL formulas include the polynomial functions
on the extended positive reals, and its judgements include inequalities between
polynomials.
<br />We present an inference system for PL and prove a series of completeness and
incompleteness results relying and the Krivine-Stengle Positivstellensatz (a
variant of Hilbert's Nullstellensatz) including completeness for finitely
axiomatisable PL theories.
<br />We also study complexity results both for both PL and its affine fragment
(AL). We demonstrate that the satisfiability of a finite set of judgements is
NP-complete in AL and in PSPACE for PL; and that deciding the semantical
consequence from a finite set of judgements is co-NP complete in AL and in
PSPACE in PL.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03545" title="Abstract">arXiv:2402.03545</a> [<a href="/pdf/2402.03545" title="Download PDF">pdf</a>, <a href="/format/2402.03545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Feature Updates Improve Online (Generalized) Label Shift  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Siddhartha Datta</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yi Su</a>, 
<a href="/search/cs?searchtype=author&query=Baby%2C+D">Dheeraj Baby</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper addresses the prevalent issue of label shift in an online setting
with missing labels, where data distributions change over time and obtaining
timely labels is challenging. While existing methods primarily focus on
adjusting or updating the final layer of a pre-trained classifier, we explore
the untapped potential of enhancing feature representations using unlabeled
data at test-time. Our novel method, Online Label Shift adaptation with Online
Feature Updates (OLS-OFU), leverages self-supervised learning to refine the
feature extraction process, thereby improving the prediction model. Theoretical
analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on
self-supervised learning for feature refinement. Empirical studies on various
datasets, under both online label shift and generalized label shift conditions,
underscore the effectiveness and robustness of OLS-OFU, especially in cases of
domain shifts.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03548" title="Abstract">arXiv:2402.03548</a> [<a href="/pdf/2402.03548" title="Download PDF">pdf</a>, <a href="/format/2402.03548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-GPU GNN Systems: Traps and Pitfalls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yidong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Tarafder%2C+A">Arnab Tarafder</a>, 
<a href="/search/cs?searchtype=author&query=Afrin%2C+S">Saima Afrin</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Pradeep Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The current graph neural network (GNN) systems have established a clear trend
of not showing training accuracy results, and directly or indirectly relying on
smaller datasets for evaluations majorly. Our in-depth analysis shows that it
leads to a chain of pitfalls in the system design and evaluation process,
questioning the practicality of many of the proposed system optimizations, and
affecting conclusions and lessons learned. We analyze many single-GPU systems
and show the fundamental impact of these pitfalls. We further develop
hypotheses, recommendations, and evaluation methodologies, and provide future
directions. Finally, a new reference system is developed to establish a new
line of optimizations rooted in solving the system-design pitfalls efficiently
and practically. The proposed design can productively be integrated into prior
works, thereby truly advancing the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03549" title="Abstract">arXiv:2402.03549</a> [<a href="/pdf/2402.03549" title="Download PDF">pdf</a>, <a href="/ps/2402.03549" title="Download PostScript">ps</a>, <a href="/format/2402.03549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnaMoDiff: 2D Analogical Motion Diffusion via Disentangled Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+M">Maham Tanveer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+N">Nanxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi-Amiri%2C+A">Ali Mahdavi-Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present AnaMoDiff, a novel diffusion-based method for 2D motion analogies
that is applied to raw, unannotated videos of articulated characters. Our goal
is to accurately transfer motions from a 2D driving video onto a source
character, with its identity, in terms of appearance and natural movement, well
preserved, even when there may be significant discrepancies between the source
and driving characters in their part proportions and movement speed and styles.
Our diffusion model transfers the input motion via a latent optical flow (LOF)
network operating in a noised latent space, which is spatially aware, efficient
to process compared to the original RGB videos, and artifact-resistant through
the diffusion denoising process even amid dense movements. To accomplish both
motion analogy and identity preservation, we train our denoising model in a
feature-disentangled manner, operating at two noise levels. While
identity-revealing features of the source are learned via conventional noise
injection, motion features are learned from LOF-warped videos by only injecting
noise with large values, with the stipulation that motion properties involving
pose and limbs are encoded by higher-level features. Experiments demonstrate
that our method achieves the best trade-off between motion analogy and identity
preservation.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03550" title="Abstract">arXiv:2402.03550</a> [<a href="/pdf/2402.03550" title="Download PDF">pdf</a>, <a href="/format/2402.03550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Green Mirage: Impact of Location- and Market-based Carbon Intensity  Estimation on Carbon Optimization Efficacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maji%2C+D">Diptyaroop Maji</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+D">David Irwin</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>, 
<a href="/search/cs?searchtype=author&query=Sitaraman%2C+R+K">Ramesh K. Sitaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In recent years, there has been an increased emphasis on reducing the carbon
emissions from electricity consumption. Many organizations have set ambitious
targets to reduce the carbon footprint of their operations as a part of their
sustainability goals. The carbon footprint of any consumer of electricity is
computed as the product of the total energy consumption and the carbon
intensity of electricity. Third-party carbon information services provide
information on carbon intensity across regions that consumers can leverage to
modulate their energy consumption patterns to reduce their overall carbon
footprint. In addition, to accelerate their decarbonization process, large
electricity consumers increasingly acquire power purchase agreements (PPAs)
from renewable power plants to obtain renewable energy credits that offset
their "brown" energy consumption. There are primarily two methods for
attributing carbon-free energy, or renewable energy credits, to electricity
consumers: location-based and market-based. These two methods yield
significantly different carbon intensity values for various consumers. As there
is a lack of consensus which method to use for carbon-free attribution, a
concurrent application of both approaches is observed in practice. In this
paper, we show that such concurrent applications can cause discrepancies in the
carbon savings reported by carbon optimization techniques. Our analysis across
three state-of-the-art carbon optimization techniques shows possible
overestimation of up to 55.1% in the carbon reductions reported by the
consumers and even increased emissions for consumers in some cases. We also
find that carbon optimization techniques make different decisions under the
market-based method and location-based method, and the market-based method can
yield up to 28.2% less carbon savings than those claimed by the location-based
method for consumers without PPAs.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03551" title="Abstract">arXiv:2402.03551</a> [<a href="/pdf/2402.03551" title="Download PDF">pdf</a>, <a href="/format/2402.03551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A retrospective analysis of Montana&#x27;s 2020 congressional redistricting  map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McKinnie%2C+K">Kelly McKinnie</a>, 
<a href="/search/cs?searchtype=author&query=Szalda-Petree%2C+E">Erin Szalda-Petree</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The 2020 decennial census data resulted in an increase from one to two
congressional representatives in the state of Montana. The state underwent its
redistricting process in 2021 in time for the November 2022 congressional
elections, carving the state into two districts. This paper analyzes the
redistricting process and compares the adopted congressional map to the space
of all other possible maps. In particular, we look at the population deviation,
compactness and political outcomes of these maps. We also consider how well two
popular sampling techniques, that sample from the space of possible maps,
approximate the true distributions of these measures.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03553" title="Abstract">arXiv:2402.03553</a> [<a href="/pdf/2402.03553" title="Download PDF">pdf</a>, <a href="/format/2402.03553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-shot Neural Face Reenactment via Finding Directions in GAN&#x27;s Latent  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bounareli%2C+S">Stella Bounareli</a>, 
<a href="/search/cs?searchtype=author&query=Tzelepis%2C+C">Christos Tzelepis</a>, 
<a href="/search/cs?searchtype=author&query=Argyriou%2C+V">Vasileios Argyriou</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>, 
<a href="/search/cs?searchtype=author&query=Tzimiropoulos%2C+G">Georgios Tzimiropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version, accepted for publication in International Journal of Computer Vision (IJCV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present our framework for neural face/head reenactment
whose goal is to transfer the 3D head orientation and expression of a target
face to a source face. Previous methods focus on learning embedding networks
for identity and head pose/expression disentanglement which proves to be a
rather hard task, degrading the quality of the generated images. We take a
different approach, bypassing the training of such networks, by using
(fine-tuned) pre-trained GANs which have been shown capable of producing
high-quality facial images. Because GANs are characterized by weak
controllability, the core of our approach is a method to discover which
directions in latent GAN space are responsible for controlling head pose and
expression variations. We present a simple pipeline to learn such directions
with the aid of a 3D shape model which, by construction, inherently captures
disentangled directions for head pose, identity, and expression. Moreover, we
show that by embedding real images in the GAN latent space, our method can be
successfully used for the reenactment of real-world faces. Our method features
several favorable properties including using a single source image (one-shot)
and enabling cross-person reenactment. Extensive qualitative and quantitative
results show that our approach typically produces reenacted faces of notably
higher quality than those produced by state-of-the-art methods for the standard
benchmarks of VoxCeleb1 &amp; 2.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03554" title="Abstract">arXiv:2402.03554</a> [<a href="/pdf/2402.03554" title="Download PDF">pdf</a>, <a href="/ps/2402.03554" title="Download PostScript">ps</a>, <a href="/format/2402.03554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Formula for Partial Information Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+A">Aobo Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+A">Andrew Clark</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+N">Netanel Raviv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR)

</div>
<p class="mathjax">Mutual information between two random variables is a well-studied notion,
whose understanding is fairly complete. Mutual information between one random
variable and a pair of other random variables, however, is a far more involved
notion. Specifically, Shannon's mutual information does not capture
fine-grained interactions between those three variables, resulting in limited
insights in complex systems. To capture these fine-grained interactions, in
2010 Williams and Beer proposed to decompose this mutual information to
information atoms, called unique, redundant, and synergistic, and proposed
several operational axioms that these atoms must satisfy. In spite of numerous
efforts, a general formula which satisfies these axioms has yet to be found.
Inspired by Judea Pearl's do-calculus, we resolve this open problem by
introducing the do-operation, an operation over the variable system which sets
a certain marginal to a desired value, which is distinct from any existing
approaches. Using this operation, we provide the first explicit formula for
calculating the information atoms so that Williams and Beer's axioms are
satisfied, as well as additional properties from subsequent studies in the
field.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03555" title="Abstract">arXiv:2402.03555</a> [<a href="/pdf/2402.03555" title="Download PDF">pdf</a>, <a href="/ps/2402.03555" title="Download PostScript">ps</a>, <a href="/format/2402.03555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A security framework for Ethereum smart contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vivar%2C+A+L">Antonio L&#xf3;pez Vivar</a>, 
<a href="/search/cs?searchtype=author&query=Orozco%2C+A+L+S">Ana Lucila Sandoval Orozco</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Communications. Vol. 172, pp. 119-129, April 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The use of blockchain and smart contracts have not stopped growing in recent
years. Like all software that begins to expand its use, it is also beginning to
be targeted by hackers who will try to exploit vulnerabilities in both the
underlying technology and the smart contract code itself. While many tools
already exist for analyzing vulnerabilities in smart contracts, the
heterogeneity and variety of approaches and differences in providing the
analysis data makes the learning curve for the smart contract developer steep.
In this article the authors present ESAF (Ethereum Security Analysis
Framework), a framework for analysis of smart contracts that aims to unify and
facilitate the task of analyzing smart contract vulnerabilities which can be
used as a persistent security monitoring tool for a set of target contracts as
well as a classic vulnerability analysis tool among other uses.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03557" title="Abstract">arXiv:2402.03557</a> [<a href="/pdf/2402.03557" title="Download PDF">pdf</a>, <a href="/format/2402.03557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Analysis of Multi-Task Learning on a Complex Vision System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+D">Dayou Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gilles%2C+M">Maximilian Gilles</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alexander Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-task learning (MTL) has been widely studied in the past decade. In
particular, dozens of optimization algorithms have been proposed for different
settings. While each of them claimed improvement when applied to certain models
on certain datasets, there is still lack of deep understanding on the
performance in complex real-worlds scenarios. We identify the gaps between
research and application and make the following 4 contributions. (1) We
comprehensively evaluate a large set of existing MTL optimization algorithms on
the MetaGraspNet dataset designed for robotic grasping task, which is complex
and has high real-world application values, and conclude the best-performing
methods. (2) We empirically compare the method performance when applied on
feature-level gradients versus parameter-level gradients over a large set of
MTL optimization algorithms, and conclude that this feature-level gradients
surrogate is reasonable when there are method-specific theoretical guarantee
but not generalizable to all methods. (3) We provide insights on the problem of
task interference and show that the existing perspectives of gradient angles
and relative gradient norms do not precisely reflect the challenges of MTL, as
the rankings of the methods based on these two indicators do not align well
with those based on the test-set performance. (4) We provide a novel view of
the task interference problem from the perspective of the latent space induced
by the feature extractor and provide training monitoring results based on
feature disentanglement.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03558" title="Abstract">arXiv:2402.03558</a> [<a href="/pdf/2402.03558" title="Download PDF">pdf</a>, <a href="/format/2402.03558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Signatures and Graph Neural Networks for Slow Earthquake Analysis:  Better Together?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riess%2C+H">Hans Riess</a>, 
<a href="/search/cs?searchtype=author&query=Veveakis%2C+M">Manolis Veveakis</a>, 
<a href="/search/cs?searchtype=author&query=Zavlanos%2C+M+M">Michael M. Zavlanos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Geophysics (physics.geo-ph)

</div>
<p class="mathjax">The path signature, having enjoyed recent success in the machine learning
community, is a theoretically-driven method for engineering features from
irregular paths. On the other hand, graph neural networks (GNN), neural
architectures for processing data on graphs, excel on tasks with irregular
domains, such as sensor networks. In this paper, we introduce a novel approach,
Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path
signatures into graph convolutional neural networks (GCNN), and leveraging the
strengths of both path signatures, for feature extraction, and GCNNs, for
handling spatial interactions. We apply our method to analyze slow earthquake
sequences, also called slow slip events (SSE), utilizing data from GPS
timeseries, with a case study on a GPS sensor network on the east coast of New
Zealand's north island. We also establish benchmarks for our method on
simulated stochastic differential equations, which model similar
reaction-diffusion phenomenon. Our methodology shows promise for future
advancement in earthquake prediction and sensor network analysis.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03559" title="Abstract">arXiv:2402.03559</a> [<a href="/pdf/2402.03559" title="Download PDF">pdf</a>, <a href="/format/2402.03559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projected Generative Diffusion Models for Constraint Satisfaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christopher%2C+J+K">Jacob K Christopher</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S">Stephen Baek</a>, 
<a href="/search/cs?searchtype=author&query=Fioretto%2C+F">Ferdinando Fioretto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative diffusion models excel at robustly synthesizing coherent content
from raw noise through a sequential process. However, their direct application
in scenarios requiring outputs to adhere to specific, stringent criteria faces
several severe challenges. This paper aims at overcome these challenges and
introduces Projected Generative Diffusion Models (PGDM), an approach that
recast traditional diffusion models sampling into a constrained-optimization
problem. This enables the application of an iterative projections method to
ensure that generated data faithfully adheres to specified constraints or
physical principles. This paper provides theoretical support for the ability of
PGDM to synthesize outputs from a feasible subdistribution under a restricted
class of constraints while also providing large empirical evidence in the case
of complex non-convex constraints and ordinary differential equations. These
capabilities are demonstrated by physics-informed motion in video generation,
trajectory optimization in path planning, and morphometric properties adherence
in material science.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03560" title="Abstract">arXiv:2402.03560</a> [<a href="/pdf/2402.03560" title="Download PDF">pdf</a>, <a href="/format/2402.03560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic flux surrogate-based partitioned methods for interface problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bochev%2C+P">Pavel Bochev</a>, 
<a href="/search/cs?searchtype=author&query=Owen%2C+J">Justin Owen</a>, 
<a href="/search/cs?searchtype=author&query=Kuberry%2C+P">Paul Kuberry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Partitioned methods for coupled problems rely on data transfers between
subdomains to synchronize the subdomain equations and enable their independent
solution. By treating each subproblem as a separate entity, these methods
enable code reuse, increase concurrency and provide a convenient framework for
plug-and-play multiphysics simulations. However, accuracy and stability of
partitioned methods depends critically on the type of information exchanged
between the subproblems. The exchange mechanisms can vary from minimally
intrusive remap across interfaces to more accurate but also more intrusive and
expensive estimates of the necessary information based on monolithic
formulations of the coupled system. These transfer mechanisms are separated by
accuracy, performance and intrusiveness gaps that tend to limit the scope of
the resulting partitioned methods to specific simulation scenarios. Data-driven
system identification techniques provide an opportunity to close these gaps by
enabling the construction of accurate, computationally efficient and minimally
intrusive data transfer surrogates. This approach shifts the principal
computational burden to an offline phase, leaving the application of the
surrogate as the sole additional cost during the online simulation phase. In
this paper we formulate and demonstrate such a \emph{dynamic flux
surrogate-based} partitioned method for a model advection-diffusion
transmission problem by using Dynamic Mode Decomposition (DMD) to learn the
dynamics of the interface flux from data. The accuracy of the resulting DMD
flux surrogate is comparable to that of a dual Schur complement reconstruction,
yet its application cost is significantly lower. Numerical results confirm the
attractive properties of the new partitioned approach.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03561" title="Abstract">arXiv:2402.03561</a> [<a href="/pdf/2402.03561" title="Download PDF">pdf</a>, <a href="/format/2402.03561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+A">Aishwarya Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G">Gaurav Sukhatme</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate
through realistic 3D outdoor environments based on natural language
instructions. The performance of existing VLN methods is limited by
insufficient diversity in navigation environments and limited training data. To
address these issues, we propose VLN-Video, which utilizes the diverse outdoor
environments present in driving videos in multiple cities in the U.S. augmented
with automatically generated navigation instructions and actions to improve
outdoor VLN performance. VLN-Video combines the best of intuitive classical
approaches and modern deep learning techniques, using template infilling to
generate grounded navigation instructions, combined with an image rotation
similarity-based navigation action predictor to obtain VLN style data from
driving videos for pretraining deep learning VLN models. We pre-train the model
on the Touchdown dataset and our video-augmented dataset created from driving
videos with three proxy tasks: Masked Language Modeling, Instruction and
Trajectory Matching, and Next Action Prediction, so as to learn
temporally-aware and visually-aligned instruction representations. The learned
instruction representation is adapted to the state-of-the-art navigator when
fine-tuning on the Touchdown dataset. Empirical results demonstrate that
VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in
task completion rate, achieving a new state-of-the-art on the Touchdown
dataset.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03562" title="Abstract">arXiv:2402.03562</a> [<a href="/pdf/2402.03562" title="Download PDF">pdf</a>, <a href="/ps/2402.03562" title="Download PostScript">ps</a>, <a href="/format/2402.03562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel pattern recognition system for detecting Android malware by  analyzing suspicious boot sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidal%2C+J+M">Jorge Maestre Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Monge%2C+M+A+S">Marco Antonio Sotelo Monge</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Knowledge-Based Systems. Vol. 150, pp. 198-217, June 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper introduces a malware detection system for smartphones based on
studying the dynamic behavior of suspicious applications. The main goal is to
prevent the installation of the malicious software on the victim systems. The
approach focuses on identifying malware addressed against the Android platform.
For that purpose, only the system calls performed during the boot process of
the recently installed applications are studied. Thereby the amount of
information to be considered is reduced, since only activities related with
their initialization are taken into account. The proposal defines a pattern
recognition system with three processing layers: monitoring, analysis and
decision-making. First, in order to extract the sequences of system calls, the
potentially compromised applications are executed on a safe and isolated
environment. Then the analysis step generates the metrics required for
decision-making. This level combines sequence alignment algorithms with
bagging, which allow scoring the similarity between the extracted sequences
considering their regions of greatest resemblance. At the decision-making
stage, the Wilcoxon signed-rank test is implemented, which determines if the
new software is labeled as legitimate or malicious. The proposal has been
tested in different experiments that include an in-depth study of a particular
use case, and the evaluation of its effectiveness when analyzing samples of
well-known public datasets. Promising experimental results have been shown,
hence demonstrating that the approach is a good complement to the strategies of
the bibliography.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03563" title="Abstract">arXiv:2402.03563</a> [<a href="/pdf/2402.03563" title="Download PDF">pdf</a>, <a href="/format/2402.03563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distinguishing the Knowable from the Unknowable with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahdritz%2C+G">Gustaf Ahdritz</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tian Qin</a>, 
<a href="/search/cs?searchtype=author&query=Vyas%2C+N">Nikhil Vyas</a>, 
<a href="/search/cs?searchtype=author&query=Barak%2C+B">Boaz Barak</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We study the feasibility of identifying epistemic uncertainty (reflecting a
lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in
the underlying distribution), in the outputs of large language models (LLMs)
over free-form text. In the absence of ground-truth probabilities, we explore a
setting where, in order to (approximately) disentangle a given LLM's
uncertainty, a significantly larger model stands in as a proxy for the ground
truth. We show that small linear probes trained on the embeddings of frozen,
pretrained models accurately predict when larger models will be more confident
at the token level and that probes trained on one text domain generalize to
others. Going further, we propose a fully unsupervised method that achieves
non-trivial accuracy on the same task. Taken together, we interpret these
results as evidence that LLMs naturally contain internal representations of
different types of uncertainty that could potentially be leveraged to devise
more informative indicators of model confidence in diverse practical settings.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03564" title="Abstract">arXiv:2402.03564</a> [<a href="/pdf/2402.03564" title="Download PDF">pdf</a>, <a href="/format/2402.03564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkipPredict: When to Invest in Predictions for Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahout%2C+R">Rana Shahout</a>, 
<a href="/search/cs?searchtype=author&query=Mitzenmacher%2C+M">Michael Mitzenmacher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In light of recent work on scheduling with predicted job sizes, we consider
the effect of the cost of predictions in queueing systems, removing the
assumption in prior research that predictions are external to the system's
resources and/or cost-free. In particular, we introduce a novel approach to
utilizing predictions, SkipPredict, designed to address their inherent cost.
Rather than uniformly applying predictions to all jobs, we propose a tailored
approach that categorizes jobs based on their prediction requirements. To
achieve this, we employ one-bit "cheap predictions" to classify jobs as either
short or long. SkipPredict prioritizes predicted short jobs over long jobs, and
for the latter, SkipPredict applies a second round of more detailed "expensive
predictions" to approximate Shortest Remaining Processing Time for these jobs.
Our analysis takes into account the cost of prediction. We examine the effect
of this cost for two distinct models. In the external cost model, predictions
are generated by some external method without impacting job service times but
incur a cost. In the server time cost model, predictions themselves require
server processing time, and are scheduled on the same server as the jobs.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03569" title="Abstract">arXiv:2402.03569</a> [<a href="/pdf/2402.03569" title="Download PDF">pdf</a>, <a href="/format/2402.03569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Invisible Game on the Internet: A Case Study of Decoding Deceptive  Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zewei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoxi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jieshan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiamou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Minhui Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Deceptive patterns are design practices embedded in digital platforms to
manipulate users, representing a widespread and long-standing issue in the web
and mobile software development industry. Legislative actions highlight the
urgency of globally regulating deceptive patterns. However, despite
advancements in detection tools, a significant gap exists in assessing
deceptive pattern risks. In this study, we introduce a comprehensive approach
involving the interactions between the Adversary, Watchdog (e.g., detection
tools), and Challengers (e.g., users) to formalize and decode deceptive pattern
threats. Based on this, we propose a quantitative risk assessment system.
Representative cases are analyzed to showcase the practicability of the
proposed risk scoring system, emphasizing the importance of involving human
factors in deceptive pattern risk assessment.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03570" title="Abstract">arXiv:2402.03570</a> [<a href="/pdf/2402.03570" title="Download PDF">pdf</a>, <a href="/format/2402.03570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion World Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zihan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinqing Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce Diffusion World Model (DWM), a conditional diffusion model
capable of predicting multistep future states and rewards concurrently. As
opposed to traditional one-step dynamics models, DWM offers long-horizon
predictions in a single forward pass, eliminating the need for recursive
quires. We integrate DWM into model-based value estimation, where the
short-term return is simulated by future trajectories sampled from DWM. In the
context of offline reinforcement learning, DWM can be viewed as a conservative
value regularization through generative modeling. Alternatively, it can be seen
as a data source that enables offline Q-learning with synthetic data. Our
experiments on the D4RL dataset confirm the robustness of DWM to long-horizon
simulation. In terms of absolute performance, DWM significantly surpasses
one-step dynamics models with a $44\%$ performance gain, and achieves
state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03574" title="Abstract">arXiv:2402.03574</a> [<a href="/pdf/2402.03574" title="Download PDF">pdf</a>, <a href="/ps/2402.03574" title="Download PostScript">ps</a>, <a href="/format/2402.03574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connections Between Finite Difference and Finite Element Approximations  for a Convection-Diffusion Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bacuta%2C+C">Constantin Bacuta</a>, 
<a href="/search/math?searchtype=author&query=Bacuta%2C+C">Cristina Bacuta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider a model convection-diffusion problem and present useful
connections between the finite differences and finite element discretization
methods. We introduce a general upwinding Petrov-Galerkin discretization based
on bubble modification of the test space and connect the method with the
general upwinding approach used in finite difference discretization. We write
the finite difference and the finite element systems such that the two
corresponding linear systems have the same stiffness matrices, and compare the
right hand side load vectors for the two methods. This new approach allows for
improving well known upwinding finite difference methods and for obtaining new
error estimates. We prove that the exponential bubble Petrov-Galerkin
discretization can recover the interpolant of the exact solution. As a
consequence, we estimate the closeness of the related finite difference
solutions to the interpolant. The ideas we present in this work, can lead to
building efficient new discretization methods for multidimensional convection
dominated problems.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03575" title="Abstract">arXiv:2402.03575</a> [<a href="/pdf/2402.03575" title="Download PDF">pdf</a>, <a href="/format/2402.03575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Human-AI Alignment in Large-Scale Multi-Player Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sugandha Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+G">Guy Davidson</a>, 
<a href="/search/cs?searchtype=author&query=Khetarpal%2C+K">Khimya Khetarpal</a>, 
<a href="/search/cs?searchtype=author&query=Kanervisto%2C+A">Anssi Kanervisto</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+U">Udit Arora</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+K">Katja Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Achieving human-AI alignment in complex multi-agent games is crucial for
creating trustworthy AI agents that enhance gameplay. We propose a method to
evaluate this alignment using an interpretable task-sets framework, focusing on
high-level behavioral tasks instead of low-level policies. Our approach has
three components. First, we analyze extensive human gameplay data from Xbox's
Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task
space. This task space serves as a basis set for a behavior manifold capturing
interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.
Second, we train an AI agent to play Bleeding Edge using a Generative
Pretrained Causal Transformer and measure its behavior. Third, we project human
and AI gameplay to the proposed behavior manifold to compare and contrast. This
allows us to interpret differences in policy as higher-level behavioral
concepts, e.g., we find that while human players exhibit variability in
fight-flight and explore-exploit behavior, AI players tend towards uniformity.
Furthermore, AI agents predominantly engage in solo play, while humans often
engage in cooperative and competitive multi-agent patterns. These stark
differences underscore the need for interpretable evaluation, design, and
integration of AI in human-aligned applications. Our study advances the
alignment discussion in AI and especially generative AI research, offering a
measurable framework for interpretable human-agent alignment in multiplayer
gaming.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03576" title="Abstract">arXiv:2402.03576</a> [<a href="/pdf/2402.03576" title="Download PDF">pdf</a>, <a href="/ps/2402.03576" title="Download PostScript">ps</a>, <a href="/format/2402.03576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Properties of Adversarial Training for $\ell_0$-Bounded  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgosha%2C+P">Payam Delgosha</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Pedarsani%2C+R">Ramtin Pedarsani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We have widely observed that neural networks are vulnerable to small additive
perturbations to the input causing misclassification. In this paper, we focus
on the $\ell_0$-bounded adversarial attacks, and aim to theoretically
characterize the performance of adversarial training for an important class of
truncated classifiers. Such classifiers are shown to have strong performance
empirically, as well as theoretically in the Gaussian mixture model, in the
$\ell_0$-adversarial setting. The main contribution of this paper is to prove a
novel generalization bound for the binary classification setting with
$\ell_0$-bounded adversarial perturbation that is distribution-independent.
Deriving a generalization bound in this setting has two main challenges: (i)
the truncated inner product which is highly non-linear; and (ii) maximization
over the $\ell_0$ ball due to adversarial training is non-convex and highly
non-smooth. To tackle these challenges, we develop new coding techniques for
bounding the combinatorial dimension of the truncated hypothesis class.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03577" title="Abstract">arXiv:2402.03577</a> [<a href="/pdf/2402.03577" title="Download PDF">pdf</a>, <a href="/format/2402.03577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Dataset Bias Problem from a Statistical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Kien Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thao Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Harikumar%2C+H">Haripriya Harikumar</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Truyen Tran</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+S">Santu Rana</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+S">Svetha Venkatesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we study the "dataset bias" problem from a statistical
standpoint, and identify the main cause of the problem as the strong
correlation between a class attribute u and a non-class attribute b in the
input x, represented by p(u|b) differing significantly from p(u). Since p(u|b)
appears as part of the sampling distributions in the standard maximum
log-likelihood (MLL) objective, a model trained on a biased dataset via MLL
inherently incorporates such correlation into its parameters, leading to poor
generalization to unbiased test data. From this observation, we propose to
mitigate dataset bias via either weighting the objective of each sample n by
\frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to
\frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the
former proves more stable and effective in practice. Additionally, we establish
a connection between our debiasing approach and causal reasoning, reinforcing
our method's theoretical foundation. However, when the bias label is
unavailable, computing p(u|b) exactly is difficult. To overcome this challenge,
we propose to approximate \frac{1}{p(u|b)} using a biased classifier trained
with "bias amplification" losses. Extensive experiments on various biased
datasets demonstrate the superiority of our method over existing debiasing
techniques in most settings, validating our theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03578" title="Abstract">arXiv:2402.03578</a> [<a href="/pdf/2402.03578" title="Download PDF">pdf</a>, <a href="/format/2402.03578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Multi-Agent Systems: Challenges and Open Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuhang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weizhao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoyang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores existing works of multi-agent systems and identifies
challenges that remain inadequately addressed. By leveraging the diverse
capabilities and roles of individual agents within a multi-agent system, these
systems can tackle complex tasks through collaboration. We discuss optimizing
task allocation, fostering robust reasoning through iterative debates, managing
complex and layered context information, and enhancing memory management to
support the intricate interactions within multi-agent systems. We also explore
the potential application of multi-agent systems in blockchain systems to shed
light on their future development and application in real-world distributed
systems.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03579" title="Abstract">arXiv:2402.03579</a> [<a href="/pdf/2402.03579" title="Download PDF">pdf</a>, <a href="/format/2402.03579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deconstructing the Goldilocks Zone of Neural Network Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vysogorets%2C+A">Artem Vysogorets</a>, 
<a href="/search/cs?searchtype=author&query=Dawid%2C+A">Anna Dawid</a>, 
<a href="/search/cs?searchtype=author&query=Kempe%2C+J">Julia Kempe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The second-order properties of the training loss have a massive impact on the
optimization dynamics of deep learning models. Fort &amp; Scherlis (2019)
discovered that a high positive curvature and local convexity of the loss
Hessian are associated with highly trainable initial points located in a region
coined the "Goldilocks zone". Only a handful of subsequent studies touched upon
this relationship, so it remains largely unexplained. In this paper, we present
a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous
neural networks. In particular, we derive the fundamental condition resulting
in non-zero positive curvature of the loss Hessian and argue that it is only
incidentally related to the initialization norm, contrary to prior beliefs.
Further, we relate high positive curvature to model confidence, low initial
loss, and a previously unknown type of vanishing cross-entropy loss gradient.
To understand the importance of positive curvature for trainability of deep
networks, we optimize both fully-connected and convolutional architectures
outside the Goldilocks zone and analyze the emergent behaviors. We find that
strong model performance is not necessarily aligned with the Goldilocks zone,
which questions the practical significance of this concept.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03582" title="Abstract">arXiv:2402.03582</a> [<a href="/pdf/2402.03582" title="Download PDF">pdf</a>, <a href="/format/2402.03582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cranor%2C+L+F">Lorrie Faith Cranor</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+Y">Yuvraj Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J+I">Jason I. Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Apple and Google introduced their versions of privacy nutrition labels to the
mobile app stores to better inform users of the apps' data practices. However,
these labels are self-reported by developers and have been found to contain
many inaccuracies due to misunderstandings of the label taxonomy. In this work,
we present Matcha, an IDE plugin that uses automated code analysis to help
developers create accurate Google Play data safety labels. Developers can
benefit from Matcha's ability to detect user data accesses and transmissions
while staying in control of the generated label by adding custom Java
annotations and modifying an auto-generated XML specification. Our evaluation
with 12 developers showed that Matcha helped our participants improved the
accuracy of a label they created with Google's official tool for a real-world
app they developed. We found that participants preferred Matcha for its
accuracy benefits. Drawing on Matcha, we discuss general design recommendations
for developer tools used to create accurate standardized privacy notices.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03583" title="Abstract">arXiv:2402.03583</a> [<a href="/pdf/2402.03583" title="Download PDF">pdf</a>, <a href="/format/2402.03583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MQuinE: a cure for &quot;Z-paradox&#x27;&#x27; in knowledge graph embedding models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yunfeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingming Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge graph embedding (KGE) models achieved state-of-the-art results on
many knowledge graph tasks including link prediction and information retrieval.
Despite the superior performance of KGE models in practice, we discover a
deficiency in the expressiveness of some popular existing KGE models called
\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE
model called \emph{MQuinE} that does not suffer from Z-paradox while preserves
strong expressiveness to model various relation patterns including
symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with
theoretical justification. Experiments on real-world knowledge bases indicate
that Z-paradox indeed degrades the performance of existing KGE models, and can
cause more than 20\% accuracy drop on some challenging test samples. Our
experiments further demonstrate that MQuinE can mitigate the negative impact of
Z-paradox and outperform existing KGE models by a visible margin on link
prediction tasks.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03585" title="Abstract">arXiv:2402.03585</a> [<a href="/pdf/2402.03585" title="Download PDF">pdf</a>, <a href="/format/2402.03585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoder-Only Image Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wenqi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinxing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinming Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In unsupervised medical image registration, the predominant approaches
involve the utilization of a encoder-decoder network architecture, allowing for
precise prediction of dense, full-resolution displacement fields from given
paired images. Despite its widespread use in the literature, we argue for the
necessity of making both the encoder and decoder learnable in such an
architecture. For this, we propose a novel network architecture, termed LessNet
in this paper, which contains only a learnable decoder, while entirely omitting
the utilization of a learnable encoder. LessNet substitutes the learnable
encoder with simple, handcrafted features, eliminating the need to learn
(optimize) network parameters in the encoder altogether. Consequently, this
leads to a compact, efficient, and decoder-only architecture for 3D medical
image registration. Evaluated on two publicly available brain MRI datasets, we
demonstrate that our decoder-only LessNet can effectively and efficiently learn
both dense displacement and diffeomorphic deformation fields in 3D.
Furthermore, our decoder-only LessNet can achieve comparable registration
performance to state-of-the-art methods such as VoxelMorph and TransMorph,
while requiring significantly fewer computational resources. Our code and
pre-trained models are available at https://github.com/xi-jia/LessNet.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03586" title="Abstract">arXiv:2402.03586</a> [<a href="/pdf/2402.03586" title="Download PDF">pdf</a>, <a href="/format/2402.03586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error estimates for SUPG-stabilised Dynamical Low Rank Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nobile%2C+F">Fabio Nobile</a>, 
<a href="/search/math?searchtype=author&query=Trindade%2C+T+T">Thomas Trigo Trindade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We perform an error analysis of a fully discretised Streamline Upwind Petrov
Galerkin Dynamical Low Rank (SUPG-DLR) method for random time-dependent
advection-dominated problems. The time integration scheme has a splitting-like
nature, allowing for potentially efficient computations of the factors
characterising the discretised random field. The method allows to efficiently
compute a low-rank approximation of the true solution, while naturally
"inbuilding" the SUPG stabilisation. Standard error rates in the L2 and
SUPG-norms are recovered. Numerical experiments validate the predicted rates.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03587" title="Abstract">arXiv:2402.03587</a> [<a href="/pdf/2402.03587" title="Download PDF">pdf</a>, <a href="/format/2402.03587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Acquisition Functions for Active Correlation Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aronsson%2C+L">Linus Aronsson</a>, 
<a href="/search/cs?searchtype=author&query=Chehreghani%2C+M+H">Morteza Haghir Chehreghani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Correlation clustering is a powerful unsupervised learning paradigm that
supports positive and negative similarities. In this paper, we assume the
similarities are not known in advance. Instead, we employ active learning to
iteratively query similarities in a cost-efficient way. In particular, we
develop three effective acquisition functions to be used in this setting. One
is based on the notion of inconsistency (i.e., when similarities violate the
transitive property). The remaining two are based on information-theoretic
quantities, i.e., entropy and information gain.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03588" title="Abstract">arXiv:2402.03588</a> [<a href="/pdf/2402.03588" title="Download PDF">pdf</a>, <a href="/format/2402.03588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Domain Adversarial Adaptation via Double-Head Discriminators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhanghexuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chunwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingchen Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Domain adversarial adaptation in a continual setting poses a significant
challenge due to the limitations on accessing previous source domain data.
Despite extensive research in continual learning, the task of adversarial
adaptation cannot be effectively accomplished using only a small number of
stored source domain data, which is a standard setting in memory replay
approaches. This limitation arises from the erroneous empirical estimation of
$\gH$-divergence with few source domain samples. To tackle this problem, we
propose a double-head discriminator algorithm, by introducing an addition
source-only domain discriminator that are trained solely on source learning
phase. We prove that with the introduction of a pre-trained source-only domain
discriminator, the empirical estimation error of $\gH$-divergence related
adversarial loss is reduced from the source domain side. Further experiments on
existing domain adaptation benchmark show that our proposed algorithm achieves
more than 2$\%$ improvement on all categories of target domain adaptation task
while significantly mitigating the forgetting on source domain.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03589" title="Abstract">arXiv:2402.03589</a> [<a href="/pdf/2402.03589" title="Download PDF">pdf</a>, <a href="/format/2402.03589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reinforcement Learning Approach for Dynamic Rebalancing in  Bike-Sharing System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiaqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Jena%2C+S+D">Sanjay Dominik Jena</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Defeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lodi%2C+A">Andrea Lodi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the
alleviation of traffic congestion and to healthier lifestyles. Efficiently
operating such systems and maintaining high customer satisfaction is
challenging due to the stochastic nature of trip demand, leading to full or
empty stations. Devising effective rebalancing strategies using vehicles to
redistribute bikes among stations is therefore of uttermost importance for
operators. As a promising alternative to classical mathematical optimization,
reinforcement learning is gaining ground to solve sequential decision-making
problems. This paper introduces a spatio-temporal reinforcement learning
algorithm for the dynamic rebalancing problem with multiple vehicles. We first
formulate the problem as a Multi-agent Markov Decision Process in a continuous
time framework. This allows for independent and cooperative vehicle
rebalancing, eliminating the impractical restriction of time-discretized models
where vehicle departures are synchronized. A comprehensive simulator under the
first-arrive-first-serve rule is then developed to facilitate the learning
process by computing immediate rewards under diverse demand scenarios. To
estimate the value function and learn the rebalancing policy, various Deep
Q-Network configurations are tested, minimizing the lost demand. Experiments
are carried out on various datasets generated from historical data, affected by
both temporal and weather factors. The proposed algorithms outperform
benchmarks, including a multi-period Mixed-Integer Programming model, in terms
of lost demand. Once trained, it yields immediate decisions, making it suitable
for real-time applications. Our work offers practical insights for operators
and enriches the integration of reinforcement learning into dynamic rebalancing
problems, paving the way for more intelligent and robust urban mobility
solutions.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03590" title="Abstract">arXiv:2402.03590</a> [<a href="/pdf/2402.03590" title="Download PDF">pdf</a>, <a href="/format/2402.03590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Impact of Distribution Shift on Reinforcement Learning  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+T">Ted Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Suetterlein%2C+J">Joshua Suetterlein</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Samrat Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Auroop Ganguly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Poster at the Workshop on Regulatable Machine Learning at the 37th Conference on Neural Information Processing Systems (RegML @ NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Research in machine learning is making progress in fixing its own
reproducibility crisis. Reinforcement learning (RL), in particular, faces its
own set of unique challenges. Comparison of point estimates, and plots that
show successful convergence to the optimal policy during training, may
obfuscate overfitting or dependence on the experimental setup. Although
researchers in RL have proposed reliability metrics that account for
uncertainty to better understand each algorithm's strengths and weaknesses, the
recommendations of past work do not assume the presence of out-of-distribution
observations. We propose a set of evaluation methods that measure the
robustness of RL algorithms under distribution shifts. The tools presented here
argue for the need to account for performance over time while the agent is
acting in its environment. In particular, we recommend time series analysis as
a method of observational RL evaluation. We also show that the unique
properties of RL and simulated dynamic environments allow us to make stronger
assumptions to justify the measurement of causal impact in our evaluations. We
then apply these tools to single-agent and multi-agent environments to show the
impact of introducing distribution shifts during test time. We present this
methodology as a first step toward rigorous RL evaluation in the presence of
distribution shifts.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03591" title="Abstract">arXiv:2402.03591</a> [<a href="/pdf/2402.03591" title="Download PDF">pdf</a>, <a href="/ps/2402.03591" title="Download PostScript">ps</a>, <a href="/format/2402.03591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse Engineering and Security Evaluation of Commercial Tags for  RFID-Based IoT Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez-Caram%C3%A9s%2C+T+M">Tiago M. Fern&#xe1;ndez-Caram&#xe9;s</a>, 
<a href="/search/eess?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/eess?searchtype=author&query=Su%C3%A1rez-Albela%2C+M">Manuel Su&#xe1;rez-Albela</a>, 
<a href="/search/eess?searchtype=author&query=Castedo%2C+L">Luis Castedo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 26 figures, accepted version of Sensors journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2017, 17(1), 28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The Internet of Things (IoT) is a distributed system of physical objects that
requires the seamless integration of hardware (e.g., sensors, actuators,
electronics) and network communications in order to collect and exchange data.
IoT smart objects need to be somehow identified to determine the origin of the
data and to automatically detect the elements around us. One of the best
positioned technologies to perform identification is RFID (Radio Frequency
Identification), which in the last years has gained a lot of popularity in
applications like access control, payment cards or logistics. Despite its
popularity, RFID security has not been properly handled in numerous
applications. To foster security in such applications, this article includes
three main contributions. First, in order to establish the basics, a detailed
review of the most common flaws found in RFID-based IoT systems is provided,
including the latest attacks described in the literature. Second, a novel
methodology that eases the detection and mitigation of such flaws is presented.
Third, the latest RFID security tools are analyzed and the methodology proposed
is applied through one of them (Proxmark 3) to validate it. Thus, the
methodology is tested in different scenarios where tags are commonly used for
identification. In such systems it was possible to clone transponders, extract
information, and even emulate both tags and readers. Therefore, it is shown
that the methodology proposed is useful for auditing security and reverse
engineering RFID communications in IoT applications. It must be noted that,
although this paper is aimed at fostering RFID communications security in IoT
applications, the methodology can be applied to any RFID communications
protocol.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03592" title="Abstract">arXiv:2402.03592</a> [<a href="/pdf/2402.03592" title="Download PDF">pdf</a>, <a href="/format/2402.03592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirabadi%2C+A+K">Ali Khajegili Mirabadi</a>, 
<a href="/search/cs?searchtype=author&query=Archibald%2C+G">Graham Archibald</a>, 
<a href="/search/cs?searchtype=author&query=Darbandsari%2C+A">Amirali Darbandsari</a>, 
<a href="/search/cs?searchtype=author&query=Contreras-Sanz%2C+A">Alberto Contreras-Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Nakhli%2C+R+E">Ramin Ebrahim Nakhli</a>, 
<a href="/search/cs?searchtype=author&query=Asadi%2C+M">Maryam Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Allen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gilks%2C+C+B">C. Blake Gilks</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+P">Peter Black</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Farahani%2C+H">Hossein Farahani</a>, 
<a href="/search/cs?searchtype=author&query=Bashashati%2C+A">Ali Bashashati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Early version: To be updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cancer subtyping is one of the most challenging tasks in digital pathology,
where Multiple Instance Learning (MIL) by processing gigapixel whole slide
images (WSIs) has been in the spotlight of recent research. However, MIL
approaches do not take advantage of inter- and intra-magnification information
contained in WSIs. In this work, we present GRASP, a novel graph-structured
multi-magnification framework for processing WSIs in digital pathology. Our
approach is designed to dynamically emulate the pathologist's behavior in
handling WSIs and benefits from the hierarchical structure of WSIs. GRASP,
which introduces a convergence-based node aggregation instead of traditional
pooling mechanisms, outperforms state-of-the-art methods over two distinct
cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times
smaller than the closest-performing state-of-the-art model in terms of the
number of parameters. Our results show that GRASP is dynamic in finding and
consulting with different magnifications for subtyping cancers and is reliable
and stable across different hyperparameters. The model's behavior has been
evaluated by two expert pathologists confirming the interpretability of the
model's dynamic. We also provide a theoretical foundation, along with empirical
evidence, for our work, explaining how GRASP interacts with different
magnifications and nodes in the graph to make predictions. We believe that the
strong characteristics yet simple structure of GRASP will encourage the
development of interpretable, structure-based designs for WSI representation in
digital pathology. Furthermore, we publish two large graph datasets of rare
Ovarian and Bladder cancers to contribute to the field.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03597" title="Abstract">arXiv:2402.03597</a> [<a href="/pdf/2402.03597" title="Download PDF">pdf</a>, <a href="/ps/2402.03597" title="Download PostScript">ps</a>, <a href="/format/2402.03597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Reasons for Contraceptive Switching from Real-World Data  Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+B+Y">Brenda Y. Miao</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+C+Y">Christopher YK Williams</a>, 
<a href="/search/cs?searchtype=author&query=Chinedu-Eneh%2C+E">Ebenezer Chinedu-Eneh</a>, 
<a href="/search/cs?searchtype=author&query=Zack%2C+T">Travis Zack</a>, 
<a href="/search/cs?searchtype=author&query=Alsentzer%2C+E">Emily Alsentzer</a>, 
<a href="/search/cs?searchtype=author&query=Butte%2C+A+J">Atul J. Butte</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+I+Y">Irene Y. Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prescription contraceptives play a critical role in supporting women's
reproductive health. With nearly 50 million women in the United States using
contraceptives, understanding the factors that drive contraceptives selection
and switching is of significant interest. However, many factors related to
medication switching are often only captured in unstructured clinical notes and
can be difficult to extract. Here, we evaluate the zero-shot abilities of a
recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft
Azure API), to identify reasons for switching between classes of contraceptives
from the UCSF Information Commons clinical notes dataset. We demonstrate that
GPT-4 can accurately extract reasons for contraceptive switching, outperforming
baseline BERT-based models with microF1 scores of 0.849 and 0.881 for
contraceptive start and stop extraction, respectively. Human evaluation of
GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal
hallucinations. Using extracted reasons, we identified patient preference,
adverse events, and insurance as key reasons for switching using unsupervised
topic modeling approaches. Notably, we also showed using our approach that
"weight gain/mood change" and "insurance coverage" are disproportionately found
as reasons for contraceptive switching in specific demographic populations. Our
code and supplemental data are available at
https://github.com/BMiao10/contraceptive-switching.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03599" title="Abstract">arXiv:2402.03599</a> [<a href="/pdf/2402.03599" title="Download PDF">pdf</a>, <a href="/format/2402.03599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Internet of Things for Defense and Public Safety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez-Caram%C3%A9s%2C+T+M">Tiago M. Fern&#xe1;ndez-Caram&#xe9;s</a>, 
<a href="/search/eess?searchtype=author&query=Su%C3%A1rez-Albela%2C+M">Manuel Su&#xe1;rez-Albela</a>, 
<a href="/search/eess?searchtype=author&query=Castedo%2C+L">Luis Castedo</a>, 
<a href="/search/eess?searchtype=author&query=Gonz%C3%A1lez-L%C3%B3pez%2C+M">Miguel Gonz&#xe1;lez-L&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 14 figures. Accepted version of Sensors journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2016, 16(10), 1644
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">The Internet of Things (IoT) is undeniably transforming the way that
organizations communicate and organize everyday businesses and industrial
procedures. Its adoption has proven well suited for sectors that manage a large
number of assets and coordinate complex and distributed processes. This survey
analyzes the great potential for applying IoT technologies (i.e., data-driven
applications or embedded automation and intelligent adaptive systems) to
revolutionize modern warfare and provide benefits similar to those in industry.
It identifies scenarios where Defense and Public Safety (PS) could leverage
better commercial IoT capabilities to deliver greater survivability to the
warfighter or first responders, while reducing costs and increasing operation
efficiency and effectiveness. This article reviews the main tactical
requirements and the architecture, examining gaps and shortcomings in existing
IoT systems across the military field and mission-critical scenarios. The
review characterizes the open challenges for a broad deployment and presents a
research roadmap for enabling an affordable IoT for defense and PS.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03600" title="Abstract">arXiv:2402.03600</a> [<a href="/pdf/2402.03600" title="Download PDF">pdf</a>, <a href="/format/2402.03600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Counteracting Feature-Level Bias in Click-Through Rate  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jinqiu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Sihao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Common click-through rate (CTR) prediction recommender models tend to exhibit
feature-level bias, which leads to unfair recommendations among item groups and
inaccurate recommendations for users. While existing methods address this issue
by adjusting the learning of CTR models, such as through additional
optimization objectives, they fail to consider how the bias is caused within
these models. To address this research gap, our study performs a top-down
analysis on representative CTR models. Through blocking different components of
a trained CTR model one by one, we identify the key contribution of the linear
component to feature-level bias. We conduct a theoretical analysis of the
learning process for the weights in the linear component, revealing how
group-wise properties of training data influence them. Our experimental and
statistical analyses demonstrate a strong correlation between imbalanced
positive sample ratios across item groups and feature-level bias. Based on this
understanding, we propose a minimally invasive yet effective strategy to
counteract feature-level bias in CTR models by removing the biased linear
weights from trained models. Additionally, we present a linear weight adjusting
strategy that requires fewer random exposure records than relevant debiasing
methods. The superiority of our proposed strategies are validated through
extensive experiments on three real-world datasets.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03602" title="Abstract">arXiv:2402.03602</a> [<a href="/pdf/2402.03602" title="Download PDF">pdf</a>, <a href="/ps/2402.03602" title="Download PostScript">ps</a>, <a href="/format/2402.03602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of 4D BIM and Robot Task Planning: Creation and Flow of  Construction-Related Information for Action-Level Simulation of Indoor Wall  Frame Installation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oyediran%2C+H">Hafiz Oyediran</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+W">William Turner</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyungki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Barrows%2C+M">Matthew Barrows</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">An obstacle toward construction robotization is the lack of methods to plan
robot operations within the entire construction planning process. Despite the
strength in modeling construction site conditions, 4D BIM technologies cannot
perform construction robot task planning considering the contexts of given work
environments. To address this limitation, this study presents a framework that
integrates 4D BIM and robot task planning, presents an information flow for the
integration, and performs high-level robot task planning and detailed
simulation. The framework uniquely incorporates a construction robot knowledge
base that derives robot-related modeling requirements to augment a 4D BIM
model. Then, the 4D BIM model is converted into a robot simulation world where
a robot performs a sequence of actions retrieving construction-related
information. A case study focusing on the interior wall frame installation
demonstrates the potential of systematic integration in achieving context-aware
robot task planning and simulation in construction environments.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03607" title="Abstract">arXiv:2402.03607</a> [<a href="/pdf/2402.03607" title="Download PDF">pdf</a>, <a href="/format/2402.03607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Contextual Congruence Across Modalities for Effective  Multimodal Marketing using Knowledge-infused Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padhi%2C+T">Trilok Padhi</a>, 
<a href="/search/cs?searchtype=author&query=Kursuncu%2C+U">Ugur Kursuncu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+Y">Yaman Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shalin%2C+V+L">Valerie L. Shalin</a>, 
<a href="/search/cs?searchtype=author&query=Fronczek%2C+L+P">Lane Peterson Fronczek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The prevalence of smart devices with the ability to capture moments in
multiple modalities has enabled users to experience multimodal information
online. However, large Language (LLMs) and Vision models (LVMs) are still
limited in capturing holistic meaning with cross-modal semantic relationships.
Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual
Language Models (VLMs) only learn implicit representations by capturing
high-level patterns in vast corpora, missing essential contextual cross-modal
cues. In this work, we design a framework to couple explicit commonsense
knowledge in the form of knowledge graphs with large VLMs to improve the
performance of a downstream task, predicting the effectiveness of multi-modal
marketing campaigns. While the marketing application provides a compelling
metric for assessing our methods, our approach enables the early detection of
likely persuasive multi-modal campaigns and the assessment and augmentation of
marketing theory.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03610" title="Abstract">arXiv:2402.03610</a> [<a href="/pdf/2402.03610" title="Download PDF">pdf</a>, <a href="/format/2402.03610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal  LLM Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kagaya%2C+T">Tomoyuki Kagaya</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T+J">Thong Jing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yuxuan Lou</a>, 
<a href="/search/cs?searchtype=author&query=Karlekar%2C+J">Jayashree Karlekar</a>, 
<a href="/search/cs?searchtype=author&query=Pranata%2C+S">Sugiri Pranata</a>, 
<a href="/search/cs?searchtype=author&query=Kinose%2C+A">Akira Kinose</a>, 
<a href="/search/cs?searchtype=author&query=Oguri%2C+K">Koki Oguri</a>, 
<a href="/search/cs?searchtype=author&query=Wick%2C+F">Felix Wick</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Owing to recent advancements, Large Language Models (LLMs) can now be
deployed as agents for increasingly complex decision-making applications in
areas including robotics, gaming, and API integration. However, reflecting past
experiences in current decision-making processes, an innate human behavior,
continues to pose significant challenges. Addressing this, we propose
Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage
past experiences corresponding to the current situation and context, thereby
enhancing agents' planning capabilities. RAP distinguishes itself by being
versatile: it excels in both text-only and multimodal environments, making it
suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's
effectiveness, where it achieves SOTA performance in textual scenarios and
notably enhances multimodal LLM agents' performance for embodied tasks. These
results highlight RAP's potential in advancing the functionality and
applicability of LLM agents in complex, real-world applications.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03612" title="Abstract">arXiv:2402.03612</a> [<a href="/pdf/2402.03612" title="Download PDF">pdf</a>, <a href="/format/2402.03612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy risk in GeoData: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorestani%2C+M+A">Mahrokh Abdollahi Lorestani</a>, 
<a href="/search/cs?searchtype=author&query=Ranbaduge%2C+T">Thilina Ranbaduge</a>, 
<a href="/search/cs?searchtype=author&query=Rakotoarivelo%2C+T">Thierry Rakotoarivelo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the ubiquitous use of location-based services, large-scale
individual-level location data has been widely collected through
location-awareness devices. The exposure of location data constitutes a
significant privacy risk to users as it can lead to de-anonymisation, the
inference of sensitive information, and even physical threats. Geoprivacy
concerns arise on the issues of user identity de-anonymisation and location
exposure. In this survey, we analyse different geomasking techniques that have
been proposed to protect the privacy of individuals in geodata. We present a
taxonomy to characterise these techniques along different dimensions, and
conduct a survey of geomasking techniques. We then highlight shortcomings of
current techniques and discuss avenues for future research.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03614" title="Abstract">arXiv:2402.03614</a> [<a href="/pdf/2402.03614" title="Download PDF">pdf</a>, <a href="/format/2402.03614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of automatically discovering Granger causal relations
from observational multivariate time-series data. Vector autoregressive (VAR)
models have been time-tested for this problem, including Bayesian variants and
more recent developments using deep neural networks. Most existing VAR methods
for Granger causality use sparsity-inducing penalties/priors or post-hoc
thresholds to interpret their coefficients as Granger causal graphs. Instead,
we propose a new Bayesian VAR model with a hierarchical graph prior over binary
Granger causal graphs, separately from the VAR coefficients. We develop an
efficient algorithm to infer the posterior over binary Granger causal graphs.
Our method provides better uncertainty quantification, has less
hyperparameters, and achieves better performance than competing approaches,
especially on sparse multivariate time-series data.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03616" title="Abstract">arXiv:2402.03616</a> [<a href="/pdf/2402.03616" title="Download PDF">pdf</a>, <a href="/format/2402.03616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Hybrid Workplace Decision Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yujin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chin-Chia Hsu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Large Language Models (LLMs) hold the potential to perform a variety of text
processing tasks and provide textual explanations for proposed actions or
decisions. In the era of hybrid work, LLMs can provide intelligent decision
support for workers who are designing their hybrid work plans. In particular,
they can offer suggestions and explanations to workers balancing numerous
decision factors, thereby enhancing their work experience. In this paper, we
present a decision support model for workspaces in hybrid work environments,
leveraging the reasoning skill of LLMs. We first examine LLM's capability of
making suitable workspace suggestions. We find that its reasoning extends
beyond the guidelines in the prompt and the LLM can manage the trade-off among
the available resources in the workspaces. We conduct an extensive user study
to understand workers' decision process for workspace choices and evaluate the
effectiveness of the system. We observe that a worker's decision could be
influenced by the LLM's suggestions and explanations. The participants in our
study find the system to be convenient, regardless of whether reasons are
provided or not. Our results show that employees can benefit from the
LLM-empowered system for their workspace selection in hybrid workplace.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03617" title="Abstract">arXiv:2402.03617</a> [<a href="/pdf/2402.03617" title="Download PDF">pdf</a>, <a href="/format/2402.03617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Environment-Centric Learning Approach for Gait Synthesis in Terrestrial  Soft Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freeman%2C+C">Caitlin Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Mahendran%2C+A+N">Arun Niddish Mahendran</a>, 
<a href="/search/cs?searchtype=author&query=Vikas%2C+V">Vishesh Vikas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE Transactions on Robotics for possible publication. Details: 17 pages, 18 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Locomotion gaits are fundamental for control of soft terrestrial robots.
However, synthesis of these gaits is challenging due to modeling of
robot-environment interaction and lack of a mathematical framework. This work
presents an environment-centric, data-driven and fault-tolerant probabilistic
Model-Free Control (pMFC) framework that allows for soft multi-limb robots to
learn from their environment and synthesize diverse sets of locomotion gaits
for realizing open-loop control. Here, discretization of factors dominating
robot-environment interactions enables an environment-specific graphical
representation where the edges encode experimental locomotion data
corresponding to the robot motion primitives. In this graph, locomotion gaits
are defined as simple cycles that are transformation invariant, i.e., the
locomotion is independent of the starting vertex of these periodic cycles. Gait
synthesis, the problem of finding optimal locomotion gaits for a given
substrate, is formulated as Binary Integer Linear Programming (BILP) problems
with a linearized cost function, linear constraints, and iterative simple cycle
detection. Experimentally, gaits are synthesized for varying robot-environment
interactions. Variables include robot morphology - three-limb and four-limb
robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and
carpet; and actuator functionality - simulated loss of robot limb actuation. On
an average, gait synthesis improves the translation and rotation speeds by 82%
and 97% respectively. The results highlight that data-driven methods are vital
to soft robot locomotion control due to the significant influence of unexpected
asymmetries in the system and the dependence of optimal gait sequences on the
experimental robot-environment interaction.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03618" title="Abstract">arXiv:2402.03618</a> [<a href="/pdf/2402.03618" title="Download PDF">pdf</a>, <a href="/format/2402.03618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Abstraction in Humans and Large Language Models Using  Multimodal Serial Reproduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sreejan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Marjieh%2C+R">Raja Marjieh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byron Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+D">Declan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M+Y">Michael Y. Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+U">Umang Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Lake%2C+B">Brenden Lake</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Humans extract useful abstractions of the world from noisy sensory data.
Serial reproduction allows us to study how people construe the world through a
paradigm similar to the game of telephone, where one person observes a stimulus
and reproduces it for the next to form a chain of reproductions. Past serial
reproduction experiments typically employ a single sensory modality, but humans
often communicate abstractions of the world to each other through language. To
investigate the effect language on the formation of abstractions, we implement
a novel multimodal serial reproduction framework by asking people who receive a
visual stimulus to reproduce it in a linguistic format, and vice versa. We ran
unimodal and multimodal chains with both humans and GPT-4 and find that adding
language as a modality has a larger effect on human reproductions than GPT-4's.
This suggests human visual and linguistic representations are more dissociable
than those of GPT-4.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03620" title="Abstract">arXiv:2402.03620</a> [<a href="/pdf/2402.03620" title="Download PDF">pdf</a>, <a href="/format/2402.03620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Discover: Large Language Models Self-Compose Reasoning Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Heng-Tze Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q+V">Quoc V. Le</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H+S">Huaixiu Steven Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the
task-intrinsic reasoning structures to tackle complex reasoning problems that
are challenging for typical prompting methods. Core to the framework is a
self-discovery process where LLMs select multiple atomic reasoning modules such
as critical thinking and step-by-step thinking, and compose them into an
explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER
substantially improves GPT-4 and PaLM 2's performance on challenging reasoning
benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as
much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER
outperforms inference-intensive methods such as CoT-Self-Consistency by more
than 20%, while requiring 10-40x fewer inference compute. Finally, we show that
the self-discovered reasoning structures are universally applicable across
model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share
commonalities with human reasoning patterns.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03621" title="Abstract">arXiv:2402.03621</a> [<a href="/pdf/2402.03621" title="Download PDF">pdf</a>, <a href="/format/2402.03621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network Approximators for Marginal MAP in Probabilistic Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arya%2C+S">Shivvrat Arya</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tahrima Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Gogate%2C+V">Vibhav Gogate</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will appear in AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Probabilistic circuits (PCs) such as sum-product networks efficiently
represent large multi-variate probability distributions. They are preferred in
practice over other probabilistic representations such as Bayesian and Markov
networks because PCs can solve marginal inference (MAR) tasks in time that
scales linearly in the size of the network. Unfortunately, the
maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in
these models. Inspired by the recent work on using neural networks for
generating near-optimal solutions to optimization problems such as integer
linear programming, we propose an approach that uses neural networks to
approximate (M)MAP inference in PCs. The key idea in our approach is to
approximate the cost of an assignment to the query variables using a continuous
multilinear function, and then use the latter as a loss function. The two main
benefits of our new method are that it is self-supervised and after the neural
network is learned, it requires only linear time to output a solution. We
evaluate our new approach on several benchmark datasets and show that it
outperforms three competing linear time approximations, max-product inference,
max-marginal inference and sequential estimation, which are used in practice to
solve MMAP tasks in PCs.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03624" title="Abstract">arXiv:2402.03624</a> [<a href="/pdf/2402.03624" title="Download PDF">pdf</a>, <a href="/ps/2402.03624" title="Download PostScript">ps</a>, <a href="/format/2402.03624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QQMR: A Structure-Preserving Quaternion Quasi-Minimal Residual Method  for Non-Hermitian Quaternion Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Q">Qing-Wen Wang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xin-Fang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The quaternion biconjugate gradient (QBiCG) method, as a novel variant of
quaternion Lanczos-type methods for solving the non-Hermitian quaternion linear
systems, does not yield a minimization property. This means that the method
possesses a rather irregular convergence behavior, which leads to numerical
instability. In this paper, we propose a new structure-preserving quaternion
quasi-minimal residual method, based on the quaternion biconjugate
orthonormalization procedure with coupled two-term recurrences, which overcomes
the drawback of QBiCG. The computational cost and storage required by the
proposed method are much less than the traditional QMR iterations for the real
representation of quaternion linear systems. Some convergence properties of
which are also established. Finally, we report the numerical results to show
the robustness and effectiveness of the proposed method compared with QBiCG.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03625" title="Abstract">arXiv:2402.03625</a> [<a href="/pdf/2402.03625" title="Download PDF">pdf</a>, <a href="/format/2402.03625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex Relaxations of ReLU Neural Networks Approximate Global Optima in  Polynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungyoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we study the optimality gap between two-layer ReLU networks
regularized with weight decay and their convex relaxations. We show that when
the training data is random, the relative optimality gap between the original
problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$,
where $n$ is the number of training samples. A simple application leads to a
tractable polynomial-time algorithm that is guaranteed to solve the original
non-convex problem up to a logarithmic factor. Moreover, under mild
assumptions, we show that with random initialization on the parameters local
gradient methods almost surely converge to a point that has low training loss.
Our result is an exponential improvement compared to existing results and sheds
new light on understanding why local gradient methods work well.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03627" title="Abstract">arXiv:2402.03627</a> [<a href="/pdf/2402.03627" title="Download PDF">pdf</a>, <a href="/ps/2402.03627" title="Download PostScript">ps</a>, <a href="/format/2402.03627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Recentralization Softmax Loss for Vision-Language Models  Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jinzhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yaqian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Large Language Models make a breakthrough in natural language processing
tasks (NLP), multimodal technique becomes extremely popular. However, it has
been shown that multimodal NLP are vulnerable to adversarial attacks, where the
outputs of a model can be dramatically changed by a perturbation to the input.
While several defense techniques have been proposed both in computer vision and
NLP models, the multimodal robustness of models have not been fully explored.
In this paper, we study the adversarial robustness provided by modifying loss
function of pre-trained multimodal models, by restricting top K softmax
outputs. Based on the evaluation and scoring, our experiments show that after a
fine-tuning, adversarial robustness of pre-trained models can be significantly
improved, against popular attacks. Further research should be studying, such as
output diversity, generalization and the robustness-performance trade-off of
this kind of loss functions. Our code will be available after this paper is
accepted
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03628" title="Abstract">arXiv:2402.03628</a> [<a href="/pdf/2402.03628" title="Download PDF">pdf</a>, <a href="/format/2402.03628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Professional Agents -- Evolving Large Language Models into Autonomous  Experts with Human-Level Competencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4
has catalyzed remarkable advances in natural language processing, demonstrating
human-like language fluency and reasoning capacities. This position paper
introduces the concept of Professional Agents (PAgents), an application
framework harnessing LLM capabilities to create autonomous agents with
controllable, specialized, interactive, and professional-level competencies. We
posit that PAgents can reshape professional services through continuously
developed expertise. Our proposed PAgents framework entails a tri-layered
architecture for genesis, evolution, and synergy: a base tool layer, a middle
agent layer, and a top synergy layer. This paper aims to spur discourse on
promising real-world applications of LLMs. We argue the increasing
sophistication and integration of PAgents could lead to AI systems exhibiting
professional mastery over complex domains, serving critical needs, and
potentially achieving artificial general intelligence.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03629" title="Abstract">arXiv:2402.03629</a> [<a href="/pdf/2402.03629" title="Download PDF">pdf</a>, <a href="/format/2402.03629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disparate Impact on Group Accuracy of Linearization for Private  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Saswat Das</a>, 
<a href="/search/cs?searchtype=author&query=Romanelli%2C+M">Marco Romanelli</a>, 
<a href="/search/cs?searchtype=author&query=Fioretto%2C+F">Ferdinando Fioretto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">Ensuring privacy-preserving inference on cryptographically secure data is a
well-known computational challenge. To alleviate the bottleneck of costly
cryptographic computations in non-linear activations, recent methods have
suggested linearizing a targeted portion of these activations in neural
networks. This technique results in significantly reduced runtimes with often
negligible impacts on accuracy. In this paper, we demonstrate that such
computational benefits may lead to increased fairness costs. Specifically, we
find that reducing the number of ReLU activations disproportionately decreases
the accuracy for minority groups compared to majority groups. To explain these
observations, we provide a mathematical interpretation under restricted
assumptions about the nature of the decision boundary, while also showing the
prevalence of this problem across widely used datasets and architectures.
Finally, we show how a simple procedure altering the fine-tuning step for
linearized models can serve as an effective mitigation strategy.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03630" title="Abstract">arXiv:2402.03630</a> [<a href="/pdf/2402.03630" title="Download PDF">pdf</a>, <a href="/format/2402.03630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing LLM-Based Coding Tools through Native Integration of  IDE-Derived Static Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable success in code
completion, as evidenced by their essential roles in developing code assistant
services such as Copilot. Being trained on in-file contexts, current LLMs are
quite effective in completing code for single source files. However, it is
challenging for them to conduct repository-level code completion for large
software projects that require cross-file information. Existing research on
LLM-based repository-level code completion identifies and integrates cross-file
contexts, but it suffers from low accuracy and limited context length of LLMs.
In this paper, we argue that Integrated Development Environments (IDEs) can
provide direct, accurate and real-time cross-file information for
repository-level code completion. We propose IDECoder, a practical framework
that leverages IDE native static contexts for cross-context construction and
diagnosis results for self-refinement. IDECoder utilizes the rich cross-context
information available in IDEs to enhance the capabilities of LLMs of
repository-level code completion. We conducted preliminary experiments to
validate the performance of IDECoder and observed that this synergy represents
a promising trend for future exploration.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03631" title="Abstract">arXiv:2402.03631</a> [<a href="/pdf/2402.03631" title="Download PDF">pdf</a>, <a href="/format/2402.03631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of  Segmentation Anything Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+A">Aoran Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+W">Weihao Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Heli Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yun Xing</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Ruijie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoqin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://xiaoaoran.github.io/projects/CAT-SAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot
capability and flexible geometric prompting in general image segmentation.
However, SAM often struggles when handling various unconventional images, such
as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a
ConditionAl Tuning network that adapts SAM toward various unconventional target
tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and
adapts its mask decoder and image encoder simultaneously with a small number of
learnable parameters. The core design is a prompt bridge structure that enables
decoder-conditioned joint tuning of the heavyweight image encoder and the
lightweight mask decoder. The bridging maps the prompt token of the mask
decoder to the image encoder, fostering synergic adaptation of the encoder and
the decoder with mutual benefits. We develop two representative tuning
strategies for the image encoder which leads to two CAT-SAM variants: one
injecting learnable prompt tokens in the input space and the other inserting
lightweight adapter networks. Extensive experiments over 11 unconventional
tasks show that both CAT-SAM variants achieve superior target segmentation
performance consistently even under the very challenging one-shot adaptation
setup. Project page: \url{https://xiaoaoran.github.io/projects/CAT-SAM}
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03633" title="Abstract">arXiv:2402.03633</a> [<a href="/pdf/2402.03633" title="Download PDF">pdf</a>, <a href="/ps/2402.03633" title="Download PostScript">ps</a>, <a href="/format/2402.03633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lossy Cryptography from Code-Based Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dao%2C+Q">Quang Dao</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Aayush Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computational Complexity (cs.CC); Information Theory (cs.IT)

</div>
<p class="mathjax">Over the past few decades, we have seen a proliferation of advanced
cryptographic primitives with lossy or homomorphic properties built from
various assumptions such as Quadratic Residuosity, Decisional Diffie-Hellman,
and Learning with Errors. These primitives imply hard problems in the
complexity class $SZK$ (statistical zero-knowledge); as a consequence, they can
only be based on assumptions that are broken in $BPP^{SZK}$. This poses a
barrier for building advanced primitives from code-based assumptions, as the
only known such assumption is Learning Parity with Noise (LPN) with an
extremely low noise rate $\frac{\log^2 n}{n}$, which is broken in
quasi-polynomial time.
<br />In this work, we propose a new code-based assumption: Dense-Sparse LPN, that
falls in the complexity class $BPP^{SZK}$ and is conjectured to be secure
against subexponential time adversaries. Our assumption is a variant of LPN
that is inspired by McEliece's cryptosystem and random $k\mbox{-}$XOR in
average-case complexity.
<br />We leverage our assumption to build lossy trapdoor functions (Peikert-Waters
STOC 08). This gives the first post-quantum alternative to the lattice-based
construction in the original paper. Lossy trapdoor functions, being a
fundamental cryptographic tool, are known to enable a broad spectrum of both
lossy and non-lossy cryptographic primitives; our construction thus implies
these primitives in a generic manner. In particular, we achieve
collision-resistant hash functions with plausible subexponential security,
improving over a prior construction from LPN with noise rate $\frac{\log^2
n}{n}$ that is only quasi-polynomially secure.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03634" title="Abstract">arXiv:2402.03634</a> [<a href="/pdf/2402.03634" title="Download PDF">pdf</a>, <a href="/format/2402.03634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tengteng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qianjing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Haotian Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qixiang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yanzhao Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-view 3D object detectors struggle with duplicate predictions due to the
lack of depth information, resulting in false positive detections. In this
study, we introduce BEAM, a novel Beta Distribution Ray Denoising approach that
can be applied to any DETR-style multi-view 3D detector to explicitly
incorporate structure prior knowledge of the scene. By generating rays from
cameras to objects and sampling spatial denoising queries from the Beta
distribution family along these rays, BEAM enhances the model's ability to
distinguish spatial hard negative samples arising from ambiguous depths. BEAM
is a plug-and-play technique that adds only marginal computational costs during
training, while impressively preserving the inference speed. Extensive
experiments and ablation studies on the NuScenes dataset demonstrate
significant improvements over strong baselines, outperforming the
state-of-the-art method StreamPETR by 1.9% mAP. The code will be available at
https://github.com/LiewFeng/BEAM.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03635" title="Abstract">arXiv:2402.03635</a> [<a href="/pdf/2402.03635" title="Download PDF">pdf</a>, <a href="/ps/2402.03635" title="Download PostScript">ps</a>, <a href="/format/2402.03635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&amp;A Sites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sijin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pengyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongjian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Liping Jing</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jian Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Posts in software Q\&amp;A sites often consist of three main parts: title,
description and code, which are interconnected and jointly describe the
question. Existing tag recommendation methods often treat different modalities
as a whole or inadequately consider the interaction between different
modalities. Additionally, they focus on extracting information directly from
the post itself, neglecting the information from external knowledge sources.
Therefore, we propose a Retrieval Augmented Cross-Modal (RACM) Tag
Recommendation Model in Software Q\&amp;A Sites. Specifically, we first use the
input post as a query and enhance the representation of different modalities by
retrieving information from external knowledge sources. For the
retrieval-augmented representations, we employ a cross-modal context-aware
attention to leverage the main modality description for targeted feature
extraction across the submodalities title and code. In the fusion process, a
gate mechanism is employed to achieve fine-grained feature selection,
controlling the amount of information extracted from the submodalities.
Finally, the fused information is used for tag recommendation. Experimental
results on three real-world datasets demonstrate that our model outperforms the
state-of-the-art counterparts.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03636" title="Abstract">arXiv:2402.03636</a> [<a href="/pdf/2402.03636" title="Download PDF">pdf</a>, <a href="/format/2402.03636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Informative Sampling using Semantic Features in Underwater  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thengane%2C+S+V">Shrutika Vishal Thengane</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y+X">Yu Xiang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Prasetyo%2C+M+B">Marcel Bartholomeus Prasetyo</a>, 
<a href="/search/cs?searchtype=author&query=Meghjani%2C+M">Malika Meghjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In proceeding of IEEE/MTS OCEANS, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The underwater world remains largely unexplored, with Autonomous Underwater
Vehicles (AUVs) playing a crucial role in sub-sea explorations. However,
continuous monitoring of underwater environments using AUVs can generate a
significant amount of data. In addition, sending live data feed from an
underwater environment requires dedicated on-board data storage options for
AUVs which can hinder requirements of other higher priority tasks. Informative
sampling techniques offer a solution by condensing observations. In this paper,
we present a semantically-aware online informative sampling (ON-IS) approach
which samples an AUV's visual experience in real-time. Specifically, we obtain
visual features from a fine-tuned object detection model to align the sampling
outcomes with the desired semantic information. Our contributions are (a) a
novel Semantic Online Informative Sampling (SON-IS) algorithm, (b) a user study
to validate the proposed approach and (c) a novel evaluation metric to score
our proposed algorithm with respect to the suggested samples by human subjects
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03640" title="Abstract">arXiv:2402.03640</a> [<a href="/pdf/2402.03640" title="Download PDF">pdf</a>, <a href="/format/2402.03640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosny%2C+A">Abdelrahman Hosny</a>, 
<a href="/search/cs?searchtype=author&query=Reda%2C+S">Sherief Reda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The remarkable achievements of machine learning techniques in analyzing
discrete structures have drawn significant attention towards their integration
into combinatorial optimization algorithms. Typically, these methodologies
improve existing solvers by injecting learned models within the solving loop to
enhance the efficiency of the search process. In this work, we derive a single
differentiable function capable of approximating solutions for the Maximum
Satisfiability Problem (MaxSAT). Then, we present a novel neural network
architecture to model our differentiable function, and progressively solve
MaxSAT using backpropagation. This approach eliminates the need for labeled
data or a neural network training phase, as the training process functions as
the solving algorithm. Additionally, we leverage the computational power of
GPUs to accelerate these computations. Experimental results on challenging
MaxSAT instances show that our proposed methodology outperforms two existing
MaxSAT solvers, and is on par with another in terms of solution cost, without
necessitating any training or access to an underlying SAT solver. Given that
numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves
the way for a new generation of solvers poised to benefit from neural network
GPU acceleration.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03641" title="Abstract">arXiv:2402.03641</a> [<a href="/pdf/2402.03641" title="Download PDF">pdf</a>, <a href="/ps/2402.03641" title="Download PostScript">ps</a>, <a href="/format/2402.03641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable BDF time discretization of BGN-based parametric finite element  methods for geometric flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+C">Chunmei Su</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+G">Ganghui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel class of temporal high-order parametric finite element
methods for solving a wide range of geometric flows of curves and surfaces. By
incorporating the backward differentiation formulae (BDF) for time
discretization into the BGN formulation, originally proposed by Barrett,
Garcke, and N\"urnberg (J. Comput. Phys., 222 (2007), pp.~441--467), we
successfully develop high-order BGN/BDF$k$ schemes. The proposed BGN/BDF$k$
schemes not only retain almost all the advantages of the classical first-order
BGN scheme such as computational efficiency and good mesh quality, but also
exhibit the desired $k$th-order temporal accuracy in terms of shape metrics,
ranging from second-order to fourth-order accuracy. Furthermore, we validate
the performance of our proposed BGN/BDF$k$ schemes through extensive numerical
examples, demonstrating their high-order temporal accuracy for various types of
geometric flows while maintaining good mesh quality throughout the evolution.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03642" title="Abstract">arXiv:2402.03642</a> [<a href="/pdf/2402.03642" title="Download PDF">pdf</a>, <a href="/format/2402.03642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish  Misinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lavrouk%2C+A">Anton Lavrouk</a>, 
<a href="/search/cs?searchtype=author&query=Ligon%2C+I">Ian Ligon</a>, 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jonathan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WNUT2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide
high-quality, annotated, 5-way stance data extracted from Twitter, suitable for
analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus
2.0 iteration, we extend this framework to encompass Russian and Spanish. The
former is of current significance due to prevalent misinformation amid
escalating tensions with the West and the violent incursion into Ukraine. The
latter, meanwhile, represents an enormous community that has been largely
overlooked on major social media platforms. By incorporating an additional
3,874 Spanish and Russian tweets over 41 misinformation claims, our objective
is to support research focused on these issues. To demonstrate the value of
this data, we employed zero-shot cross-lingual transfer on multilingual BERT,
yielding results on par with the initial Stanceosaurus study with a macro F1
score of 43 for both languages. This underlines the viability of stance
classification as an effective tool for identifying multicultural
misinformation.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03646" title="Abstract">arXiv:2402.03646</a> [<a href="/pdf/2402.03646" title="Download PDF">pdf</a>, <a href="/format/2402.03646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lens: A Foundation Model for Network Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qineng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaochang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Huajie Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Network traffic refers to the amount of information being sent and received
over the internet or any system that connects computers. Analyzing and
understanding network traffic is vital for improving network security and
management. However, the analysis of network traffic poses great challenges due
to the unique characteristics of data packets, such as heterogeneous headers
and encrypted payload lacking semantics. To capture the latent semantics of
traffic, a few studies have adopted pre-training techniques based on the
Transformer encoder or decoder to learn the representations from large-scale
traffic data. However, these methods typically excel only in traffic
understanding (classification) or traffic generation tasks. To address this
issue, we develop Lens, a foundational network traffic model that leverages the
T5 architecture to learn the pre-trained representations from large-scale
unlabeled data. Harnessing the strength of the encoder-decoder framework, which
captures the global information while preserving the generative ability, our
model can better learn the representations from large-scale network traffic. To
further enhance pre-training performance, we design a novel loss that
integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet
Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation
results on multiple benchmark datasets demonstrate that the proposed Lens
outperforms the baselines in most downstream tasks related to both traffic
understanding and traffic generation. Notably, it also requires considerably
less labeled data for fine-tuning compared to current methods.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03647" title="Abstract">arXiv:2402.03647</a> [<a href="/pdf/2402.03647" title="Download PDF">pdf</a>, <a href="/format/2402.03647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMBranch: Contrastive Learning with Augmented MILPs for Branching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiacheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Meng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhihua Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huangang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements have introduced machine learning frameworks to enhance
the Branch and Bound (B\&amp;B) branching policies for solving Mixed Integer Linear
Programming (MILP). These methods, primarily relying on imitation learning of
Strong Branching, have shown superior performance. However, collecting expert
samples for imitation learning, particularly for Strong Branching, is a
time-consuming endeavor. To address this challenge, we propose
\textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for
\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs
(AMILPs) by applying variable shifting to limited expert data from their
original MILPs. This approach enables the acquisition of a considerable number
of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for
imitation learning and employs contrastive learning to enhance the model's
ability to capture MILP features, thereby improving the quality of branching
decisions. Experimental results demonstrate that CAMBranch, trained with only
10\% of the complete dataset, exhibits superior performance. Ablation studies
further validate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03651" title="Abstract">arXiv:2402.03651</a> [<a href="/pdf/2402.03651" title="Download PDF">pdf</a>, <a href="/format/2402.03651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Graph Analysis with TGX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shirzadkhani%2C+R">Razieh Shirzadkhani</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shenyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kooshafar%2C+E">Elahe Kooshafar</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>, 
<a href="/search/cs?searchtype=author&query=Poursafaei%2C+F">Farimah Poursafaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-world networks, with their evolving relations, are best captured as
temporal graphs. However, existing software libraries are largely designed for
static graphs where the dynamic nature of temporal graphs is ignored. Bridging
this gap, we introduce TGX, a Python package specially designed for analysis of
temporal networks that encompasses an automated pipeline for data loading, data
processing, and analysis of evolving graphs. TGX provides access to eleven
built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as
well as any novel datasets in the .csv format. Beyond data loading, TGX
facilitates data processing functionalities such as discretization of temporal
graphs and node subsampling to accelerate working with larger datasets. For
comprehensive investigation, TGX offers network analysis by providing a diverse
set of measures, including average node degree and the evolving number of nodes
and edges per timestamp. Additionally, the package consolidates meaningful
visualization plots indicating the evolution of temporal patterns, such as
Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots. The TGX
package is a robust tool for examining the features of temporal graphs and can
be used in various areas like studying social networks, citation networks, and
tracking user interactions. We plan to continuously support and update TGX
based on community feedback. TGX is publicly available on:
https://github.com/ComplexData-MILA/TGX.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03653" title="Abstract">arXiv:2402.03653</a> [<a href="/pdf/2402.03653" title="Download PDF">pdf</a>, <a href="/ps/2402.03653" title="Download PostScript">ps</a>, <a href="/format/2402.03653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-Based Triangle Counting and its Applications in Anonymous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chand%2C+P+K">Prabhat Kumar Chand</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Apurba Das</a>, 
<a href="/search/cs?searchtype=author&query=Molla%2C+A+R">Anisur Rahaman Molla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Triangle counting in a graph is a fundamental problem and has a wide range of
applications in various domains. It is crucial in understanding the structural
properties of a graph and is often used as a building block for more complex
graph analytics. In this paper, we solve the triangle counting problem in an
anonymous graph in a distributed setting using mobile agents and subsequently
use this as a subroutine to tackle the truss decomposition and triangle
centrality problem. The paper employs mobile agents, placed on the nodes of the
graph to coordinate among themselves to solve the triangle enumeration problem
for the graph. Following the literature, we consider the synchronous systems
where each robot executes its tasks concurrently with all others and hence time
complexity can be measured as the number of rounds needed to complete the task.
The graph is anonymous, i.e., without any node labels or IDs, but the agents
are autonomous with distinct IDs and have limited memory. Agents can only
communicate with other agents locally i.e., if and only if they are at the same
node. The goal is to devise algorithms that minimise both the time required for
triangle counting and the memory usage at each agent. We further demonstrate
how the triangle count obtained through the mobile agent approach can be
leveraged to address the truss decomposition, triangle centrality and local
clustering coefficient problems, which involves finding maximal sub-graphs with
strong interconnections. Truss decomposition helps in identifying maximal,
highly interconnected sub-graphs, or trusses, within a network, thus, revealing
the structural cohesion and tight-knit communities in complex graphs,
facilitating the analysis of relationships and information flow in various
fields, such as social networks, biology, and recommendation systems.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03654" title="Abstract">arXiv:2402.03654</a> [<a href="/pdf/2402.03654" title="Download PDF">pdf</a>, <a href="/ps/2402.03654" title="Download PostScript">ps</a>, <a href="/format/2402.03654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reviewing FID and SID Metrics on Generative Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Deijn%2C+R">Ricardo de Deijn</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+A">Aishwarya Batra</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+B">Brandon Koch</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+N">Naseef Mansoor</a>, 
<a href="/search/cs?searchtype=author&query=Makkena%2C+H">Hema Makkena</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages 9 figures 1 table Included in IOTBS, NLTM, AIMLA, DBDM - 2024 Conference Proceedings Editor: David C. Wyld et al
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CS &amp; IT - CSCP (2024) 111-124
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The growth of generative adversarial network (GAN) models has increased the
ability of image processing and provides numerous industries with the
technology to produce realistic image transformations. However, with the field
being recently established there are new evaluation metrics that can further
this research. Previous research has shown the Fr\'echet Inception Distance
(FID) to be an effective metric when testing these image-to-image GANs in
real-world applications. Signed Inception Distance (SID), a founded metric in
2023, expands on FID by allowing unsigned distances. This paper uses public
datasets that consist of fa\c{c}ades, cityscapes, and maps within Pix2Pix and
CycleGAN models. After training these models are evaluated on both inception
distance metrics which measure the generating performance of the trained
models. Our findings indicate that usage of the metric SID incorporates an
efficient and effective metric to complement, or even exceed the ability shown
using the FID for the image-to-image GANs
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03655" title="Abstract">arXiv:2402.03655</a> [<a href="/pdf/2402.03655" title="Download PDF">pdf</a>, <a href="/format/2402.03655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator SVD with Neural Networks via Nested Low-Rank Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J+J">J. Jon Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangxiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Erol%2C+H+S+M">H. S. Melihcan Erol</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Y">Yuheng Bu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lizhong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wornell%2C+G+W">Gregory W. Wornell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Computing eigenvalue decomposition (EVD) of a given linear operator, or
finding its leading eigenvalues and eigenfunctions, is a fundamental task in
many machine learning and scientific computing problems. For high-dimensional
eigenvalue problems, training neural networks to parameterize the
eigenfunctions is considered as a promising alternative to the classical
numerical linear algebra techniques. This paper proposes a new optimization
framework based on the low-rank approximation characterization of a truncated
singular value decomposition, accompanied by new techniques called nesting for
learning the top-$L$ singular values and singular functions in the correct
order. The proposed method promotes the desired orthogonality in the learned
functions implicitly and efficiently via an unconstrained optimization
formulation, which is easy to solve with off-the-shelf gradient-based
optimization algorithms. We demonstrate the effectiveness of the proposed
optimization framework for use cases in computational physics and machine
learning.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03656" title="Abstract">arXiv:2402.03656</a> [<a href="/pdf/2402.03656" title="Download PDF">pdf</a>, <a href="/format/2402.03656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in  Spent Nuclear Fuel Treatment Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vo%2C+D">Duc-Tri Vo</a>, 
<a href="/search/eess?searchtype=author&query=Prodan%2C+I">Ionela Prodan</a>, 
<a href="/search/eess?searchtype=author&query=Lef%C3%A8vre%2C+L">Laurent Lef&#xe8;vre</a>, 
<a href="/search/eess?searchtype=author&query=Vanel%2C+V">Vincent Vanel</a>, 
<a href="/search/eess?searchtype=author&query=Costenoble%2C+S">Sylvain Costenoble</a>, 
<a href="/search/eess?searchtype=author&query=Dinh%2C+B">Binh Dinh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper addresses the particularities of adaptive optimal control of the
uranium extraction-scrubbing operation in the PUREX process. The process
dynamics are nonlinear, high dimensional, and have limited online measurements.
In addition, analysis and developments are based on a qualified simulation
program called PAREX, which was validated with laboratory and industrial data.
The control objective is to stabilize the process at a desired solvent
saturation level, guaranteeing constraints and handling disturbances. The
developed control strategy relies on optimization-based methods for computing
control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC)
and Nonlinear Moving Horizon Estimation (NMHE). The designs of these two
associated algorithms are tailored for this process's particular dynamics and
are implemented through an enhanced Particle Swarm Optimization (PSO) to
guarantee constraint satisfaction. Software-in-the-loop simulations using PAREX
show that the designed control scheme effectively satisfies control objectives
and guarantees constraints during operation.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03658" title="Abstract">arXiv:2402.03658</a> [<a href="/pdf/2402.03658" title="Download PDF">pdf</a>, <a href="/format/2402.03658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+K">Kun Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Liqiang Jing</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuemeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yupeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which
aims to generate a natural language explanation for the given sarcastic
dialogue that involves multiple modalities (i.e., utterance, video, and audio).
Although existing studies have achieved great success based on the generative
pretrained language model BART, they overlook exploiting the sentiments
residing in the utterance, video and audio, which are vital clues for sarcasm
explanation. In fact, it is non-trivial to incorporate sentiments for boosting
SED performance, due to three main challenges: 1) diverse effects of utterance
tokens on sentiments; 2) gap between video-audio sentiment signals and the
embedding space of BART; and 3) various relations among utterances, utterance
sentiments, and video-audio sentiments. To tackle these challenges, we propose
a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation
framework, named EDGE. In particular, we first propose a lexicon-guided
utterance sentiment inference module, where a heuristic utterance sentiment
refinement strategy is devised. We then develop a module named Joint Cross
Attention-based Sentiment Inference (JCA-SI) by extending the multimodal
sentiment analysis model JCA to derive the joint sentiment label for each
video-audio clip. Thereafter, we devise a context-sentiment graph to
comprehensively model the semantic relations among the utterances, utterance
sentiments, and video-audio sentiments, to facilitate sarcasm explanation
generation. Extensive experiments on the publicly released dataset WITS verify
the superiority of our model over cutting-edge methods.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03659" title="Abstract">arXiv:2402.03659</a> [<a href="/pdf/2402.03659" title="Download PDF">pdf</a>, <a href="/format/2402.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Explainable Stock Predictions using Self-Reflective  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koa%2C+K+J+L">Kelvin J.L. Koa</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunshan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+R">Ritchie Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">Explaining stock predictions is generally a difficult task for traditional
non-generative deep learning models, where explanations are limited to
visualizing the attention weights on important texts. Today, Large Language
Models (LLMs) present a solution to this problem, given their known
capabilities to generate human-readable explanations for their decision-making
process. However, the task of stock prediction remains challenging for LLMs, as
it requires the ability to weigh the varying impacts of chaotic social texts on
stock prices. The problem gets progressively harder with the introduction of
the explanation component, which requires LLMs to explain verbally why certain
factors are more important than the others. On the other hand, to fine-tune
LLMs for such a task, one would need expert-annotated samples of explanation
for every stock movement in the training set, which is expensive and
impractical to scale. To tackle these issues, we propose our
Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective
agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to
generate explainable stock predictions in a fully autonomous manner. The
reflective agent learns how to explain past stock movements through
self-reasoning, while the PPO trainer trains the model to generate the most
likely explanations from input texts. The training samples for the PPO trainer
are also the responses generated during the reflective process, which
eliminates the need for human annotators. Using our SEP framework, we fine-tune
a LLM that can outperform both traditional deep-learning and LLM methods in
prediction accuracy and Matthews correlation coefficient for the stock
classification task. To justify the generalization capability of our framework,
we further test it on the portfolio construction task, and demonstrate its
effectiveness through various portfolio metrics.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03660" title="Abstract">arXiv:2402.03660</a> [<a href="/pdf/2402.03660" title="Download PDF">pdf</a>, <a href="/format/2402.03660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanpeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yilan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The pretraining-finetuning paradigm has become the prevailing trend in modern
deep learning. In this work, we discover an intriguing linear phenomenon in
models that are initialized from a common pretrained checkpoint and finetuned
on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we
linearly interpolate the weights of two finetuned models, the features in the
weight-interpolated model are approximately equal to the linear interpolation
of features in two finetuned models at each layer. Such cross-task linearity
has not been noted in peer literature. We provide comprehensive empirical
evidence supporting that CTL consistently occurs for finetuned models that
start from the same pretrained checkpoint. We conjecture that in the
pretraining-finetuning paradigm, neural networks essentially function as linear
maps, mapping from the parameter space to the feature space. Based on this
viewpoint, our study unveils novel insights into explaining model
merging/editing, particularly by translating operations from the parameter
space to the feature space. Furthermore, we delve deeper into the underlying
factors for the emergence of CTL, emphasizing the impact of pretraining.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03661" title="Abstract">arXiv:2402.03661</a> [<a href="/pdf/2402.03661" title="Download PDF">pdf</a>, <a href="/format/2402.03661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transductive Reward Inference on Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+B">Bohao Qu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaofeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we present a transductive inference approach on that reward
information propagation graph, which enables the effective estimation of
rewards for unlabelled data in offline reinforcement learning. Reward inference
is the key to learning effective policies in practical scenarios, while direct
environmental interactions are either too costly or unethical and the reward
functions are rarely accessible, such as in healthcare and robotics. Our
research focuses on developing a reward inference method based on the
contextual properties of information propagation on graphs that capitalizes on
a constrained number of human reward annotations to infer rewards for
unlabelled data. We leverage both the available data and limited reward
annotations to construct a reward propagation graph, wherein the edge weights
incorporate various influential factors pertaining to the rewards.
Subsequently, we employ the constructed graph for transductive reward
inference, thereby estimating rewards for unlabelled data. Furthermore, we
establish the existence of a fixed point during several iterations of the
transductive inference process and demonstrate its at least convergence to a
local optimum. Empirical evaluations on locomotion and robotic manipulation
tasks validate the effectiveness of our approach. The application of our
inferred rewards improves the performance in offline reinforcement learning
tasks.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03663" title="Abstract">arXiv:2402.03663</a> [<a href="/pdf/2402.03663" title="Download PDF">pdf</a>, <a href="/format/2402.03663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbol Correctness in Deep Neural Networks Containing Symbolic Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bembenek%2C+A">Aaron Bembenek</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+T">Toby Murray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To handle AI tasks that combine perception and logical reasoning, recent work
introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in
addition to traditional neural layers -- symbolic layers: symbolic expressions
(e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers
during inference. We identify and formalize an intuitive, high-level principle
that can guide the design and analysis of NS-DNNs: symbol correctness, the
correctness of the intermediate symbols predicted by the neural layers with
respect to a (generally unknown) ground-truth symbolic representation of the
input data. We demonstrate that symbol correctness is a necessary property for
NS-DNN explainability and transfer learning (despite being in general
impossible to train for). Moreover, we show that the framework of symbol
correctness provides a precise way to reason and communicate about model
behavior at neural-symbolic boundaries, and gives insight into the fundamental
tradeoffs faced by NS-DNN training algorithms. In doing so, we both identify
significant points of ambiguity in prior work, and provide a framework to
support further NS-DNN developments.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03664" title="Abstract">arXiv:2402.03664</a> [<a href="/pdf/2402.03664" title="Download PDF">pdf</a>, <a href="/format/2402.03664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Solvers for Partial Gromov-Wasserstein
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yikun Bai</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+R+D">Rocio Diaz Martin</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hengrong Du</a>, 
<a href="/search/cs?searchtype=author&query=Shahbazi%2C+A">Ashkan Shahbazi</a>, 
<a href="/search/cs?searchtype=author&query=Kolouri%2C+S">Soheil Kolouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of
measures with unequal masses residing in potentially distinct metric spaces,
thereby enabling unbalanced and partial matching across these spaces. In this
paper, we demonstrate that the PGW problem can be transformed into a variant of
the Gromov-Wasserstein problem, akin to the conversion of the partial optimal
transport problem into an optimal transport problem. This transformation leads
to two new solvers, mathematically and computationally equivalent, based on the
Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We
further establish that the PGW problem constitutes a metric for metric measure
spaces. Finally, we validate the effectiveness of our proposed solvers in terms
of computation time and performance on shape-matching and positive-unlabeled
learning problems, comparing them against existing baselines.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03666" title="Abstract">arXiv:2402.03666</a> [<a href="/pdf/2402.03666" title="Download PDF">pdf</a>, <a href="/format/2402.03666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuEST: Low-bit Diffusion Model Quantization via Efficient Selective  Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Yuzhang Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have achieved remarkable success in image generation tasks,
yet their practical deployment is restrained by the high memory and time
consumption. While quantization paves a way for diffusion model compression and
acceleration, existing methods totally fail when the models are quantized to
low-bits. In this paper, we unravel three properties in quantized diffusion
models that compromise the efficacy of current methods: imbalanced activation
distributions, imprecise temporal information, and vulnerability to
perturbations of specific modules. To alleviate the intensified low-bit
quantization difficulty stemming from the distribution imbalance, we propose
finetuning the quantized model to better adapt to the activation distribution.
Building on this idea, we identify two critical types of quantized layers:
those holding vital temporal information and those sensitive to reduced
bit-width, and finetune them to mitigate performance degradation with
efficiency. We empirically verify that our approach modifies the activation
distribution and provides meaningful temporal information, facilitating easier
and more accurate quantization. Our method is evaluated over three
high-resolution image generation tasks and achieves state-of-the-art
performance under various bit-width settings, as well as being the first method
to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03667" title="Abstract">arXiv:2402.03667</a> [<a href="/pdf/2402.03667" title="Download PDF">pdf</a>, <a href="/format/2402.03667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as an Indirect Reasoner: Contrapositive and  Contradiction for Automated Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanfang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiliu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dapeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chen Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages,13 figures,4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, increasing attention has been focused drawn on to improve the
ability of Large Language Models (LLMs) to perform complex reasoning. However,
previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow
Direct Reasoning (DR) frameworks, so they will meet difficulty in solving
numerous real-world tasks which can hardly be solved via DR. Therefore, to
strengthen the reasoning power of LLMs, this paper proposes a novel Indirect
Reasoning (IR) method that employs the logic of contrapositives and
contradictions to tackle IR tasks such as factual reasoning and mathematic
proof. Specifically, our methodology comprises two steps. Firstly, we leverage
the logical equivalence of contrapositive to augment the data and rules to
enhance the comprehensibility of LLMs. Secondly, we design a set of prompt
templates to trigger LLMs to conduct IR based on proof by contradiction that is
logically equivalent to the original DR process. Our IR method is simple yet
effective and can be straightforwardly integrated with existing DR methods to
further boost the reasoning abilities of LLMs. The experimental results on
popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method
enhances the overall accuracy of factual reasoning by 27.33% and mathematical
proof by 31.43%, when compared with traditional DR methods. Moreover, the
methods combining IR and DR significantly outperform the methods solely using
IR or DR, further demonstrating the effectiveness of our strategy.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03669" title="Abstract">arXiv:2402.03669</a> [<a href="/pdf/2402.03669" title="Download PDF">pdf</a>, <a href="/format/2402.03669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis of Distributed Generalized Nash Equilibria Seeking  Algorithm with Asynchrony and Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huaqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+L">Liang Ran</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lifeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinhui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tingwen Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper considers a class of noncooperative games in which the feasible
decision sets of all players are coupled together by a coupled inequality
constraint. Adopting the variational inequality formulation of the game, we
first introduce a new local edge-based equilibrium condition and develop a
distributed primal-dual proximal algorithm with full information. Considering
challenges when communication delays occur, we devise an asynchronous
distributed algorithm to seek a generalized Nash equilibrium. This asynchronous
scheme arbitrarily activates one player to start new computations independently
at different iteration instants, which means that the picked player can use the
involved out-dated information from itself and its neighbors to perform new
updates. A distinctive attribute is that the proposed algorithms enable the
derivation of new distributed forward-backward-like extensions. In theoretical
aspect, we provide explicit conditions on algorithm parameters, for instance,
the step-sizes to establish a sublinear convergence rate for the proposed
synchronous algorithm. Moreover, the asynchronous algorithm guarantees almost
sure convergence in expectation under the same step-size conditions and some
standard assumptions. An interesting observation is that our analysis approach
improves the convergence rate of prior synchronous distributed
forward-backward-based algorithms. Finally, the viability and performance of
the proposed algorithms are demonstrated by numerical studies on the networked
Cournot competition.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03671" title="Abstract">arXiv:2402.03671</a> [<a href="/pdf/2402.03671" title="Download PDF">pdf</a>, <a href="/format/2402.03671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on  Multi-Core Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yi-Chien Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gobriel%2C+S">Sameh Gobriel</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+N">Nilesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+G+K">Gopi Krishna Jha</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE International Parallel and Distributed Processing Symposium (IPDPS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">As Graph Neural Networks (GNNs) become popular, libraries like
PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these
libraries have emerged as the de facto standard for implementing GNNs because
they provide graph-oriented APIs and are purposefully designed to manage the
inherent sparsity and irregularity in graph structures. However, these
libraries show poor scalability on multi-core processors, which under-utilizes
the available platform resources and limits the performance. This is because
GNN training is a resource-intensive workload with high volume of irregular
data accessing, and existing libraries fail to utilize the memory bandwidth
efficiently. To address this challenge, we propose ARGO, a novel runtime system
for GNN training that offers scalable performance. ARGO exploits
multi-processing and core-binding techniques to improve platform resource
utilization. We further develop an auto-tuner that searches for the optimal
configuration for multi-processing and core-binding. The auto-tuner works
automatically, making it completely transparent from the user. Furthermore, the
auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets,
etc. We evaluate ARGO on two representative GNN models and four widely-used
datasets on two platforms. With the proposed autotuner, ARGO is able to select
a near-optimal configuration by exploring only 5% of the design space. ARGO
speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a
four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids
machine with 64 cores, respectively. Finally, ARGO can seamlessly integrate
into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and
speed up GNN training.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03674" title="Abstract">arXiv:2402.03674</a> [<a href="/pdf/2402.03674" title="Download PDF">pdf</a>, <a href="/ps/2402.03674" title="Download PostScript">ps</a>, <a href="/format/2402.03674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum-Norm Error Estimates of Fourth-Order Compact and ADI Compact  Finite Difference Methods for Nonlinear Coupled Bacterial Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+S">Shusen Xie</a>, 
<a href="/search/math?searchtype=author&query=Fu%2C+H">Hongfei Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, by introducing two temporal-derivative-dependent auxiliary
variables, a linearized and decoupled fourth-order compact finite difference
method is developed and analyzed for the nonlinear coupled bacterial systems.
The temporal-spatial error splitting technique and discrete energy method are
employed to prove the unconditional stability and convergence of the method in
discrete maximum norm. Furthermore, to improve the computational efficiency, an
alternating direction implicit (ADI) compact difference algorithm is proposed,
and the unconditional stability and optimal-order maximum-norm error estimate
for the ADI scheme are also strictly established. Finally, several numerical
experiments are conducted to validate the theoretical convergence and to
simulate the phenomena of bacterial extinction as well as the formation of
endemic diseases.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03678" title="Abstract">arXiv:2402.03678</a> [<a href="/pdf/2402.03678" title="Download PDF">pdf</a>, <a href="/format/2402.03678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Specifications-guided Dynamic Task Sampling for Reinforcement  Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+Y">Yash Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenchang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sarathy%2C+V">Vasanth Sarathy</a>, 
<a href="/search/cs?searchtype=author&query=Velasquez%2C+A">Alvaro Velasquez</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+R">Robert Wright</a>, 
<a href="/search/cs?searchtype=author&query=Sinapov%2C+J">Jivko Sinapov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement Learning (RL) has made significant strides in enabling
artificial agents to learn diverse behaviors. However, learning an effective
policy often requires a large number of environment interactions. To mitigate
sample complexity issues, recent approaches have used high-level task
specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward
Machines (RM), to guide the learning progress of the agent. In this work, we
propose a novel approach, called Logical Specifications-guided Dynamic Task
Sampling (LSTS), that learns a set of RL policies to guide an agent from an
initial state to a goal state based on a high-level task specification, while
minimizing the number of environmental interactions. Unlike previous work, LSTS
does not assume information about the environment dynamics or the Reward
Machine, and dynamically samples promising tasks that lead to successful goal
policies. We evaluate LSTS on a gridworld and show that it achieves improved
time-to-threshold performance on complex sequential decision-making problems
compared to state-of-the-art RM and Automaton-guided RL baselines, such as
Q-Learning for Reward Machines and Compositional RL from logical Specifications
(DIRL). Moreover, we demonstrate that our method outperforms RM and
Automaton-guided RL baselines in terms of sample-efficiency, both in a
partially observable robotic task and in a continuous control robotic
manipulation task.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03681" title="Abstract">arXiv:2402.03681</a> [<a href="/pdf/2402.03681" title="Download PDF">pdf</a>, <a href="/format/2402.03681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhanyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Z">Zhou Xian</a>, 
<a href="/search/cs?searchtype=author&query=Biyik%2C+E">Erdem Biyik</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reward engineering has long been a challenge in Reinforcement Learning (RL)
research, as it often requires extensive human effort and iterative processes
of trial-and-error to design effective reward functions. In this paper, we
propose RL-VLM-F, a method that automatically generates reward functions for
agents to learn new tasks, using only a text description of the task goal and
the agent's visual observations, by leveraging feedbacks from vision language
foundation models (VLMs). The key to our approach is to query these models to
give preferences over pairs of the agent's image observations based on the text
description of the task goal, and then learn a reward function from the
preference labels, rather than directly prompting these models to output a raw
reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F
successfully produces effective rewards and policies across various domains -
including classic control, as well as manipulation of rigid, articulated, and
deformable objects - without the need for human supervision, outperforming
prior methods that use large pretrained models for reward generation under the
same assumptions.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03686" title="Abstract">arXiv:2402.03686</a> [<a href="/pdf/2402.03686" title="Download PDF">pdf</a>, <a href="/format/2402.03686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minds versus Machines: Rethinking Entailment Verification with Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Soumya Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tianyi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans make numerous inferences in text comprehension to understand
discourse. This paper aims to understand the commonalities and disparities in
the inference judgments between humans and state-of-the-art Large Language
Models (LLMs). Leveraging a comprehensively curated entailment verification
benchmark, we evaluate both human and LLM performance across various reasoning
categories. Our benchmark includes datasets from three categories (NLI,
contextual QA, and rationales) that include multi-sentence premises and
different knowledge types, thereby evaluating the inference capabilities in
complex reasoning instances. Notably, our findings reveal LLMs' superiority in
multi-hop reasoning across extended contexts, while humans excel in tasks
necessitating simple deductive reasoning. Leveraging these insights, we
introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with
GPT-4, offering a robust open-source solution for entailment verification. As a
practical application, we showcase the efficacy of our finetuned model in
enhancing self-consistency in model-generated explanations, resulting in a 6%
performance boost on average across three multiple-choice question-answering
datasets.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03687" title="Abstract">arXiv:2402.03687</a> [<a href="/pdf/2402.03687" title="Download PDF">pdf</a>, <a href="/format/2402.03687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pard: Permutation-Invariant Autoregressive Diffusion for Graph  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingxiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xueying Ding</a>, 
<a href="/search/cs?searchtype=author&query=Akoglu%2C+L">Leman Akoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Diffusion Model on Graphs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph generation has been dominated by autoregressive models due to their
simplicity and effectiveness, despite their sensitivity to ordering. Yet
diffusion models have garnered increasing attention, as they offer comparable
performance while being permutation-invariant. Current graph diffusion models
generate graphs in a one-shot fashion, but they require extra features and
thousands of denoising steps to achieve optimal performance. We introduce PARD,
a Permutation-invariant Auto Regressive Diffusion model that integrates
diffusion models with autoregressive methods. PARD harnesses the effectiveness
and efficiency of the autoregressive model while maintaining permutation
invariance without ordering sensitivity. Specifically, we show that contrary to
sets, elements in a graph are not entirely unordered and there is a unique
partial order for nodes and edges. With this partial order, PARD generates a
graph in a block-by-block, autoregressive fashion, where each block's
probability is conditionally modeled by a shared diffusion model with an
equivariant network. To ensure efficiency while being expressive, we further
propose a higher-order graph transformer, which integrates transformer with
PPGN. Like GPT, we extend the higher-order graph transformer to support
parallel training of all blocks. Without any extra features, PARD achieves
state-of-the-art performance on molecular and non-molecular datasets, and
scales to large datasets like MOSES containing 1.9M molecules.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03688" title="Abstract">arXiv:2402.03688</a> [<a href="/pdf/2402.03688" title="Download PDF">pdf</a>, <a href="/format/2402.03688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Privacy Threats and Defense in Vertical Federated Learning:  From Model Life Cycle Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Meng Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Changting Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+H">Haiqin Weng</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+Y">Yuseok Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+K">Ka-Ho Chow</a>, 
<a href="/search/cs?searchtype=author&query=Patterson%2C+S">Stacy Patterson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vertical Federated Learning (VFL) is a federated learning paradigm where
multiple participants, who share the same set of samples but hold different
features, jointly train machine learning models. Although VFL enables
collaborative machine learning without sharing raw data, it is still
susceptible to various privacy threats. In this paper, we conduct the first
comprehensive survey of the state-of-the-art in privacy attacks and defenses in
VFL. We provide taxonomies for both attacks and defenses, based on their
characterizations, and discuss open challenges and future research directions.
Specifically, our discussion is structured around the model's life cycle, by
delving into the privacy threats encountered during different stages of machine
learning and their corresponding countermeasures. This survey not only serves
as a resource for the research community but also offers clear guidance and
actionable insights for practitioners to safeguard data privacy throughout the
model's life cycle.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03690" title="Abstract">arXiv:2402.03690</a> [<a href="/pdf/2402.03690" title="Download PDF">pdf</a>, <a href="/format/2402.03690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3Doodle: Compact Abstraction of Objects with 3D Strokes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Changwoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaeah Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaesik Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+M">Young Min Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While free-hand sketching has long served as an efficient representation to
convey characteristics of an object, they are often subjective, deviating
significantly from realistic representations. Moreover, sketches are not
consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We
propose 3Dooole, generating descriptive and view-consistent sketch images given
multi-view images of the target object. Our method is based on the idea that a
set of 3D strokes can efficiently represent 3D structural information and
render view-consistent 2D sketches. We express 2D sketches as a union of
view-independent and view-dependent components. 3D cubic B ezier curves
indicate view-independent 3D feature lines, while contours of superquadrics
express a smooth outline of the volume of varying viewpoints. Our pipeline
directly optimizes the parameters of 3D stroke primitives to minimize
perceptual losses in a fully differentiable manner. The resulting sparse set of
3D strokes can be rendered as abstract sketches containing essential 3D
characteristic shapes of various objects. We demonstrate that 3Doodle can
faithfully express concepts of the original images compared with recent sketch
generation approaches.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03691" title="Abstract">arXiv:2402.03691</a> [<a href="/pdf/2402.03691" title="Download PDF">pdf</a>, <a href="/format/2402.03691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robots as Creative Collaborators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Shayla Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+W">Wendy Ju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This research explores whether the interaction between adversarial robots and
creative practitioners can push artists to rethink their initial ideas. It also
explores how working with these robots may influence artists' views of machines
designed for creative tasks or collaboration. Many existing robots developed
for creativity and the arts focus on complementing creative practices, but what
if robots challenged ideas instead? To begin investigating this, I designed
UnsTable, a robot drawing desk that moves the paper while participants (N=19)
draw to interfere with the process. This inquiry invites further research into
adversarial robots designed to challenge creative practitioners.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03694" title="Abstract">arXiv:2402.03694</a> [<a href="/pdf/2402.03694" title="Download PDF">pdf</a>, <a href="/format/2402.03694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shinan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shaowang%2C+T">Ted Shaowang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+G">Gerry Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+J">Jeewon Chae</a>, 
<a href="/search/cs?searchtype=author&query=Marques%2C+J">Jonatas Marques</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sanjay Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Feamster%2C+N">Nick Feamster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Network traffic analysis increasingly uses complex machine learning models as
the internet consolidates and traffic gets more encrypted. However, over
high-bandwidth networks, flows can easily arrive faster than model inference
rates. The temporal nature of network flows limits simple scale-out approaches
leveraged in other high-traffic machine learning applications. Accordingly,
this paper presents ServeFlow, a solution for machine-learning model serving
aimed at network traffic analysis tasks, which carefully selects the number of
packets to collect and the models to apply for individual flows to achieve a
balance between minimal latency, high service rate, and high accuracy. We
identify that on the same task, inference time across models can differ by
2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of
magnitude higher than the inference time! ServeFlow is able to make inferences
on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median
end-to-end serving latency while increasing the service rate and maintaining
similar accuracy. Even with thousands of features per flow, it achieves a
service rate of over 48.5k new flows per second on a 16-core CPU commodity
server, which matches the order of magnitude of flow rates observed on
city-level network backbones.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03697" title="Abstract">arXiv:2402.03697</a> [<a href="/pdf/2402.03697" title="Download PDF">pdf</a>, <a href="/format/2402.03697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHMC-Net: A Mask-guided Feature Fusion Network for Sperm Head Morphology  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+N">Nishchal Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yejia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sirui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peixian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D+Z">Danny Z Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A shorter version is published on ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Male infertility accounts for about one-third of global infertility cases.
Manual assessment of sperm abnormalities through head morphology analysis
encounters issues of observer variability and diagnostic discrepancies among
experts. Its alternative, Computer-Assisted Semen Analysis (CASA), suffers from
low-quality sperm images, small datasets, and noisy class labels. We propose a
new approach for sperm head morphology classification, called SHMC-Net, which
uses segmentation masks of sperm heads to guide the morphology classification
of sperm images. SHMC-Net generates reliable segmentation masks using image
priors, refines object boundaries with an efficient graph-based method, and
trains an image network with sperm head crops and a mask network with the
corresponding masks. In the intermediate stages of the networks, image and mask
features are fused with a fusion scheme to better learn morphological features.
To handle noisy class labels and regularize training on small datasets,
SHMC-Net applies Soft Mixup to combine mixup augmentation and a loss function.
We achieve state-of-the-art results on SCIAN and HuSHeM datasets, outperforming
methods that use additional pre-training or costly ensembling techniques.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03698" title="Abstract">arXiv:2402.03698</a> [<a href="/pdf/2402.03698" title="Download PDF">pdf</a>, <a href="/format/2402.03698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the Local Learning Coefficient at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furman%2C+Z">Zach Furman</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Edmund Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The \textit{local learning coefficient} (LLC) is a principled way of
quantifying model complexity, originally derived in the context of Bayesian
statistics using singular learning theory (SLT). Several methods are known for
numerically estimating the local learning coefficient, but so far these methods
have not been extended to the scale of modern deep learning architectures or
data sets. Using a method developed in {\tt <a href="/abs/2308.12108">arXiv:2308.12108</a> [stat.ML]} we
empirically show how the LLC may be measured accurately and self-consistently
for deep linear networks (DLNs) up to 100M parameters. We also show that the
estimated LLC has the rescaling invariance that holds for the theoretical
quantity.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03699" title="Abstract">arXiv:2402.03699</a> [<a href="/pdf/2402.03699" title="Download PDF">pdf</a>, <a href="/ps/2402.03699" title="Download PostScript">ps</a>, <a href="/format/2402.03699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Robotic Development through Collaborative Framework by Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luan%2C+Z">Zhirong Luan</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yujun Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Despite the remarkable code generation abilities of large language models
LLMs, they still face challenges in complex task handling. Robot development, a
highly intricate field, inherently demands human involvement in task allocation
and collaborative teamwork . To enhance robot development, we propose an
innovative automated collaboration framework inspired by real-world robot
developers. This framework employs multiple LLMs in distinct roles analysts,
programmers, and testers. Analysts delve deep into user requirements, enabling
programmers to produce precise code, while testers fine-tune the parameters
based on user feedback for practical robot application. Each LLM tackles
diverse, critical tasks within the development process. Clear collaboration
rules emulate real world teamwork among LLMs. Analysts, programmers, and
testers form a cohesive team overseeing strategy, code, and parameter
adjustments . Through this framework, we achieve complex robot development
without requiring specialized knowledge, relying solely on non experts
participation.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03700" title="Abstract">arXiv:2402.03700</a> [<a href="/pdf/2402.03700" title="Download PDF">pdf</a>, <a href="/format/2402.03700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenLens: A Systematic Evaluation of Visual GenAI Model Outputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tica Lin</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jui-Hsien Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in IEEE PacificVis 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid development of generative AI (GenAI) models in computer vision
necessitates effective evaluation methods to ensure their quality and fairness.
Existing tools primarily focus on dataset quality assurance and model
explainability, leaving a significant gap in GenAI output evaluation during
model development. Current practices often depend on developers' subjective
visual assessments, which may lack scalability and generalizability. This paper
bridges this gap by conducting a formative study with GenAI model developers in
an industrial setting. Our findings led to the development of GenLens, a visual
analytic interface designed for the systematic evaluation of GenAI model
outputs during the early stages of model development. GenLens offers a
quantifiable approach for overviewing and annotating failure cases, customizing
issue tags and classifications, and aggregating annotations from multiple users
to enhance collaboration. A user study with model developers reveals that
GenLens effectively enhances their workflow, evidenced by high satisfaction
rates and a strong intent to integrate it into their practices. This research
underscores the importance of robust early-stage evaluation tools in GenAI
development, contributing to the advancement of fair and high-quality GenAI
models.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03701" title="Abstract">arXiv:2402.03701</a> [<a href="/pdf/2402.03701" title="Download PDF">pdf</a>, <a href="/format/2402.03701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving and Unifying Discrete&amp;Continuous-time Discrete Denoising  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingxiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xueying Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Akoglu%2C+L">Leman Akoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Unify Discrete Denoising Diffusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Discrete diffusion models have seen a surge of attention with applications on
naturally discrete data such as language and graphs. Although discrete-time
discrete diffusion has been established for a while, only recently Campbell et
al. (2022) introduced the first framework for continuous-time discrete
diffusion. However, their training and sampling processes differ significantly
from the discrete-time version, necessitating nontrivial approximations for
tractability. In this paper, we first present a series of mathematical
simplifications of the variational lower bound that enable more accurate and
easy-to-optimize training for discrete diffusion. In addition, we derive a
simple formulation for backward denoising that enables exact and accelerated
sampling, and importantly, an elegant unification of discrete-time and
continuous-time discrete diffusion. Thanks to simpler analytical formulations,
both forward and now also backward probabilities can flexibly accommodate any
noise distribution, including different noise distributions for multi-element
objects. Experiments show that our proposed USD3 (for Unified Simplified
Discrete Denoising Diffusion) outperform all SOTA baselines on established
datasets. We open-source our unified code at
https://github.com/LingxiaoShawn/USD3.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03702" title="Abstract">arXiv:2402.03702</a> [<a href="/pdf/2402.03702" title="Download PDF">pdf</a>, <a href="/ps/2402.03702" title="Download PostScript">ps</a>, <a href="/format/2402.03702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning Spatial Provenance in Privacy-Constrained Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Manish Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+P">Pramsu Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Harshan%2C+J">J. Harshan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented in IEEE WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In Vehicle-to-Everything networks that involve multi-hop communication, the
Road Side Units (RSUs) typically aim to collect location information from the
participating vehicles to provide security and network diagnostics features.
While the vehicles commonly use the Global Positioning System (GPS) for
navigation, they may refrain from sharing their precise GPS coordinates with
the RSUs due to privacy concerns. Therefore, to jointly address the high
localization requirements by the RSUs as well as the vehicles' privacy, we
present a novel spatial-provenance framework wherein each vehicle uses Bloom
filters to embed their partial location information when forwarding the
packets. In this framework, the RSUs and the vehicles agree upon fragmenting
the coverage area into several smaller regions so that the vehicles can embed
the identity of their regions through Bloom filters. Given the probabilistic
nature of Bloom filters, we derive an analytical expression on the error-rates
in provenance recovery and then pose an optimization problem to choose the
underlying parameters. With the help of extensive simulation results, we show
that our method offers near-optimal Bloom filter parameters in learning spatial
provenance. Some interesting trade-offs between the communication-overhead,
spatial privacy of the vehicles and the error rates in provenance recovery are
also discussed.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03703" title="Abstract">arXiv:2402.03703</a> [<a href="/pdf/2402.03703" title="Download PDF">pdf</a>, <a href="/ps/2402.03703" title="Download PostScript">ps</a>, <a href="/format/2402.03703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Large Language Models in Cloud Edge End Architecture for  Heterogeneous Robot Cluster Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luan%2C+Z">Zhirong Luan</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yujun Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Despite their powerful semantic understanding and code generation
capabilities, Large Language Models (LLMs) still face challenges when dealing
with complex tasks. Multi agent strategy generation and motion control are
highly complex domains that inherently require experts from multiple fields to
collaborate. To enhance multi agent strategy generation and motion control, we
propose an innovative architecture that employs the concept of a cloud edge end
hierarchical structure. By leveraging multiple large language models with
distinct areas of expertise, we can efficiently generate strategies and perform
task decomposition. Introducing the cosine similarity approach,aligning task
decomposition instructions with robot task sequences at the vector level, we
can identify subtasks with incomplete task decomposition and iterate on them
multiple times to ultimately generate executable machine task sequences.The
robot is guided through these task sequences to complete tasks of higher
complexity. With this architecture, we implement the process of natural
language control of robots to perform complex tasks, and successfully address
the challenge of multi agent execution of open tasks in open scenarios and the
problem of task decomposition.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03704" title="Abstract">arXiv:2402.03704</a> [<a href="/pdf/2402.03704" title="Download PDF">pdf</a>, <a href="/format/2402.03704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WhisperFuzz: White-Box Fuzzing for Detecting and Locating Timing  Vulnerabilities in Processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borkar%2C+P">Pallavi Borkar</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rostami%2C+M">Mohamadreza Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+N">Nikhilesh Singh</a>, 
<a href="/search/cs?searchtype=author&query=Kande%2C+R">Rahul Kande</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+A">Ahmad-Reza Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Rebeiro%2C+C">Chester Rebeiro</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">Jeyavijayan Rajendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Timing vulnerabilities in processors have emerged as a potent threat. As
processors are the foundation of any computing system, identifying these flaws
is imperative. Recently fuzzing techniques, traditionally used for detecting
software vulnerabilities, have shown promising results for uncovering
vulnerabilities in large-scale hardware designs, such as processors.
Researchers have adapted black-box or grey-box fuzzing to detect timing
vulnerabilities in processors. However, they cannot identify the locations or
root causes of these timing vulnerabilities, nor do they provide coverage
feedback to enable the designer's confidence in the processor's security.
<br />To address the deficiencies of the existing fuzzers, we present
WhisperFuzz--the first white-box fuzzer with static analysis--aiming to detect
and locate timing vulnerabilities in processors and evaluate the coverage of
microarchitectural timing behaviors. WhisperFuzz uses the fundamental nature of
processors' timing behaviors, microarchitectural state transitions, to localize
timing vulnerabilities. WhisperFuzz automatically extracts microarchitectural
state transitions from a processor design at the register-transfer level (RTL)
and instruments the design to monitor the state transitions as coverage.
Moreover, WhisperFuzz measures the time a design-under-test (DUT) takes to
process tests, identifying any minor, abnormal variations that may hint at a
timing vulnerability. WhisperFuzz detects 12 new timing vulnerabilities across
advanced open-sourced RISC-V processors: BOOM, Rocket Core, and CVA6. Eight of
these violate the zero latency requirements of the Zkt extension and are
considered serious security vulnerabilities. Moreover, WhisperFuzz also
pinpoints the locations of the new and the existing vulnerabilities.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03705" title="Abstract">arXiv:2402.03705</a> [<a href="/pdf/2402.03705" title="Download PDF">pdf</a>, <a href="/format/2402.03705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FoolSDEdit: Deceptively Steering Your Edits Towards Targeted  Attribute-aware Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dongxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhihong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kui Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Guided image synthesis methods, like SDEdit based on the diffusion model,
excel at creating realistic images from user inputs such as stroke paintings.
However, existing efforts mainly focus on image quality, often overlooking a
key point: the diffusion model represents a data distribution, not individual
images. This introduces a low but critical chance of generating images that
contradict user intentions, raising ethical concerns. For example, a user
inputting a stroke painting with female characteristics might, with some
probability, get male faces from SDEdit. To expose this potential
vulnerability, we aim to build an adversarial attack forcing SDEdit to generate
a specific data distribution aligned with a specified attribute (e.g., female),
without changing the input's attribute characteristics. We propose the Targeted
Attribute Generative Attack (TAGA), using an attribute-aware objective function
and optimizing the adversarial noise added to the input stroke painting.
Empirical studies reveal that traditional adversarial noise struggles with
TAGA, while natural perturbations like exposure and motion blur easily alter
generated images' attributes. To execute effective attacks, we introduce
FoolSDEdit: We design a joint adversarial exposure and blur attack, adding
exposure and motion blur to the stroke painting and optimizing them together.
We optimize the execution strategy of various perturbations, framing it as a
network architecture search problem. We create the SuperPert, a graph
representing diverse execution strategies for different perturbations. After
training, we obtain the optimized execution strategy for effective TAGA against
SDEdit. Comprehensive experiments on two datasets show our method compelling
SDEdit to generate a targeted attribute-aware data distribution, significantly
outperforming baselines.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03706" title="Abstract">arXiv:2402.03706</a> [<a href="/pdf/2402.03706" title="Download PDF">pdf</a>, <a href="/format/2402.03706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature  Drone Threats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shenghai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+H">Thien Hoang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thien-Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianping Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lihua Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In response to the evolving challenges posed by small unmanned aerial
vehicles (UAVs), which possess the potential to transport harmful payloads or
independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal
Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat
detection methodologies by focusing on drone detection, UAV-type
classification, and trajectory estimation. MMAUD stands out by combining
diverse sensory inputs, including stereo vision, various Lidars, Radars, and
audio arrays. It offers a unique overhead aerial detection vital for addressing
real-world scenarios with higher fidelity than datasets captured on specific
vantage points using thermal and RGB. Additionally, MMAUD provides accurate
Leica-generated ground truth data, enhancing credibility and enabling confident
refinement of algorithms and models, which has never been seen in other
datasets. Most existing works do not disclose their datasets, making MMAUD an
invaluable resource for developing accurate and efficient solutions. Our
proposed modalities are cost-effective and highly adaptable, allowing users to
experiment and implement new UAV threat detection tools. Our dataset closely
simulates real-world scenarios by incorporating ambient heavy machinery sounds.
This approach enhances the dataset's applicability, capturing the exact
challenges faced during proximate vehicular operations. It is expected that
MMAUD can play a pivotal role in advancing UAV threat detection,
classification, trajectory estimation capabilities, and beyond. Our dataset,
codes, and designs will be available in https://github.com/ntu-aris/MMAUD.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03707" title="Abstract">arXiv:2402.03707</a> [<a href="/pdf/2402.03707" title="Download PDF">pdf</a>, <a href="/ps/2402.03707" title="Download PostScript">ps</a>, <a href="/format/2402.03707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLAs for 2-Seat STV Elections: Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blom%2C+M">Michelle Blom</a>, 
<a href="/search/cs?searchtype=author&query=Stuckey%2C+P+J">Peter J. Stuckey</a>, 
<a href="/search/cs?searchtype=author&query=Teague%2C+V">Vanessa Teague</a>, 
<a href="/search/cs?searchtype=author&query=Vukcevic%2C+D">Damjan Vukcevic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Single Transferable Vote (STV) elections are a principled approach to
electing multiple candidates in a single election. Each ballot has a starting
value of 1, and a candidate is elected if they gather a total vote value more
than a defined quota. Votes over the quota have their value reduced by a
transfer value so as to remove the quota, and are passed to the next candidate
on the ballot. Risk-limiting audits (RLAs) are a statistically sound approach
to election auditing which guarantees that failure to detect an error in the
result is bounded by a limit. A first approach to RLAs for 2-seat STV elections
has been defined. In this paper we show how we can improve this approach by
reasoning about lower bounds on transfer values, and how we can extend the
approach to partially audit an election, if the method does not support a full
audit.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03708" title="Abstract">arXiv:2402.03708</a> [<a href="/pdf/2402.03708" title="Download PDF">pdf</a>, <a href="/format/2402.03708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in  Panchromatic Satellite Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+P">Pengming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M">Mingjie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuanjia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guangjun He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jian Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-grained ship instance segmentation in satellite images holds
considerable significance for monitoring maritime activities at sea. However,
existing datasets often suffer from the scarcity of fine-grained information or
pixel-wise localization annotations, as well as the insufficient image
diversity and variations, thus limiting the research of this task. To this end,
we propose a benchmark dataset for fine-grained Ship Instance Segmentation in
Panchromatic satellite images, namely SISP, which contains 56,693
well-annotated ship instances with four fine-grained categories across 10,000
sliced images, and all the images are collected from SuperView-1 satellite with
the resolution of 0.5m. Targets in the proposed SISP dataset have
characteristics that are consistent with real satellite scenes, such as high
class imbalance, various scenes, large variations in target densities and
scales, and high inter-class similarity and intra-class diversity, all of which
make the SISP dataset more suitable for real-world applications. In addition,
we introduce a Dynamic Feature Refinement-assist Instance segmentation network,
namely DFRInst, as the benchmark method for ship instance segmentation in
satellite images, which can fortify the explicit representation of crucial
features, thus improving the performance of ship instance segmentation.
Experiments and analysis are performed on the proposed SISP dataset to evaluate
the benchmark method and several state-of-the-art methods to establish
baselines for facilitating future research. The proposed dataset and source
codes will be available at: https://github.com/Justlovesmile/SISP.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03714" title="Abstract">arXiv:2402.03714</a> [<a href="/pdf/2402.03714" title="Download PDF">pdf</a>, <a href="/format/2402.03714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Location-Invariant and Device-Agnostic Motion Activity  Recognition on Wearable Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adaimi%2C+R">Rebecca Adaimi</a>, 
<a href="/search/cs?searchtype=author&query=Bedri%2C+A">Abdelkareem Bedri</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+R">Richard Kang</a>, 
<a href="/search/cs?searchtype=author&query=Arreaza-Taylor%2C+J">Joanna Arreaza-Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Pascual%2C+G">Gerri-Michelle Pascual</a>, 
<a href="/search/cs?searchtype=author&query=Ralph%2C+M">Michael Ralph</a>, 
<a href="/search/cs?searchtype=author&query=Laput%2C+G">Gierad Laput</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Wearable sensors have permeated into people's lives, ushering impactful
applications in interactive systems and activity recognition. However,
practitioners face significant obstacles when dealing with sensing
heterogeneities, requiring custom models for different platforms. In this
paper, we conduct a comprehensive evaluation of the generalizability of motion
models across sensor locations. Our analysis highlights this challenge and
identifies key on-body locations for building location-invariant models that
can be integrated on any device. For this, we introduce the largest
multi-location activity dataset (N=50, 200 cumulative hours), which we make
publicly available. We also present deployable on-device motion models reaching
91.41% frame-level F1-score from a single model irrespective of sensor
placements. Lastly, we investigate cross-location data synthesis, aiming to
alleviate the laborious data collection tasks by synthesizing data in one
location given data from another. These contributions advance our vision of
low-barrier, location-invariant activity recognition systems, catalyzing
research in HCI and ubiquitous computing.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03715" title="Abstract">arXiv:2402.03715</a> [<a href="/pdf/2402.03715" title="Download PDF">pdf</a>, <a href="/format/2402.03715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clarify: Improving Model Robustness With Natural Language Corrections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+S">Michelle S. Lam</a>, 
<a href="/search/cs?searchtype=author&query=Vasconcelos%2C+H">Helena Vasconcelos</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+M+S">Michael S. Bernstein</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In supervised learning, models are trained to extract correlations from a
static dataset. This often leads to models that rely on high-level
misconceptions. To prevent such misconceptions, we must necessarily provide
additional information beyond the training data. Existing methods incorporate
forms of additional instance-level supervision, such as labels for spurious
features or additional labeled data from a balanced distribution. Such
strategies can become prohibitively costly for large-scale datasets since they
require additional annotation at a scale close to the original training data.
We hypothesize that targeted natural language feedback about a model's
misconceptions is a more efficient form of additional supervision. We introduce
Clarify, a novel interface and method for interactively correcting model
misconceptions. Through Clarify, users need only provide a short text
description to describe a model's consistent failure patterns. Then, in an
entirely automated way, we use such descriptions to improve the training
process by reweighting the training data or gathering additional targeted data.
Our user studies show that non-expert users can successfully describe model
misconceptions via Clarify, improving worst-group accuracy by an average of
17.1% in two datasets. Additionally, we use Clarify to find and rectify 31
novel hard subpopulations in the ImageNet dataset, improving minority-split
accuracy from 21.1% to 28.7%.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03716" title="Abstract">arXiv:2402.03716</a> [<a href="/pdf/2402.03716" title="Download PDF">pdf</a>, <a href="/format/2402.03716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Shape and Gait Representations Learning for Video-based  Cloth-Changing Person Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+D">Vuong D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mirza%2C+S">Samiha Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Mantini%2C+P">Pranav Mantini</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+K">Shishir K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current state-of-the-art Video-based Person Re-Identification (Re-ID)
primarily relies on appearance features extracted by deep learning models.
These methods are not applicable for long-term analysis in real-world scenarios
where persons have changed clothes, making appearance information unreliable.
In this work, we deal with the practical problem of Video-based Cloth-Changing
Person Re-ID (VCCRe-ID) by proposing "Attention-based Shape and Gait
Representations Learning" (ASGL) for VCCRe-ID. Our ASGL framework improves
Re-ID performance under clothing variations by learning clothing-invariant gait
cues using a Spatial-Temporal Graph Attention Network (ST-GAT). Given the
3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises
multi-head attention modules, which are able to enhance the robustness of gait
embeddings under viewpoint changes and occlusions. The ST-GAT amplifies the
important motion ranges and reduces the influence of noisy poses. Then, the
multi-head learning module effectively reserves beneficial local temporal
dynamics of movement. We also boost discriminative power of person
representations by learning body shape cues using a GAT. Experiments on two
large-scale VCCRe-ID datasets demonstrate that our proposed framework
outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in
mAP.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03719" title="Abstract">arXiv:2402.03719</a> [<a href="/pdf/2402.03719" title="Download PDF">pdf</a>, <a href="/format/2402.03719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Language Models with Active Inquiry for Deeper Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jing-Cheng Pang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Heng-Bo Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jia-Hao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+N">Nan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Si-Hang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chengxing Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng-Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rise of large language models (LLMs) has revolutionized the way that we
interact with artificial intelligence systems through natural language.
However, LLMs often misinterpret user queries because of their uncertain
intention, leading to less helpful responses. In natural human interactions,
clarification is sought through targeted questioning to uncover obscure
information. Thus, in this paper, we introduce LaMAI (Language Model with
Active Inquiry), designed to endow LLMs with this same level of interactive
engagement. LaMAI leverages active learning techniques to raise the most
informative questions, fostering a dynamic bidirectional dialogue. This
approach not only narrows the contextual gap but also refines the output of the
LLMs, aligning it more closely with user expectations. Our empirical studies,
across a variety of complex datasets where LLMs have limited conversational
context, demonstrate the effectiveness of LaMAI. The method improves answer
accuracy from 31.9% to 50.9%, outperforming other leading question-answering
frameworks. Moreover, in scenarios involving human participants, LaMAI
consistently generates responses that are superior or comparable to baseline
methods in more than 82% of the cases. The applicability of LaMAI is further
evidenced by its successful integration with various LLMs, highlighting its
potential for the future of interactive language models.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03720" title="Abstract">arXiv:2402.03720</a> [<a href="/pdf/2402.03720" title="Download PDF">pdf</a>, <a href="/format/2402.03720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Similarity-based Neighbor Selection for Graph LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Text-attributed graphs (TAGs) present unique challenges for direct processing
by Language Learning Models (LLMs), yet their extensive commonsense knowledge
and robust reasoning capabilities offer great promise for node classification
in TAGs. Prior research in this field has grappled with issues such as
over-squashing, heterophily, and ineffective graph information integration,
further compounded by inconsistencies in dataset partitioning and
underutilization of advanced LLMs. To address these challenges, we introduce
Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor
selection techniques, SNS effectively improves the quality of selected
neighbors, thereby improving graph representation and alleviating issues like
over-squashing and heterophily. Besides, as an inductive and training-free
approach, SNS demonstrates superior generalization and scalability over
traditional GNN methods. Our comprehensive experiments, adhering to standard
dataset partitioning practices, demonstrate that SNS, through simple prompt
interactions with LLMs, consistently outperforms vanilla GNNs and achieves
state-of-the-art results on datasets like PubMed in node classification,
showcasing LLMs' potential in graph structure understanding. Our research
further underscores the significance of graph structure integration in LLM
applications and identifies key factors for their success in node
classification. Code is available at https://github.com/ruili33/SNS.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03721" title="Abstract">arXiv:2402.03721</a> [<a href="/pdf/2402.03721" title="Download PDF">pdf</a>, <a href="/format/2402.03721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Embodied Object Detection through Language-Image Pre-training  and Implicit Object Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chapman%2C+N+H">Nicolas Harvey Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Dayoub%2C+F">Feras Dayoub</a>, 
<a href="/search/cs?searchtype=author&query=Browne%2C+W">Will Browne</a>, 
<a href="/search/cs?searchtype=author&query=Lehnert%2C+C">Chris Lehnert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Deep-learning and large scale language-image training have produced image
object detectors that generalise well to diverse environments and semantic
classes. However, single-image object detectors trained on internet data are
not optimally tailored for the embodied conditions inherent in robotics.
Instead, robots must detect objects from complex multi-modal data streams
involving depth, localisation and temporal correlation, a task termed embodied
object detection. Paradigms such as Video Object Detection (VOD) and Semantic
Mapping have been proposed to leverage such embodied data streams, but existing
work fails to enhance performance using language-image training. In response,
we investigate how an image object detector pre-trained using language-image
data can be extended to perform embodied object detection. We propose a novel
implicit object memory that uses projective geometry to aggregate the features
of detected objects across long temporal horizons. The spatial and temporal
information accumulated in memory is then used to enhance the image features of
the base detector. When tested on embodied data streams sampled from diverse
indoor scenes, our approach improves the base object detector by 3.09 mAP,
outperforming alternative external memories designed for VOD and Semantic
Mapping. Our method also shows a significant improvement of 16.90 mAP relative
to baselines that perform embodied object detection without first training on
language-image data, and is robust to sensor noise and domain shift experienced
in real-world deployment.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03723" title="Abstract">arXiv:2402.03723</a> [<a href="/pdf/2402.03723" title="Download PDF">pdf</a>, <a href="/format/2402.03723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rivero%2C+A">Alfredo Rivero</a>, 
<a href="/search/cs?searchtype=author&query=Athar%2C+S">ShahRukh Athar</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Z">Zhixin Shu</a>, 
<a href="/search/cs?searchtype=author&query=Samaras%2C+D">Dimitris Samaras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
<a href="http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03726" title="Abstract">arXiv:2402.03726</a> [<a href="/pdf/2402.03726" title="Download PDF">pdf</a>, <a href="/format/2402.03726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Granger Causality from Instance-wise Self-attentive Hawkes  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Id%C3%A9%2C+T">Tsuyoshi Id&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Aur&#xe9;lie Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Kollias%2C+G">Georgios Kollias</a>, 
<a href="/search/cs?searchtype=author&query=Navr%C3%A1til%2C+J">Ji&#x159;&#xed; Navr&#xe1;til</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+N">Naoki Abe</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We address the problem of learning Granger causality from asynchronous,
interdependent, multi-type event sequences. In particular, we are interested in
discovering instance-level causal structures in an unsupervised manner.
Instance-level causality identifies causal relationships among individual
events, providing more fine-grained information for decision-making. Existing
work in the literature either requires strong assumptions, such as linearity in
the intensity function, or heuristically defined model parameters that do not
necessarily meet the requirements of Granger causality. We propose
Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning
framework that can directly infer the Granger causality at the event instance
level. ISAHP is the first neural point process model that meets the
requirements of Granger causality. It leverages the self-attention mechanism of
the transformer to align with the principles of Granger causality. We
empirically demonstrate that ISAHP is capable of discovering complex
instance-level causal structures that cannot be handled by classical models. We
also show that ISAHP achieves state-of-the-art performance in proxy tasks
involving type-level causal discovery and instance-level event type prediction.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03728" title="Abstract">arXiv:2402.03728</a> [<a href="/pdf/2402.03728" title="Download PDF">pdf</a>, <a href="/format/2402.03728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Joint Decision-Making with Heterogeneous Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faghihi%2C+H+R">Hossein Rajaby Faghihi</a>, 
<a href="/search/cs?searchtype=author&query=Kordjamshidi%2C+P">Parisa Kordjamshidi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Findings - Short Paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">This paper introduces a novel decision-making framework that promotes
consistency among decisions made by diverse models while utilizing external
knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map
predictions from various models into globally normalized and comparable values
by incorporating information about decisions' prior probability, confidence
(uncertainty), and the models' expected accuracy. Our empirical study
demonstrates the superiority of our approach over conventional baselines on
multiple datasets.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03731" title="Abstract">arXiv:2402.03731</a> [<a href="/pdf/2402.03731" title="Download PDF">pdf</a>, <a href="/format/2402.03731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a positive-preserving, energy-stable numerical scheme to mass-action  kinetics with detailed balance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Chun Liu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we provide a detailed theoretical analysis of the numerical
scheme introduced in J. Comput. Phys. 436 (2021) 110253 for the reaction
kinetics of a class of chemical reaction networks that satisfies detailed
balance condition. In contrast to conventional numerical approximations, which
are typically constructed based on ordinary differential equations (ODEs) for
the concentrations of all involved species, the scheme is developed using the
equations of reaction trajectories, which can be viewed as a generalized
gradient flow of physically relevant free energy. The unique solvability,
positivity-preserving, and energy-stable properties are proved for the general
case involving multiple reactions, under a mild condition on the stoichiometric
matrix.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03732" title="Abstract">arXiv:2402.03732</a> [<a href="/pdf/2402.03732" title="Download PDF">pdf</a>, <a href="/format/2402.03732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Outdated Fact Detection in Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+H">Huiling Tu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shuo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Saikrishna%2C+V">Vidya Saikrishna</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Feng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Verspoor%2C+K">Karin Verspoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Data Mining Workshops
  (ICDMW), December 1-4, 2023, Shanghai, China
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Digital Libraries (cs.DL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge graphs (KGs) have garnered significant attention for their vast
potential across diverse domains. However, the issue of outdated facts poses a
challenge to KGs, affecting their overall quality as real-world information
evolves. Existing solutions for outdated fact detection often rely on manual
recognition. In response, this paper presents DEAN (Deep outdatEd fAct
detectioN), a novel deep learning-based framework designed to identify outdated
facts within KGs. DEAN distinguishes itself by capturing implicit structural
information among facts through comprehensive modeling of both entities and
relations. To effectively uncover latent out-of-date information, DEAN employs
a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph,
weighted by the number of entities. Experimental results demonstrate the
effectiveness and superiority of DEAN over state-of-the-art baseline methods.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03735" title="Abstract">arXiv:2402.03735</a> [<a href="/pdf/2402.03735" title="Download PDF">pdf</a>, <a href="/format/2402.03735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Utility of ChatGPT in the Issue Tracking System: An  Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+J+K">Joy Krishan Das</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Saikat Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+C+K">Chanchal K.Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in MSR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Issue tracking systems serve as the primary tool for incorporating external
users and customizing a software project to meet the users' requirements.
However, the limited number of contributors and the challenge of identifying
the best approach for each issue often impede effective resolution. Recently,
an increasing number of developers are turning to AI tools like ChatGPT to
enhance problem-solving efficiency. While previous studies have demonstrated
the potential of ChatGPT in areas such as automatic program repair, debugging,
and code generation, there is a lack of study on how developers explicitly
utilize ChatGPT to resolve issues in their tracking system. Hence, this study
aims to examine the interaction between ChatGPT and developers to analyze their
prevalent activities and provide a resolution. In addition, we assess the code
reliability by confirming if the code produced by ChatGPT was integrated into
the project's codebase using the clone detection tool NiCad. Our investigation
reveals that developers mainly use ChatGPT for brainstorming solutions but
often opt to write their code instead of using ChatGPT-generated code, possibly
due to concerns over the generation of "hallucinated code", as highlighted in
the literature.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03736" title="Abstract">arXiv:2402.03736</a> [<a href="/pdf/2402.03736" title="Download PDF">pdf</a>, <a href="/format/2402.03736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Effective Branch-and-Bound Algorithm with New Bounding Methods for  the Maximum $s$-Bundle Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jinghui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiongzhi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingming Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG)

</div>
<p class="mathjax">The Maximum s-Bundle Problem (MBP) addresses the task of identifying a
maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if
its vertex connectivity is at least |V|-s, where the vertex connectivity equals
the minimum number of vertices whose deletion yields a disconnected or trivial
graph. MBP is NP-hard and holds relevance in numerous realworld scenarios
emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the
branch-and-bound (BnB) framework, whose performance heavily depends on the
quality of the upper bound on the cardinality of a maximum s-bundle and the
initial lower bound with graph reduction. In this work, we introduce a novel
Partition-based Upper Bound (PUB) that leverages the graph partitioning
technique to achieve a tighter upper bound compared to existing ones. To
increase the lower bound, we propose to do short random walks on a clique to
generate larger initial solutions. Then, we propose a new BnB algorithm that
uses the initial lower bound and PUB in preprocessing for graph reduction, and
uses PUB in the BnB search process for branch pruning. Extensive experiments
with diverse s values demonstrate the significant progress of our algorithm
over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can
also be generalized to other relaxation clique problems.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03737" title="Abstract">arXiv:2402.03737</a> [<a href="/pdf/2402.03737" title="Download PDF">pdf</a>, <a href="/ps/2402.03737" title="Download PostScript">ps</a>, <a href="/format/2402.03737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private High Dimensional Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+A">Apurv Shukla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Systems and Control (eess.SY); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider a high-dimensional stochastic contextual linear bandit problem
when the parameter vector is $s_{0}$-sparse and the decision maker is subject
to privacy constraints under both central and local models of differential
privacy. We present PrivateLASSO, a differentially private LASSO bandit
algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse
hard-thresholding-based privacy mechanism and (ii) an episodic thresholding
rule for identifying the support of the parameter $\theta$. We prove minimax
private lower bounds and establish privacy and utility guarantees for
PrivateLASSO for the central model under standard assumptions.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03738" title="Abstract">arXiv:2402.03738</a> [<a href="/pdf/2402.03738" title="Download PDF">pdf</a>, <a href="/format/2402.03738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AoSRNet: All-in-One Scene Recovery Networks via Multi-knowledge  Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R+W">Ryan Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yu Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scattering and attenuation of light in no-homogeneous imaging media or
inconsistent light intensity will cause insufficient contrast and color
distortion in the collected images, which limits the developments such as
vision-driven smart urban, autonomous vehicles, and intelligent robots. In this
paper, we propose an all-in-one scene recovery network via multi-knowledge
integration (termed AoSRNet) to improve the visibility of imaging devices in
typical low-visibility imaging scenes (e.g., haze, sand dust, and low light).
It combines gamma correction (GC) and optimized linear stretching (OLS) to
create the detail enhancement module (DEM) and color restoration module (CRM).
Additionally, we suggest a multi-receptive field extraction module (MEM) to
attenuate the loss of image texture details caused by GC nonlinear and OLS
linear transformations. Finally, we refine the coarse features generated by
DEM, CRM, and MEM through Encoder-Decoder to generate the final restored image.
Comprehensive experimental results demonstrate the effectiveness and stability
of AoSRNet compared to other state-of-the-art methods. The source code is
available at \url{https://github.com/LouisYuxuLu/AoSRNet}.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03740" title="Abstract">arXiv:2402.03740</a> [<a href="/pdf/2402.03740" title="Download PDF">pdf</a>, <a href="/format/2402.03740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+M">Mohammad Majid Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Bhuiyan%2C+N+S">Navid Shadman Bhuiyan</a>, 
<a href="/search/cs?searchtype=author&query=Masood%2C+R">Rahat Masood</a>, 
<a href="/search/cs?searchtype=author&query=Ikram%2C+M">Muhammad Ikram</a>, 
<a href="/search/cs?searchtype=author&query=Kanhere%2C+S+S">Salil S. Kanhere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">The detection of automated accounts, also known as "social bots", has been an
increasingly important concern for online social networks (OSNs). While several
methods have been proposed for detecting social bots, significant research gaps
remain. First, current models exhibit limitations in detecting sophisticated
bots that aim to mimic genuine OSN users. Second, these methods often rely on
simplistic profile features, which are susceptible to manipulation. In addition
to their vulnerability to adversarial manipulations, these models lack
generalizability, resulting in subpar performance when trained on one dataset
and tested on another.
<br />To address these challenges, we propose a novel framework for social Bot
detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework
leverages contrastive learning to distinguish between social bots and humans in
the embedding space to improve linear separability. The high-level
representations derived by BotSSCL enhance its resilience to variations in data
distribution and ensure generalizability. We evaluate BotSSCL's robustness
against adversarial attempts to manipulate bot accounts to evade detection.
Experiments on two datasets featuring sophisticated bots demonstrate that
BotSSCL outperforms other supervised, unsupervised, and self-supervised
baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance
than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when
trained on one dataset and tested with another, demonstrating its
generalizability. Lastly, BotSSCL increases adversarial complexity and only
allows 4% success to the adversary in evading detection.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03741" title="Abstract">arXiv:2402.03741</a> [<a href="/pdf/2402.03741" title="Download PDF">pdf</a>, <a href="/format/2402.03741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent  Reinforcement Learning Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+O">Oubo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yuwen Pu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Linkang Du</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recent advances in multi-agent reinforcement learning (MARL) have opened up
vast application prospects, including swarm control of drones, collaborative
manipulation by robotic arms, and multi-target encirclement. However, potential
security threats during the MARL deployment need more attention and thorough
investigation. Recent researches reveal that an attacker can rapidly exploit
the victim's vulnerabilities and generate adversarial policies, leading to the
victim's failure in specific tasks. For example, reducing the winning rate of a
superhuman-level Go AI to around 20%. They predominantly focus on two-player
competitive environments, assuming attackers possess complete global state
observation.
<br />In this study, we unveil, for the first time, the capability of attackers to
generate adversarial policies even when restricted to partial observations of
the victims in multi-agent competitive environments. Specifically, we propose a
novel black-box attack (SUB-PLAY), which incorporates the concept of
constructing multiple subgames to mitigate the impact of partial observability
and suggests the sharing of transitions among subpolicies to improve the
exploitative ability of attackers. Extensive evaluations demonstrate the
effectiveness of SUB-PLAY under three typical partial observability
limitations. Visualization results indicate that adversarial policies induce
significantly different activations of the victims' policy networks.
Furthermore, we evaluate three potential defenses aimed at exploring ways to
mitigate security threats posed by adversarial policies, providing constructive
recommendations for deploying MARL in competitive environments.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03744" title="Abstract">arXiv:2402.03744</a> [<a href="/pdf/2402.03744" title="Download PDF">pdf</a>, <a href="/format/2402.03744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSIDE: LLMs&#x27; Internal States Retain the Power of Hallucination  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingyuan Tao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zhihang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jieping Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge hallucination have raised widespread concerns for the security and
reliability of deployed LLMs. Previous efforts in detecting hallucinations have
been employed at logit-level uncertainty estimation or language-level
self-consistency evaluation, where the semantic information is inevitably lost
during the token-decoding procedure. Thus, we propose to explore the dense
semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates
for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular,
a simple yet effective \textbf{EigenScore} metric is proposed to better
evaluate responses' self-consistency, which exploits the eigenvalues of
responses' covariance matrix to measure the semantic consistency/diversity in
the dense embedding space. Furthermore, from the perspective of self-consistent
hallucination detection, a test time feature clipping approach is explored to
truncate extreme activations in the internal states, which reduces
overconfident generations and potentially benefits the detection of
overconfident hallucinations. Extensive experiments and ablation studies are
performed on several popular LLMs and question-answering (QA) benchmarks,
showing the effectiveness of our proposal.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03746" title="Abstract">arXiv:2402.03746</a> [<a href="/pdf/2402.03746" title="Download PDF">pdf</a>, <a href="/format/2402.03746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Large Multimodal Models for Videos using Reinforcement Learning  from AI Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+D">Daechul Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yura Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jonghyun Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in large language models have influenced the development
of video large multimodal models (VLMMs). The previous approaches for VLMMs
involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets,
integrating LLM with visual encoders, and adding additional learnable modules.
Video and text multimodal alignment remains challenging, primarily due to the
deficient volume and quality of multimodal instruction-tune data compared to
text-only data. We present a novel alignment strategy that employs multimodal
AI system to oversee itself called Reinforcement Learning from AI Feedback
(RLAIF), providing self-preference feedback to refine itself and facilitating
the alignment of video and text modalities. In specific, we propose
context-aware reward modeling by providing detailed video descriptions as
context during the generation of preference feedback in order to enrich the
understanding of video content. Demonstrating enhanced performance across
diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms
existing approaches, including the SFT model. We commit to open-sourcing our
code, models, and datasets to foster further research in this area.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03747" title="Abstract">arXiv:2402.03747</a> [<a href="/pdf/2402.03747" title="Download PDF">pdf</a>, <a href="/ps/2402.03747" title="Download PostScript">ps</a>, <a href="/format/2402.03747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An invariance constrained deep learning network for PDE discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaowei Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The discovery of partial differential equations (PDEs) from datasets has
attracted increased attention. However, the discovery of governing equations
from sparse data with high noise is still very challenging due to the
difficulty of derivatives computation and the disturbance of noise. Moreover,
the selection principles for the candidate library to meet physical laws need
to be further studied. The invariance is one of the fundamental laws for
governing equations. In this study, we propose an invariance constrained deep
learning network (ICNet) for the discovery of PDEs. Considering that temporal
and spatial translation invariance (Galilean invariance) is a fundamental
property of physical laws, we filter the candidates that cannot meet the
requirement of the Galilean transformations. Subsequently, we embedded the
fixed and possible terms into the loss function of neural network,
significantly countering the effect of sparse data with high noise. Then, by
filtering out redundant terms without fixing learnable parameters during the
training process, the governing equations discovered by the ICNet method can
effectively approximate the real governing equations. We select the 2D Burgers
equation, the equation of 2D channel flow over an obstacle, and the equation of
3D intracranial aneurysm as examples to verify the superiority of the ICNet for
fluid mechanics. Furthermore, we extend similar invariance methods to the
discovery of wave equation (Lorentz Invariance) and verify it through Single
and Coupled Klein-Gordon equation. The results show that the ICNet method with
physical constraints exhibits excellent performance in governing equations
discovery from sparse and noisy data.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03748" title="Abstract">arXiv:2402.03748</a> [<a href="/pdf/2402.03748" title="Download PDF">pdf</a>, <a href="/format/2402.03748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Succinct Data Structure for Chordal Graphs with Bounded Vertex Leafage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balakrishnan%2C+G">Girish Balakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Sankardeep Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Narayanaswamy%2C+N+S">N S Narayanaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Sadakane%2C+K">Kunihiko Sadakane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Chordal graphs is a well-studied large graph class that is also a strict
super-class of path graphs. Munro and Wu (ISAAC 2018) have given an
$(n^2/4+o(n^2))-$bit succinct representation for $n-$vertex unlabeled chordal
graphs. A chordal graph $G=(V,E)$ is the intersection graph of sub-trees of a
tree $T$. Based on this characterization, the two parameters of chordal graphs
which we consider in this work are \textit{leafage}, introduced by Lin, McKee
and West (Discussiones Mathematicae Graph Theory 1998) and \textit{vertex
leafage}, introduced by Chaplick and Stacho (Discret. Appl. Math. 2014).
Leafage is the minimum number of leaves in any possible tree $T$ characterizing
$G$. Let $L(u)$ denote the number of leaves of the sub-tree in $T$
corresponding to $u \in V$ and $k=\max\limits_{u \in V} L(u)$. The smallest $k$
for which there exists a tree $T$ for $G$ is called its vertex leafage.
<br />In this work, we improve the worst-case information theoretic lower bound of
Munro and Wu (ISAAC 2018) for chordal graphs when vertex leafage is bounded and
leafage is unbounded. The class of unlabeled $k-$vertex leafage chordal graphs
that consists of all chordal graphs with vertex leafage at most $k$ and
unbounded leafage, denoted $\mathcal{G}_k$, is introduced for the first time.
For $k&gt;1$ in $o(n/\log n)$, we obtain a lower bound of $((k-1)n \log n - kn
\log k - O(\log n))-$bits on the size of any data structure that encodes a
graph in $\mathcal{G}_k$. Further, for every $k-$vertex leafage chordal graph
$G$ such that $k&gt;1$ in $o(n/\log n)$, we present a $((k-1)n \log n + o(kn \log
n))-$bit data structure, constructed using the succinct data structure for path
graphs with $kn/2$ vertices. Our data structure supports adjacency query in
$O(k \log n)$ time and using additional $2n \log n$ bits, an $O(k^2 d_v \log n
+ \log^2 n)$ time neighbourhood query where $d_v$ is degree of $v \in V$.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03749" title="Abstract">arXiv:2402.03749</a> [<a href="/pdf/2402.03749" title="Download PDF">pdf</a>, <a href="/format/2402.03749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision Superalignment: Weak-to-Strong Generalization for Vision  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in large language models have sparked interest in their
extraordinary and near-superhuman capabilities, leading researchers to explore
methods for evaluating and optimizing these abilities, which is called
superalignment. In this context, our paper delves into the realm of vision
foundation models, focusing on the concept of weak-to-strong generalization,
which involves using a weaker model to supervise a stronger one, aiming to
enhance the latter's capabilities beyond the former's limits. We introduce a
novel and adaptively adjustable loss function for weak-to-strong supervision.
Our comprehensive experiments span various scenarios, including few-shot
learning, transfer learning, noisy label learning, and common knowledge
distillation settings. The results are striking: our approach not only exceeds
the performance benchmarks set by strong-to-strong generalization but also
surpasses the outcomes of fine-tuning strong models with whole datasets. This
compelling evidence underscores the significant potential of weak-to-strong
generalization, showcasing its capability to substantially elevate the
performance of vision foundation models. The code is available at
https://github.com/ggjy/vision_weak_to_strong.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03750" title="Abstract">arXiv:2402.03750</a> [<a href="/pdf/2402.03750" title="Download PDF">pdf</a>, <a href="/format/2402.03750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+M">Mingliang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+A">Achhardeep Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Feng Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 7th IEEE International Conference on Data Science and Systems
  (DSS), Dec 20 - 22, 2021, Haikou, China
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">With the arrival of the big data era, mobility profiling has become a viable
method of utilizing enormous amounts of mobility data to create an intelligent
transportation system. Mobility profiling can extract potential patterns in
urban traffic from mobility data and is critical for a variety of
traffic-related applications. However, due to the high level of complexity and
the huge amount of data, mobility profiling faces huge challenges. Digital Twin
(DT) technology paves the way for cost-effective and performance-optimised
management by digitally creating a virtual representation of the network to
simulate its behaviour. In order to capture the complex spatio-temporal
features in traffic scenario, we construct alignment diagrams to assist in
completing the spatio-temporal correlation representation and design dilated
alignment convolution network (DACN) to learn the fine-grained correlations,
i.e., spatio-temporal interactions. We propose a digital twin mobility
profiling (DTMP) framework to learn node profiles on a mobility network DT
model. Extensive experiments have been conducted upon three real-world
datasets. Experimental results demonstrate the effectiveness of DTMP.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03752" title="Abstract">arXiv:2402.03752</a> [<a href="/pdf/2402.03752" title="Download PDF">pdf</a>, <a href="/format/2402.03752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training of Lightweight Vision Transformers on Small Datasets with  Minimally Scaled Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J+H">Jen Hong Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Can a lightweight Vision Transformer (ViT) match or exceed the performance of
Convolutional Neural Networks (CNNs) like ResNet on small datasets with small
image resolutions? This report demonstrates that a pure ViT can indeed achieve
superior performance through pre-training, using a masked auto-encoder
technique with minimal image scaling. Our experiments on the CIFAR-10 and
CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters
and a multiply-accumulate (MAC) count below 0.27G, qualifying them as
'lightweight' models. Unlike previous approaches, our method attains
state-of-the-art performance among similar lightweight transformer-based
architectures without significantly scaling up images from CIFAR-10 and
CIFAR-100. This achievement underscores the efficiency of our model, not only
in handling small datasets but also in effectively processing images close to
their original scale.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03753" title="Abstract">arXiv:2402.03753</a> [<a href="/pdf/2402.03753" title="Download PDF">pdf</a>, <a href="/format/2402.03753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced sampling of robust molecular datasets with uncertainty-based  collective variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+A+R">Aik Rui Tan</a>, 
<a href="/search/cs?searchtype=author&query=Dietschreit%2C+J+C+B">Johannes C. B. Dietschreit</a>, 
<a href="/search/cs?searchtype=author&query=Gomez-Bombarelli%2C+R">Rafael Gomez-Bombarelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures, 10 pages of Supplementary Information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Generating a data set that is representative of the accessible configuration
space of a molecular system is crucial for the robustness of machine learned
interatomic potentials (MLIP). However, the complexity of molecular systems,
characterized by intricate potential energy surfaces (PESs) with numerous local
minima and energy barriers, presents a significant challenge. Traditional
methods of data generation, such as random sampling or exhaustive exploration,
are either intractable or may not capture rare, but highly informative
configurations. In this study, we propose a method that leverages uncertainty
as the collective variable (CV) to guide the acquisition of chemically-relevant
data points, focusing on regions of the configuration space where ML model
predictions are most uncertain. This approach employs a Gaussian Mixture
Model-based uncertainty metric from a single model as the CV for biased
molecular dynamics simulations. The effectiveness of our approach in overcoming
energy barriers and exploring unseen energy minima, thereby enhancing the data
set in an active learning framework, is demonstrated on the alanine dipeptide
benchmark system.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03754" title="Abstract">arXiv:2402.03754</a> [<a href="/pdf/2402.03754" title="Download PDF">pdf</a>, <a href="/format/2402.03754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intensive Vision-guided Network for Radiology Report Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Fudan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ying Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weijiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+N">Nong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yutong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Physics in Medicine &amp; Biology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automatic radiology report generation is booming due to its huge application
potential for the healthcare industry. However, existing computer vision and
natural language processing approaches to tackle this problem are limited in
two aspects. First, when extracting image features, most of them neglect
multi-view reasoning in vision and model single-view structure of medical
images, such as space-view or channel-view. However, clinicians rely on
multi-view imaging information for comprehensive judgment in daily clinical
diagnosis. Second, when generating reports, they overlook context reasoning
with multi-modal information and focus on pure textual optimization utilizing
retrieval-based methods. We aim to address these two issues by proposing a
model that better simulates clinicians' perspectives and generates more
accurate reports. Given the above limitation in feature extraction, we propose
a Globally-intensive Attention (GIA) module in the medical image encoder to
simulate and integrate multi-view vision perception. GIA aims to learn three
types of vision perception: depth view, space view, and pixel view. On the
other hand, to address the above problem in report generation, we explore how
to involve multi-modal signals to generate precisely matched reports, i.e., how
to integrate previously predicted words with region-aware visual content in
next word prediction. Specifically, we design a Visual Knowledge-guided Decoder
(VKGD), which can adaptively consider how much the model needs to rely on
visual information and previously predicted text to assist next word
prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework
includes a GIA-guided Visual Encoder and the VKGD. Experiments on two
commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability
of our method compared with other state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03755" title="Abstract">arXiv:2402.03755</a> [<a href="/pdf/2402.03755" title="Download PDF">pdf</a>, <a href="/format/2402.03755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Saizhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+L+M">Lionel M. Ni</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Finance (q-fin.CP)

</div>
<p class="mathjax">Autonomous agents based on Large Language Models (LLMs) that devise plans and
tackle real-world challenges have gained prominence.However, tailoring these
agents for specialized domains like quantitative investment remains a
formidable task. The core challenge involves efficiently building and
integrating a domain-specific knowledge base for the agent's learning process.
This paper introduces a principled framework to address this challenge,
comprising a two-layer loop.In the inner loop, the agent refines its responses
by drawing from its knowledge base, while in the outer loop, these responses
are tested in real-world scenarios to automatically enhance the knowledge base
with new insights.We demonstrate that our approach enables the agent to
progressively approximate optimal behavior with provable
efficiency.Furthermore, we instantiate this framework through an autonomous
agent for mining trading signals named QuantAgent. Empirical results showcase
QuantAgent's capability in uncovering viable financial signals and enhancing
the accuracy of financial forecasts.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03757" title="Abstract">arXiv:2402.03757</a> [<a href="/pdf/2402.03757" title="Download PDF">pdf</a>, <a href="/format/2402.03757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Q">Qing Lian</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+R">Rui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+R">Renjie Pi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have recently experienced remarkable progress,
where the advent of multi-modal large language models (MLLMs) has endowed LLMs
with visual capabilities, leading to impressive performances in various
multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail
spectacularly when presented with certain image and text inputs. In this paper,
we identify a typical class of inputs that baffles MLLMs, which consist of
images that are highly relevant but inconsistent with answers, causing MLLMs to
suffer from hallucination. To quantify the effect, we propose CorrelationQA,
the first benchmark that assesses the hallucination level given spurious
images. This benchmark contains 7,308 text-image pairs across 13 categories.
Based on the proposed CorrelationQA, we conduct a thorough analysis on 9
mainstream MLLMs, illustrating that they universally suffer from this
instinctive bias to varying degrees. We hope that our curated benchmark and
evaluation results aid in better assessments of the MLLMs' robustness in the
presence of misleading images. The resource is available in
https://github.com/MasaiahHan/CorrelationQA.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03758" title="Abstract">arXiv:2402.03758</a> [<a href="/pdf/2402.03758" title="Download PDF">pdf</a>, <a href="/format/2402.03758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtual Classification: Modulating Domain-Specific Knowledge for  Multidomain Crowd Counting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mingyue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Binghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhaoyi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qixiang Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Multidomain learning; Domain-guided virtual classifier; Instance-specific batch normalization
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Neural Networks and Learning Systems,2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multidomain crowd counting aims to learn a general model for multiple diverse
datasets. However, deep networks prefer modeling distributions of the dominant
domains instead of all domains, which is known as domain bias. In this study,
we propose a simple-yet-effective Modulating Domain-specific Knowledge Network
(MDKNet) to handle the domain bias issue in multidomain crowd counting. MDKNet
is achieved by employing the idea of `modulating', enabling deep network
balancing and modeling different distributions of diverse datasets with little
bias. Specifically, we propose an Instance-specific Batch Normalization (IsBN)
module, which serves as a base modulator to refine the information flow to be
adaptive to domain distributions. To precisely modulating the domain-specific
information, the Domain-guided Virtual Classifier (DVC) is then introduced to
learn a domain-separable latent space. This space is employed as an input
guidance for the IsBN modulator, such that the mixture distributions of
multiple datasets can be well treated. Extensive experiments performed on
popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the
superiority of MDKNet in tackling multidomain crowd counting and the
effectiveness for multidomain learning. Code is available at
\url{https://github.com/csguomy/MDKNet}.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03760" title="Abstract">arXiv:2402.03760</a> [<a href="/pdf/2402.03760" title="Download PDF">pdf</a>, <a href="/format/2402.03760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeMarking: A Defense for Network Flow Watermarking in Real-Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yali Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The network flow watermarking technique associates the two communicating
parties by actively modifying certain characteristics of the stream generated
by the sender so that it covertly carries some special marking information.
Some curious users communicating with the hidden server as a Tor client may
attempt de-anonymization attacks to uncover the real identity of the hidden
server by using this technique. This compromises the privacy of the anonymized
communication system. Therefore, we propose a defense scheme against flow
watermarking. The scheme is based on deep neural networks and utilizes
generative adversarial networks to convert the original Inter-Packet Delays
(IPD) into new IPDs generated by the model. We also adopt the concept of
adversarial attacks to ensure that the detector will produce an incorrect
classification when detecting these new IPDs. This approach ensures that these
IPDs are considered "clean", effectively covering the potential watermarks.
This scheme is effective against time-based flow watermarking techniques.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03762" title="Abstract">arXiv:2402.03762</a> [<a href="/pdf/2402.03762" title="Download PDF">pdf</a>, <a href="/format/2402.03762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhetao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lechen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingrui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Neural implicit representations have recently been demonstrated in many
fields including Simultaneous Localization And Mapping (SLAM). Current neural
SLAM can achieve ideal results in reconstructing bounded scenes, but this
relies on the input of RGB-D images. Neural-based SLAM based only on RGB images
is unable to reconstruct the scale of the scene accurately, and it also suffers
from scale drift due to errors accumulated during tracking. To overcome these
limitations, we present MoD-SLAM, a monocular dense mapping method that allows
global pose optimization and 3D reconstruction in real-time in unbounded
scenes. Optimizing scene reconstruction by monocular depth estimation and using
loop closure detection to update camera pose enable detailed and precise
reconstruction on large scenes. Compared to previous work, our approach is more
robust, scalable and versatile. Our experiments demonstrate that MoD-SLAM has
more excellent mapping performance than prior neural SLAM methods, especially
in large borderless scenes.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03763" title="Abstract">arXiv:2402.03763</a> [<a href="/pdf/2402.03763" title="Download PDF">pdf</a>, <a href="/format/2402.03763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Misinformation and Polarization around COVID-19 vaccines in France,  Germany, and Italy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nogara%2C+G">Gianluca Nogara</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=Cresci%2C+S">Stefano Cresci</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Giordano%2C+S">Silvia Giordano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages (including references), 14 figures, 1 table, to be published at 16th ACM Web Science Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The kick-off of vaccination campaigns in Europe, starting in late December
2020, has been followed by the online spread of controversies and conspiracies
surrounding vaccine validity and efficacy. We study Twitter discussions in
three major European languages (Italian, German, and French) during the
vaccination campaign. Moving beyond content analysis to explore the structural
aspects of online discussions, our investigation includes an analysis of
polarization and the potential formation of echo chambers, revealing nuanced
behavioral and topical differences in user interactions across the analyzed
countries. Notably, we identify strong anti- and pro-vaccine factions
exhibiting heterogeneous temporal polarization patterns in different countries.
Through a detailed examination of news-sharing sources, we uncover the
widespread use of other media platforms like Telegram and YouTube for
disseminating low-credibility information, indicating a concerning trend of
diminishing news credibility over time. Our findings on Twitter discussions
during the COVID-19 vaccination campaign in major European languages expose
nuanced behavioral distinctions, revealing the profound impact of polarization
and the emergence of distinct anti-vaccine and pro-vaccine advocates over time.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03766" title="Abstract">arXiv:2402.03766</a> [<a href="/pdf/2402.03766" title="Download PDF">pdf</a>, <a href="/format/2402.03766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileVLM V2: Faster and Stronger Baseline for Vision Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiangxiang Chu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+L">Limeng Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yiming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinyang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce MobileVLM V2, a family of significantly improved vision language
models upon MobileVLM, which proves that a delicate orchestration of novel
architectural design, an improved training scheme tailored for mobile VLMs, and
rich high-quality dataset curation can substantially benefit VLMs' performance.
Specifically, MobileVLM V2 1.7B achieves better or on-par performance on
standard VLM benchmarks compared with much larger VLMs at the 3B scale.
Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our
models will be released at https://github.com/Meituan-AutoML/MobileVLM .
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03769" title="Abstract">arXiv:2402.03769</a> [<a href="/pdf/2402.03769" title="Download PDF">pdf</a>, <a href="/format/2402.03769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttackNet: Enhancing Biometric Security via Tailored Convolutional  Neural Network Architectures for Liveness Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+O">Oleksandr Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Zakharov%2C+D">Dmytro Zakharov</a>, 
<a href="/search/cs?searchtype=author&query=Frontoni%2C+E">Emanuele Frontoni</a>, 
<a href="/search/cs?searchtype=author&query=Maranesi%2C+A">Andrea Maranesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Biometric security is the cornerstone of modern identity verification and
authentication systems, where the integrity and reliability of biometric
samples is of paramount importance. This paper introduces AttackNet, a bespoke
Convolutional Neural Network architecture, meticulously designed to combat
spoofing threats in biometric systems. Rooted in deep learning methodologies,
this model offers a layered defense mechanism, seamlessly transitioning from
low-level feature extraction to high-level pattern discernment. Three
distinctive architectural phases form the crux of the model, each underpinned
by judiciously chosen activation functions, normalization techniques, and
dropout layers to ensure robustness and resilience against adversarial attacks.
Benchmarking our model across diverse datasets affirms its prowess, showcasing
superior performance metrics in comparison to contemporary models. Furthermore,
a detailed comparative analysis accentuates the model's efficacy, drawing
parallels with prevailing state-of-the-art methodologies. Through iterative
refinement and an informed architectural strategy, AttackNet underscores the
potential of deep learning in safeguarding the future of biometric security.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03770" title="Abstract">arXiv:2402.03770</a> [<a href="/pdf/2402.03770" title="Download PDF">pdf</a>, <a href="/format/2402.03770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fed-CVLC: Compressing Federated Learning Communications with  Variable-Length Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaoxin Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Laizhong Cui</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiangchuan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in 2024 IEEE International Conference on Computer Communications(INFOCOM 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In Federated Learning (FL) paradigm, a parameter server (PS) concurrently
communicates with distributed participating clients for model collection,
update aggregation, and model distribution over multiple rounds, without
touching private data owned by individual clients. FL is appealing in
preserving data privacy; yet the communication between the PS and scattered
clients can be a severe bottleneck. Model compression algorithms, such as
quantization and sparsification, have been suggested but they generally assume
a fixed code length, which does not reflect the heterogeneity and variability
of model updates. In this paper, through both analysis and experiments, we show
strong evidences that variable-length is beneficial for compression in FL. We
accordingly present Fed-CVLC (Federated Learning Compression with
Variable-Length Codes), which fine-tunes the code length in response of the
dynamics of model updates. We develop optimal tuning strategy that minimizes
the loss function (equivalent to maximizing the model utility) subject to the
budget for communication. We further demonstrate that Fed-CVLC is indeed a
general compression design that bridges quantization and sparsification, with
greater flexibility. Extensive experiments have been conducted with public
datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art
baselines, improving model utility by 1.50%-5.44%, or shrinking communication
traffic by 16.67%-41.61%.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03771" title="Abstract">arXiv:2402.03771</a> [<a href="/pdf/2402.03771" title="Download PDF">pdf</a>, <a href="/format/2402.03771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning from Bagged Reward: A Transformer-based Approach  for Instance-Level Reward Redistribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuting Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xin-Qiang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yao-Xiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guoqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In reinforcement Learning (RL), an instant reward signal is generated for
each action of the agent, such that the agent learns to maximize the cumulative
reward to obtain the optimal policy. However, in many real-world applications,
the instant reward signals are not obtainable by the agent. Instead, the
learner only obtains rewards at the ends of bags, where a bag is defined as a
partial sequence of a complete trajectory. In this situation, the learner has
to face the significant difficulty of exploring the unknown instant rewards in
the bags, which could not be addressed by existing approaches, including those
trajectory-based approaches that consider only complete trajectories and ignore
the inner reward distributions. To formally study this situation, we introduce
a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR),
where only the bagged rewards of sequences can be obtained. We provide the
theoretical study to establish the connection between RLBR and standard RL in
Markov Decision Processes (MDPs). To effectively explore the reward
distributions within the bagged rewards, we propose a Transformer-based reward
model, the Reward Bag Transformer (RBT), which uses the self-attention
mechanism for interpreting the contextual nuances and temporal dependencies
within each bag. Extensive experimental analyses demonstrate the superiority of
our method, particularly in its ability to mimic the original MDP's reward
distribution, highlighting its proficiency in contextual understanding and
adaptability to environmental dynamics.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03772" title="Abstract">arXiv:2402.03772</a> [<a href="/pdf/2402.03772" title="Download PDF">pdf</a>, <a href="/format/2402.03772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zeyan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongfang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Multi-antenna relays and intelligent reflecting surfaces (IRSs) have been
utilized to construct favorable channels to improve the performance of wireless
systems. A common feature between relay systems and IRS-aided systems is the
two-hop multiple-input multiple-output (MIMO) channel. As a result, the mutual
information (MI) of two-hop MIMO channels has been widely investigated with
very engaging results. However, a rigorous investigation on the fundamental
limits of two-hop MIMO channels, i.e., the first and second-order analysis, is
not yet available in the literature, due to the difficulties caused by the
two-hop (product) channel and the noise introduced by the relay (active IRS).
In this paper, we employ large-scale random matrix theory (RMT), specifically
Gaussian tools, to derive the closed-form deterministic approximation for the
mean and variance of the MI. Additionally, we determine the convergence rate
for the mean, variance and the characteristic function of the MI, and prove the
asymptotic Gaussianity. Furthermore, we also investigate the analytical
properties of the fundamental equations that describe the closed-form
approximation and prove the existence and uniqueness of the solution. An
iterative algorithm is then proposed to obtain the solution for the fundamental
equations. Numerical results validate the accuracy of the theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03773" title="Abstract">arXiv:2402.03773</a> [<a href="/pdf/2402.03773" title="Download PDF">pdf</a>, <a href="/format/2402.03773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding Version History Context for Better Code Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>, 
<a href="/search/cs?searchtype=author&query=Thongtanunam%2C+P">Patanamon Thongtanunam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages (plus 1 for references), 1 figure, 3 tables, paper was accepted to 21st International Conference on Mining Software Repositories (MSR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">With the exponential growth of AI tools that generate source code,
understanding software has become crucial. When developers comprehend a
program, they may refer to additional contexts to look for information, e.g.
program documentation or historical code versions. Therefore, we argue that
encoding this additional contextual information could also benefit code
representation for deep learning. Recent papers incorporate contextual data
(e.g. call hierarchy) into vector representation to address program
comprehension problems. This motivates further studies to explore additional
contexts, such as version history, to enhance models' understanding of
programs. That is, insights from version history enable recognition of patterns
in code evolution over time, recurring issues, and the effectiveness of past
solutions. Our paper presents preliminary evidence of the potential benefit of
encoding contextual information from the version history to predict code clones
and perform code classification. We experiment with two representative deep
learning models, ASTNN and CodeBERT, to investigate whether combining
additional contexts with different aggregations may benefit downstream
activities. The experimental result affirms the positive impact of combining
version history into source code representation in all scenarios; however, to
ensure the technique performs consistently, we need to conduct a holistic
investigation on a larger code base using different combinations of contexts,
aggregation, and models. Therefore, we propose a research agenda aimed at
exploring various aspects of encoding additional context to improve code
representation and its optimal utilisation in specific situations.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03774" title="Abstract">arXiv:2402.03774</a> [<a href="/pdf/2402.03774" title="Download PDF">pdf</a>, <a href="/format/2402.03774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Decision Tree Algorithm with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yufan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Decision trees are renowned for their interpretability capability to achieve
high predictive performance, especially on tabular data. Traditionally, they
are constructed through recursive algorithms, where they partition the data at
every node in a tree. However, identifying the best partition is challenging,
as decision trees optimized for local segments may not bring global
generalization. To address this, we introduce MetaTree, which trains a
transformer-based model on filtered outputs from classical algorithms to
produce strong decision trees for classification. Specifically, we fit both
greedy decision trees and optimized decision trees on a large number of
datasets. We then train MetaTree to produce the trees that achieve strong
generalization performance. This training enables MetaTree to not only emulate
these algorithms, but also to intelligently adapt its strategy according to the
context, thereby achieving superior generalization performance.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03776" title="Abstract">arXiv:2402.03776</a> [<a href="/pdf/2402.03776" title="Download PDF">pdf</a>, <a href="/ps/2402.03776" title="Download PostScript">ps</a>, <a href="/format/2402.03776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models As MOOCs Graders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Garuda%2C+N">Nikhil Garuda</a>, 
<a href="/search/cs?searchtype=author&query=Impey%2C+C">Christopher Impey</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+M">Matthew Wenger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Massive open online courses (MOOCs) unlock the doors to free education for
anyone around the globe with access to a computer and the internet. Despite
this democratization of learning, the massive enrollment in these courses means
it is almost impossible for one instructor to assess every student's writing
assignment. As a result, peer grading, often guided by a straightforward
rubric, is the method of choice. While convenient, peer grading often falls
short in terms of reliability and validity. In this study, using 18 distinct
settings, we explore the feasibility of leveraging large language models (LLMs)
to replace peer grading in MOOCs. Specifically, we focus on two
state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses:
Introductory Astronomy, Astrobiology, and the History and Philosophy of
Astronomy. To instruct LLMs, we use three different prompts based on a variant
of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique:
Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT
in conjunction with both instructor-formulated answers and rubrics; and
Zero-shot-CoT with instructor-offered correct answers and LLM-generated
rubrics. Our results show that Zero-shot-CoT, when integrated with
instructor-provided answers and rubrics, produces grades that are more aligned
with those assigned by instructors compared to peer grading. However, the
History and Philosophy of Astronomy course proves to be more challenging in
terms of grading as opposed to other courses. Finally, our study reveals a
promising direction for automating grading systems for MOOCs, especially in
subjects with well-defined rubrics.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03777" title="Abstract">arXiv:2402.03777</a> [<a href="/pdf/2402.03777" title="Download PDF">pdf</a>, <a href="/format/2402.03777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Automated Code Reviews: Learning from Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H+Y">Hong Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Thongtanunam%2C+P">Patanamon Thongtanunam</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>, 
<a href="/search/cs?searchtype=author&query=Charoenwet%2C+W">Wachiraphan Charoenwet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 21st International Conference on Mining Software Repositories (MSR 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern code review is a critical quality assurance process that is widely
adopted in both industry and open source software environments. This process
can help newcomers learn from the feedback of experienced reviewers; however,
it often brings a large workload and stress to reviewers. To alleviate this
burden, the field of automated code reviews aims to automate the process,
teaching large language models to provide reviews on submitted code, just as a
human would. A recent approach pre-trained and fine-tuned the code intelligent
language model on a large-scale code review corpus. However, such techniques
did not fully utilise quality reviews amongst the training data. Indeed,
reviewers with a higher level of experience or familiarity with the code will
likely provide deeper insights than the others. In this study, we set out to
investigate whether higher-quality reviews can be generated from automated code
review models that are trained based on an experience-aware oversampling
technique. Through our quantitative and qualitative evaluation, we find that
experience-aware oversampling can increase the correctness, level of
information, and meaningfulness of reviews generated by the current
state-of-the-art model without introducing new data. The results suggest that a
vast amount of high-quality reviews are underutilised with current training
strategies. This work sheds light on resource-efficient ways to boost automated
code review models.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03780" title="Abstract">arXiv:2402.03780</a> [<a href="/pdf/2402.03780" title="Download PDF">pdf</a>, <a href="/format/2402.03780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing propaganda: an analysis of stylistic cues comparing human  annotations and machine classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faye%2C+G">G&#xe9;raud Faye</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+B">Benjamin Icard</a>, 
<a href="/search/cs?searchtype=author&query=Casanova%2C+M">Morgane Casanova</a>, 
<a href="/search/cs?searchtype=author&query=Chanson%2C+J">Julien Chanson</a>, 
<a href="/search/cs?searchtype=author&query=Maine%2C+F">Fran&#xe7;ois Maine</a>, 
<a href="/search/cs?searchtype=author&query=Bancilhon%2C+F">Fran&#xe7;ois Bancilhon</a>, 
<a href="/search/cs?searchtype=author&query=Gadek%2C+G">Guillaume Gadek</a>, 
<a href="/search/cs?searchtype=author&query=Gravier%2C+G">Guillaume Gravier</a>, 
<a href="/search/cs?searchtype=author&query=%C3%89gr%C3%A9%2C+P">Paul &#xc9;gr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper to appear in the EACL 2024 Proceedings of the Third Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper investigates the language of propaganda and its stylistic
features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a
multisource, multilingual, multimodal dataset composed of news articles
extracted from websites identified as propaganda sources by expert agencies. A
limited sample from this set was randomly mixed with papers from the regular
French press, and their URL masked, to conduct an annotation-experiment by
humans, using 11 distinct labels. The results show that human annotators were
able to reliably discriminate between the two types of press across each of the
labels. We propose different NLP techniques to identify the cues used by the
annotators, and to compare them with machine classification. They include the
analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to
serve as a baseline, and four different classifiers: two RoBERTa-based models,
CATS using syntax, and one XGBoost combining syntactic and semantic features.
<br />Keywords: Propaganda, Fake News, Explainability, AI alignment, Vagueness,
Subjectivity, Exaggeration, Stylistic analysis
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03782" title="Abstract">arXiv:2402.03782</a> [<a href="/pdf/2402.03782" title="Download PDF">pdf</a>, <a href="/format/2402.03782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Philippy%2C+F">Fred Philippy</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Haddadan%2C+S">Shohreh Haddadan</a>, 
<a href="/search/cs?searchtype=author&query=Lothritz%2C+C">Cedric Lothritz</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyand%C3%A9%2C+T+F">Tegawend&#xe9; F. Bissyand&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 1st Workshop on Modular and Open Multilingual NLP (co-located with EACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting
pre-trained language models (PLMs) to specific tasks by inserting learnable
embeddings, or soft prompts, at the input layer of the PLM, without modifying
its parameters. This paper investigates the potential of SPT for cross-lingual
transfer. Unlike previous studies on SPT for cross-lingual transfer that often
fine-tune both the soft prompt and the model parameters, we adhere to the
original intent of SPT by keeping the model parameters frozen and only training
the soft prompt. This does not only reduce the computational cost and storage
overhead of full-model fine-tuning, but we also demonstrate that this very
parameter efficiency intrinsic to SPT can enhance cross-lingual transfer
performance to linguistically distant languages. Moreover, we explore how
different factors related to the prompt, such as the length or its
reparameterization, affect cross-lingual transfer performance.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03783" title="Abstract">arXiv:2402.03783</a> [<a href="/pdf/2402.03783" title="Download PDF">pdf</a>, <a href="/format/2402.03783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Low-Resource Medical Image Classification with Weakly  Supervised Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Fudan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jindong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weijiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+N">Nong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yutong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most advances in medical image recognition supporting clinical auxiliary
diagnosis meet challenges due to the low-resource situation in the medical
field, where annotations are highly expensive and professional. This
low-resource problem can be alleviated by leveraging the transferable
representations of large-scale pre-trained vision-language models via relevant
medical text prompts. However, existing pre-trained vision-language models
require domain experts to carefully design the medical prompts, which greatly
increases the burden on clinicians. To address this problem, we propose a
weakly supervised prompt learning method MedPrompt to automatically generate
medical prompts, which includes an unsupervised pre-trained vision-language
model and a weakly supervised prompt learning model. The unsupervised
pre-trained vision-language model utilizes the natural correlation between
medical images and corresponding medical texts for pre-training, without any
manual annotations. The weakly supervised prompt learning model only utilizes
the classes of images in the dataset to guide the learning of the specific
class vector in the prompt, while the learning of other context vectors in the
prompt requires no manual annotations for guidance. To the best of our
knowledge, this is the first model to automatically generate medical prompts.
With these prompts, the pre-trained vision-language model can be freed from the
strong expert dependency of manual annotation and manual prompt design.
Experimental results show that the model using our automatically generated
prompts outperforms its full-shot learning hand-crafted prompts counterparts
with only a minimal number of labeled samples for few-shot learning, and
reaches superior or comparable accuracy on zero-shot image classification. The
proposed prompt generator is lightweight and therefore can be embedded into any
network architecture.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03784" title="Abstract">arXiv:2402.03784</a> [<a href="/pdf/2402.03784" title="Download PDF">pdf</a>, <a href="/format/2402.03784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hettige%2C+K+H">Kethmi Hirushini Hettige</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiahao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+S">Shili Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Cheng Long</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gao Cong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Air quality prediction and modelling plays a pivotal role in public health
and environment management, for individuals and authorities to make informed
decisions. Although traditional data-driven models have shown promise in this
domain, their long-term prediction accuracy can be limited, especially in
scenarios with sparse or incomplete data and they often rely on black-box deep
learning structures that lack solid physical foundation leading to reduced
transparency and interpretability in predictions. To address these limitations,
this paper presents a novel approach named Physics guided Neural Network for
Air Quality Prediction (AirPhyNet). Specifically, we leverage two
well-established physics principles of air particle movement (diffusion and
advection) by representing them as differential equation networks. Then, we
utilize a graph structure to integrate physics knowledge into a neural network
architecture and exploit latent representations to capture spatio-temporal
relationships within the air quality data. Experiments on two real-world
benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art
models for different testing scenarios including different lead time (24h, 48h,
72h), sparse data and sudden change prediction, achieving reduction in
prediction errors up to 10%. Moreover, a case study further validates that our
model captures underlying physical processes of particle movement and generates
accurate predictions with real physical meaning.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03785" title="Abstract">arXiv:2402.03785</a> [<a href="/pdf/2402.03785" title="Download PDF">pdf</a>, <a href="/format/2402.03785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Anomaly Detection via Knowledge-Data Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haihong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zi%2C+C">Chenyi Zi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Anomaly detection (AD) plays a pivotal role in numerous web-based
applications, including malware detection, anti-money laundering, device
failure detection, and network fault analysis. Most methods, which rely on
unsupervised learning, are hard to reach satisfactory detection accuracy due to
the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been
introduced with a limited number of labeled anomaly samples to enhance model
performance. Nevertheless, it is still challenging for models, trained on an
inadequate amount of labeled data, to generalize to unseen anomalies. In this
paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to
integrate rule knowledge, typically summarized by human experts, to supplement
the limited labeled data. Specifically, we transpose these rules into the
knowledge space and subsequently recast the incorporation of knowledge as the
alignment of knowledge and data. To facilitate this alignment, we employ the
Optimal Transport (OT) technique. We then incorporate the OT distance as an
additional loss term to the original objective function of WSAD methodologies.
Comprehensive experimental results on five real-world datasets demonstrate that
our proposed KDAlign framework markedly surpasses its state-of-the-art
counterparts, achieving superior performance across various anomaly types.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03790" title="Abstract">arXiv:2402.03790</a> [<a href="/pdf/2402.03790" title="Download PDF">pdf</a>, <a href="/ps/2402.03790" title="Download PostScript">ps</a>, <a href="/format/2402.03790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong approximation of the time-fractional Cahn--Hilliard equation  driven by a fractionally integrated additive noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Al-Maskari%2C+M">Mariam Al-Maskari</a>, 
<a href="/search/math?searchtype=author&query=Karaa%2C+S">Samir Karaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we consider the numerical approximation of a time-fractional
stochastic Cahn--Hilliard equation driven by an additive fractionally
integrated Gaussian noise. The model involves a Caputo fractional derivative in
time of order $\alpha\in(0,1)$ and a fractional time-integral noise of order
$\gamma\in[0,1]$. The numerical scheme approximates the model by a piecewise
linear finite element method in space and a convolution quadrature in time (for
both time-fractional operators), along with the $L^2$-projection for the noise.
We carefully investigate the spatially semidiscrete and fully discrete schemes,
and obtain strong convergence rates by using clever energy arguments. The
temporal H\"older continuity property of the solution played a key role in the
error analysis. Unlike the stochastic Allen--Cahn equation, the presence of the
unbounded elliptic operator in front of the cubic nonlinearity in the
underlying model adds complexity and challenges to the error analysis. To
overcome these difficulties, several new techniques and error estimates are
developed. The study concludes with numerical examples that validate the
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03791" title="Abstract">arXiv:2402.03791</a> [<a href="/pdf/2402.03791" title="Download PDF">pdf</a>, <a href="/format/2402.03791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Blockwise Task-interleaved Pipeline Parallelism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Ding Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lijuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Minxi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiecheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hengjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingcheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Z">Zhilin Pei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Efficient distributed training serves as a powerful catalyst and an essential
foundation for the development of large-scale neural networks. In distributed
training scenarios, various pipeline parallelism methods are cleverly designed
and widely employed. In this paper, we propose ZeroPP, a highly efficient and
flexible pipeline parallelism method that trades off pipeline bubbles, memory
usage, and communication through adaptive scheduling units. ZeroPP achieves
minimal pipeline bubbles by carefully staggering the computation tasks of
forward, input gradient, and weight gradient within a scheduling unit.
Additionally, ZeroPP optimizes the combination of pipeline parallelism and
fully sharded data parallelism using a blockwise schedule. We conduct
experiments with popular GPT-style models and observe up to a 30% increase in
throughput compared to the state-of-the-art breath-first pipeline parallelism.
Besides, our evaluation also demonstrates up to a 68% increase in throughput
and a 10% reduction in memory consumption compared to the memory-efficient 1F1B
method.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03792" title="Abstract">arXiv:2402.03792</a> [<a href="/pdf/2402.03792" title="Download PDF">pdf</a>, <a href="/format/2402.03792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Reinforcement Learning in Smooth MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maran%2C+D">Davide Maran</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>, 
<a href="/search/cs?searchtype=author&query=Papini%2C+M">Matteo Papini</a>, 
<a href="/search/cs?searchtype=author&query=Restell%2C+M">Marcello Restell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Obtaining no-regret guarantees for reinforcement learning (RL) in the case of
problems with continuous state and/or action spaces is still one of the major
open challenges in the field. Recently, a variety of solutions have been
proposed, but besides very specific settings, the general problem remains
unsolved. In this paper, we introduce a novel structural assumption on the
Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes
most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To
face this challenging scenario, we propose two algorithms for regret
minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of
constructing an MDP representation through an orthogonal feature map based on
Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives
the no-regret property under weaker assumptions but is computationally
inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial
time, although for a smaller class of problems. After analyzing their regret
properties, we compare our results with state-of-the-art ones from RL theory,
showing that our algorithms achieve the best guarantees.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03795" title="Abstract">arXiv:2402.03795</a> [<a href="/pdf/2402.03795" title="Download PDF">pdf</a>, <a href="/format/2402.03795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-based Domain-Adaptive Segmentation with Depth Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinjing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhedong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tae-Kyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent endeavors have been made to leverage self-supervised depth estimation
as guidance in unsupervised domain adaptation (UDA) for semantic segmentation.
Prior arts, however, overlook the discrepancy between semantic and depth
features, as well as the reliability of feature fusion, thus leading to
suboptimal segmentation performance. To address this issue, we propose a novel
UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy
esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive
features and achieve reliable feature fusion for semantic segmentation with
self-supervised depth estimates. Our framework incorporates two novel
components: energy-based feature fusion (EB2F) and energy-based reliable fusion
Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and
depth features by explicitly measuring and reducing their discrepancy using
Hopfield energy for better feature fusion. The RFA module evaluates the
reliability of the feature fusion using an energy score to improve the
effectiveness of depth guidance. Extensive experiments on two datasets
demonstrate that our method achieves significant performance gains over prior
works, validating the effectiveness of our energy-based learning approach.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03796" title="Abstract">arXiv:2402.03796</a> [<a href="/pdf/2402.03796" title="Download PDF">pdf</a>, <a href="/format/2402.03796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Face Detection: Present State and Research Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhat%2C+P">Purnendu Prabhat</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Vishwakarma%2C+A+K">Ajeet Kumar Vishwakarma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The majority of computer vision applications that handle images featuring
humans use face detection as a core component. Face detection still has issues,
despite much research on the topic. Face detection's accuracy and speed might
yet be increased. This review paper shows the progress made in this area as
well as the substantial issues that still need to be tackled. The paper
provides research directions that can be taken up as research projects in the
field of face detection.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03801" title="Abstract">arXiv:2402.03801</a> [<a href="/pdf/2402.03801" title="Download PDF">pdf</a>, <a href="/format/2402.03801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Practical Diversified Recommendation with Controllable Category  Diversity Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Luwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhibo Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+W">Wei Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Two-stage Controllable Category Diversity Framework for Recommendation
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender systems have made significant strides in various industries,
primarily driven by extensive efforts to enhance recommendation accuracy.
However, this pursuit of accuracy has inadvertently given rise to echo
chamber/filter bubble effects. Especially in industry, it could impair user's
experiences and prevent user from accessing a wider range of items. One of the
solutions is to take diversity into account. However, most of existing works
focus on user's explicit preferences, while rarely exploring user's
non-interaction preferences. These neglected non-interaction preferences are
especially important for broadening user's interests in alleviating echo
chamber/filter bubble effects.Therefore, in this paper, we first define
diversity as two distinct definitions, i.e., user-explicit diversity
(U-diversity) and user-item non-interaction diversity (N-diversity) based on
user historical behaviors. Then, we propose a succinct and effective method,
named as Controllable Category Diversity Framework (CCDF) to achieve both high
U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two
stages, User-Category Matching and Constrained Item Matching. The User-Category
Matching utilizes the DeepU2C model and a combined loss to capture user's
preferences in categories, and then selects the top-$K$ categories with a
controllable parameter $K$.These top-$K$ categories will be used as trigger
information in Constrained Item Matching. Offline experimental results show
that our proposed DeepU2C outperforms state-of-the-art diversity-oriented
methods, especially on N-diversity task. The whole framework is validated in a
real-world production environment by conducting online A/B testing.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03803" title="Abstract">arXiv:2402.03803</a> [<a href="/pdf/2402.03803" title="Download PDF">pdf</a>, <a href="/ps/2402.03803" title="Download PostScript">ps</a>, <a href="/format/2402.03803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot voice a voice controlled robot using arduino
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teeda%2C+V">Vineeth Teeda</a>, 
<a href="/search/cs?searchtype=author&query=Sujatha%2C+K">K Sujatha</a>, 
<a href="/search/cs?searchtype=author&query=Mutukuru%2C+R">Rakesh Mutukuru</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Engineering and Advanced Technology,
  2016, Volume 5, Issue 6, 60-67
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Robotic assistants reduce the manual efforts being put in by humans in their
day-to-day tasks. In this paper, we develop a voice-controlled personal
assistant robot. The robot takes the human voice commands by its own built-in
microphone. This robot not only takes the commands and executes them but also
acknowledges them through speech output. This robot can perform different
movements, turns, wakeup/shutdown operations, relocate an object from one place
to another, and can also develop a conversation with humans. The voice commands
are processed in real time using an offline server. The speech signal commands
are directly communicated to the server using a USB cable. The personal
assistant robot is developed on a microcontroller-based platform. Performance
evaluation is carried out with encouraging results of the initial experiments.
Possible improvements for applications in homes, hospitals, car systems, and
industries are also discussed.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03804" title="Abstract">arXiv:2402.03804</a> [<a href="/pdf/2402.03804" title="Download PDF">pdf</a>, <a href="/format/2402.03804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yixin Song</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guanghui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chenyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Z">Zeyu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sparse computation offers a compelling solution for the inference of Large
Language Models (LLMs) in low-resource scenarios by dynamically skipping the
computation of inactive neurons. While traditional approaches focus on
ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of
sparse LLMs beyond zero activation values. We introduce a general method that
defines neuron activation through neuron output magnitudes and a tailored
magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse
activation. To find the most efficient activation function for sparse
computation, we propose a systematic framework to examine the sparsity of LLMs
from three aspects: the trade-off between sparsity and performance, the
predictivity of sparsity, and the hardware affinity. We conduct thorough
experiments on LLMs utilizing different activation functions, including ReLU,
SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing
ReLU$^2$ excel across all three evaluation aspects, highlighting its potential
as an efficient activation function for sparse LLMs. We will release the code
to facilitate future research.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03805" title="Abstract">arXiv:2402.03805</a> [<a href="/pdf/2402.03805" title="Download PDF">pdf</a>, <a href="/format/2402.03805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Description Generation for Software Patches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vu%2C+T+T">Thanh Trong Vu</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+T">Tuan-Dung Bui</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T">Thanh-Dat Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thu-Trang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+H+D">Hieu Dinh Vo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S">Son Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print version of PATCHEXPLAINER
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software patches are pivotal in refining and evolving codebases, addressing
bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed
accounts of changes, aiding comprehension and collaboration among developers.
However, manual description creation poses challenges in terms of time
consumption and variations in quality and detail. In this paper, we propose
PATCHEXPLAINER, an approach that addresses these challenges by framing patch
description generation as a machine translation task. In PATCHEXPLAINER, we
leverage explicit representations of critical elements, historical context, and
syntactic conventions. Moreover, the translation model in PATCHEXPLAINER is
designed with an awareness of description similarity. Particularly, the model
is explicitly trained to recognize and incorporate similarities present in
patch descriptions clustered into groups, improving its ability to generate
accurate and consistent descriptions across similar patches. The dual
objectives maximize similarity and accurately predict affiliating groups. Our
experimental results on a large dataset of real-world software patches show
that PATCHEXPLAINER consistently outperforms existing methods, with
improvements up to 189% in BLEU, 5.7X in Exact Match rate, and 154% in Semantic
Similarity, affirming its effectiveness in generating software patch
descriptions.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03807" title="Abstract">arXiv:2402.03807</a> [<a href="/pdf/2402.03807" title="Download PDF">pdf</a>, <a href="/format/2402.03807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEABO: A Simple Search-Based Method for Offline Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiafei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoteng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Le Wan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Runze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning (RL) has attracted much attention due to its
ability in learning from static offline datasets and eliminating the need of
interacting with the environment. Nevertheless, the success of offline RL
relies heavily on the offline transitions annotated with reward labels. In
practice, we often need to hand-craft the reward function, which is sometimes
difficult, labor-intensive, or inefficient. To tackle this challenge, we set
our focus on the offline imitation learning (IL) setting, and aim at getting a
reward function based on the expert data and unlabeled data. To that end, we
propose a simple yet effective search-based offline IL method, tagged SEABO.
SEABO allocates a larger reward to the transition that is close to its closest
neighbor in the expert demonstration, and a smaller reward otherwise, all in an
unsupervised learning manner. Experimental results on a variety of D4RL
datasets indicate that SEABO can achieve competitive performance to offline RL
algorithms with ground-truth rewards, given only a single expert trajectory,
and can outperform prior reward learning and offline IL methods across many
tasks. Moreover, we demonstrate that SEABO also works well if the expert
demonstrations contain only observations. Our code is publicly available at
https://github.com/dmksjfl/SEABO.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03812" title="Abstract">arXiv:2402.03812</a> [<a href="/pdf/2402.03812" title="Download PDF">pdf</a>, <a href="/format/2402.03812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FDO Manager: Minimum Viable FAIR Digital Object Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zoubia%2C+O">Oussama Zoubia</a>, 
<a href="/search/cs?searchtype=author&query=Boukhers%2C+Z">Zeyd Boukhers</a>, 
<a href="/search/cs?searchtype=author&query=Asundi%2C+N+B">Nagaraj Bahubali Asundi</a>, 
<a href="/search/cs?searchtype=author&query=Dogan%2C+S">Sezin Dogan</a>, 
<a href="/search/cs?searchtype=author&query=Koumpis%2C+A">Adamantios Koumpis</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+C">Christoph Lange</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+O">Oya Beyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The concept of FAIR Digital Objects (FDOs) aims to revolutionise the field of
digital preservation and accessibility in the next few years. Central to this
revolution is the alignment of FDOs with the FAIR (Findable, Accessible,
Interoperable, Reusable) Principles, particularly emphasizing
machine-actionability and interoperability across diverse data ecosystems. This
abstract introduces the "FDO Manager", a Minimum Viable Implementation,
designed to optimize the management of FDOs following these principles and the
FDO specifications. The FDO Manager is tailored to manage research artefacts
such as datasets, codes, and publications, to foster increased transparency and
reproducibility in research. The abstract presents the implementation details
of the FDO Manager, its underlying architecture, and the metadata schemas it
employs, thereby offering a clear and comprehensive understanding of its
functionalities and impact on the research domain.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03813" title="Abstract">arXiv:2402.03813</a> [<a href="/pdf/2402.03813" title="Download PDF">pdf</a>, <a href="/ps/2402.03813" title="Download PostScript">ps</a>, <a href="/format/2402.03813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NK Hybrid Genetic Algorithm for Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tin%C3%B3s%2C+R">Renato Tin&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>, 
<a href="/search/cs?searchtype=author&query=Whitley%2C+D">Darrell Whitley</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Trans. Evol. Comput. 22(5): 748-761 (2018)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The NK hybrid genetic algorithm for clustering is proposed in this paper. In
order to evaluate the solutions, the hybrid algorithm uses the NK clustering
validation criterion 2 (NKCV2). NKCV2 uses information about the disposition of
$N$ small groups of objects. Each group is composed of $K+1$ objects of the
dataset. Experimental results show that density-based regions can be identified
by using NKCV2 with fixed small $K$. In NKCV2, the relationship between
decision variables is known, which in turn allows us to apply gray box
optimization. Mutation operators, a partition crossover, and a local search
strategy are proposed, all using information about the relationship between
decision variables. In partition crossover, the evaluation function is
decomposed into $q$ independent components; partition crossover then
deterministically returns the best among $2^q$ possible offspring with
computational complexity $O(N)$. The NK hybrid genetic algorithm allows the
detection of clusters with arbitrary shapes and the automatic estimation of the
number of clusters. In the experiments, the NK hybrid genetic algorithm
produced very good results when compared to another genetic algorithm approach
and to state-of-art clustering algorithms.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03814" title="Abstract">arXiv:2402.03814</a> [<a href="/pdf/2402.03814" title="Download PDF">pdf</a>, <a href="/format/2402.03814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Graph Autoencoder with Non-discrete Bandwidths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziwen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhua Li</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yixiong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruixuan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version (17 pages, 8 figures, 12 tables), accepted by TheWebConf 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Masked graph autoencoders have emerged as a powerful graph self-supervised
learning method that has yet to be fully explored. In this paper, we unveil
that the existing discrete edge masking and binary link reconstruction
strategies are insufficient to learn topologically informative representations,
from the perspective of message propagation on graph neural networks. These
limitations include blocking message flows, vulnerability to over-smoothness,
and suboptimal neighborhood discriminability. Inspired by these understandings,
we explore non-discrete edge masks, which are sampled from a continuous and
dispersive probability distribution instead of the discrete Bernoulli
distribution. These masks restrict the amount of output messages for each edge,
referred to as "bandwidths". We propose a novel, informative, and effective
topological masked graph autoencoder using bandwidth masking and a layer-wise
bandwidth prediction objective. We demonstrate its powerful graph topological
learning ability both theoretically and empirically. Our proposed framework
outperforms representative baselines in both self-supervised link prediction
(improving the discrete edge reconstructors by at most 20%) and node
classification on numerous datasets, solely with a structure-learning pretext.
Our implementation is available at https://github.com/Newiz430/Bandana.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03815" title="Abstract">arXiv:2402.03815</a> [<a href="/pdf/2402.03815" title="Download PDF">pdf</a>, <a href="/format/2402.03815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expediting In-Network Federated Learning by Voting-Based Consensus Model  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaoxin Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Laizhong Cui</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in 2024 IEEE International Conference on Computer Communications(INFOCOM 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, federated learning (FL) has gained momentum because of its
capability in preserving data privacy. To conduct model training by FL,
multiple clients exchange model updates with a parameter server via Internet.
To accelerate the communication speed, it has been explored to deploy a
programmable switch (PS) in lieu of the parameter server to coordinate clients.
The challenge to deploy the PS in FL lies in its scarce memory space,
prohibiting running memory consuming aggregation algorithms on the PS. To
overcome this challenge, we propose Federated Learning in-network Aggregation
with Compression (FediAC) algorithm, consisting of two phases: client voting
and model aggregating. In the former phase, clients report their significant
model update indices to the PS to estimate global significant model updates. In
the latter phase, clients upload global significant model updates to the PS for
aggregation. FediAC consumes much less memory space and communication traffic
than existing works because the first phase can guarantee consensus compression
across clients. The PS easily aligns model update indices to swiftly complete
aggregation in the second phase. Finally, we conduct extensive experiments by
using public datasets to demonstrate that FediAC remarkably surpasses the
state-of-the-art baselines in terms of model accuracy and communication
traffic.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03817" title="Abstract">arXiv:2402.03817</a> [<a href="/pdf/2402.03817" title="Download PDF">pdf</a>, <a href="/ps/2402.03817" title="Download PostScript">ps</a>, <a href="/format/2402.03817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improvement of Frequency Source Phase Noise Reduction Design under  Vibration Condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yin%2C+L">Liwei Yin</a>, 
<a href="/search/eess?searchtype=author&query=Shu%2C+Y">Yongjiang Shu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+Y">Yuefei Dai</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+X">Xiaopeng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Lian%2C+Y">Yunlong Lian</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhonghua Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+Y">Yong Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages,29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Reasonable vibration reduction design is an important way to achieve low
phase noise index of airborne frequency source output signal. Aiming at the
problem of phase noise deterioration of an airborne frequency source under
random condition, this paper proposes to improve the vibration reduction mode
crystal oscillator and reduce the distance between the barycenter of frequency
source and crystal oscillator vibration based on the analysis of the
relationship between the frequency source and the phase noise of output signal.
Experimental results show that the active noise control system achieves 62dB
phase noise compensation under the random vibration of 0.04-0.1g*g/Hz amplitude
range and 5-2000 Hz frequency range.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03818" title="Abstract">arXiv:2402.03818</a> [<a href="/pdf/2402.03818" title="Download PDF">pdf</a>, <a href="/format/2402.03818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic generalization error of a single-layer graph convolutional  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duranthon%2C+O">O. Duranthon</a>, 
<a href="/search/cs?searchtype=author&query=Zdeborov%C3%A1%2C+L">L. Zdeborov&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
<p class="mathjax">While graph convolutional networks show great practical promises, the
theoretical understanding of their generalization properties as a function of
the number of samples is still in its infancy compared to the more broadly
studied case of supervised fully connected neural networks. In this article, we
predict the performances of a single-layer graph convolutional network (GCN)
trained on data produced by attributed stochastic block models (SBMs) in the
high-dimensional limit. Previously, only ridge regression on contextual-SBM
(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to
arbitrary convex loss and regularization for the CSBM and add the analysis for
another data model, the neural-prior SBM. We also study the high
signal-to-noise ratio limit, detail the convergence rates of the GCN and show
that, while consistent, it does not reach the Bayes-optimal rate for any of the
considered cases.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03820" title="Abstract">arXiv:2402.03820</a> [<a href="/pdf/2402.03820" title="Download PDF">pdf</a>, <a href="/format/2402.03820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMSM transient response optimization by end-to-end optimal control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kawachi%2C+Y">Yuta Kawachi</a>, 
<a href="/search/eess?searchtype=author&query=Ambai%2C+M">Mitsuru Ambai</a>, 
<a href="/search/eess?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>, 
<a href="/search/eess?searchtype=author&query=Takano%2C+G">Gaku Takano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Speed responses of motors, especially Permanent Magnet Synchronous Motors
(PMSMs), are increasing in importance for recent applications, such as electric
vehicles or quadrotors. These applications require quick acceleration
performance. However, commercial controllers are based mainly on
Proportional-Integral (PI) controllers, which are suitable for eliminating
steady-state errors but unsuitable for transient response optimization. In this
paper, we replaced whole conventional controllers with an end-to-end Recurrent
Neural Network (RNN) that has a regularized transition matrix. Our end-to-end
controller directly minimizes the transient response time on the basis of
optimal control theory. Computer-simulated results show that speed response
indices improved using the RNN rather than a PI controller, while both were
under comparable power losses. The current vector trajectories of the RNN
showed that the RNN could automatically determine arbitrary trajectories in the
flux-weakening region in accordance with an arbitrarily designed loss function.
In contrast, the traditional flux-weakening methods using PI controllers have
pre-determined current vector trajectories.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03821" title="Abstract">arXiv:2402.03821</a> [<a href="/pdf/2402.03821" title="Download PDF">pdf</a>, <a href="/format/2402.03821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite volumes for the Gross-Pitaevskii equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chauleur%2C+Q">Quentin Chauleur</a> (LPP, PhLAM, Paradyse)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study the approximation by a Voronoi finite-volume scheme of the
Gross-Pitaevskii equation with time-dependent potential in two and three
dimensions. We perform an explicit splitting scheme for the time integration
alongside a two-point flux approximation scheme in space. We rigorously analyze
the error bounds relying on discrete uniform Sobolev inequalities. We also
prove the convergence of the pseudo-vorticity of the wave function. We finally
perform some numerical simulations to illustrate our theoretical results.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03822" title="Abstract">arXiv:2402.03822</a> [<a href="/pdf/2402.03822" title="Download PDF">pdf</a>, <a href="/format/2402.03822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Si Shen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+P">Peijun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Danhao Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents RevOrder, a novel technique aimed at improving arithmetic
operations in large language models (LLMs) by reversing the output digits in
addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.
Our method significantly reduces the Count of Sequential Intermediate Digits
(CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation
complexity. Through comprehensive testing, RevOrder not only achieves perfect
accuracy in basic arithmetic operations but also substantially boosts LLM
performance in division tasks, particularly with large numbers where
traditional models struggle. Implementation of RevOrder is cost-effective for
both training and inference phases. Moreover, applying RevOrder to fine-tune
the LLaMA2-7B model on the GSM8K math task results in a considerable
improvement, reducing equation calculation errors by 46% and increasing overall
scores from 41.6 to 44.4.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03824" title="Abstract">arXiv:2402.03824</a> [<a href="/pdf/2402.03824" title="Download PDF">pdf</a>, <a href="/ps/2402.03824" title="Download PostScript">ps</a>, <a href="/format/2402.03824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A call for embodied AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paolo%2C+G">Giuseppe Paolo</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Billandon%2C+J">Jonas Gonzalez-Billandon</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A9gl%2C+B">Bal&#xe1;zs K&#xe9;gl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024 Position paper track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We propose Embodied AI as the next fundamental step in the pursuit of
Artificial General Intelligence, juxtaposing it against current AI
advancements, particularly Large Language Models. We traverse the evolution of
the embodiment concept across diverse fields - philosophy, psychology,
neuroscience, and robotics - to highlight how EAI distinguishes itself from the
classical paradigm of static learning. By broadening the scope of Embodied AI,
we introduce a theoretical framework based on cognitive architectures,
emphasizing perception, action, memory, and learning as essential components of
an embodied agent. This framework is aligned with Friston's active inference
principle, offering a comprehensive approach to EAI development. Despite the
progress made in the field of AI, substantial challenges, such as the
formulation of a novel AI learning theory and the innovation of advanced
hardware, persist. Our discussion lays down a foundational guideline for future
Embodied AI research. Highlighting the importance of creating Embodied AI
agents capable of seamless communication, collaboration, and coexistence with
humans and other intelligent entities within real-world environments, we aim to
steer the AI community towards addressing the multifaceted challenges and
seizing the opportunities that lie ahead in the quest for AGI.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03828" title="Abstract">arXiv:2402.03828</a> [<a href="/pdf/2402.03828" title="Download PDF">pdf</a>, <a href="/format/2402.03828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Barycenters of Distributions with Neural Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Udovichenko%2C+I">Igor Udovichenko</a>, 
<a href="/search/cs?searchtype=author&query=Gazdieva%2C+M">Milena Gazdieva</a>, 
<a href="/search/cs?searchtype=author&query=Pammer%2C+G">Gudmund Pammer</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Given a collection of probability measures, a practitioner sometimes needs to
find an "average" distribution which adequately aggregates reference
distributions. A theoretically appealing notion of such an average is the
Wasserstein barycenter, which is the primal focus of our work. By building upon
the dual formulation of Optimal Transport (OT), we propose a new scalable
approach for solving the Wasserstein barycenter problem. Our methodology is
based on the recent Neural OT solver: it has bi-level adversarial learning
objective and works for general cost functions. These are key advantages of our
method, since the typical adversarial algorithms leveraging barycenter tasks
utilize tri-level optimization and focus mostly on quadratic cost. We also
establish theoretical error bounds for our proposed approach and showcase its
applicability and effectiveness on illustrative scenarios and image data
setups.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03830" title="Abstract">arXiv:2402.03830</a> [<a href="/pdf/2402.03830" title="Download PDF">pdf</a>, <a href="/format/2402.03830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OASim: an Open and Adaptive Simulator based on Neural Rendering for  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Guohang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+J">Jiahao Pi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianfei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhaotong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+M">Min Dou</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Nianchen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiusheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Daocheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Licheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pinlong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xing Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuemeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yeqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongbin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With deep learning and computer vision technology development, autonomous
driving provides new solutions to improve traffic safety and efficiency. The
importance of building high-quality datasets is self-evident, especially with
the rise of end-to-end autonomous driving algorithms in recent years. Data
plays a core role in the algorithm closed-loop system. However, collecting
real-world data is expensive, time-consuming, and unsafe. With the development
of implicit rendering technology and in-depth research on using generative
models to produce data at scale, we propose OASim, an open and adaptive
simulator and autonomous driving data generator based on implicit neural
rendering. It has the following characteristics: (1) High-quality scene
reconstruction through neural implicit surface reconstruction technology. (2)
Trajectory editing of the ego vehicle and participating vehicles. (3) Rich
vehicle model library that can be freely selected and inserted into the scene.
(4) Rich sensors model library where you can select specified sensors to
generate data. (5) A highly customizable data generation system can generate
data according to user needs. We demonstrate the high quality and fidelity of
the generated data through perception performance evaluation on the Carla
simulator and real-world data acquisition. Code is available at
https://github.com/PJLab-ADG/OASim.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03832" title="Abstract">arXiv:2402.03832</a> [<a href="/pdf/2402.03832" title="Download PDF">pdf</a>, <a href="/format/2402.03832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Skill Extraction in the Job Market Domain using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K+C">Khanh Cao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mike Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Montariol%2C+S">Syrielle Montariol</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NLP4HR 2024 (EACL Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Skill Extraction involves identifying skills and qualifications mentioned in
documents such as job postings and resumes. The task is commonly tackled by
training supervised models using a sequence labeling approach with BIO tags.
However, the reliance on manually annotated data limits the generalizability of
such approaches. Moreover, the common BIO setting limits the ability of the
models to capture complex skill patterns and handle ambiguous mentions. In this
paper, we explore the use of in-context learning to overcome these challenges,
on a benchmark of 6 uniformized skill extraction datasets. Our approach
leverages the few-shot learning capabilities of large language models (LLMs) to
identify and extract skills from sentences. We show that LLMs, despite not
being on par with traditional supervised models in terms of performance, can
better handle syntactically complex skill mentions in skill extraction tasks.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03833" title="Abstract">arXiv:2402.03833</a> [<a href="/pdf/2402.03833" title="Download PDF">pdf</a>, <a href="/format/2402.03833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An SVD-free Approach to Nonlinear Dictionary Learning based on RVFL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhuri%2C+G">G.Madhuri</a>, 
<a href="/search/cs?searchtype=author&query=Negi%2C+A">Atul Negi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a novel nonlinear dictionary learning algorithm
leveraging the theory of a feed-forward neural network called Random Vector
Functional Link (RVFL). The proposed RVFL-based nonlinear Dictionary Learning
(RVFLDL) learns a dictionary as a sparse-to-dense feature map from nonlinear
sparse coefficients to the dense input features. Kernel-based nonlinear
dictionary learning methods operate in a feature space obtained by an implicit
feature map, and they are not independent of computationally expensive
operations like Singular Value Decomposition (SVD). Training the RVFL-based
dictionary is free from SVD computation as RVFL generates weights from the
input to the output layer analytically. Sparsity-inducing Horse-shoe prior is
assumed on the coefficients to generate a sparse coefficient matrix w.r.t an
initial random dictionary. Higher-order dependencies between the input sparse
coefficients and the dictionary atoms are incorporated into the training
process by nonlinearly transforming the sparse coefficients and adding them as
enhanced features. Thus the method projects sparse coefficients to a higher
dimensional space while inducing nonlinearities into the dictionary. For
classification using RVFL-net, a classifier matrix is learned as a transform
that maps nonlinear sparse coefficients to the labels. The performance of the
method illustrated in image classification and reconstruction applications is
comparable to that of other nonlinear dictionary learning methods. Experiments
show that RVFLDL is scalable and provides a solution better than those obtained
using other nonlinear dictionary learning methods.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03834" title="Abstract">arXiv:2402.03834</a> [<a href="/pdf/2402.03834" title="Download PDF">pdf</a>, <a href="/ps/2402.03834" title="Download PostScript">ps</a>, <a href="/format/2402.03834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Security and Efficiency in Blockchain with Aggregated  Zero-Knowledge Proof Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+O">Oleksandr Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Rusnak%2C+A">Alex Rusnak</a>, 
<a href="/search/cs?searchtype=author&query=Yezhov%2C+A">Anton Yezhov</a>, 
<a href="/search/cs?searchtype=author&query=Kanonik%2C+D">Dzianis Kanonik</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+K">Kateryna Kuznetsova</a>, 
<a href="/search/cs?searchtype=author&query=Karashchuk%2C+S">Stanislav Karashchuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Blockchain technology has emerged as a revolutionary tool in ensuring data
integrity and security in digital transactions. However, the current approaches
to data verification in blockchain systems, particularly in Ethereum, face
challenges in terms of efficiency and computational overhead. The traditional
use of Merkle Trees and cryptographic hash functions, while effective, leads to
significant resource consumption, especially for large datasets. This
highlights a gap in existing research: the need for more efficient methods of
data verification in blockchain networks. Our study addresses this gap by
proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the
structure of Merkle Trees. We develop a system that significantly reduces the
size of the proof and the computational resources needed for its generation and
verification. Our approach represents a paradigm shift in blockchain data
verification, balancing security with efficiency. We conducted extensive
experimental evaluations using real Ethereum block data to validate the
effectiveness of our proposed scheme. The results demonstrate a drastic
reduction in proof size and computational requirements compared to traditional
methods, making the verification process more efficient and economically
viable. Our contribution fills a critical research void, offering a scalable
and secure solution for blockchain data verification. The implications of our
work are far-reaching, enhancing the overall performance and adaptability of
blockchain technology in various applications, from financial transactions to
supply chain management.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03837" title="Abstract">arXiv:2402.03837</a> [<a href="/pdf/2402.03837" title="Download PDF">pdf</a>, <a href="/format/2402.03837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressivity of Geometric Inhomogeneous Random Graphs -- Metric and  Non-Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dayan%2C+B">Benjamin Dayan</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+M">Marc Kaufmann</a>, 
<a href="/search/cs?searchtype=author&query=Schaller%2C+U">Ulysse Schaller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Recently there has been increased interest in fitting generative graph models
to real-world networks. In particular, Bl\"asius et al. have proposed a
framework for systematic evaluation of the expressivity of random graph models.
We extend this framework to Geometric Inhomogeneous Random Graphs (GIRGs). This
includes a family of graphs induced by non-metric distance functions which
allow capturing more complex models of partial similarity between nodes as a
basis of connection - as well as homogeneous and non-homogeneous feature
spaces. As part of the extension, we develop schemes for estimating the
multiplicative constant and the long-range parameter in the connection
probability. Moreover, we devise an algorithm for sampling
Minimum-Component-Distance GIRGs whose runtime is linear both in the number of
vertices and in the dimension of the underlying geometric space. Our results
provide evidence that GIRGs are more realistic candidates with respect to
various graph features such as closeness centrality, betweenness centrality,
local clustering coefficient, and graph effective diameter, while they face
difficulties to replicate higher variance and more extreme values of graph
statistics observed in real-world networks.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03840" title="Abstract">arXiv:2402.03840</a> [<a href="/pdf/2402.03840" title="Download PDF">pdf</a>, <a href="/format/2402.03840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Belief Scene Graphs: Expanding Partial Scenes with Objects through  Computation of Expectation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saucedo%2C+M+A+V">Mario A.V. Saucedo</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Akash Patel</a>, 
<a href="/search/cs?searchtype=author&query=Saradagi%2C+A">Akshit Saradagi</a>, 
<a href="/search/cs?searchtype=author&query=Kanellakis%2C+C">Christoforos Kanellakis</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this article, we propose the novel concept of Belief Scene Graphs, which
are utility-driven extensions of partial 3D scene graphs, that enable efficient
high-level task planning with partial information. We propose a graph-based
learning methodology for the computation of belief (also referred to as
expectation) on any given 3D scene graph, which is then used to strategically
add new nodes (referred to as blind nodes) that are relevant for a robotic
mission. We propose the method of Computation of Expectation based on
Correlation Information (CECI), to reasonably approximate real
Belief/Expectation, by learning histograms from available training data. A
novel Graph Convolutional Neural Network (GCN) model is developed, to learn
CECI from a repository of 3D scene graphs. As no database of 3D scene graphs
exists for the training of the novel CECI model, we present a novel methodology
for generating a 3D scene graph dataset based on semantically annotated
real-life 3D spaces. The generated dataset is then utilized to train the
proposed CECI model and for extensive validation of the proposed method. We
establish the novel concept of \textit{Belief Scene Graphs} (BSG), as a core
component to integrate expectations into abstract representations. This new
concept is an evolution of the classical 3D scene graph concept and aims to
enable high-level reasoning for the task planning and optimization of a variety
of robotics missions. The efficacy of the overall framework has been evaluated
in an object search scenario, and has also been tested on a real-life
experiment to emulate human common sense of unseen-objects.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03843" title="Abstract">arXiv:2402.03843</a> [<a href="/pdf/2402.03843" title="Download PDF">pdf</a>, <a href="/format/2402.03843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new method for optical steel rope non-destructive damage detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yunqing Bao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a novel algorithm for non-destructive damage detection
for steel ropes in high-altitude environments (aerial ropeway). The algorithm
comprises two key components: First, a segmentation model named RGBD-UNet is
designed to accurately extract steel ropes from complex backgrounds. This model
is equipped with the capability to process and combine color and depth
information through the proposed CMA module. Second, a detection model named
VovNetV3.5 is developed to differentiate between normal and abnormal steel
ropes. It integrates the VovNet architecture with a DBB module to enhance
performance. Besides, a novel background augmentation method is proposed to
enhance the generalization ability of the segmentation model. Datasets
containing images of steel ropes in different scenarios are created for the
training and testing of both the segmentation and detection models. Experiments
demonstrate a significant improvement over baseline models. On the proposed
dataset, the highest accuracy achieved by the detection model reached 0.975,
and the maximum F-measure achieved by the segmentation model reached 0.948.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03845" title="Abstract">arXiv:2402.03845</a> [<a href="/pdf/2402.03845" title="Download PDF">pdf</a>, <a href="/format/2402.03845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On gauge freedom, conservativity and intrinsic dimensionality estimation  in diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horvat%2C+C">Christian Horvat</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+J">Jean-Pascal Pfister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diffusion models are generative models that have recently demonstrated
impressive performances in terms of sampling quality and density estimation in
high dimensions. They rely on a forward continuous diffusion process and a
backward continuous denoising process, which can be described by a
time-dependent vector field and is used as a generative model. In the original
formulation of the diffusion model, this vector field is assumed to be the
score function (i.e. it is the gradient of the log-probability at a given time
in the diffusion process). Curiously, on the practical side, most studies on
diffusion models implement this vector field as a neural network function and
do not constrain it be the gradient of some energy function (that is, most
studies do not constrain the vector field to be conservative). Even though some
studies investigated empirically whether such a constraint will lead to a
performance gain, they lead to contradicting results and failed to provide
analytical results. Here, we provide three analytical results regarding the
extent of the modeling freedom of this vector field. {Firstly, we propose a
novel decomposition of vector fields into a conservative component and an
orthogonal component which satisfies a given (gauge) freedom. Secondly, from
this orthogonal decomposition, we show that exact density estimation and exact
sampling is achieved when the conservative component is exactly equals to the
true score and therefore conservativity is neither necessary nor sufficient to
obtain exact density estimation and exact sampling. Finally, we show that when
it comes to inferring local information of the data manifold, constraining the
vector field to be conservative is desirable.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03846" title="Abstract">arXiv:2402.03846</a> [<a href="/pdf/2402.03846" title="Download PDF">pdf</a>, <a href="/format/2402.03846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Generation of Hidden Outliers for Improved Outlier Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cribeiro-Ramallo%2C+J">Jose Cribeiro-Ramallo</a>, 
<a href="/search/cs?searchtype=author&query=Arzamasov%2C+V">Vadim Arzamasov</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hm%2C+K">Klemens B&#xf6;hm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Outlier generation is a popular technique used for solving important outlier
detection tasks. Generating outliers with realistic behavior is challenging.
Popular existing methods tend to disregard the 'multiple views' property of
outliers in high-dimensional spaces. The only existing method accounting for
this property falls short in efficiency and effectiveness. We propose BISECT, a
new outlier generation method that creates realistic outliers mimicking said
property. To do so, BISECT employs a novel proposition introduced in this
article stating how to efficiently generate said realistic outliers. Our method
has better guarantees and complexity than the current methodology for
recreating 'multiple views'. We use the synthetic outliers generated by BISECT
to effectively enhance outlier detection in diverse datasets, for multiple use
cases. For instance, oversampling with BISECT reduced the error by up to 3
times when compared with the baselines.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03848" title="Abstract">arXiv:2402.03848</a> [<a href="/pdf/2402.03848" title="Download PDF">pdf</a>, <a href="/format/2402.03848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANLS* -- A Universal Document Processing Metric for Generative Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peer%2C+D">David Peer</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6pf%2C+P">Philemon Sch&#xf6;pf</a>, 
<a href="/search/cs?searchtype=author&query=Nebendahl%2C+V">Volckmar Nebendahl</a>, 
<a href="/search/cs?searchtype=author&query=Rietzler%2C+A">Alexander Rietzler</a>, 
<a href="/search/cs?searchtype=author&query=Stabinger%2C+S">Sebastian Stabinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs. This paper introduces a new metric for generative models called ANLS*
for evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets and 3 different GLLMs using the
ANLS* metric is also provided, demonstrating the importance of the proposed
metric. We also benchmark a novel approach to generate prompts for documents,
called SFT, against other prompting techniques such as LATIN. In 15 out of 21
cases, SFT outperforms other techniques and improves the state-of-the-art,
sometimes by as much as $15$ percentage points.
<br />Sources are available at https://github.com/deepopinion/anls_star_metric
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03849" title="Abstract">arXiv:2402.03849</a> [<a href="/pdf/2402.03849" title="Download PDF">pdf</a>, <a href="/ps/2402.03849" title="Download PostScript">ps</a>, <a href="/format/2402.03849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global certification via perfect hashing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/cs?searchtype=author&query=Feuilloley%2C+L">Laurent Feuilloley</a>, 
<a href="/search/cs?searchtype=author&query=Zeitoun%2C+S">S&#xe9;bastien Zeitoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this work, we provide an upper bound for global certification of graph
homomorphism, a generalization of graph coloring. In certification, the nodes
of a network should decide if the network satisfies a given property, thanks to
small pieces of information called certificates. Here, there is only one global
certificate which is shared by all the nodes, and the property we want to
certify is the existence of a graph homomorphism to a given graph.
<br />For bipartiteness, a special case of graph homomorphism, Feuilloley and
Hirvonen proved in~\cite{FeuilloleyH18} some upper and lower bounds on the size
of the optimal certificate, and made the conjecture that their lower bound
could be improved to match their upper bound. We prove that this conjecture is
false: their lower bound was in fact optimal, and we prove it by providing the
matching upper bound using a known result of perfect hashing.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03855" title="Abstract">arXiv:2402.03855</a> [<a href="/pdf/2402.03855" title="Download PDF">pdf</a>, <a href="/format/2402.03855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Toward New Frameworks for Studying Model Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golechha%2C+S">Satvik Golechha</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+J">James Dao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mechanistic interpretability (MI) aims to understand AI models by
reverse-engineering the exact algorithms neural networks learn. Most works in
MI so far have studied behaviors and capabilities that are trivial and
token-aligned. However, most capabilities are not that trivial, which advocates
for the study of hidden representations inside these networks as the unit of
analysis. We do a literature review, formalize representations for features and
behaviors, highlight their importance and evaluation, and perform some basic
exploration in the mechanistic interpretability of representations. With
discussion and exploratory results, we justify our position that studying
representations is an important and under-studied field, and that currently
established methods in MI are not sufficient to understand representations,
thus pushing for the research community to work toward new frameworks for
studying representations.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03860" title="Abstract">arXiv:2402.03860</a> [<a href="/pdf/2402.03860" title="Download PDF">pdf</a>, <a href="/format/2402.03860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AED: Adaptable Error Detection for Few-shot Imitation Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+J">Jia-Fong Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+K">Kuo-Han Hung</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+P">Pang-Chi Lo</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+C">Chi-Ming Chung</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tsung-Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hung-Ting Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-Ting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W+H">Winston H. Hsu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We study how to report few-shot imitation (FSI) policies' behavior errors in
novel environments, a novel task named adaptable error detection (AED). The
potential to cause serious damage to surrounding areas limits the application
of FSI policies in real-world scenarios. Thus, a robust system is necessary to
notify operators when FSI policies are inconsistent with the intent of
demonstrations. We develop a cross-domain benchmark for the challenging AED
task, consisting of 329 base and 158 novel environments. This task introduces
three challenges, including (1) detecting behavior errors in novel
environments, (2) behavior errors occurring without revealing notable changes,
and (3) lacking complete temporal information of the rollout due to the
necessity of online detection. To address these challenges, we propose Pattern
Observer (PrObe) to parse discernible patterns in the policy feature
representations of normal or error states, whose effectiveness is verified in
the proposed benchmark. Through our comprehensive evaluation, PrObe
consistently surpasses strong baselines and demonstrates a robust capability to
identify errors arising from a wide range of FSI policies. Moreover, we conduct
comprehensive ablations and experiments (error correction, demonstration
quality, etc.) to validate the practicality of our proposed task and
methodology.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03861" title="Abstract">arXiv:2402.03861</a> [<a href="/pdf/2402.03861" title="Download PDF">pdf</a>, <a href="/format/2402.03861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bernoulli-barycentric rational matrix collocation method with  preconditioning for a class of evolutionary PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+W">Wei-Hua Luo</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+X">Xian-Ming Gu</a>, 
<a href="/search/math?searchtype=author&query=Carpentieri%2C+B">Bruno Carpentieri</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+J">Jun Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a Bernoulli-barycentric rational matrix collocation method for
two-dimensional evolutionary partial differential equations (PDEs) with
variable coefficients that combines Bernoulli polynomials with barycentric
rational interpolations in time and space, respectively. The theoretical
accuracy $O\left((2\pi)^{-N}+h_x^{d_x-1}+h_y^{d_y-1}\right)$ of our numerical
scheme is proven, where $N$ is the number of basis functions in time, $h_x$ and
$h_y$ are the grid sizes in the $x$, $y$-directions, respectively, and $0\leq
d_x\leq \frac{b-a}{h_x},~0\leq d_y\leq\frac{d-c}{h_y}$. For the efficient
solution of the relevant linear system arising from the discretizations, we
introduce a class of dimension expanded preconditioners that take the advantage
of structural properties of the coefficient matrices, and we present a
theoretical analysis of eigenvalue distributions of the preconditioned
matrices. The effectiveness of our proposed method and preconditioners are
studied for solving some real-world examples represented by the heat conduction
equation, the advection-diffusion equation, the wave equation and telegraph
equations.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03864" title="Abstract">arXiv:2402.03864</a> [<a href="/pdf/2402.03864" title="Download PDF">pdf</a>, <a href="/format/2402.03864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Challenges of the Nonlinear Regime for Physics-Informed Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonfanti%2C+A">Andrea Bonfanti</a>, 
<a href="/search/cs?searchtype=author&query=Bruno%2C+G">Giuseppe Bruno</a>, 
<a href="/search/cs?searchtype=author&query=Cipriani%2C+C">Cristina Cipriani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, appendix of 10 additional pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to
examine the training dynamics of Physics-Informed Neural Networks (PINNs) in
the infinite width limit. We leverage this perspective and focus on the case of
nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide
theoretical results on the different behaviors of the NTK depending on the
linearity of the differential operator. Moreover, inspired by our theoretical
results, we emphasize the advantage of employing second-order methods for
training PINNs. Additionally, we explore the convergence capabilities of
second-order methods and address the challenges of spectral bias and slow
convergence. Every theoretical result is supported by numerical examples with
both linear and nonlinear PDEs, and we validate our training method on
benchmark test cases.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03865" title="Abstract">arXiv:2402.03865</a> [<a href="/pdf/2402.03865" title="Download PDF">pdf</a>, <a href="/ps/2402.03865" title="Download PostScript">ps</a>, <a href="/format/2402.03865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and implementation of multiprotocol framework for residential  prosumer incorporation in flexibility markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gayo%2C+M">Miguel Gayo</a>, 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez%2C+F+J">Francisco Javier Rodr&#xed;guez</a>, 
<a href="/search/eess?searchtype=author&query=Santos%2C+C">Carlos Santos</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Ying Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yanpeng Wu</a>, 
<a href="/search/eess?searchtype=author&query=Vasquez%2C+J+C">Juan C. Vasquez</a>, 
<a href="/search/eess?searchtype=author&query=Guerrero%2C+J+M">Josep M. Guerrero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Internet of Things, Volume 23, 2023, 100897, ISSN 2542-6605
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The growth of distributed renewable energy in the electrical grid presents
challenges to its stability and quality. To address this at the local level,
flexibility energy strategies emerge as an innovative technique. However,
managing these strategies in residential areas becomes complex due to the
unique characteristics of each prosumer. A major challenge lies in managing
communication among diverse devices with different protocols. To address these
issues, a comprehensive framework is designed and implemented to facilitate
prosumers' integration in flexibility strategies, addressing communication at
various levels. The effectiveness of the proposed framework is demonstrated
through its implementation in a real smart home environment with diverse
devices. The framework enables seamless integration and communication between
IoT devices and IEC 61,850-compliant power devices. This research presents a
novel approach to address the challenges of managing flexibility strategies in
residential areas, providing a practical solution for prosumers to actively
participate in optimizing energy consumption and enhancing the stability and
quality of the electricity system amidst the growing integration of distributed
renewable energy.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03867" title="Abstract">arXiv:2402.03867</a> [<a href="/pdf/2402.03867" title="Download PDF">pdf</a>, <a href="/format/2402.03867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binaural sound source localization using a hybrid time and frequency  domain model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geva%2C+G">Gil Geva</a>, 
<a href="/search/cs?searchtype=author&query=Warusfel%2C+O">Olivier Warusfel</a>, 
<a href="/search/cs?searchtype=author&query=Dubnov%2C+S">Shlomo Dubnov</a>, 
<a href="/search/cs?searchtype=author&query=Dubnov%2C+T">Tammuz Dubnov</a>, 
<a href="/search/cs?searchtype=author&query=Amedi%2C+A">Amir Amedi</a>, 
<a href="/search/cs?searchtype=author&query=Hel-Or%2C+Y">Yacov Hel-Or</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper introduces a new approach to sound source localization using
head-related transfer function (HRTF) characteristics, which enable precise
full-sphere localization from raw data. While previous research focused
primarily on using extensive microphone arrays in the frontal plane, this
arrangement often encountered limitations in accuracy and robustness when
dealing with smaller microphone arrays. Our model proposes using both time and
frequency domain for sound source localization while utilizing Deep Learning
(DL) approach. The performance of our proposed model, surpasses the current
state-of-the-art results. Specifically, it boasts an average angular error of
$0.24 degrees and an average Euclidean distance of 0.01 meters, while the known
state-of-the-art gives average angular error of 19.07 degrees and average
Euclidean distance of 1.08 meters. This level of accuracy is of paramount
importance for a wide range of applications, including robotics, virtual
reality, and aiding individuals with cochlear implants (CI).
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03870" title="Abstract">arXiv:2402.03870</a> [<a href="/pdf/2402.03870" title="Download PDF">pdf</a>, <a href="/ps/2402.03870" title="Download PostScript">ps</a>, <a href="/format/2402.03870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less than one percent of words would be affected by gender-inclusive  language in German press texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Spitzer%2C+C">Carolin M&#xfc;ller-Spitzer</a>, 
<a href="/search/cs?searchtype=author&query=Ochs%2C+S">Samira Ochs</a>, 
<a href="/search/cs?searchtype=author&query=Koplenig%2C+A">Alexander Koplenig</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCdiger%2C+J">Jan-Oliver R&#xfc;diger</a>, 
<a href="/search/cs?searchtype=author&query=Wolfer%2C+S">Sascha Wolfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Research on gender and language is tightly knitted to social debates on
gender equality and non-discriminatory language use. Psycholinguistic scholars
have made significant contributions in this field. However, corpus-based
studies that investigate these matters within the context of language use are
still rare. In our study, we address the question of how much textual material
would actually have to be changed if non-gender-inclusive texts were rewritten
to be gender-inclusive. This quantitative measure is an important empirical
insight, as a recurring argument against the use of gender-inclusive German is
that it supposedly makes written texts too long and complicated. It is also
argued that gender-inclusive language has negative effects on language
learners. However, such effects are only likely if gender-inclusive texts are
very different from those that are not gender-inclusive. In our
corpus-linguistic study, we manually annotated German press texts to identify
the parts that would have to be changed. Our results show that, on average,
less than 1% of all tokens would be affected by gender-inclusive language. This
small proportion calls into question whether gender-inclusive German presents a
substantial barrier to understanding and learning the language, particularly
when we take into account the potential complexities of interpreting masculine
generics.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03877" title="Abstract">arXiv:2402.03877</a> [<a href="/pdf/2402.03877" title="Download PDF">pdf</a>, <a href="/format/2402.03877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mouselinos%2C+S">Spyridon Mouselinos</a>, 
<a href="/search/cs?searchtype=author&query=Michalewski%2C+H">Henryk Michalewski</a>, 
<a href="/search/cs?searchtype=author&query=Malinowski%2C+M">Mateusz Malinowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate ever-increasing abilities in
mathematical and algorithmic tasks, yet their geometric reasoning skills are
underexplored. We investigate LLMs' abilities in constructive geometric
problem-solving one of the most fundamental steps in the development of human
mathematical reasoning. Our work reveals notable challenges that the
state-of-the-art LLMs face in this domain despite many successes in similar
areas. LLMs exhibit biases in target variable selection and struggle with 2D
spatial relationships, often misrepresenting and hallucinating objects and
their placements. To this end, we introduce a framework that formulates an
LLMs-based multi-agents system that enhances their existing reasoning potential
by conducting an internal dialogue. This work underscores LLMs' current
limitations in geometric reasoning and improves geometric reasoning
capabilities through self-correction, collaboration, and diverse role
specializations.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03880" title="Abstract">arXiv:2402.03880</a> [<a href="/pdf/2402.03880" title="Download PDF">pdf</a>, <a href="/ps/2402.03880" title="Download PostScript">ps</a>, <a href="/format/2402.03880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aperiodic two-layer energy management system for community microgrids  based on blockchain strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Abeleira%2C+M+G">Miguel Gayo Abeleira</a>, 
<a href="/search/eess?searchtype=author&query=P%C3%A9rez%2C+C+S">Carlos Santos P&#xe9;rez</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A1nchez%2C+F+J+R">Francisco Javier Rodr&#xed;guez S&#xe1;nchez</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A1nchez%2C+P+M">Pedro Mart&#xed;n S&#xe1;nchez</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%B3mez%2C+E+S">Enrique Santiso G&#xf3;mez</a>, 
<a href="/search/eess?searchtype=author&query=Calvo%2C+J+A+J">Jos&#xe9; Antonio Jim&#xe9;nez Calvo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Energy, Volume 324, 2022, 119847, ISSN 0306-2619
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work proposes a geographically-based split of the community microgrids
into clusters of members that tend to have similar consumption and generation
profiles. Assuming a community microgrid divided into clusters, a two-layer
architecture is developed to facilitate the greater penetration of distributed
energy resources in an efficient way. The first layer, referred as the market
layer, is responsible for creating local energy markets with the aim of
maximising the economic benefits for community microgrid members. The second
layer is responsible for the network reconfiguration, which is based on the
energy balance within each cluster. This layer complies with the IEC 61850
communication standard, in order to control commercial sectionalizing and tie
switches. This allows the community microgrid network to be reconfigured to
minimise energy exchanges with the main grid. To implement this two-layer
energy management strategy, an aperiodic market approach based on Blockchain
technology, and the additional functionality offered by Smart Contracts is
adopted. This embraces the concept of energy communities since it decentralizes
the control and eliminates intermediaries. The use of aperiodic control
techniques helps to overcome the challenges of using Blockchain technology in
terms of storage, computational requirements and member privacy. The
scalability and modularity of the Smart Contract-based system allow each
cluster of members to be designed by tailoring the system to their specific
needs. The implementation of this strategy is based on low-cost off-the-shelf
devices, such as Raspberry Pi 4 Model B boards, which operate as Blockchain
nodes of community microgrid members. Finally, the strategy has been validated
by emulating two use cases based on the IEEE 123-node system network model
highlighting the benefits of the proposal.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03881" title="Abstract">arXiv:2402.03881</a> [<a href="/pdf/2402.03881" title="Download PDF">pdf</a>, <a href="/format/2402.03881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEthna: Accurate Ethereum Network Topology Discovery with Marked  Transactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chonghe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Taotao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q+Z">Quan Z. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE INFOCOM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In Ethereum, the ledger exchanges messages along an underlying Peer-to-Peer
(P2P) network to reach consistency. Understanding the underlying network
topology of Ethereum is crucial for network optimization, security and
scalability. However, the accurate discovery of Ethereum network topology is
non-trivial due to its deliberately designed security mechanism. Consequently,
existing measuring schemes cannot accurately infer the Ethereum network
topology with a low cost. To address this challenge, we propose the Distributed
Ethereum Network Analyzer (DEthna) tool, which can accurately and efficiently
measure the Ethereum network topology. In DEthna, a novel parallel measurement
model is proposed that can generate marked transactions to infer link
connections based on the transaction replacement and propagation mechanism in
Ethereum. Moreover, a workload offloading scheme is designed so that DEthna can
be deployed on multiple distributed probing nodes so as to measure a
large-scale Ethereum network at a low cost. We run DEthna on Goerli (the most
popular Ethereum test network) to evaluate its capability in discovering
network topology. The experimental results demonstrate that DEthna
significantly outperforms the state-of-the-art baselines. Based on DEthna, we
further analyze characteristics of the Ethereum network revealing that there
exist more than 50% low-degree Ethereum nodes that weaken the network
robustness.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03885" title="Abstract">arXiv:2402.03885</a> [<a href="/pdf/2402.03885" title="Download PDF">pdf</a>, <a href="/format/2402.03885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOMENT: A Family of Open Time-series Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+M">Mononito Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Szafer%2C+K">Konrad Szafer</a>, 
<a href="/search/cs?searchtype=author&query=Choudhry%2C+A">Arjun Choudhry</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yifu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Dubrawski%2C+A">Artur Dubrawski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce MOMENT, a family of open-source foundation models for
general-purpose time-series analysis. Pre-training large models on time-series
data is challenging due to (1) the absence of a large and cohesive public
time-series repository, and (2) diverse time-series characteristics which make
multi-dataset training onerous. Additionally, (3) experimental benchmarks to
evaluate these models, especially in scenarios with limited resources, time,
and supervision, are still in their nascent stages. To address these
challenges, we compile a large and diverse collection of public time-series,
called the Time-series Pile, and systematically tackle time-series-specific
challenges to unlock large-scale multi-dataset pre-training. Finally, we build
on recent work to design a benchmark to evaluate time-series foundation models
on diverse tasks and datasets in limited supervision settings. Experiments on
this benchmark demonstrate the effectiveness of our pre-trained models with
minimal data and task-specific fine-tuning. Finally, we present several
interesting empirical observations about large pre-trained time-series models.
Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03886" title="Abstract">arXiv:2402.03886</a> [<a href="/pdf/2402.03886" title="Download PDF">pdf</a>, <a href="/format/2402.03886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sattari%2C+M">Mehdi Sattari</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hao Guo</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>, 
<a href="/search/cs?searchtype=author&query=Panahi%2C+A">Ashkan Panahi</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+T">Tommy Svensson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality
with great potential for further improvement. We study full-duplex
transmissions as an effective way to improve mmWave MIMO systems. Compared to
half-duplex systems, full-duplex transmissions may offer higher data rates and
lower latency. However, full-duplex transmission is hindered by
self-interference (SI) at the receive antennas, and SI channel estimation
becomes a crucial step to make the full-duplex systems feasible. In this paper,
we address the problem of channel estimation in full-duplex mmWave MIMO systems
using neural networks (NNs). Our approach involves sharing pilot resources
between user equipments (UEs) and transmit antennas at the base station (BS),
aiming to reduce the pilot overhead in full-duplex systems and to achieve a
comparable level to that of a half-duplex system. Additionally, in the case of
separate antenna configurations in a full-duplex BS, providing channel
estimates of transmit antenna (TX) arrays to the downlink UEs poses another
challenge, as the TX arrays are not capable of receiving pilot signals. To
address this, we employ an NN to map the channel from the downlink UEs to the
receive antenna (RX) arrays to the channel from the TX arrays to the downlink
UEs. We further elaborate on how NNs perform the estimation with different
architectures, (e.g., different numbers of hidden layers), the introduction of
non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)),
and different channel conditions (e.g., low-correlated and high-correlated
channels). Our work provides novel insights into NN-based channel estimators.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03887" title="Abstract">arXiv:2402.03887</a> [<a href="/pdf/2402.03887" title="Download PDF">pdf</a>, <a href="/ps/2402.03887" title="Download PostScript">ps</a>, <a href="/format/2402.03887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shifting social norms as a driving force for linguistic change:  Struggles about language and gender in the German Bundestag
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Spitzer%2C+C">Carolin M&#xfc;ller-Spitzer</a>, 
<a href="/search/cs?searchtype=author&query=Ochs%2C+S">Samira Ochs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper focuses on language change based on shifting social norms, in
particular with regard to the debate on language and gender. It is a recurring
argument in this debate that language develops "naturally" and that "severe
interventions" - such as gender-inclusive language is often claimed to be - in
the allegedly "organic" language system are inappropriate and even "dangerous".
Such interventions are, however, not unprecedented. Socially motivated
processes of language change are neither unusual nor new. We focus in our
contribution on one important political-social space in Germany, the German
Bundestag. Taking other struggles about language and gender in the plenaries of
the Bundestag as a starting point, our article illustrates that language and
gender has been a recurring issue in the German Bundestag since the 1980s. We
demonstrate how this is reflected in linguistic practices of the Bundestag, by
the use of a) designations for gays and lesbians; b) pair forms such as
B\"urgerinnen und B\"urger (female and male citizens); and c) female forms of
addresses and personal nouns ('Pr\"asidentin' in addition to 'Pr\"asident').
Lastly, we will discuss implications of these earlier language battles for the
currently very heated debate about gender-inclusive language, especially
regarding new forms with gender symbols like the asterisk or the colon
(Lehrer*innen, Lehrer:innen; male*female teachers) which are intended to
encompass all gender identities.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03891" title="Abstract">arXiv:2402.03891</a> [<a href="/pdf/2402.03891" title="Download PDF">pdf</a>, <a href="/format/2402.03891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control-Flow Refinement for Probabilistic Programs in KoAT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lommen%2C+N">Nils Lommen</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+%C3%89">&#xc9;l&#xe9;anore Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Recently, we showed how to use control-flow refinement (CFR) to improve
automatic complexity analysis of integer programs. While up to now CFR was
limited to classical programs, in this paper we extend CFR to probabilistic
programs and show its soundness for complexity analysis. To demonstrate its
benefits, we implemented our new CFR technique in our complexity analysis tool
KoAT.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03892" title="Abstract">arXiv:2402.03892</a> [<a href="/pdf/2402.03892" title="Download PDF">pdf</a>, <a href="/format/2402.03892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bezier surfaces with prescribed diagonals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arnal%2C+A">A. Arnal</a>, 
<a href="/search/cs?searchtype=author&query=Monterde%2C+J">J. Monterde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">The affine space of all tensor product B\'ezier patches of degree nxn with
prescribed main diagonal curves is determined. First, the pair of B\'ezier
curves which can be diagonals of a B\'ezier patch is characterized. Besides
prescribing the diagonal curves, other related problems are considered, those
where boundary curves or tangent planes along boundary curves are also
prescribed.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03893" title="Abstract">arXiv:2402.03893</a> [<a href="/pdf/2402.03893" title="Download PDF">pdf</a>, <a href="/format/2402.03893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction Horizon Requirements for Automated Driving: Optimizing  Safety, Comfort, and Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez%2C+M+M">Manuel Mu&#xf1;oz S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Ploeg%2C+C">Chris van der Ploeg</a>, 
<a href="/search/cs?searchtype=author&query=Smit%2C+R">Robin Smit</a>, 
<a href="/search/cs?searchtype=author&query=Elfring%2C+J">Jos Elfring</a>, 
<a href="/search/cs?searchtype=author&query=Silvas%2C+E">Emilia Silvas</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Molengraft%2C+R">Ren&#xe9; van de Molengraft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Intelligent Vehicles Symposium. 9 pages. 10 figures. 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Predicting the movement of other road users is beneficial for improving
automated vehicle (AV) performance. However, the relationship between the time
horizon associated with these predictions and AV performance remains unclear.
Despite the existence of numerous trajectory prediction algorithms, no studies
have been conducted on how varying prediction lengths affect AV safety and
other vehicle performance metrics, resulting in undefined horizon requirements
for prediction methods. Our study addresses this gap by examining the effects
of different prediction horizons on AV performance, focusing on safety,
comfort, and efficiency. Through multiple experiments using a state-of-the-art,
risk-based predictive trajectory planner, we simulated predictions with
horizons up to 20 seconds. Based on our simulations, we propose a framework for
specifying the minimum required and optimal prediction horizons based on
specific AV performance criteria and application needs. Our results indicate
that a horizon of 1.6 seconds is required to prevent collisions with crossing
pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up
to 15 seconds improve passenger comfort. We conclude that prediction horizon
requirements are application-dependent, and recommend aiming for a prediction
horizon of 11.8 seconds as a general guideline for applications involving
crossing pedestrians.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03896" title="Abstract">arXiv:2402.03896</a> [<a href="/pdf/2402.03896" title="Download PDF">pdf</a>, <a href="/format/2402.03896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convincing Rationales for Visual Question Answering Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Vosselman%2C+G">George Vosselman</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M+Y">Michael Ying Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual Question Answering (VQA) is a challenging task of predicting the
answer to a question about the content of an image. It requires deep
understanding of both the textual question and visual image. Prior works
directly evaluate the answering models by simply calculating the accuracy of
the predicted answers. However, the inner reasoning behind the prediction is
disregarded in such a "black box" system, and we do not even know if one can
trust the predictions. In some cases, the models still get the correct answers
even when they focus on irrelevant visual regions or textual tokens, which
makes the models unreliable and illogical. To generate both visual and textual
rationales next to the predicted answer to the given image/question pair, we
propose Convincing Rationales for VQA, CRVQA. Considering the extra annotations
brought by the new outputs, {CRVQA} is trained and evaluated by samples
converted from some existing VQA datasets and their visual labels. The
extensive experiments demonstrate that the visual and textual rationales
support the prediction of the answers, and further improve the accuracy.
Furthermore, {CRVQA} achieves competitive performance on generic VQA datatsets
in the zero-shot evaluation setting. The dataset and source code will be
released under https://github.com/lik1996/CRVQA2024.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03897" title="Abstract">arXiv:2402.03897</a> [<a href="/pdf/2402.03897" title="Download PDF">pdf</a>, <a href="/format/2402.03897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Data-EnablEd Predictive Leading Cruise Control via Reachability  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaoyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haotian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiawei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Keqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Data-driven predictive control promises modelfree wave-dampening strategies
for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow. However,
the performance suffers from unknown noise and disturbances, which could occur
in offline data collection and online predictive control. In this paper, we
propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC)
method based on reachability analysis, aiming to achieve safe and optimal
control of CAVs under bounded process noise and external disturbances.
Precisely, we decouple the mixed platoon system into an error system and a
nominal system, and tighten the constraint via the data-driven reachable set
technique. Then, the enhanced safety constraint is integrated with the
data-driven predictive control formulation to achieve stronger robust control
performance for CAVs. Simulations validate the effectiveness of the proposed
method in mitigating traffic waves with better robustness.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03898" title="Abstract">arXiv:2402.03898</a> [<a href="/pdf/2402.03898" title="Download PDF">pdf</a>, <a href="/format/2402.03898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistiLLM: Towards Streamlined Distillation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jongwoo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungnyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/jongwooko/distillm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge distillation (KD) is widely used for compressing a teacher model to
a smaller student model, reducing its inference cost and memory footprint while
preserving model capabilities. However, current KD methods for auto-regressive
sequence models (e.g., large language models) suffer from missing a
standardized objective function. Moreover, the recent use of student-generated
outputs to address training-inference mismatches has significantly escalated
computational costs. To tackle these issues, we introduce DistiLLM, a more
effective and efficient KD framework for auto-regressive language models.
DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence
loss, where we unveil and leverage its theoretical properties, and (2) an
adaptive off-policy approach designed to enhance the efficiency in utilizing
student-generated outputs. Extensive experiments, including
instruction-following tasks, demonstrate the effectiveness of DistiLLM in
building high-performing student models while achieving up to 4.3$\times$
speedup compared to recent KD methods.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03900" title="Abstract">arXiv:2402.03900</a> [<a href="/pdf/2402.03900" title="Download PDF">pdf</a>, <a href="/format/2402.03900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based  Spoken Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teng%2C+D">Dechuan Teng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chunlin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Libo Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, Profile-based Spoken Language Understanding (SLU) has gained
increasing attention, which aims to incorporate various types of supplementary
profile information (i.e., Knowledge Graph, User Profile, Context Awareness) to
eliminate the prevalent ambiguities in user utterances. However, existing
approaches can only separately model different profile information, without
considering their interrelationships or excluding irrelevant and conflicting
information within them. To address the above issues, we introduce a
Heterogeneous Graph Attention Network to perform reasoning across multiple
Profile information, called Pro-HAN. Specifically, we design three types of
edges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture
interrelationships among multiple Pros. We establish a new state-of-the-art on
the ProSLU dataset, with an improvement of approximately 8% across all three
metrics. Further analysis experiments also confirm the effectiveness of our
method in modeling multi-source profile information.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03901" title="Abstract">arXiv:2402.03901</a> [<a href="/pdf/2402.03901" title="Download PDF">pdf</a>, <a href="/ps/2402.03901" title="Download PostScript">ps</a>, <a href="/format/2402.03901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch Universal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bondaschi%2C+M">Marco Bondaschi</a>, 
<a href="/search/cs?searchtype=author&query=Gastpar%2C+M">Michael Gastpar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large language models (LLMs) have recently gained much popularity due to
their surprising ability at generating human-like English sentences. LLMs are
essentially predictors, estimating the probability of a sequence of words given
the past. Therefore, it is natural to evaluate their performance from a
universal prediction perspective. In order to do that fairly, we introduce the
notion of batch regret as a modification of the classical average regret, and
we study its asymptotical value for add-constant predictors, in the case of
memoryless sources and first-order Markov sources.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03902" title="Abstract">arXiv:2402.03902</a> [<a href="/pdf/2402.03902" title="Download PDF">pdf</a>, <a href="/format/2402.03902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A phase transition between positional and semantic learning in a  solvable model of dot-product attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hugo Cui</a>, 
<a href="/search/cs?searchtype=author&query=Behrens%2C+F">Freya Behrens</a>, 
<a href="/search/cs?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>, 
<a href="/search/cs?searchtype=author&query=Zdeborov%C3%A1%2C+L">Lenka Zdeborov&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate how a dot-product attention layer learns a positional
attention matrix (with tokens attending to each other based on their respective
positions) and a semantic attention matrix (with tokens attending to each other
based on their meaning). For an algorithmic task, we experimentally show how
the same simple architecture can learn to implement a solution using either the
positional or semantic mechanism. On the theoretical side, we study the
learning of a non-linear self-attention layer with trainable tied and low-rank
query and key matrices. In the asymptotic limit of high-dimensional data and a
comparably large number of training samples, we provide a closed-form
characterization of the global minimum of the non-convex empirical loss
landscape. We show that this minimum corresponds to either a positional or a
semantic mechanism and evidence an emergent phase transition from the former to
the latter with increasing sample complexity. Finally, we compare the
dot-product attention layer to linear positional baseline, and show that it
outperforms the latter using the semantic mechanism provided it has access to
sufficient data.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03903" title="Abstract">arXiv:2402.03903</a> [<a href="/pdf/2402.03903" title="Download PDF">pdf</a>, <a href="/format/2402.03903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compound Returns Reduce Variance in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daley%2C+B">Brett Daley</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Martha White</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+M+C">Marlos C. Machado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 8 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multistep returns, such as $n$-step returns and $\lambda$-returns, are
commonly used to improve the sample efficiency of reinforcement learning (RL)
methods. The variance of the multistep returns becomes the limiting factor in
their length; looking too far into the future increases variance and reverses
the benefits of multistep learning. In our work, we demonstrate the ability of
compound returns -- weighted averages of $n$-step returns -- to reduce
variance. We prove for the first time that any compound return with the same
contraction modulus as a given $n$-step return has strictly lower variance. We
additionally prove that this variance-reduction property improves the
finite-sample complexity of temporal-difference learning under linear function
approximation. Because general compound returns can be expensive to implement,
we introduce two-bootstrap returns which reduce variance while remaining
efficient, even when using minibatched experience replay. We conduct
experiments showing that two-bootstrap returns can improve the sample
efficiency of $n$-step deep RL agents, with little additional computational
cost.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03904" title="Abstract">arXiv:2402.03904</a> [<a href="/pdf/2402.03904" title="Download PDF">pdf</a>, <a href="/format/2402.03904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep  Functional Maps for Unsupervised Shape Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feifan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingsong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Ling Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinru Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengjun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a novel constraint called Multiple Spectral filter Operators
Preservation (MSFOR) to compute functional maps and based on it, develop an
efficient deep functional map architecture called Deep MSFOP for shape
matching. The core idea is that, instead of using the general descriptor
preservation constraint, we require our maps to preserve multiple spectral
filter operators. This allows us to incorporate more informative geometrical
information, contained in different frequency bands of functions, into the
functional map computing. This can be confirmed by that some previous
techniques like wavelet preservation and LBO commutativity are actually our
special cases. Moreover, we also develop a very efficient way to compute the
maps with MSFOP constraint, which can be conveniently embedded into the deep
learning, especially having learnable filter operators. Utilizing the above
results, we finally design our Deep MSFOP pipeline, equipped with a suitable
unsupervised loss jointly penalizing the functional map and the underlying
pointwise map. Our deep functional map has notable advantages, including that
the functional map is more geometrically informative and guaranteed to be
proper, and the computing is numerically stable. Extensive experimental results
on different datasets demonstrate that our approach outperforms the existing
state-of-the-art methods, especially in challenging settings like non-isometric
and inconsistent topology datasets.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03905" title="Abstract">arXiv:2402.03905</a> [<a href="/pdf/2402.03905" title="Download PDF">pdf</a>, <a href="/ps/2402.03905" title="Download PostScript">ps</a>, <a href="/format/2402.03905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Employee Turnover Analysis Using Machine Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karimi%2C+M">Mahyar Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Viliyani%2C+K+S">Kamyar Seyedkazem Viliyani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 11 feagures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Employee's knowledge is an organization asset. Turnover may impose apparent
and hidden costs and irreparable damages. To overcome and mitigate this risk,
employee's condition should be monitored. Due to high complexity of analyzing
well-being features, employee's turnover predicting can be delegated to machine
learning techniques. In this paper, we discuss employee's attrition rate. Three
different supervised learning algorithms comprising AdaBoost, SVM and
RandomForest are used to benchmark employee attrition accuracy. Attained models
can help out at establishing predictive analytics.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03907" title="Abstract">arXiv:2402.03907</a> [<a href="/pdf/2402.03907" title="Download PDF">pdf</a>, <a href="/format/2402.03907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Large Language Models into Extended Reality: Opportunities and  Challenges for Inclusion, Engagement, and Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozkir%2C+E">Efe Bozkir</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zdel%2C+S">S&#xfc;leyman &#xd6;zdel</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+K+H+C">Ka Hei Carrie Lau</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent developments in computer graphics, hardware, artificial intelligence
(AI), and human-computer interaction likely lead to extended reality (XR)
devices and setups being more pervasive. While these devices and setups provide
users with interactive, engaging, and immersive experiences with different
sensing modalities, such as eye and hand trackers, many non-player characters
are utilized in a pre-scripted way or by conventional AI techniques. In this
paper, we argue for using large language models (LLMs) in XR by embedding them
in virtual avatars or as narratives to facilitate more inclusive experiences
through prompt engineering according to user profiles and fine-tuning the LLMs
for particular purposes. We argue that such inclusion will facilitate diversity
for XR use. In addition, we believe that with the versatile conversational
capabilities of LLMs, users will engage more with XR environments, which might
help XR be more used in everyday life. Lastly, we speculate that combining the
information provided to LLM-powered environments by the users and the biometric
data obtained through the sensors might lead to novel privacy invasions. While
studying such possible privacy invasions, user privacy concerns and preferences
should also be investigated. In summary, despite some challenges, embedding
LLMs into XR is a promising and novel research area with several opportunities.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03908" title="Abstract">arXiv:2402.03908</a> [<a href="/pdf/2402.03908" title="Download PDF">pdf</a>, <a href="/format/2402.03908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EscherNet: A Generative Model for Scalable View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xin Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shikun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xiaoyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Taher%2C+M">Marwan Taher</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Davison%2C+A+J">Andrew J. Davison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://kxhit.github.io/EscherNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce EscherNet, a multi-view conditioned diffusion model for view
synthesis. EscherNet learns implicit and generative 3D representations coupled
with a specialised camera positional encoding, allowing precise and continuous
relative control of the camera transformation between an arbitrary number of
reference and target views. EscherNet offers exceptional generality,
flexibility, and scalability in view synthesis -- it can generate more than 100
consistent target views simultaneously on a single consumer-grade GPU, despite
being trained with a fixed number of 3 reference views to 3 target views. As a
result, EscherNet not only addresses zero-shot novel view synthesis, but also
naturally unifies single- and multi-image 3D reconstruction, combining these
diverse tasks into a single, cohesive framework. Our extensive experiments
demonstrate that EscherNet achieves state-of-the-art performance in multiple
benchmarks, even when compared to methods specifically tailored for each
individual problem. This remarkable versatility opens up new directions for
designing scalable neural architectures for 3D vision. Project page:
\url{https://kxhit.github.io/EscherNet}.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03910" title="Abstract">arXiv:2402.03910</a> [<a href="/pdf/2402.03910" title="Download PDF">pdf</a>, <a href="/format/2402.03910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A  Network Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalhor%2C+G">Ghazal Kalhor</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Studying acquisitions offers invaluable insights into startup trends, aiding
informed investment decisions for businesses. However, the scarcity of studies
in this domain prompts our focus on shedding light in this area. Employing
Crunchbase data, our study delves into the global network of company
acquisitions using diverse network analysis techniques. Our findings unveil an
acquisition network characterized by a primarily sparse structure comprising
localized dense connections. We reveal a prevalent tendency among organizations
to acquire companies within their own country and industry. Furthermore, our
temporal analysis indicates a growth in network communities over time,
accompanied by a trend toward a sparser network. Through centrality metrics
computation in the cross-city acquisition network, we identify New York,
London, and San Francisco as pivotal and central hubs in the global economic
landscape. Finally, we show that the United States, United Kingdom, and Germany
are predominant countries in international acquisitions.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03915" title="Abstract">arXiv:2402.03915</a> [<a href="/pdf/2402.03915" title="Download PDF">pdf</a>, <a href="/format/2402.03915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Metrics that Maximise Power for Accelerated A/B-Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeunen%2C+O">Olivier Jeunen</a>, 
<a href="/search/cs?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Online controlled experiments are a crucial tool to allow for confident
decision-making in technology companies. A North Star metric is defined (such
as long-term revenue or user retention), and system variants that statistically
significantly improve on this metric in an A/B-test can be considered superior.
North Star metrics are typically delayed and insensitive. As a result, the cost
of experimentation is high: experiments need to run for a long time, and even
then, type-II errors (i.e. false negatives) are prevalent.
<br />We propose to tackle this by learning metrics from short-term signals that
directly maximise the statistical power they harness with respect to the North
Star. We show that existing approaches are prone to overfitting, in that higher
average metric sensitivity does not imply improved type-II errors, and propose
to instead minimise the $p$-values a metric would have produced on a log of
past experiments. We collect such datasets from two social media applications
with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.
Empirical results show that we are able to increase statistical power by up to
78% when using our learnt metrics stand-alone, and by up to 210% when used in
tandem with the North Star. Alternatively, we can obtain constant statistical
power at a sample size that is down to 12% of what the North Star requires,
significantly reducing the cost of experimentation.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03916" title="Abstract">arXiv:2402.03916</a> [<a href="/pdf/2402.03916" title="Download PDF">pdf</a>, <a href="/format/2402.03916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Detect Rumors on Social Media?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this work, we investigate to use Large Language Models (LLMs) for rumor
detection on social media. However, it is challenging for LLMs to reason over
the entire propagation information on social media, which contains news
contents and numerous comments, due to LLMs may not concentrate on key clues in
the complex propagation information, and have trouble in reasoning when facing
massive and redundant information. Accordingly, we propose an LLM-empowered
Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to
reason over important clues in news and comments, and divide the entire
propagation information into a Chain-of-Propagation for reducing LLMs' burden.
We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD
outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%.
Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus
shows more promising rumor detection ability in few-shot or zero-shot
scenarios.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03917" title="Abstract">arXiv:2402.03917</a> [<a href="/pdf/2402.03917" title="Download PDF">pdf</a>, <a href="/format/2402.03917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elastic Feature Consolidation for Cold Start Exemplar-free Incremental  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magistri%2C+S">Simone Magistri</a>, 
<a href="/search/cs?searchtype=author&query=Trinci%2C+T">Tomaso Trinci</a>, 
<a href="/search/cs?searchtype=author&query=Soutif-Cormerais%2C+A">Albin Soutif-Cormerais</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>, 
<a href="/search/cs?searchtype=author&query=Bagdanov%2C+A+D">Andrew D. Bagdanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Twelfth International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a
sequence of tasks without having access to previous task data. In this paper,
we consider the challenging Cold Start scenario in which insufficient data is
available in the first task to learn a high-quality backbone. This is
especially challenging for EFCIL since it requires high plasticity, which
results in feature drift which is difficult to compensate for in the
exemplar-free setting. To address this problem, we propose a simple and
effective approach that consolidates feature representations by regularizing
drift in directions highly relevant to previous tasks and employs prototypes to
reduce task-recency bias. Our method, called Elastic Feature Consolidation
(EFC), exploits a tractable second-order approximation of feature drift based
on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in
feature space which we use to regularize feature drift in important directions
and to update Gaussian prototypes used in a novel asymmetric cross entropy loss
which effectively balances prototype rehearsal with data from new tasks.
Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and
ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to
learn new tasks by maintaining model plasticity and significantly outperform
the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03918" title="Abstract">arXiv:2402.03918</a> [<a href="/pdf/2402.03918" title="Download PDF">pdf</a>, <a href="/format/2402.03918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynastic Potential Crossover Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>, 
<a href="/search/cs?searchtype=author&query=Ochoa%2C+G">Gabriela Ochoa</a>, 
<a href="/search/cs?searchtype=author&query=Whitley%2C+D">Darrell Whitley</a>, 
<a href="/search/cs?searchtype=author&query=Tin%C3%B3s%2C+R">Renato Tin&#xf3;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Evol. Comput. 30(3): 409-446 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">An optimal recombination operator for two parent solutions provides the best
solution among those that take the value for each variable from one of the
parents (gene transmission property). If the solutions are bit strings, the
offspring of an optimal recombination operator is optimal in the smallest
hyperplane containing the two parent solutions. Exploring this hyperplane is
computationally costly, in general, requiring exponential time in the worst
case. However, when the variable interaction graph of the objective function is
sparse, exploration can be done in polynomial time.
<br />In this paper, we present a recombination operator, called Dynastic Potential
Crossover (DPX), that runs in polynomial time and behaves like an optimal
recombination operator for low-epistasis combinatorial problems. We compare
this operator, both theoretically and experimentally, with traditional
crossover operators, like uniform crossover and network crossover, and with two
recently defined efficient recombination operators: partition crossover and
articulation points partition crossover. The empirical comparison uses NKQ
Landscapes and MAX-SAT instances. DPX outperforms the other crossover operators
in terms of quality of the offspring and provides better results included in a
trajectory and a population-based metaheuristic, but it requires more time and
memory to compute the offspring.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03919" title="Abstract">arXiv:2402.03919</a> [<a href="/pdf/2402.03919" title="Download PDF">pdf</a>, <a href="/format/2402.03919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensing Mutual Information with Random Signals in Gaussian Channels:  Bridging Sensing and Communication Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiajin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2311.07081">arXiv:2311.07081</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Sensing performance is typically evaluated by classical radar metrics, such
as Cramer-Rao bound and signal-to-clutter-plus-noise ratio. The recent
development of the integrated sensing and communication (ISAC) framework
motivated the efforts to unify the performance metric for sensing and
communication, where mutual information (MI) was proposed as a sensing
performance metric with deterministic signals. However, the need of
communication in ISAC systems necessitates the transmission of random signals
for sensing applications, whereas an explicit evaluation for the sensing mutual
information (SMI) with random signals is not yet available in the literature.
This paper aims to fill the research gap and investigate the unification of
sensing and communication performance metrics. For that purpose, we first
derive the explicit expression for the SMI with random signals utilizing random
matrix theory. On top of that, we further build up the connections between SMI
and traditional sensing metrics, such as ergodic minimum mean square error
(EMMSE), ergodic linear minimum mean square error (ELMMSE), and ergodic
Bayesian Cram\'{e}r-Rao bound (EBCRB). Such connections open up the opportunity
to unify sensing and communication performance metrics, which facilitates the
analysis and design for ISAC systems. Finally, SMI is utilized to optimize the
precoder for both sensing-only and ISAC applications. Simulation results
validate the accuracy of the theoretical results and the effectiveness of the
proposed precoding designs.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03921" title="Abstract">arXiv:2402.03921</a> [<a href="/pdf/2402.03921" title="Download PDF">pdf</a>, <a href="/format/2402.03921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models to Enhance Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tennison Liu</a>, 
<a href="/search/cs?searchtype=author&query=Astorga%2C+N">Nicol&#xe1;s Astorga</a>, 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Poster at ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Bayesian optimization (BO) is a powerful approach for optimizing complex and
expensive-to-evaluate black-box functions. Its importance is underscored in
many applications, notably including hyperparameter tuning, but its efficacy
depends on efficiently balancing exploration and exploitation. While there has
been substantial progress in BO methods, striking this balance still remains a
delicate process. In this light, we present \texttt{LLAMBO}, a novel approach
that integrates the capabilities of large language models (LLM) within BO. At a
high level, we frame the BO problem in natural language terms, enabling LLMs to
iteratively propose promising solutions conditioned on historical evaluations.
More specifically, we explore how combining contextual understanding, few-shot
learning proficiency, and domain knowledge of LLMs can enhance various
components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is
effective at zero-shot warmstarting, and improves surrogate modeling and
candidate sampling, especially in the early stages of search when observations
are sparse. Our approach is performed in context and does not require LLM
finetuning. Additionally, it is modular by design, allowing individual
components to be integrated into existing BO frameworks, or function cohesively
as an end-to-end method. We empirically validate \texttt{LLAMBO}'s efficacy on
the problem of hyperparameter tuning, highlighting strong empirical performance
across a range of diverse benchmarks, proprietary, and synthetic tasks.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03922" title="Abstract">arXiv:2402.03922</a> [<a href="/pdf/2402.03922" title="Download PDF">pdf</a>, <a href="/format/2402.03922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive advantage of URLLC vs. eMBB for supporting  timeliness-relevant services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guijarro%2C+L">Luis Guijarro</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+J">Jose-Ramon Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Pla%2C+V">Vicent Pla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">5G specifications promise a common and flexible-enough network infrastructure
capable of satisfying diverse requirements of both current and future use
cases. Two service types standardized in 5G are eMBB, without stringent delay
guarantee, and URLLC, with stringent delay guarantee. We focus on a use case
where data timeliness is the relevant quality parameter. We provide an economic
rationale for the support of data-based services, that is, from the point of
view of the profits attained by the service providers and operators (SP). More
specifically, we focus on data-based services the quality of which is related
to the Age of Information, and we assess two alternatives for the support of
this sort of services by means of a 5G network: one that is based on the eMBB
service type, and one that is based on the URLLC service type. These assessment
is conducted in a duopoly scenario. We conclude that URLLC support provides a
competitive advantage to an SP against a competitor SP that supports its
service offering on eMBB. And that there is a slightly better situation for the
users when the URLLC QoS constraint is stringent.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03923" title="Abstract">arXiv:2402.03923</a> [<a href="/pdf/2402.03923" title="Download PDF">pdf</a>, <a href="/format/2402.03923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Return-Aligned Decision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+T">Tsunehiko Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>, 
<a href="/search/cs?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>, 
<a href="/search/cs?searchtype=author&query=Morimura%2C+T">Tetsuro Morimura</a>, 
<a href="/search/cs?searchtype=author&query=Simo-Serra%2C+E">Edgar Simo-Serra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traditional approaches in offline reinforcement learning aim to learn the
optimal policy that maximizes the cumulative reward, also known as return.
However, as applications broaden, it becomes increasingly crucial to train
agents that not only maximize the returns, but align the actual return with a
specified target return, giving control over the agent's performance. Decision
Transformer (DT) optimizes a policy that generates actions conditioned on the
target return through supervised learning and is equipped with a mechanism to
control the agent using the target return. Despite being designed to align the
actual return with the target return, we have empirically identified a
discrepancy between the actual return and the target return in DT. In this
paper, we propose Return-Aligned Decision Transformer (RADT), designed to
effectively align the actual return with the target return. Our model decouples
returns from the conventional input sequence, which typically consists of
returns, states, and actions, to enhance the relationships between returns and
states, as well as returns and actions. Extensive experiments show that RADT
reduces the discrepancies between the actual return and the target return of
DT-based methods.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03924" title="Abstract">arXiv:2402.03924</a> [<a href="/pdf/2402.03924" title="Download PDF">pdf</a>, <a href="/format/2402.03924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Analysis of U.S. Non-Fatal Opioid-Involved Overdose Journeys,  2018-2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCabe%2C+L+H">Lucas H. McCabe</a>, 
<a href="/search/cs?searchtype=author&query=Masuda%2C+N">Naoki Masuda</a>, 
<a href="/search/cs?searchtype=author&query=Casillas%2C+S">Shannon Casillas</a>, 
<a href="/search/cs?searchtype=author&query=Danneman%2C+N">Nathan Danneman</a>, 
<a href="/search/cs?searchtype=author&query=Alic%2C+A">Alen Alic</a>, 
<a href="/search/cs?searchtype=author&query=Law%2C+R">Royal Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">We present a nation-wide network analysis of non-fatal opioid-involved
overdose journeys in the United States. Leveraging a unique proprietary dataset
of Emergency Medical Services incidents, we construct a journey-to-overdose
geospatial network capturing nearly half a million opioid-involved overdose
events spanning 2018-2023. We analyze the structure and sociological profile of
the nodes, which are counties or their equivalents, characterize the
distribution of overdose journey lengths, and investigate changes in the
journey network between 2018 and 2023. Our findings include that authority and
hub nodes identified by the HITS algorithm tend to be located in urban areas
and involved in overdose journeys with particularly long geographical
distances.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03927" title="Abstract">arXiv:2402.03927</a> [<a href="/pdf/2402.03927" title="Download PDF">pdf</a>, <a href="/format/2402.03927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in  Closed-Source LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balloccu%2C+S">Simone Balloccu</a>, 
<a href="/search/cs?searchtype=author&query=Schmidtov%C3%A1%2C+P">Patr&#xed;cia Schmidtov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Lango%2C+M">Mateusz Lango</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Natural Language Processing (NLP) research is increasingly focusing on the
use of Large Language Models (LLMs), with some of the most popular ones being
either fully or partially closed-source. The lack of access to model details,
especially regarding training data, has repeatedly raised concerns about data
contamination among researchers. Several attempts have been made to address
this issue, but they are limited to anecdotal evidence and trial and error.
Additionally, they overlook the problem of \emph{indirect} data leaking, where
models are iteratively improved by using data coming from users. In this work,
we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and
GPT-4, the most prominently used LLMs today, in the context of data
contamination. By analysing 255 papers and considering OpenAI's data usage
policy, we extensively document the amount of data leaked to these models
during the first year after the model's release. We report that these models
have been globally exposed to $\sim$4.7M samples from 263 benchmarks. At the
same time, we document a number of evaluation malpractices emerging in the
reviewed papers, such as unfair or missing baseline comparisons and
reproducibility issues. We release our results as a collaborative project on
https://leak-llm.github.io/, where other researchers can contribute to our
efforts.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03928" title="Abstract">arXiv:2402.03928</a> [<a href="/pdf/2402.03928" title="Download PDF">pdf</a>, <a href="/format/2402.03928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating the Core via Iterative Coalition Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gemp%2C+I">Ian Gemp</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Marris%2C+L">Luke Marris</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yiran Mao</a>, 
<a href="/search/cs?searchtype=author&query=Du%C3%A9%C3%B1ez-Guzm%C3%A1n%2C+E">Edgar Du&#xe9;&#xf1;ez-Guzm&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Perrin%2C+S">Sarah Perrin</a>, 
<a href="/search/cs?searchtype=author&query=Gyorgy%2C+A">Andras Gyorgy</a>, 
<a href="/search/cs?searchtype=author&query=Elie%2C+R">Romuald Elie</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Kaisers%2C+M">Michael Kaisers</a>, 
<a href="/search/cs?searchtype=author&query=Hennes%2C+D">Daniel Hennes</a>, 
<a href="/search/cs?searchtype=author&query=Bullard%2C+K">Kalesha Bullard</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+K">Kate Larson</a>, 
<a href="/search/cs?searchtype=author&query=Bachrach%2C+Y">Yoram Bachrach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The core is a central solution concept in cooperative game theory, defined as
the set of feasible allocations or payments such that no subset of agents has
incentive to break away and form their own subgroup or coalition. However, it
has long been known that the core (and approximations, such as the least-core)
are hard to compute. This limits our ability to analyze cooperative games in
general, and to fully embrace cooperative game theory contributions in domains
such as explainable AI (XAI), where the core can complement the Shapley values
to identify influential features or instances supporting predictions by
black-box models. We propose novel iterative algorithms for computing variants
of the core, which avoid the computational bottleneck of many other approaches;
namely solving large linear programs. As such, they scale better to very large
problems as we demonstrate across different classes of cooperative games,
including weighted voting games, induced subgraph games, and marginal
contribution networks. We also explore our algorithms in the context of XAI,
providing further evidence of the power of the core for such applications.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03929" title="Abstract">arXiv:2402.03929</a> [<a href="/pdf/2402.03929" title="Download PDF">pdf</a>, <a href="/format/2402.03929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Viscous regularization of the MHD equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dao%2C+T+A">Tuan Anh Dao</a>, 
<a href="/search/math?searchtype=author&query=Lundgren%2C+L">Lukas Lundgren</a>, 
<a href="/search/math?searchtype=author&query=Nazarov%2C+M">Murtazo Nazarov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Nonlinear conservation laws such as the system of ideal magnetohydrodynamics
(MHD) equations may develop singularities over time. In these situations,
viscous regularization is a common approach to regain regularity of the
solution. In this paper, we present a new viscous flux to regularize the MHD
equations which holds many attractive properties. In particular, we prove that
the proposed viscous flux preserves positivity of density and internal energy,
satisfies the minimum entropy principle, is consistent with all generalized
entropies, and is Galilean and rotationally invariant. We also provide a
variation of the viscous flux that conserves angular momentum. To make the
analysis more useful for numerical schemes, the divergence of the magnetic
field is not assumed to be zero. Using continuous finite elements, we show
several numerical experiments including contact waves and magnetic
reconnection.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03933" title="Abstract">arXiv:2402.03933</a> [<a href="/pdf/2402.03933" title="Download PDF">pdf</a>, <a href="/ps/2402.03933" title="Download PostScript">ps</a>, <a href="/format/2402.03933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development of a Evaluation Tool for Age-Appropriate Software in Aging  Environments: A Delphi Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zhenggang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yougxiang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongtu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinru Chen</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+N">Ning An</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rui%2C+G">Guoxin Rui</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jing Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Objective: We aimed to develop a dependable reliable tool for assessing
software ageappropriateness. Methods: We conducted a systematic review to get
the indicators of technology ageappropriateness from studies from January 2000
to April 2023.This study engaged 25 experts from the fields of anthropology,
sociology,and social technology research across, three rounds of Delphi
consultations were conducted. Experts were asked to screen, assess, add and
provide feedback on the preliminary indicators identified in the initial
indicator pool. Result: We found 76 criterias for evaluating quality criteria
was extracted, grouped into 11 distinct domains. After completing three rounds
of Delphi consultations,experts drew upon their personal
experiences,theoretical frameworks,and industry insights to arrive at a
three-dimensional structure for the evaluation tooluser experience,product
quality,and social promotion.These metrics were further distilled into a
16-item scale, and a corresponding questionnaire was formulated.The developed
tool exhibited strong internal reliability(Cronbach's Alpha is 0.867)and
content validity(S-CVI is 0.93). Conclusion: This tool represents a
straightforward,objective,and reliable mechanism for evaluating software's
appropriateness across age groups. Moreover,it offers valuable insights and
practical guidance for designing and developing of high-quality age-appropriate
software,and assisst age groups to select software they like.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03938" title="Abstract">arXiv:2402.03938</a> [<a href="/pdf/2402.03938" title="Download PDF">pdf</a>, <a href="/ps/2402.03938" title="Download PostScript">ps</a>, <a href="/format/2402.03938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apparent Distance and a Notion of BCH Multivariate Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernal%2C+J+J">Jos&#xe9; Joaqu&#xed;n Bernal</a>, 
<a href="/search/cs?searchtype=author&query=Bueno-Carre%C3%B1o%2C+D+H">Diana H. Bueno-Carre&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B3n%2C+J+J">Juan Jacobo Sim&#xf3;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper is devoted to studying two main problems: 1) computing the
apparent distance of an Abelian code and 2) giving a notion of Bose,
Ray-Chaudhuri, Hocquenghem (BCH) multivariate code. To do this, we first
strengthen the notion of an apparent distance by introducing the notion of a
strong apparent distance; then, we present an algorithm to compute the strong
apparent distance of an Abelian code, based on some manipulations of
hypermatrices associated with its generating idempotent. Our method uses less
computations than those given by Camion and Sabin; furthermore, in the
bivariate case, the order of computation complexity is reduced from exponential
to linear. Then, we use our techniques to develop a notion of a BCH code in the
multivariate case, and we extend most of the classical results on cyclic BCH
codes. Finally, we apply our method to the design of Abelian codes with maximum
dimension with respect to a fixed apparent distance and a fixed length.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03941" title="Abstract">arXiv:2402.03941</a> [<a href="/pdf/2402.03941" title="Download PDF">pdf</a>, <a href="/format/2402.03941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovery of the Hidden World with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Mingming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">James Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version of an ongoing project; Chenxi and Yongqiang contributed equally; 26 pages, 41 figures; Project page: <a href="https://causalcoat.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">Science originates with discovering new causal knowledge from a combination
of known facts and observations. Traditional causal discovery approaches mainly
rely on high-quality measured variables, usually given by human experts, to
find causal relations. However, the causal variables are usually unavailable in
a wide range of real-world applications. The rise of large language models
(LLMs) that are trained to learn rich knowledge from the massive observations
of the world, provides a new opportunity to assist with discovering high-level
hidden variables from the raw observational data. Therefore, we introduce COAT:
Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer
that extracts the potential causal factors from unstructured data. Moreover,
LLMs can also be instructed to provide additional information used to collect
data values (e.g., annotation criteria) and to further parse the raw
unstructured data into structured data. The annotated data will be fed to a
causal learning module (e.g., the FCI algorithm) that provides both rigorous
explanations of the data, as well as useful feedback to further improve the
extraction of causal factors by LLMs. We verify the effectiveness of COAT in
uncovering the underlying causal system with two case studies of review rating
analysis and neuropathic diagnosis.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03944" title="Abstract">arXiv:2402.03944</a> [<a href="/pdf/2402.03944" title="Download PDF">pdf</a>, <a href="/format/2402.03944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMUSIC: IMU-based Facial Expression Capture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Youjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yiwen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruiqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hengan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongyang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yingwenqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yingsheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guanpeng Long</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Go to IMUSIC project page <a href="https://sites.google.com/view/projectpage-imusic">this https URL</a> watch our video at <a href="https://youtu.be/rPusR6b43ng">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">For facial motion capture and analysis, the dominated solutions are generally
based on visual cues, which cannot protect privacy and are vulnerable to
occlusions. Inertial measurement units (IMUs) serve as potential rescues yet
are mainly adopted for full-body motion capture. In this paper, we propose
IMUSIC to fill the gap, a novel path for facial expression capture using purely
IMU signals, significantly distant from previous visual solutions.The key
design in our IMUSIC is a trilogy. We first design micro-IMUs to suit facial
capture, companion with an anatomy-driven IMU placement scheme. Then, we
contribute a novel IMU-ARKit dataset, which provides rich paired IMU/visual
signals for diverse facial expressions and performances. Such unique
multi-modality brings huge potential for future directions like IMU-based
facial behavior analysis. Moreover, utilizing IMU-ARKit, we introduce a strong
baseline approach to accurately predict facial blendshape parameters from
purely IMU signals. Specifically, we tailor a Transformer diffusion model with
a two-stage training strategy for this novel tracking task. The IMUSIC
framework empowers us to perform accurate facial capture in scenarios where
visual methods falter and simultaneously safeguard user privacy. We conduct
extensive experiments about both the IMU configuration and technical components
to validate the effectiveness of our IMUSIC approach. Notably, IMUSIC enables
various potential and novel applications, i.e., privacy-protecting facial
capture, hybrid capture against occlusions, or detecting minute facial
movements that are often invisible through visual cues. We will release our
dataset and implementations to enrich more possibilities of facial capture and
analysis in our community.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03945" title="Abstract">arXiv:2402.03945</a> [<a href="/pdf/2402.03945" title="Download PDF">pdf</a>, <a href="/format/2402.03945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using metaheuristics for the location of bicycle stations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cintrano%2C+C">Christian Cintrano</a>, 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>, 
<a href="/search/cs?searchtype=author&query=Alba%2C+E">Enrique Alba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Syst. Appl. 161: 113684 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">In this work, we solve the problem of finding the best locations to place
stations for depositing/collecting shared bicycles. To do this, we model the
problem as the p-median problem, that is a major existing localization problem
in optimization. The p-median problem seeks to place a set of facilities
(bicycle stations) in a way that minimizes the distance between a set of
clients (citizens) and their closest facility (bike station). We have used a
genetic algorithm, iterated local search, particle swarm optimization,
simulated annealing, and variable neighbourhood search, to find the best
locations for the bicycle stations and study their comparative advantages. We
use irace to parameterize each algorithm automatically, to contribute with a
methodology to fine-tune algorithms automatically. We have also studied
different real data (distance and weights) from diverse open data sources from
a real city, Malaga (Spain), hopefully leading to a final smart city
application. We have compared our results with the implemented solution in
Malaga. Finally, we have analyzed how we can use our proposal to improve the
existing system in the city by adding more stations.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03946" title="Abstract">arXiv:2402.03946</a> [<a href="/pdf/2402.03946" title="Download PDF">pdf</a>, <a href="/ps/2402.03946" title="Download PostScript">ps</a>, <a href="/format/2402.03946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioNet-XR: Biological Network Visualization Framework for Virtual  Reality and Mixed Reality Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senderin%2C+B">Busra Senderin</a>, 
<a href="/search/cs?searchtype=author&query=Tuncbag%2C+N">Nurcan Tuncbag</a>, 
<a href="/search/cs?searchtype=author&query=Surer%2C+E">Elif Surer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Protein-protein interaction networks (PPIN) enable the study of cellular
processes in organisms. Visualizing PPINs in extended reality (XR), including
virtual reality (VR) and mixed reality (MR), is crucial for exploring
subnetworks, evaluating protein positions, and collaboratively analyzing and
discussing on networks with the help of recent technological advancements.
Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in
VR and MR environments. BioNet-XR was developed with the Unity3D game engine.
Our framework provides state-of-the-art methods and visualization features
including teleportation between nodes, general and first-person view to explore
the network, subnetwork construction via PageRank, Steiner tree, and all-pair
shortest path algorithms for a given set of initial nodes. We used usability
tests to gather feedback from both specialists (bioinformaticians) and
generalists (multidisciplinary groups), addressing the need for usability
evaluations of visualization tools. In the MR version of BioNet-XR, users can
seamlessly transition to real-world environments and interact with protein
interaction networks. BioNet-XR is highly modular and adaptable for
visualization of other biological networks, such as metabolic and regulatory
networks, and extension with additional network methods.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03947" title="Abstract">arXiv:2402.03947</a> [<a href="/pdf/2402.03947" title="Download PDF">pdf</a>, <a href="/format/2402.03947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Collision-free Flight Exploiting Deep  Collision Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+M">Mihir Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Alexis%2C+K">Kostas Alexis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures. Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work contributes a novel deep navigation policy that enables
collision-free flight of aerial robots based on a modular approach exploiting
deep collision encoding and reinforcement learning. The proposed solution
builds upon a deep collision encoder that is trained on both simulated and real
depth images using supervised learning such that it compresses the
high-dimensional depth data to a low-dimensional latent space encoding
collision information while accounting for the robot size. This compressed
encoding is combined with an estimate of the robot's odometry and the desired
target location to train a deep reinforcement learning navigation policy that
offers low-latency computation and robust sim2real performance. A set of
simulation and experimental studies in diverse environments are conducted and
demonstrate the efficiency of the emerged behavior and its resilience in
real-life deployments.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03948" title="Abstract">arXiv:2402.03948</a> [<a href="/pdf/2402.03948" title="Download PDF">pdf</a>, <a href="/format/2402.03948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Student Profiles Within Online Judge Systems Using  Explainable Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rico-Juan%2C+J+R">Juan Ram&#xf3;n Rico-Juan</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Cartagena%2C+V+M">V&#xed;ctor M. S&#xe1;nchez-Cartagena</a>, 
<a href="/search/cs?searchtype=author&query=Valero-Mas%2C+J+J">Jose J. Valero-Mas</a>, 
<a href="/search/cs?searchtype=author&query=Gallego%2C+A+J">Antonio Javier Gallego</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Learning Technologies ( Volume: 16, Issue: 6,
  December 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Online Judge (OJ) systems are typically considered within programming-related
courses as they yield fast and objective assessments of the code developed by
the students. Such an evaluation generally provides a single decision based on
a rubric, most commonly whether the submission successfully accomplished the
assignment. Nevertheless, since in an educational context such information may
be deemed insufficient, it would be beneficial for both the student and the
instructor to receive additional feedback about the overall development of the
task. This work aims to tackle this limitation by considering the further
exploitation of the information gathered by the OJ and automatically inferring
feedback for both the student and the instructor. More precisely, we consider
the use of learning-based schemes -- particularly, multi-instance learning
(MIL) and classical machine learning formulations -- to model student behavior.
Besides, explainable artificial intelligence (XAI) is contemplated to provide
human-understandable feedback. The proposal has been evaluated considering a
case of study comprising 2500 submissions from roughly 90 different students
from a programming-related course in a computer science degree. The results
obtained validate the proposal: The model is capable of significantly
predicting the user outcome (either passing or failing the assignment) solely
based on the behavioral pattern inferred by the submissions provided to the OJ.
Moreover, the proposal is able to identify prone-to-fail student groups and
profiles as well as other relevant information, which eventually serves as
feedback to both the student and the instructor.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03949" title="Abstract">arXiv:2402.03949</a> [<a href="/pdf/2402.03949" title="Download PDF">pdf</a>, <a href="/format/2402.03949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Beamforming Design for the STAR-RIS-Enabled ISAC Systems with  Multiple Targets and Multiple Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wanming Hao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Gangcan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingwang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In this paper, the sensing beam pattern gain under simultaneously
transmitting and reflecting reconfigurable intelligent surfaces
(STAR-RIS)-enabled integrated sensing and communications (ISAC) systems is
investigated, in which multiple targets and multiple users exist. However,
multiple targets detection introduces new challenges, since the STAR-RIS cannot
directly send sensing beams and detect targets, the dual-functional base
station (DFBS) is required to analyze the echoes of the targets. While the
echoes reflected by different targets through STAR-RIS come from the same
direction for the DFBS, making it impossible to distinguish them. To address
the issue, we first introduce the signature sequence (SS) modulation scheme to
the ISAC system, and thus, the DFBS can detect different targets by the
SS-modulated sensing beams. Next, via the joint beamforming design of DFBS and
STAR-RIS, we develop a maxmin sensing beam pattern gain problem, and meanwhile,
considering the communication quality requirements, the interference
limitations of other targets and users, the passive nature constraint of
STAR-RIS, and the total transmit power limitation. Then, to tackle the complex
non-convex problem, we propose an alternating optimization method to divide it
into two quadratic semidefinite program subproblems and decouple the coupled
variables. Drawing on mathematical transformation, semidefinite programming, as
well as semidefinite relaxation techniques, these two subproblems are
iteratively sloved until convergence, and the ultimate solutions are obtained.
Finally, simulation results are conducted to validate the benefits and
efficiency of our proposed scheme.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03951" title="Abstract">arXiv:2402.03951</a> [<a href="/pdf/2402.03951" title="Download PDF">pdf</a>, <a href="/format/2402.03951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Adversarial Transferability across Model Genus by  Deformation-Constrained Warping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qinliang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Cheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zenghao Niu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xilin He</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weicheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yuanbo Hou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linlin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Siyang Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Adversarial examples generated by a surrogate model typically exhibit limited
transferability to unknown target systems. To address this problem, many
transferability enhancement approaches (e.g., input transformation and model
augmentation) have been proposed. However, they show poor performances in
attacking systems having different model genera from the surrogate model. In
this paper, we propose a novel and generic attacking strategy, called
Deformation-Constrained Warping Attack (DeCoWA), that can be effectively
applied to cross model genus attack. Specifically, DeCoWA firstly augments
input examples via an elastic deformation, namely Deformation-Constrained
Warping (DeCoW), to obtain rich local details of the augmented input. To avoid
severe distortion of global semantics led by random deformation, DeCoW further
constrains the strength and direction of the warping transformation by a novel
adaptive control strategy. Extensive experiments demonstrate that the
transferable examples crafted by our DeCoWA on CNN surrogates can significantly
hinder the performance of Transformers (and vice versa) on various tasks,
including image classification, video action recognition, and audio
recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03955" title="Abstract">arXiv:2402.03955</a> [<a href="/pdf/2402.03955" title="Download PDF">pdf</a>, <a href="/format/2402.03955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A linear dissipativity approach to incremental input-to-state stability  for a class of positive Lur&#x27;e systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Piengeon%2C+V">Violaine Piengeon</a>, 
<a href="/search/eess?searchtype=author&query=Guiver%2C+C">Chris Guiver</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 3 figures (comprising 8 subpanels). Submitting to Arxiv for early dissemination of results. This version was submitted in Dec 2023 to a journal for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Incremental stability properties are considered for certain systems of
forced, nonlinear differential equations with a particular positivity
structure. An incremental stability estimate is derived for pairs of
input/state/output trajectories of the Lur'e systems under consideration, from
which a number of consequences are obtained, including the incremental
exponential input-to-state stability property and certain input-output
stability concepts with linear gain. Incremental stability estimates provide a
basis for an investigation into the response to convergent and (almost)
periodic forcing terms, and is treated presently. Our results show that an
incremental version of the real Aizerman conjecture is true for positive Lur'e
systems when an incremental gain condition is imposed on the nonlinear term, as
we describe. Our argumentation is underpinned by linear dissipativity theory --
a property of positive linear control systems.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03957" title="Abstract">arXiv:2402.03957</a> [<a href="/pdf/2402.03957" title="Download PDF">pdf</a>, <a href="/format/2402.03957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Graph Representations for Procedural Instructional Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shruti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rishabh Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Computation of document similarity is a critical task in various NLP domains
that has applications in deduplication, matching, and recommendation.
Traditional approaches for document similarity computation include learning
representations of documents and employing a similarity or a distance function
over the embeddings. However, pairwise similarities and differences are not
efficiently captured by individual representations. Graph representations such
as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a
joint undirected weighted graph. JCIGs facilitate an interpretable
representation of document pairs as a graph. However, JCIGs are undirected, and
don't consider the sequential flow of sentences in documents. We propose two
approaches to model document similarity by representing document pairs as a
directed and sparse JCIG that incorporates sequential information. We propose
two algorithms inspired by Supergenome Sorting and Hamiltonian Path that
replace the undirected edges with directed edges. Our approach also sparsifies
the graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our
sparse directed graph model architecture consisting of a Siamese encoder and
GCN achieves comparable results to the baseline on datasets not containing
sequential information and beats the baseline by ten points on an instructional
documents dataset containing sequential information.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03962" title="Abstract">arXiv:2402.03962</a> [<a href="/pdf/2402.03962" title="Download PDF">pdf</a>, <a href="/format/2402.03962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altmeyer%2C+P">Patrick Altmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Demetriou%2C+A+M">Andrew M. Demetriou</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+A">Antony Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Liem%2C+C+C+S">Cynthia C. S. Liem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 15 figures. Preliminary work. Under review by the International Conference on Machine Learning (ICML)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Humans have a tendency to see 'human'-like qualities in objects around them.
We name our cars, and talk to pets and even household appliances, as if they
could understand us as other humans do. This behavior, called anthropomorphism,
is also seeing traction in Machine Learning (ML), where human-like intelligence
is claimed to be perceived in Large Language Models (LLMs). In this position
paper, considering professional incentives, human biases, and general
methodological setups, we discuss how the current search for Artificial General
Intelligence (AGI) is a perfect storm for over-attributing human-like qualities
to LLMs. In several experiments, we demonstrate that the discovery of
human-interpretable patterns in latent spaces should not be a surprising
outcome. Also in consideration of common AI portrayal in the media, we call for
the academic community to exercise extra caution, and to be extra aware of
principles of academic integrity, in interpreting and communicating about AI
research outcomes.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03964" title="Abstract">arXiv:2402.03964</a> [<a href="/pdf/2402.03964" title="Download PDF">pdf</a>, <a href="/ps/2402.03964" title="Download PostScript">ps</a>, <a href="/format/2402.03964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Perfect Mutually Unbiased Bases that are Sparse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ajeet Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Subhamoy Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Somjit Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">In dimension $d$, Mutually Unbiased Bases (MUBs) are a collection of
orthonormal bases over $\mathbb{C}^d$ such that for any two vectors $v_1, v_2$
belonging to different bases, the dot or scalar product $|\braket{v_1|v_2}| =
\frac{1}{\sqrt{d}}$. The upper bound on the number of such bases is $d+1$.
Construction methods to achieve this bound are known for cases when $d$ is some
power of prime. The situation is more restrictive in other cases and also when
we consider the results over real rather than complex. Thus, certain
relaxations of this model are considered in literature and consequently
Approximate MUBs (AMUB) are studied. This enables one to construct potentially
large number of such objects for $\mathbb{C}^d$ as well as in $\mathbb{R}^d$.
In this regard, we propose the concept of Almost Perfect MUBs (APMUB), where we
restrict the absolute value of inner product $|\braket{v_1|v_2}|$ to be
two-valued, one being 0 and the other $ \leq
\frac{1+\mathcal{O}(d^{-\lambda})}{\sqrt{d}}$, such that $\lambda &gt; 0$ and the
numerator $1 + \mathcal{O}(d^{-\lambda}) \leq 2$. Each such vector constructed,
has an important feature that large number of its components are zero and the
non-zero components are of equal magnitude. Our techniques are based on
combinatorial structures related to Resolvable Block Designs (RBDs). We show
that for several composite dimensions $d$, one can construct
$\mathcal{O}(\sqrt{d})$ many APMUBs, in which cases the number of MUBs are
significantly small. To be specific, this result works for $d$ of the form
$(q-e)(q+f), \ q, e, f \in \mathbb{N}$, with the conditions $0 \leq f \leq e$
for constant $e, f$ and $q$ some power of prime. We also show that such APMUBs
provide sets of Bi-angular vectors which are of the order of
$\mathcal{O}(d^{3/2})$ in numbers, having high angular distances among them.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03965" title="Abstract">arXiv:2402.03965</a> [<a href="/pdf/2402.03965" title="Download PDF">pdf</a>, <a href="/ps/2402.03965" title="Download PostScript">ps</a>, <a href="/format/2402.03965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyclic and BCH Codes whose Minimum Distance Equals their Maximum BCH  bound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernal%2C+J+J">Jos&#xe9; Joaqu&#xed;n Bernal</a>, 
<a href="/search/cs?searchtype=author&query=Bueno-Carre%C3%B1o%2C+D+H">Diana H. Bueno-Carre&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B3n%2C+J+J">Juan Jacobo Sim&#xf3;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper we study the family of cyclic codes such that its minimum
distance reaches the maximum of its BCH bounds. We also show a way to construct
cyclic codes with that property by means of computations of some divisors of a
polynomial of the form X^n-1. We apply our results to the study of those BCH
codes C, with designed distance delta, that have minimum distance d(C)= delta.
Finally, we present some examples of new binary BCH codes satisfying that
condition. To do this, we make use of two related tools: the discrete Fourier
transform and the notion of apparent distance of a code, originally defined for
multivariate abelian codes.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03966" title="Abstract">arXiv:2402.03966</a> [<a href="/pdf/2402.03966" title="Download PDF">pdf</a>, <a href="/format/2402.03966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On dimensionality of feature vectors in MPNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bravo%2C+C">C&#xe9;sar Bravo</a>, 
<a href="/search/cs?searchtype=author&query=Kozachinskiy%2C+A">Alexander Kozachinskiy</a>, 
<a href="/search/cs?searchtype=author&query=Rojas%2C+C">Crist&#xf3;bal Rojas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We revisit the classical result of Morris et al.~(AAAI'19) that
message-passing graphs neural networks (MPNNs) are equal in their
distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.
<br />Morris et al.~show their simulation result with ReLU activation function and
$O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the
graph. Recently, by introducing randomness into the architecture, Aamand et
al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional
feature vectors, although at the expense of guaranteeing perfect simulation
only with high probability.
<br />In all these constructions, to guarantee equivalence to the WL test, the
dimension of feature vectors in the MPNN has to increase with the size of the
graphs. However, architectures used in practice have feature vectors of
constant dimension. Thus, there is a gap between the guarantees provided by
these results and the actual characteristics of architectures used in practice.
In this paper we close this gap by showing that, for \emph{any} non-polynomial
analytic (like the sigmoid) activation function, to guarantee that MPNNs are
equivalent to the WL test, feature vectors of dimension $d=1$ is all we need,
independently of the size of the graphs.
<br />Our main technical insight is that for simulating multi-sets in the WL-test,
it is enough to use linear independence of feature vectors over rationals
instead of reals. Countability of the set of rationals together with nice
properties of analytic functions allow us to carry out the simulation invariant
over the iterations of the WL test without increasing the dimension of the
feature vectors.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03969" title="Abstract">arXiv:2402.03969</a> [<a href="/pdf/2402.03969" title="Download PDF">pdf</a>, <a href="/format/2402.03969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context learning agents are asymmetric belief updaters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schubert%2C+J+A">Johannes A. Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Jagadish%2C+A+K">Akshay K. Jagadish</a>, 
<a href="/search/cs?searchtype=author&query=Binz%2C+M">Marcel Binz</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+E">Eric Schulz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the in-context learning dynamics of large language models (LLMs)
using three instrumental learning tasks adapted from cognitive psychology. We
find that LLMs update their beliefs in an asymmetric manner and learn more from
better-than-expected outcomes than from worse-than-expected ones. Furthermore,
we show that this effect reverses when learning about counterfactual feedback
and disappears when no agency is implied. We corroborate these findings by
investigating idealized in-context learning agents derived through
meta-reinforcement learning, where we observe similar patterns. Taken together,
our results contribute to our understanding of how in-context learning works by
highlighting that the framing of a problem significantly influences how
learning occurs, a phenomenon also observed in human cognition.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03970" title="Abstract">arXiv:2402.03970</a> [<a href="/pdf/2402.03970" title="Download PDF">pdf</a>, <a href="/format/2402.03970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tabular Data: Is Attention All You Need?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zab%C3%ABrgja%2C+G">Guri Zab&#xeb;rgja</a>, 
<a href="/search/cs?searchtype=author&query=Kadra%2C+A">Arlind Kadra</a>, 
<a href="/search/cs?searchtype=author&query=Grabocka%2C+J">Josif Grabocka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep Learning has revolutionized the field of AI and led to remarkable
achievements in applications involving image and text data. Unfortunately,
there is inconclusive evidence on the merits of neural networks for structured
tabular data. In this paper, we introduce a large-scale empirical study
comparing neural networks against gradient-boosted decision trees on tabular
data, but also transformer-based architectures against traditional multi-layer
perceptrons (MLP) with residual connections. In contrast to prior work, our
empirical findings indicate that neural networks are competitive against
decision trees. Furthermore, we assess that transformer-based architectures do
not outperform simpler variants of traditional MLP architectures on tabular
datasets. As a result, this paper helps the research and practitioner
communities make informed choices on deploying neural networks on future
tabular data applications.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03972" title="Abstract">arXiv:2402.03972</a> [<a href="/pdf/2402.03972" title="Download PDF">pdf</a>, <a href="/format/2402.03972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent  Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toquebiau%2C+M">Maxime Toquebiau</a>, 
<a href="/search/cs?searchtype=author&query=Bredeche%2C+N">Nicolas Bredeche</a>, 
<a href="/search/cs?searchtype=author&query=Benamar%2C+F">Fa&#xef;z Benamar</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+J">Jae-Yun Jun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures. Published as an extended abstract at AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-agent deep reinforcement learning (MADRL) problems often encounter the
challenge of sparse rewards. This challenge becomes even more pronounced when
coordination among agents is necessary. As performance depends not only on one
agent's behavior but rather on the joint behavior of multiple agents, finding
an adequate solution becomes significantly harder. In this context, a group of
agents can benefit from actively exploring different joint strategies in order
to determine the most efficient one. In this paper, we propose an approach for
rewarding strategies where agents collectively exhibit novel behaviors. We
present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation
method that follows the centralized learning with decentralized execution
paradigm. JIM rewards joint trajectories based on a centralized measure of
novelty designed to function in continuous environments. We demonstrate the
strengths of this approach both in a synthetic environment designed to reveal
shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks.
Results show that joint exploration is crucial for solving tasks where the
optimal strategy requires a high level of coordination.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03973" title="Abstract">arXiv:2402.03973</a> [<a href="/pdf/2402.03973" title="Download PDF">pdf</a>, <a href="/format/2402.03973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given  Enough Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ollikka%2C+N">Netta Ollikka</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A">Amro Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Perin%2C+A">Andrea Perin</a>, 
<a href="/search/cs?searchtype=author&query=Kilpel%C3%A4inen%2C+M">Markku Kilpel&#xe4;inen</a>, 
<a href="/search/cs?searchtype=author&query=Deny%2C+S">St&#xe9;phane Deny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning is closing the gap with humans on several object recognition
benchmarks. Here we investigate this gap in the context of challenging images
where objects are seen from unusual viewpoints. We find that humans excel at
recognizing objects in unusual poses, in contrast with state-of-the-art
pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are
systematically brittle in this condition. Remarkably, as we limit image
exposure time, human performance degrades to the level of deep networks,
suggesting that additional mental processes (requiring additional time) take
place when humans identify objects in unusual poses. Finally, our analysis of
error patterns of humans vs. networks reveals that even time-limited humans are
dissimilar to feed-forward deep networks. We conclude that more work is needed
to bring computer vision systems to the level of robustness of the human visual
system. Understanding the nature of the mental processes taking place during
extra viewing time may be key to attain such robustness.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03975" title="Abstract">arXiv:2402.03975</a> [<a href="/pdf/2402.03975" title="Download PDF">pdf</a>, <a href="/ps/2402.03975" title="Download PostScript">ps</a>, <a href="/format/2402.03975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed analysis of deterministic discounted and mean-payoff games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loff%2C+B">Bruno Loff</a>, 
<a href="/search/cs?searchtype=author&query=Skomra%2C+M">Mateusz Skomra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We devise a policy-iteration algorithm for deterministic two-player
discounted and mean-payoff games, that runs in polynomial time with high
probability, on any input where each payoff is chosen independently from a
sufficiently random distribution.
<br />This includes the case where an arbitrary set of payoffs has been perturbed
by a Gaussian, showing for the first time that deterministic two-player games
can be solved efficiently, in the sense of smoothed analysis.
<br />More generally, we devise a condition number for deterministic discounted and
mean-payoff games, and show that our algorithm runs in time polynomial in this
condition number.
<br />Our result confirms a previous conjecture of Boros et al., which was claimed
as a theorem and later retracted. It stands in contrast with a recent
counter-example by Christ and Yannakakis, showing that Howard's
policy-iteration algorithm does not run in smoothed polynomial time on
stochastic single-player mean-payoff games.
<br />Our approach is inspired by the analysis of random optimal assignment
instances by Frieze and Sorkin, and the analysis of bias-induced policies for
mean-payoff games by Akian, Gaubert and Hochart.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03978" title="Abstract">arXiv:2402.03978</a> [<a href="/pdf/2402.03978" title="Download PDF">pdf</a>, <a href="/format/2402.03978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Power Converters with Increased Utilization for  Unbalanced Power Distribution System Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Deakin%2C+M">Matthew Deakin</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+X">Xu Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 13th International Conference on Power Electronics, Machines and Drives (PEMD 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">A low-cost reconfiguration stage connected at the output of balanced
three-phase, multi-terminal ac/dc/ac converters can increase the feasible set
of power injections substantially, increasing converter utilization and
therefore achieving a lower system cost. However, the approach has yet to be
explored for phase unbalance mitigation in power distribution networks, an
important application for future energy systems. This study addresses this by
considering power converter reconfiguration's potential for increasing the
feasible set of power transfers of four-wire power converters. Reconfigurable
topologies are compared against both conventional four-wire designs and an
idealised, fully reconfigurable converter. Results show that conventional
converters need up to 75.3% greater capacity to yield a capability chart of
equivalent size to an idealised reconfigurable converter. The number and
capacity of legs impact the capability chart's size, as do constraints on
dc-side power injections. The proposed approach shows significant promise for
maximizing the utilization of power electronics used to mitigate impacts of
phase unbalance.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03979" title="Abstract">arXiv:2402.03979</a> [<a href="/pdf/2402.03979" title="Download PDF">pdf</a>, <a href="/format/2402.03979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Li Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+K">Keith Ross</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=George%2C+A">Andriopoulos George</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">Shuyang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yufeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zixuan Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Label smoothing loss is a widely adopted technique to mitigate overfitting in
deep neural networks. This paper studies label smoothing from the perspective
of Neural Collapse (NC), a powerful empirical and theoretical framework which
characterizes model behavior during the terminal phase of training. We first
show empirically that models trained with label smoothing converge faster to
neural collapse solutions and attain a stronger level of neural collapse.
Additionally, we show that at the same level of NC1, models under label
smoothing loss exhibit intensified NC2. These findings provide valuable
insights into the performance benefits and enhanced model calibration under
label smoothing loss. We then leverage the unconstrained feature model to
derive closed-form solutions for the global minimizers for both loss functions
and further demonstrate that models under label smoothing have a lower
conditioning number and, therefore, theoretically converge faster. Our study,
combining empirical evidence and theoretical results, not only provides nuanced
insights into the differences between label smoothing and cross-entropy losses,
but also serves as an example of how the powerful neural collapse framework can
be used to improve our understanding of DNNs.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03981" title="Abstract">arXiv:2402.03981</a> [<a href="/pdf/2402.03981" title="Download PDF">pdf</a>, <a href="/format/2402.03981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Diverse Sampling for Diffusion Based Motion Behavior  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sester%2C+M">Monika Sester</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In autonomous driving tasks, trajectory prediction in complex traffic
environments requires adherence to real-world context conditions and behavior
multimodalities. Existing methods predominantly rely on prior assumptions or
generative models trained on curated data to learn road agents' stochastic
behavior bounded by scene constraints. However, they often face mode averaging
issues due to data imbalance and simplistic priors, and could even suffer from
mode collapse due to unstable training and single ground truth supervision.
These issues lead the existing methods to a loss of predictive diversity and
adherence to the scene constraints. To address these challenges, we introduce a
novel trajectory generator named Controllable Diffusion Trajectory (CDT), which
integrates map information and social interactions into a Transformer-based
conditional denoising diffusion model to guide the prediction of future
trajectories. To ensure multimodality, we incorporate behavioral tokens to
direct the trajectory's modes, such as going straight, turning right or left.
Moreover, we incorporate the predicted endpoints as an alternative behavioral
token into the CDT model to facilitate the prediction of accurate trajectories.
Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels
in generating diverse and scene-compliant trajectories in complex urban
settings.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03985" title="Abstract">arXiv:2402.03985</a> [<a href="/pdf/2402.03985" title="Download PDF">pdf</a>, <a href="/format/2402.03985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bias-Variance Decomposition for Ensembles over Multiple Synthetic  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%A4is%C3%A4%2C+O">Ossi R&#xe4;is&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Honkela%2C+A">Antti Honkela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent studies have highlighted the benefits of generating multiple synthetic
datasets for supervised learning, from increased accuracy to more effective
model selection and uncertainty estimation. These benefits have clear empirical
support, but the theoretical understanding of them is currently very light. We
seek to increase the theoretical understanding by deriving bias-variance
decompositions for several settings of using multiple synthetic datasets. Our
theory predicts multiple synthetic datasets to be especially beneficial for
high-variance downstream predictors, and yields a simple rule of thumb to
select the appropriate number of synthetic datasets in the case of mean-squared
error and Brier score. We investigate how our theory works in practice by
evaluating the performance of an ensemble over many synthetic datasets for
several real datasets and downstream predictors. The results follow our theory,
showing that our insights are also practically relevant.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03987" title="Abstract">arXiv:2402.03987</a> [<a href="/pdf/2402.03987" title="Download PDF">pdf</a>, <a href="/format/2402.03987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tail-Erasure-Correcting Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moav%2C+B">Boaz Moav</a>, 
<a href="/search/cs?searchtype=author&query=Gabrys%2C+R">Ryan Gabrys</a>, 
<a href="/search/cs?searchtype=author&query=Yaakobi%2C+E">Eitan Yaakobi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The increasing demand for data storage has prompted the exploration of new
techniques, with molecular data storage being a promising alternative. In this
work, we develop coding schemes for a new storage paradigm that can be
represented as a collection of two-dimensional arrays. Motivated by error
patterns observed in recent prototype architectures, our study focuses on
correcting erasures in the last few symbols of each row, and also correcting
arbitrary deletions across rows. We present code constructions and explicit
encoders and decoders that are shown to be nearly optimal in many scenarios. We
show that the new coding schemes are capable of effectively mitigating these
errors, making these emerging storage platforms potentially promising
solutions.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03989" title="Abstract">arXiv:2402.03989</a> [<a href="/pdf/2402.03989" title="Download PDF">pdf</a>, <a href="/format/2402.03989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOPoint Joint Keypoint and Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Backhaus%2C+A">Anton Backhaus</a>, 
<a href="/search/cs?searchtype=author&query=Luettel%2C+T">Thorsten Luettel</a>, 
<a href="/search/cs?searchtype=author&query=Wuensche%2C+H">Hans-Joachim Wuensche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Advanced Concepts for Intelligent Vision Systems,
  14124, 112-123 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Intelligent vehicles of the future must be capable of understanding and
navigating safely through their surroundings. Camera-based vehicle systems can
use keypoints as well as objects as low- and high-level landmarks for
GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a
convolutional neural network model that simultaneously detects keypoints and
objects in an image by combining YOLOv5 and SuperPoint to create a single
forward-pass network that is both real-time capable and accurate. By using a
shared backbone and a light-weight network structure, YOLOPoint is able to
perform competitively on both the HPatches and KITTI benchmarks.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03991" title="Abstract">arXiv:2402.03991</a> [<a href="/pdf/2402.03991" title="Download PDF">pdf</a>, <a href="/format/2402.03991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Rank Collapse: Weight Decay and Small Within-Class Variability  Yield Low-Rank Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zangrando%2C+E">Emanuele Zangrando</a>, 
<a href="/search/cs?searchtype=author&query=Deidda%2C+P">Piero Deidda</a>, 
<a href="/search/cs?searchtype=author&query=Brugiapaglia%2C+S">Simone Brugiapaglia</a>, 
<a href="/search/cs?searchtype=author&query=Guglielmi%2C+N">Nicola Guglielmi</a>, 
<a href="/search/cs?searchtype=author&query=Tudisco%2C+F">Francesco Tudisco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent work in deep learning has shown strong empirical and theoretical
evidence of an implicit low-rank bias: weight matrices in deep networks tend to
be approximately low-rank and removing relatively small singular values during
training or from available trained models may significantly reduce model size
while maintaining or even improving model performance. However, the majority of
the theoretical investigations around low-rank bias in neural networks deal
with oversimplified deep linear networks. In this work, we consider general
networks with nonlinear activations and the weight decay parameter, and we show
the presence of an intriguing neural rank collapse phenomenon, connecting the
low-rank bias of trained networks with networks' neural collapse properties: as
the weight decay parameter grows, the rank of each layer in the network
decreases proportionally to the within-class variability of the hidden-space
embeddings of the previous layers. Our theoretical findings are supported by a
range of experimental evaluations illustrating the phenomenon.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03992" title="Abstract">arXiv:2402.03992</a> [<a href="/pdf/2402.03992" title="Download PDF">pdf</a>, <a href="/format/2402.03992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space Group Constrained Crystal Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+R">Rui Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenbing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Deli Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">Crystals are the foundation of numerous scientific and industrial
applications. While various learning-based approaches have been proposed for
crystal generation, existing methods seldom consider the space group constraint
which is crucial in describing the geometry of crystals and closely relevant to
many desirable properties. However, considering space group constraint is
challenging owing to its diverse and nontrivial forms. In this paper, we reduce
the space group constraint into an equivalent formulation that is more
tractable to be handcrafted into the generation process. In particular, we
translate the space group constraint into two parts: the basis constraint of
the invariant logarithmic space of the lattice matrix and the Wyckoff position
constraint of the fractional coordinates. Upon the derived constraints, we then
propose DiffCSP++, a novel diffusion model that has enhanced a previous work
DiffCSP by further taking space group constraint into account. Experiments on
several popular datasets verify the benefit of the involvement of the space
group constraint, and show that our DiffCSP++ achieves promising performance on
crystal structure prediction, ab initio crystal generation and controllable
generation with customized space groups.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03994" title="Abstract">arXiv:2402.03994</a> [<a href="/pdf/2402.03994" title="Download PDF">pdf</a>, <a href="/format/2402.03994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Sketches for Training Data Attribution and Studying the Loss  Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schioppa%2C+A">Andrea Schioppa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Random projections or sketches of gradients and Hessian vector products play
an essential role in applications where one needs to store many such vectors
while retaining accurate information about their relative geometry. Two
important scenarios are training data attribution (tracing a model's behavior
to the training data), where one needs to store a gradient for each training
example, and the study of the spectrum of the Hessian (to analyze the training
dynamics), where one needs to store multiple Hessian vector products. While
sketches that use dense matrices are easy to implement, they are memory bound
and cannot be scaled to modern neural networks. Motivated by work on the
intrinsic dimension of neural networks, we propose and study a design space for
scalable sketching algorithms. We demonstrate the efficacy of our approach in
three applications: training data attribution, the analysis of the Hessian
spectrum and the computation of the intrinsic dimension when fine-tuning
pre-trained language models.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04004" title="Abstract">arXiv:2402.04004</a> [<a href="/pdf/2402.04004" title="Download PDF">pdf</a>, <a href="/format/2402.04004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effect of Noise in LLM Training Data with Algorithmic  Chains of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havrilla%2C+A">Alex Havrilla</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+M">Maia Iyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">During both pretraining and fine-tuning, Large Language Models
(\textbf{LLMs}) are trained on trillions of tokens of text of widely varying
quality. Both phases of training typically involve heuristically filtering out
``low-quality'' or \textit{noisy} training samples, yet little is known
quantitatively about how the type or intensity of noise affects downstream
performance. In this work, we study how noise in chain of thought
(\textbf{CoT}) impacts task performance in the highly-controlled setting of
algorithmically solvable tasks. First, we develop the Traced Integer
(\textbf{TInt}) framework to generate highly customizable noised execution
traces for any arithmetic function on lists of integers. We then define two
types of noise: \textit{static} noise, a local form of noise which is applied
after the CoT trace is computed, and \textit{dynamic} noise, a global form of
noise which propagates errors in the trace as it is computed. We then evaluate
the test performance of pretrained models both prompted and fine-tuned on
noised datasets with varying levels of dataset contamination and intensity. We
find fine-tuned models are extremely robust to high levels of static noise but
struggle significantly more with lower levels of dynamic noise. In contrast,
few-shot prompted models appear more sensitive to even static noise. We
conclude with a discussion of how our findings impact noise filtering
best-practices, in particular emphasizing the importance of removing samples
containing destructive dynamic noise with global errors.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04005" title="Abstract">arXiv:2402.04005</a> [<a href="/pdf/2402.04005" title="Download PDF">pdf</a>, <a href="/format/2402.04005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Achituve%2C+I">Idan Achituve</a>, 
<a href="/search/cs?searchtype=author&query=Diamant%2C+I">Idit Diamant</a>, 
<a href="/search/cs?searchtype=author&query=Netzer%2C+A">Arnon Netzer</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As machine learning becomes more prominent there is a growing demand to
perform several inference tasks in parallel. Running a dedicated model for each
task is computationally expensive and therefore there is a great interest in
multi-task learning (MTL). MTL aims at learning a single model that solves
several tasks efficiently. Optimizing MTL models is often achieved by computing
a single gradient per task and aggregating them for obtaining a combined update
direction. However, these approaches do not consider an important aspect, the
sensitivity in the gradient dimensions. Here, we introduce a novel gradient
aggregation approach using Bayesian inference. We place a probability
distribution over the task-specific parameters, which in turn induce a
distribution over the gradients of the tasks. This additional valuable
information allows us to quantify the uncertainty in each of the gradients
dimensions, which can then be factored in when aggregating them. We empirically
demonstrate the benefits of our approach in a variety of datasets, achieving
state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04009" title="Abstract">arXiv:2402.04009</a> [<a href="/pdf/2402.04009" title="Download PDF">pdf</a>, <a href="/format/2402.04009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+N">Ningyuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+M">Minghao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Ke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianxin Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In finetuning a large pretrained model to downstream tasks,
parameter-efficient fine-tuning (PEFT) methods can effectively finetune
pretrained models with few trainable parameters, but suffer from high GPU
memory consumption and slow training speed. Because learnable parameters from
these methods are entangled with the pretrained model, gradients related to the
frozen pretrained model's parameters have to be computed and stored during
finetuning. We propose Low-rank Attention Side-Tuning (LAST), which
disentangles the trainable module from the pretrained model by freezing not
only parameters but also outputs of the pretrained network. LAST trains a
side-network composed of only low-rank self-attention modules. By viewing the
pretrained model as a frozen feature extractor, the side-network takes
intermediate output from the pretrained model and focus on learning
task-specific knowledge. We also show that LAST can be highly parallel across
multiple optimization objectives, making it very efficient in downstream task
adaptation, for example, in finding optimal hyperparameters. LAST outperforms
previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks
with roughly only 30\% of GPU memory footprint and 60\% of training time
compared to existing PEFT methods, but achieves significantly higher accuracy.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04010" title="Abstract">arXiv:2402.04010</a> [<a href="/pdf/2402.04010" title="Download PDF">pdf</a>, <a href="/format/2402.04010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Availability Attacks against Supervised and Contrastive  Learning Simultaneously
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiao-Shan Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Availability attacks can prevent the unauthorized use of private data and
commercial datasets by generating imperceptible noise and making unlearnable
examples before release. Ideally, the obtained unlearnability prevents
algorithms from training usable models. When supervised learning (SL)
algorithms have failed, a malicious data collector possibly resorts to
contrastive learning (CL) algorithms to bypass the protection. Through
evaluation, we have found that most of the existing methods are unable to
achieve both supervised and contrastive unlearnability, which poses risks to
data protection. Different from recent methods based on contrastive error
minimization, we employ contrastive-like data augmentations in supervised error
minimization or maximization frameworks to obtain attacks effective for both SL
and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case
unlearnability across SL and CL algorithms with less computation consumption,
showcasing prospects in real-world applications.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04012" title="Abstract">arXiv:2402.04012</a> [<a href="/pdf/2402.04012" title="Download PDF">pdf</a>, <a href="/format/2402.04012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantized Approximately Orthogonal Recurrent Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foucault%2C+A">Armand Foucault</a> (IMT), 
<a href="/search/cs?searchtype=author&query=Mamalet%2C+F">Franck Mamalet</a> (UT), 
<a href="/search/cs?searchtype=author&query=Malgouyres%2C+F">Fran&#xe7;ois Malgouyres</a> (IMT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Statistics Theory (math.ST)

</div>
<p class="mathjax">Orthogonal recurrent neural networks (ORNNs) are an appealing option for
learning tasks involving time series with long-term dependencies, thanks to
their simplicity and computational stability. However, these networks often
require a substantial number of parameters to perform well, which can be
prohibitive in power-constrained environments, such as compact devices. One
approach to address this issue is neural network quantization. The construction
of such networks remains an open problem, acknowledged for its inherent
instability.In this paper, we explore the quantization of the recurrent and
input weight matrices in ORNNs, leading to Quantized approximately Orthogonal
RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and
three quantization-aware training (QAT) algorithms that incorporate orthogonal
constraints and quantized weights. Empirical results demonstrate the advantages
of employing QAT over PTQ. The most efficient model achieves results similar to
state-of-the-art full-precision ORNN and LSTM on a variety of standard
benchmarks, even with 3-bits quantization.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04013" title="Abstract">arXiv:2402.04013</a> [<a href="/pdf/2402.04013" title="Download PDF">pdf</a>, <a href="/format/2402.04013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and  Defenses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yixiang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenbo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+J">Jiawei Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+B">Baoli Chong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Model Inversion (MI) attacks aim to disclose private information about the
training data by abusing access to the pre-trained models. These attacks enable
adversaries to reconstruct high-fidelity data that closely aligns with the
private training data, which has raised significant privacy concerns. Despite
the rapid advances in the field, we lack a comprehensive overview of existing
MI attacks and defenses. To fill this gap, this paper thoroughly investigates
this field and presents a holistic survey. Firstly, our work briefly reviews
the traditional MI on machine learning scenarios. We then elaborately analyze
and compare numerous recent attacks and defenses on \textbf{D}eep
\textbf{N}eural \textbf{N}etworks (DNNs) across multiple modalities and
learning tasks.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04019" title="Abstract">arXiv:2402.04019</a> [<a href="/pdf/2402.04019" title="Download PDF">pdf</a>, <a href="/ps/2402.04019" title="Download PostScript">ps</a>, <a href="/format/2402.04019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Effects of Population and Employment Characteristics on  Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M">Majbah Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuandong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Hyeonsup Lim</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In International Conference on Transportation and Development 2023
  (pp. 503-513)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Truck transportation remains the dominant mode of US freight transportation
because of its advantages, such as the flexibility of accessing pickup and
drop-off points and faster delivery. Because of the massive freight volume
transported by trucks, understanding the effects of population and employment
characteristics on truck flows is critical for better transportation planning
and investment decisions. The US Federal Highway Administration published a
truck travel origin-destination data set as part of the Next Generation
National Household Travel Survey program. This data set contains the total
number of truck trips in 2020 within and between 583 predefined zones
encompassing metropolitan and nonmetropolitan statistical areas within each
state and Washington, DC. In this study, origin-destination-level truck trip
flow data was augmented to include zone-level population and employment
characteristics from the US Census Bureau. Census population and County
Business Patterns data were included. The final data set was used to train a
machine learning algorithm-based model, Extreme Gradient Boosting (XGBoost),
where the target variable is the number of total truck trips. Shapley Additive
ExPlanation (SHAP) was adopted to explain the model results. Results showed
that the distance between the zones was the most important variable and had a
nonlinear relationship with truck flows.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04020" title="Abstract">arXiv:2402.04020</a> [<a href="/pdf/2402.04020" title="Download PDF">pdf</a>, <a href="/ps/2402.04020" title="Download PostScript">ps</a>, <a href="/format/2402.04020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Rail Transportation Route of Crude Oil in the US Using  Crowdsourced Social Media Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuandong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M">Majbah Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+S">Shih-Miao Chin</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Ho-Ling Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoli Chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transportation Research Record, 2678(1), 218-228 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Safety issues associated with transporting crude oil by rail have been a
concern since the boom of US domestic shale oil production in 2012. During the
last decade, over 300 crude oil by rail incidents have occurred in the US. Some
of them have caused adverse consequences including fire and hazardous materials
leakage. However, only limited information on the routes of crude-on-rail and
their associated risks is available to the public. To this end, this study
proposes an unconventional way to reconstruct the crude-on-rail routes using
geotagged photos harvested from the Flickr website. The proposed method links
the geotagged photos of crude oil trains posted online with national railway
networks to identify potential railway segments those crude oil trains were
traveling on. A shortest path-based method was then applied to infer the
complete crude-on-rail routes, by utilizing the confirmed railway segments as
well as their movement direction information. Validation of the inferred routes
was performed using a public map and official crude oil incident data. Results
suggested that the inferred routes based on geotagged photos have high
coverage, with approximately 96% of the documented crude oil incidents aligned
with the reconstructed crude-on-rail network. The inferred crude oil train
routes were found to pass through many metropolitan areas with dense
populations, who are exposed to potential risk. This finding could improve
situation awareness for policymakers and transportation planners. In addition,
with the inferred routes, this study establishes a good foundation for future
crude oil train risk analysis along the rail route.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04023" title="Abstract">arXiv:2402.04023</a> [<a href="/pdf/2402.04023" title="Download PDF">pdf</a>, <a href="/ps/2402.04023" title="Download PostScript">ps</a>, <a href="/format/2402.04023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Google Translate Error Analysis for Mental Healthcare Information:  Evaluating Accuracy, Comprehensibility, and Implications for Multilingual  Healthcare Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delfani%2C+J">Jaleh Delfani</a>, 
<a href="/search/cs?searchtype=author&query=Orasan%2C+C">Constantin Orasan</a>, 
<a href="/search/cs?searchtype=author&query=Saadany%2C+H">Hadeel Saadany</a>, 
<a href="/search/cs?searchtype=author&query=Temizoz%2C+O">Ozlem Temizoz</a>, 
<a href="/search/cs?searchtype=author&query=Taylor-Stilgoe%2C+E">Eleanor Taylor-Stilgoe</a>, 
<a href="/search/cs?searchtype=author&query=Kanojia%2C+D">Diptesh Kanojia</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+S">Sabine Braun</a>, 
<a href="/search/cs?searchtype=author&query=Schouten%2C+B">Barbara Schouten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study explores the use of Google Translate (GT) for translating mental
healthcare (MHealth) information and evaluates its accuracy, comprehensibility,
and implications for multilingual healthcare communication through analysing GT
output in the MHealth domain from English to Persian, Arabic, Turkish,
Romanian, and Spanish. Two datasets comprising MHealth information from the UK
National Health Service website and information leaflets from The Royal College
of Psychiatrists were used. Native speakers of the target languages manually
assessed the GT translations, focusing on medical terminology accuracy,
comprehensibility, and critical syntactic/semantic errors. GT output analysis
revealed challenges in accurately translating medical terminology, particularly
in Arabic, Romanian, and Persian. Fluency issues were prevalent across various
languages, affecting comprehension, mainly in Arabic and Spanish. Critical
errors arose in specific contexts, such as bullet-point formatting,
specifically in Persian, Turkish, and Romanian. Although improvements are seen
in longer-text translations, there remains a need to enhance accuracy in
medical and mental health terminology and fluency, whilst also addressing
formatting issues for a more seamless user experience. The findings highlight
the need to use customised translation engines for Mhealth translation and the
challenges when relying solely on machine-translated medical content,
emphasising the crucial role of human reviewers in multilingual healthcare
communication.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04028" title="Abstract">arXiv:2402.04028</a> [<a href="/pdf/2402.04028" title="Download PDF">pdf</a>, <a href="/format/2402.04028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlbNews: A Corpus of Headlines for Topic Modeling in Albanian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87ano%2C+E">Erion &#xc7;ano</a>, 
<a href="/search/cs?searchtype=author&query=Lamaj%2C+D">Dario Lamaj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The scarcity of available text corpora for low-resource languages like
Albanian is a serious hurdle for research in natural language processing tasks.
This paper introduces AlbNews, a collection of 600 topically labeled news
headlines and 2600 unlabeled ones in Albanian. The data can be freely used for
conducting topic modeling research. We report the initial classification scores
of some traditional machine learning classifiers trained with the AlbNews
samples. These results show that basic models outrun the ensemble learning ones
and can serve as a baseline for future experiments.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04029" title="Abstract">arXiv:2402.04029</a> [<a href="/pdf/2402.04029" title="Download PDF">pdf</a>, <a href="/format/2402.04029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positive concave deep equilibrium models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabor%2C+M">Mateusz Gabor</a>, 
<a href="/search/cs?searchtype=author&query=Piotrowski%2C+T">Tomasz Piotrowski</a>, 
<a href="/search/cs?searchtype=author&query=Cavalcante%2C+R+L+G">Renato L. G. Cavalcante</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep equilibrium (DEQ) models are widely recognized as a memory efficient
alternative to standard neural networks, achieving state-of-the-art performance
in language modeling and computer vision tasks. These models solve a fixed
point equation instead of explicitly computing the output, which sets them
apart from standard neural networks. However, existing DEQ models often lack
formal guarantees of the existence and uniqueness of the fixed point, and the
convergence of the numerical scheme used for computing the fixed point is not
formally established. As a result, DEQ models are potentially unstable in
practice. To address these drawbacks, we introduce a novel class of DEQ models
called positive concave deep equilibrium (pcDEQ) models. Our approach, which is
based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and
activation functions that are concave on the positive orthant. By imposing
these constraints, we can easily ensure the existence and uniqueness of the
fixed point without relying on additional complex assumptions commonly found in
the DEQ literature, such as those based on monotone operator theory in convex
analysis. Furthermore, the fixed point can be computed with the standard fixed
point algorithm, and we provide theoretical guarantees of geometric
convergence, which, in particular, simplifies the training process. Experiments
demonstrate the competitiveness of our pcDEQ models against other implicit
models.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04030" title="Abstract">arXiv:2402.04030</a> [<a href="/pdf/2402.04030" title="Download PDF">pdf</a>, <a href="/format/2402.04030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing the Cost of Quantum Chemical Data By Backpropagating Through  Density Functional Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathiasen%2C+A">Alexander Mathiasen</a>, 
<a href="/search/cs?searchtype=author&query=Helal%2C+H">Hatem Helal</a>, 
<a href="/search/cs?searchtype=author&query=Balanca%2C+P">Paul Balanca</a>, 
<a href="/search/cs?searchtype=author&query=Krzywaniak%2C+A">Adam Krzywaniak</a>, 
<a href="/search/cs?searchtype=author&query=Parviz%2C+A">Ali Parviz</a>, 
<a href="/search/cs?searchtype=author&query=Hvilsh%C3%B8j%2C+F">Frederik Hvilsh&#xf8;j</a>, 
<a href="/search/cs?searchtype=author&query=Banaszewski%2C+B">Blazej Banaszewski</a>, 
<a href="/search/cs?searchtype=author&query=Luschi%2C+C">Carlo Luschi</a>, 
<a href="/search/cs?searchtype=author&query=Fitzgibbon%2C+A+W">Andrew William Fitzgibbon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Density Functional Theory (DFT) accurately predicts the quantum chemical
properties of molecules, but scales as $O(N_{\text{electrons}}^3)$. Sch\"utt et
al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN).
Arguably, the biggest problem one faces when scaling to larger molecules is the
cost of DFT labels. For example, it took years to create the PCQ dataset
(Nakata &amp; Shimazaki, 2017) on which subsequent NNs are trained within a week.
DFT labels molecules by minimizing energy $E(\cdot )$ as a "loss function." We
bypass dataset creation by directly training NNs with $E(\cdot )$ as a loss
function. For comparison, Sch\"utt et al. (2019) spent 626 hours creating a
dataset on which they trained their NN for 160h, for a total of 786h; our
method achieves comparable performance within 31h.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04031" title="Abstract">arXiv:2402.04031</a> [<a href="/pdf/2402.04031" title="Download PDF">pdf</a>, <a href="/ps/2402.04031" title="Download PostScript">ps</a>, <a href="/format/2402.04031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorjsembe%2C+Z">Zolnamar Dorjsembe</a>, 
<a href="/search/cs?searchtype=author&query=Pao%2C+H">Hsing-Kuo Pao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+F">Furen Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study introduces Polyp-DDPM, a diffusion-based method for generating
realistic images of polyps conditioned on masks, aimed at enhancing the
segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the
challenges of data limitations, high annotation costs, and privacy concerns
associated with medical images. By conditioning the diffusion model on
segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM
outperforms state-of-the-art methods in terms of image quality (achieving a
Frechet Inception Distance (FID) score of 78.47, compared to scores above
83.79) and segmentation performance (achieving an Intersection over Union (IoU)
of 0.7156, versus less than 0.6694 for synthetic images from baseline models
and 0.7067 for real data). Our method generates a high-quality, diverse
synthetic dataset for training, thereby enhancing polyp segmentation models to
be comparable with real images and offering greater data augmentation
capabilities to improve segmentation models. The source code and pretrained
weights for Polyp-DDPM are made publicly available at
https://github.com/mobaidoctor/polyp-ddpm.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04032" title="Abstract">arXiv:2402.04032</a> [<a href="/pdf/2402.04032" title="Download PDF">pdf</a>, <a href="/format/2402.04032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEAM : Hashed Embedding Acceleration using Processing-In-Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngsuk Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyuk-Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rhee%2C+C+E">Chae Eun Rhee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In today's data centers, personalized recommendation systems face challenges
such as the need for large memory capacity and high bandwidth, especially when
performing embedding operations. Previous approaches have relied on DIMM-based
near-memory processing techniques or introduced 3D-stacked DRAM to address
memory-bound issues and expand memory bandwidth. However, these solutions fall
short when dealing with the expanding size of personalized recommendation
systems. Recommendation models have grown to sizes exceeding tens of terabytes,
making them challenging to run efficiently on traditional single-node inference
servers. Although various algorithmic methods have been proposed to reduce
embedding table capacity, they often result in increased memory access or
inefficient utilization of memory resources. This paper introduces HEAM, a
heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to
accelerate recommendation systems in which compositional embedding is
utilized-a technique aimed at reducing the size of embedding tables. The
architecture is organized into a three-tier memory hierarchy consisting of
conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory
(PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is
specifically designed to accommodate the unique aspects of compositional
embedding, such as temporal locality and embedding table capacity. This design
effectively reduces bank access, improves access efficiency, and enhances
overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings
compared to the baseline.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04033" title="Abstract">arXiv:2402.04033</a> [<a href="/pdf/2402.04033" title="Download PDF">pdf</a>, <a href="/format/2402.04033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On provable privacy vulnerabilities of graph representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Guanhua Fang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qiying Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenbiao Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph representation learning (GRL) is critical for extracting insights from
complex network structures, but it also raises security concerns due to
potential privacy vulnerabilities in these representations. This paper
investigates the structural vulnerabilities in graph neural models where
sensitive topological information can be inferred through edge reconstruction
attacks. Our research primarily addresses the theoretical underpinnings of
cosine-similarity-based edge reconstruction attacks (COSERA), providing
theoretical and empirical evidence that such attacks can perfectly reconstruct
sparse Erdos Renyi graphs with independent random features as graph size
increases. Conversely, we establish that sparsity is a critical factor for
COSERA's effectiveness, as demonstrated through analysis and experiments on
stochastic block models. Finally, we explore the resilience of (provably)
private graph representations produced via noisy aggregation (NAG) mechanism
against COSERA. We empirically delineate instances wherein COSERA demonstrates
both efficacy and deficiency in its capacity to function as an instrument for
elucidating the trade-off between privacy and utility.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04035" title="Abstract">arXiv:2402.04035</a> [<a href="/pdf/2402.04035" title="Download PDF">pdf</a>, <a href="/ps/2402.04035" title="Download PostScript">ps</a>, <a href="/format/2402.04035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Distortion Clustering with Ordinal and Limited Cardinal Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burkhardt%2C+J">Jakob Burkhardt</a>, 
<a href="/search/cs?searchtype=author&query=Caragiannis%2C+I">Ioannis Caragiannis</a>, 
<a href="/search/cs?searchtype=author&query=Fehrs%2C+K">Karl Fehrs</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+M">Matteo Russo</a>, 
<a href="/search/cs?searchtype=author&query=Schwiegelshohn%2C+C">Chris Schwiegelshohn</a>, 
<a href="/search/cs?searchtype=author&query=Shyam%2C+S">Sudarshan Shyam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Motivated by recent work in computational social choice, we extend the metric
distortion framework to clustering problems. Given a set of $n$ agents located
in an underlying metric space, our goal is to partition them into $k$ clusters,
optimizing some social cost objective. The metric space is defined by a
distance function $d$ between the agent locations. Information about $d$ is
available only implicitly via $n$ rankings, through which each agent ranks all
other agents in terms of their distance from her. Still, we would like to
evaluate clustering algorithms in terms of social cost objectives that are
defined using $d$. This is done using the notion of distortion, which measures
how far from optimality a clustering can be, taking into account all underlying
metrics that are consistent with the ordinal information available.
Unfortunately, the most important clustering objectives do not admit algorithms
with finite distortion. To sidestep this disappointing fact, we follow two
alternative approaches: We first explore whether resource augmentation can be
beneficial. We consider algorithms that use more than $k$ clusters but compare
their social cost to that of the optimal $k$-clusterings. We show that using
exponentially (in terms of $k$) many clusters, we can get low (constant or
logarithmic) distortion for the $k$-center and $k$-median objectives.
Interestingly, such an exponential blowup is shown to be necessary. More
importantly, we explore whether limited cardinal information can be used to
obtain better results. Somewhat surprisingly, for $k$-median and $k$-center, we
show that a number of queries that is polynomial in $k$ and only logarithmic in
$n$ (i.e., only sublinear in the number of agents for the most relevant
scenarios in practice) is enough to get constant distortion.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04045" title="Abstract">arXiv:2402.04045</a> [<a href="/pdf/2402.04045" title="Download PDF">pdf</a>, <a href="/format/2402.04045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mission Planning and Safety Assessment for Pipeline Inspection Using  Autonomous Underwater Vehicles: A Framework based on Behavior Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aubard%2C+M">Martin Aubard</a>, 
<a href="/search/cs?searchtype=author&query=Quijano%2C+S">Sergio Quijano</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez-Tu%C3%B1%C3%B3n%2C+O">Olaya &#xc1;lvarez-Tu&#xf1;&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Antal%2C+L">L&#xe1;szl&#xf3; Antal</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Maria Costa</a>, 
<a href="/search/cs?searchtype=author&query=Brodskiy%2C+Y">Yury Brodskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The recent advance in autonomous underwater robotics facilitates autonomous
inspection tasks of offshore infrastructure. However, current inspection
missions rely on predefined plans created offline, hampering the flexibility
and autonomy of the inspection vehicle and the mission's success in case of
unexpected events. In this work, we address these challenges by proposing a
framework encompassing the modeling and verification of mission plans through
Behavior Trees (BTs). This framework leverages the modularity of BTs to model
onboard reactive behaviors, thus enabling autonomous plan executions, and uses
BehaVerify to verify the mission's safety. Moreover, as a use case of this
framework, we present a novel AI-enabled algorithm that aims for efficient,
autonomous pipeline camera data collection. In a simulated environment, we
demonstrate the framework's application to our proposed pipeline inspection
algorithm. Our framework marks a significant step forward in the field of
autonomous underwater robotics, promising to enhance the safety and success of
underwater missions in practical, real-world applications.
https://github.com/remaro-network/pipe_inspection_mission
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04046" title="Abstract">arXiv:2402.04046</a> [<a href="/pdf/2402.04046" title="Download PDF">pdf</a>, <a href="/format/2402.04046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling of Graphs via Joint Diffusion of Node and Edge  Attributes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berman%2C+N">Nimrod Berman</a>, 
<a href="/search/cs?searchtype=author&query=Kosman%2C+E">Eitan Kosman</a>, 
<a href="/search/cs?searchtype=author&query=Di+Castro%2C+D">Dotan Di Castro</a>, 
<a href="/search/cs?searchtype=author&query=Azencot%2C+O">Omri Azencot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph generation is integral to various engineering and scientific
disciplines. Nevertheless, existing methodologies tend to overlook the
generation of edge attributes. However, we identify critical applications where
edge attributes are essential, making prior methods potentially unsuitable in
such contexts. Moreover, while trivial adaptations are available, empirical
investigations reveal their limited efficacy as they do not properly model the
interplay among graph components. To address this, we propose a joint
score-based model of nodes and edges for graph generation that considers all
graph components. Our approach offers two key novelties: (i) node and edge
attributes are combined in an attention module that generates samples based on
the two ingredients; and (ii) node, edge and adjacency information are mutually
dependent during the graph diffusion process. We evaluate our method on
challenging benchmarks involving real-world and synthetic datasets in which
edge features are crucial. Additionally, we introduce a new synthetic dataset
that incorporates edge values. Furthermore, we propose a novel application that
greatly benefits from the method due to its nature: the generation of traffic
scenes represented as graphs. Our method outperforms other graph generation
methods, demonstrating a significant advantage in edge-related measures.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04048" title="Abstract">arXiv:2402.04048</a> [<a href="/pdf/2402.04048" title="Download PDF">pdf</a>, <a href="/format/2402.04048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A nodal ghost method based on variational formulation and regular square  grid for elliptic problems on arbitrary domains in two space dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Astuto%2C+C">Clarissa Astuto</a>, 
<a href="/search/math?searchtype=author&query=Boffi%2C+D">Daniele Boffi</a>, 
<a href="/search/math?searchtype=author&query=Russo%2C+G">Giovanni Russo</a>, 
<a href="/search/math?searchtype=author&query=Zerbinati%2C+U">Umberto Zerbinati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper focuses on the numerical solution of elliptic partial differential
equations (PDEs) with Dirichlet and mixed boundary conditions, specifically
addressing the challenges arising from irregular domains. Both finite element
method (FEM) and finite difference method (FDM), face difficulties in dealing
with arbitrary domains. The paper introduces a novel nodal symmetric ghost
finite element method approach, which combines the advantages of FEM and FDM.
The method employs bilinear finite elements on a structured mesh, and provides
a detailed implementation description. A rigorous a priori convergence rate
analysis is also presented. The convergence rates are validated with many
numerical experiments, in both one and two space dimensions.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04049" title="Abstract">arXiv:2402.04049</a> [<a href="/pdf/2402.04049" title="Download PDF">pdf</a>, <a href="/format/2402.04049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Biases in LLM Simulations of Debates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taubenfeld%2C+A">Amir Taubenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Dover%2C+Y">Yaniv Dover</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+A">Ariel Goldstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in natural language processing, especially the emergence
of Large Language Models (LLMs), have opened exciting possibilities for
constructing computational simulations designed to replicate human behavior
accurately. However, LLMs are complex statistical learners without
straightforward deductive rules, making them prone to unexpected behaviors. In
this study, we highlight the limitations of LLMs in simulating human
interactions, particularly focusing on LLMs' ability to simulate political
debates. Our findings indicate a tendency for LLM agents to conform to the
model's inherent social biases despite being directed to debate from certain
political perspectives. This tendency results in behavioral patterns that seem
to deviate from well-established social dynamics among humans. We reinforce
these observations using an automatic self-fine-tuning method, which enables us
to manipulate the biases within the LLM and demonstrate that agents
subsequently align with the altered biases. These results underscore the need
for further research to develop methods that help agents overcome these biases,
a critical step toward creating more realistic simulations.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04050" title="Abstract">arXiv:2402.04050</a> [<a href="/pdf/2402.04050" title="Download PDF">pdf</a>, <a href="/format/2402.04050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting the Dots: Collaborative Fine-tuning for Black-Box  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T">Tieniu Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With the emergence of pretrained vision-language models (VLMs), considerable
efforts have been devoted to fine-tuning them for downstream tasks. Despite the
progress made in designing efficient fine-tuning methods, such methods require
access to the model's parameters, which can be challenging as model owners
often opt to provide their models as a black box to safeguard model ownership.
This paper proposes a \textbf{C}ollabo\textbf{ra}tive
\textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning
black-box VLMs to downstream tasks, where one only has access to the input
prompts and the output predictions of the model. CraFT comprises two modules, a
prompt generation module for learning text prompts and a prediction refinement
module for enhancing output predictions in residual style. Additionally, we
introduce an auxiliary prediction-consistent loss to promote consistent
optimization across these modules. These modules are optimized by a novel
collaborative training algorithm. Extensive experiments on few-shot
classification over 15 datasets demonstrate the superiority of CraFT. The
results show that CraFT achieves a decent gain of about 12\% with 16-shot
datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only
about 1/80 of the memory footprint for deployment, while sacrificing only
1.62\% compared to the white-box method.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04051" title="Abstract">arXiv:2402.04051</a> [<a href="/pdf/2402.04051" title="Download PDF">pdf</a>, <a href="/format/2402.04051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Linear Mode Connectivity via Permutation-Based Weight  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ito%2C+A">Akira Ito</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Masanori Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Kumagai%2C+A">Atsutoshi Kumagai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, Ainsworth et al. showed that using weight matching (WM) to minimize
the $L_2$ distance in a permutation search of model parameters effectively
identifies permutations that satisfy linear mode connectivity (LMC), in which
the loss along a linear path between two independently trained models with
different seeds remains nearly constant. This paper provides a theoretical
analysis of LMC using WM, which is crucial for understanding stochastic
gradient descent's effectiveness and its application in areas like model
merging. We first experimentally and theoretically show that permutations found
by WM do not significantly reduce the $L_2$ distance between two models and the
occurrence of LMC is not merely due to distance reduction by WM in itself. We
then provide theoretical insights showing that permutations can change the
directions of the singular vectors, but not the singular values, of the weight
matrices in each layer. This finding shows that permutations found by WM mainly
align the directions of singular vectors associated with large singular values
across models. This alignment brings the singular vectors with large singular
values, which determine the model functionality, closer between pre-merged and
post-merged models, so that the post-merged model retains functionality similar
to the pre-merged models, making it easy to satisfy LMC. Finally, we analyze
the difference between WM and straight-through estimator (STE), a
dataset-dependent permutation search method, and show that WM outperforms STE,
especially when merging three or more models.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04054" title="Abstract">arXiv:2402.04054</a> [<a href="/pdf/2402.04054" title="Download PDF">pdf</a>, <a href="/format/2402.04054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zakerinia%2C+H">Hossein Zakerinia</a>, 
<a href="/search/cs?searchtype=author&query=Behjati%2C+A">Amin Behjati</a>, 
<a href="/search/cs?searchtype=author&query=Lampert%2C+C+H">Christoph H. Lampert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a new framework for studying meta-learning methods using
PAC-Bayesian theory. Its main advantage over previous work is that it allows
for more flexibility in how the transfer of knowledge between tasks is
realized. For previous approaches, this could only happen indirectly, by means
of learning prior distributions over models. In contrast, the new
generalization bounds that we prove express the process of meta-learning much
more directly as learning the learning algorithm that should be used for future
tasks. The flexibility of our framework makes it suitable to analyze a wide
range of meta-learning mechanisms and even design new mechanisms. Other than
our theoretical contributions we also show empirically that our framework
improves the prediction quality in practical meta-learning mechanisms.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04058" title="Abstract">arXiv:2402.04058</a> [<a href="/pdf/2402.04058" title="Download PDF">pdf</a>, <a href="/ps/2402.04058" title="Download PostScript">ps</a>, <a href="/format/2402.04058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Digital Twin Design Methodology for Control, Simulation, and  Monitoring of Fluidic Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gunes%2C+V">Veyis Gunes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We propose a synthesis method for the design of digital twins applicable to
various systems (pneumatic, hydraulic, electrical/electronic circuits). The
methodology allows representing the operation of these systems through an
active digital twin, thereby enabling a more suitable and easier computer-aided
design, simulation, control, and monitoring. Furthermore, our methodology
enables the detection of a system's actions on its own inputs (for example, in
pneumatics: backflow of gases trapped in part of a fluidic system onto its own
inputs). During the simulation or monitoring phase, the approach also
facilitates real-time diagnosis of the controlled system. The outputs, on the
controlled physical system or its digital twin, do not depend only on the
current inputs but also on the history of the inputs and the history of
internal states and variables. In other words, the underlying sequential logic
has a memory while an only combinational logic approach does not. These
capabilities can contribute to the digital transformation of the factory of the
future.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04059" title="Abstract">arXiv:2402.04059</a> [<a href="/pdf/2402.04059" title="Download PDF">pdf</a>, <a href="/format/2402.04059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Multivariate Time Series Imputation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wenjie Du</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Keli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 5 tables, 58 referred papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ubiquitous missing values cause the multivariate time series data to be
partially observed, destroying the integrity of time series and hindering the
effective time series data analysis. Recently deep learning imputation methods
have demonstrated remarkable success in elevating the quality of corrupted time
series data, subsequently enhancing performance in downstream tasks. In this
paper, we conduct a comprehensive survey on the recently proposed deep learning
imputation methods. First, we propose a taxonomy for the reviewed methods, and
then provide a structured review of these methods by highlighting their
strengths and limitations. We also conduct empirical experiments to study
different methods and compare their enhancement for downstream tasks. Finally,
the open issues for future research on multivariate time series imputation are
pointed out. All code and configurations of this work, including a regularly
maintained multivariate time series imputation paper list, can be found in the
GitHub repository~\url{https://github.com/WenjieDu/Awesome\_Imputation}.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04061" title="Abstract">arXiv:2402.04061</a> [<a href="/pdf/2402.04061" title="Download PDF">pdf</a>, <a href="/format/2402.04061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoNav: Topological Navigation for Efficient Exploration in Sparse  Reward Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+J">Jumman Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Faridee%2C+A">Abu-Zaher Faridee</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nirmalya Roy</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+J">Jade Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+T">Timothy Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Trout%2C+T+T">Theron T. Trout</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Autonomous robots exploring unknown areas face a significant challenge --
navigating effectively without prior maps and with limited external feedback.
This challenge intensifies in sparse reward environments, where traditional
exploration techniques often fail. In this paper, we introduce TopoNav, a novel
framework that empowers robots to overcome these constraints and achieve
efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental
building blocks are active topological mapping, intrinsic reward mechanisms,
and hierarchical objective prioritization. Throughout its exploration, TopoNav
constructs a dynamic topological map that captures key locations and pathways.
It utilizes intrinsic rewards to guide the robot towards designated sub-goals
within this map, fostering structured exploration even in sparse reward
settings. To ensure efficient navigation, TopoNav employs the Hierarchical
Objective-Driven Active Topologies framework, enabling the robot to prioritize
immediate tasks like obstacle avoidance while maintaining focus on the overall
goal. We demonstrate TopoNav's effectiveness in simulated environments that
replicate real-world conditions. Our results reveal significant improvements in
exploration efficiency, navigational accuracy, and adaptability to unforeseen
obstacles, showcasing its potential to revolutionize autonomous exploration in
a wide range of applications, including search and rescue, environmental
monitoring, and planetary exploration.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04062" title="Abstract">arXiv:2402.04062</a> [<a href="/pdf/2402.04062" title="Download PDF">pdf</a>, <a href="/format/2402.04062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link Prediction with Relational Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingyue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Orth%2C+M+R">Miguel Romero Orth</a>, 
<a href="/search/cs?searchtype=author&query=Barcel%C3%B3%2C+P">Pablo Barcel&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M+M">Michael M. Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Link prediction with knowledge graphs has been thoroughly studied in graph
machine learning, leading to a rich landscape of graph neural network
architectures with successful applications. Nonetheless, it remains challenging
to transfer the success of these architectures to link prediction with
relational hypergraphs. The presence of relational hyperedges makes link
prediction a task between $k$ nodes for varying choices of $k$, which is
substantially harder than link prediction with knowledge graphs, where every
relation is binary ($k=2$). In this paper, we propose two frameworks for link
prediction with relational hypergraphs and conduct a thorough analysis of the
expressive power of the resulting model architectures via corresponding
relational Weisfeiler-Leman algorithms, and also via some natural logical
formalisms. Through extensive empirical analysis, we validate the power of the
proposed model architectures on various relational hypergraph benchmarks. The
resulting model architectures substantially outperform every baseline for
inductive link prediction, and lead to state-of-the-art results for
transductive link prediction. Our study therefore unlocks applications of graph
neural networks to fully relational structures.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04064" title="Abstract">arXiv:2402.04064</a> [<a href="/pdf/2402.04064" title="Download PDF">pdf</a>, <a href="/format/2402.04064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-class Road Defect Detection and Segmentation using Spatial and  Channel-wise Attention for Autonomous Road Repairing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jongmin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C+B">Chen Bene Chi</a>, 
<a href="/search/cs?searchtype=author&query=Fichera%2C+S">Sebastiano Fichera</a>, 
<a href="/search/cs?searchtype=author&query=Paoletti%2C+P">Paolo Paoletti</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+D">Devansh Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Road pavement detection and segmentation are critical for developing
autonomous road repair systems. However, developing an instance segmentation
method that simultaneously performs multi-class defect detection and
segmentation is challenging due to the textural simplicity of road pavement
image, the diversity of defect geometries, and the morphological ambiguity
between classes. We propose a novel end-to-end method for multi-class road
defect detection and segmentation. The proposed method comprises multiple
spatial and channel-wise attention blocks available to learn global
representations across spatial and channel-wise dimensions. Through these
attention blocks, more globally generalised representations of morphological
information (spatial characteristics) of road defects and colour and depth
information of images can be learned. To demonstrate the effectiveness of our
framework, we conducted various ablation studies and comparisons with prior
methods on a newly collected dataset annotated with nine road defect classes.
The experiments show that our proposed method outperforms existing
state-of-the-art methods for multi-class road defect detection and segmentation
methods.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04068" title="Abstract">arXiv:2402.04068</a> [<a href="/pdf/2402.04068" title="Download PDF">pdf</a>, <a href="/format/2402.04068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieve to Explain: Evidence-driven Predictions with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+R">Ravi Patel</a> (1), 
<a href="/search/cs?searchtype=author&query=Brayne%2C+A">Angus Brayne</a> (1), 
<a href="/search/cs?searchtype=author&query=Hintzen%2C+R">Rogier Hintzen</a> (1), 
<a href="/search/cs?searchtype=author&query=Jaroslawicz%2C+D">Daniel Jaroslawicz</a> (1), 
<a href="/search/cs?searchtype=author&query=Neculae%2C+G">Georgiana Neculae</a> (1), 
<a href="/search/cs?searchtype=author&query=Corneil%2C+D">Dane Corneil</a> (1) ((1) BenevolentAI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Machine learning models, particularly language models, are notoriously
difficult to introspect. Black-box models can mask both issues in model
training and harmful biases. For human-in-the-loop processes, opaque
predictions can drive lack of trust, limiting a model's impact even when it
performs effectively. To address these issues, we introduce Retrieve to Explain
(R2E). R2E is a retrieval-based language model that prioritizes amongst a
pre-defined set of possible answers to a research question based on the
evidence in a document corpus, using Shapley values to identify the relative
importance of pieces of evidence to the final prediction. R2E can adapt to new
evidence without retraining, and incorporate structured data through templating
into natural language. We assess on the use case of drug target identification
from published scientific literature, where we show that the model outperforms
an industry-standard genetics-based approach on predicting clinical trial
outcomes.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04070" title="Abstract">arXiv:2402.04070</a> [<a href="/pdf/2402.04070" title="Download PDF">pdf</a>, <a href="/format/2402.04070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial Assisted Human-Drone Collaborative Navigation and Interaction  through Immersive Mixed Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morando%2C+L">Luca Morando</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently Accepted at International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Aerial robots have the potential to play a crucial role in assisting humans
with complex and dangerous tasks. Nevertheless, the future industry demands
innovative solutions to streamline the interaction process between humans and
drones to enable seamless collaboration and efficient co-working. In this
paper, we present a novel tele-immersive framework that promotes cognitive and
physical collaboration between humans and robots through Mixed Reality (MR).
This framework incorporates a novel bi-directional spatial awareness and a
multi-modal virtual-physical interaction approaches. The former seamlessly
integrates the physical and virtual worlds, offering bidirectional egocentric
and exocentric environmental representations. The latter, leveraging the
proposed spatial representation, further enhances the collaboration combining a
robot planning algorithm for obstacle avoidance with a variable admittance
control. This allows users to issue commands based on virtual forces while
maintaining compatibility with the environment map. We validate the proposed
approach by performing several collaborative planning and exploration tasks
involving a drone and an user equipped with a MR headset.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04074" title="Abstract">arXiv:2402.04074</a> [<a href="/pdf/2402.04074" title="Download PDF">pdf</a>, <a href="/ps/2402.04074" title="Download PostScript">ps</a>, <a href="/format/2402.04074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean-Square Stability and Stabilizability for LTI and Stochastic Systems  Connected in Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Junhui Li</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jieying Lu</a>, 
<a href="/search/eess?searchtype=author&query=Su%2C+W">Weizhou Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, the feedback stabilization of a linear time-invariant (LTI)
multiple-input multiple-output (MIMO) system cascaded by a linear stochastic
system is studied in the mean-square sense. Here, the linear stochastic system
can model a class of correlated stochastic uncertainties such as channel
uncertainties induced by packet loss and random transmission delays in
networked systems. By proposing a key parameter called coefficient of frequency
variation to characterize the correlation of the stochastic uncertainties, we
present a necessary and sufficient condition of the mean-square stability for
this MIMO stochastic feedback system. After then a necessary and sufficient
condition for the mean-square stabilizability is provided, which reveals a
fundamental limit imposed by the system's unstable poles, nonminimum-phase
(NMP) zeros, relative degrees (input delays), and the coefficient of frequency
variation of the stochastic uncertainties. A numerical example is presented to
illustrate the fundamental constraints in the mean-square stabilizability of
MIMO networked systems with parallel communication channels.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04075" title="Abstract">arXiv:2402.04075</a> [<a href="/pdf/2402.04075" title="Download PDF">pdf</a>, <a href="/format/2402.04075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Prompt Refinement for Radiation Oncology Symptom Extraction  Using Teacher-Student Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khanmohammadi%2C+R">Reza Khanmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+A+I">Ahmed I Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Verdecchia%2C+K">Kyle Verdecchia</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+R">Ryan Hall</a>, 
<a href="/search/cs?searchtype=author&query=Elshaikh%2C+M">Mohamed Elshaikh</a>, 
<a href="/search/cs?searchtype=author&query=Movsas%2C+B">Benjamin Movsas</a>, 
<a href="/search/cs?searchtype=author&query=Bagher-Ebadian%2C+H">Hassan Bagher-Ebadian</a>, 
<a href="/search/cs?searchtype=author&query=Chetty%2C+I">Indrin Chetty</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M+M">Mohammad M. Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Thind%2C+K">Kundan Thind</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study introduces a novel teacher-student architecture utilizing Large
Language Models (LLMs) to improve prostate cancer radiotherapy symptom
extraction from clinical notes. Mixtral, the student model, initially extracts
symptoms, followed by GPT-4, the teacher model, which refines prompts based on
Mixtral's performance. This iterative process involved 294 single symptom
clinical notes across 12 symptoms, with up to 16 rounds of refinement per
epoch. Results showed significant improvements in extracting symptoms from both
single and multi-symptom notes. For 59 single symptom notes, accuracy increased
from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and
F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24
to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score
from 0.20 to 0.44. These results demonstrate the effectiveness of advanced
prompt engineering in LLMs for radiation oncology use.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04079" title="Abstract">arXiv:2402.04079</a> [<a href="/pdf/2402.04079" title="Download PDF">pdf</a>, <a href="/ps/2402.04079" title="Download PostScript">ps</a>, <a href="/format/2402.04079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and implementation of a real-time onboard system for a  stratospheric balloon mission using commercial off-the-self components and a  model-based approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Perez-Munoz%2C+A">Angel-Grover Perez-Munoz</a>, 
<a href="/search/eess?searchtype=author&query=Gamazo-Real%2C+J">Jose-Carlos Gamazo-Real</a>, 
<a href="/search/eess?searchtype=author&query=Gonzalez-Barcena%2C+D">David Gonzalez-Barcena</a>, 
<a href="/search/eess?searchtype=author&query=Zamorano%2C+J">Juan Zamorano</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers and Electrical Engineering, vol. 111, Part B, no.
  108953, pp. 1-22, 2023, ISSN 0045-7906
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Hardware Architecture (cs.AR); Operating Systems (cs.OS)

</div>
<p class="mathjax">Stratospheric balloons have emerged as an affordable and flexible alternative
to traditional spacecrafts as they are implemented using commercial
off-the-shelf (COTS) equipment without following strict methodologies.
HERCCULES is a stratospheric balloon mission that aims to characterize the
convective heat and radiative environment in the stratosphere. The purpose of
this article is to present the HERCCULES onboard software (OBSW) whose design
and complexity is comparable to that of satellite systems, since it must
control about sixty COTS equipment using a single Raspberry Pi 4B as onboard
computer and ensure the real-time requirements. Compared to similar systems,
novel contributions are presented as the OBSW is developed following modelbased
and component-based approaches using the TASTE toolchain from the European
Space Agency (ESA) for automatic code generation. Besides, the OBSW is verified
and validated following the ESA standards and the results obtained demonstrate
the suitability and efficiency of the solution and the selected methodologies.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04080" title="Abstract">arXiv:2402.04080</a> [<a href="/pdf/2402.04080" title="Download PDF">pdf</a>, <a href="/format/2402.04080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-regularized Diffusion Policy with Q-Ensembles for Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Ziwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+T+B">Thomas B. Sch&#xf6;n</a>, 
<a href="/search/cs?searchtype=author&query=Mattsson%2C+P">Per Mattsson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper presents advanced techniques of training diffusion policies for
offline reinforcement learning (RL). At the core is a mean-reverting stochastic
differential equation (SDE) that transfers a complex action distribution into a
standard Gaussian and then samples actions conditioned on the environment state
with a corresponding reverse-time SDE, like a typical diffusion policy. We show
that such an SDE has a solution that we can use to calculate the log
probability of the policy, yielding an entropy regularizer that improves the
exploration of offline datasets. To mitigate the impact of inaccurate value
functions from out-of-distribution data points, we further propose to learn the
lower confidence bound of Q-ensembles for more robust policy improvement. By
combining the entropy-regularized diffusion policy with Q-ensembles in offline
RL, our method achieves state-of-the-art performance on most tasks in D4RL
benchmarks. Code is available at
\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04081" title="Abstract">arXiv:2402.04081</a> [<a href="/pdf/2402.04081" title="Download PDF">pdf</a>, <a href="/format/2402.04081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Generalization of Weight Space Networks via Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D+W">David W. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04082" title="Abstract">arXiv:2402.04082</a> [<a href="/pdf/2402.04082" title="Download PDF">pdf</a>, <a href="/ps/2402.04082" title="Download PostScript">ps</a>, <a href="/format/2402.04082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal House Price Prediction Algorithm: XGBoost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+H">Hemlata Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Harsora%2C+H">Hitesh Harsora</a>, 
<a href="/search/cs?searchtype=author&query=Ogunleye%2C+B">Bayode Ogunleye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, Journal of Analytics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Analytics, 3(1), 30-45 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">An accurate prediction of house prices is a fundamental requirement for
various sectors including real estate and mortgage lending. It is widely
recognized that a property value is not solely determined by its physical
attributes but is significantly influenced by its surrounding neighbourhood.
Meeting the diverse housing needs of individuals while balancing budget
constraints is a primary concern for real estate developers. To this end, we
addressed the house price prediction problem as a regression task and thus
employed various machine learning techniques capable of expressing the
significance of independent variables. We made use of the housing dataset of
Ames City in Iowa, USA to compare support vector regressor, random forest
regressor, XGBoost, multilayer perceptron and multiple linear regression
algorithms for house price prediction. Afterwards, we identified the key
factors that influence housing costs. Our results show that XGBoost is the best
performing model for house price prediction.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04083" title="Abstract">arXiv:2402.04083</a> [<a href="/pdf/2402.04083" title="Download PDF">pdf</a>, <a href="/ps/2402.04083" title="Download PostScript">ps</a>, <a href="/format/2402.04083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperation and profit allocation in distribution chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Timmer%2C+J">Judith Timmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the coordination of actions and the allocation of profit in supply
chains under decentralized control in which a single supplier supplies several
retailers with goods for replenishment of stocks. The goal of the supplier and
the retailers is to maximize their individual profits. Since the outcome under
decentralized control is inefficient, cooperation among firms by means of
coordination of actions may improve the individual profits. Cooperation is
studied by means of cooperative game theory. Among others we show that the
corresponding games are balanced and we propose a stable solution concept for
these games.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04084" title="Abstract">arXiv:2402.04084</a> [<a href="/pdf/2402.04084" title="Download PDF">pdf</a>, <a href="/ps/2402.04084" title="Download PostScript">ps</a>, <a href="/format/2402.04084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably learning a multi-head attention layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sitan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 105 pages, comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">The multi-head attention layer is one of the key components of the
transformer architecture that sets it apart from traditional feed-forward
models. Given a sequence length $k$, attention matrices
$\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and
projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times
d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to
\mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional
tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq
\sum^m_{i=1}
\mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$.
In this work, we initiate the study of provably learning a multi-head attention
layer from random examples and give the first nontrivial upper and lower bounds
for this problem:
<br />- Provided $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ satisfy certain
non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns
$F$ to small error given random labeled examples drawn uniformly from $\{\pm
1\}^{k\times d}$.
<br />- We prove computational lower bounds showing that in the worst case,
exponential dependence on $m$ is unavoidable.
<br />We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in
large language models, though our techniques naturally extend to standard
continuous settings, e.g. Gaussian. Our algorithm, which is centered around
using examples to sculpt a convex body containing the unknown parameters, is a
significant departure from existing provable algorithms for learning
feedforward networks, which predominantly exploit algebraic and rotation
invariance properties of the Gaussian distribution. In contrast, our analysis
is more flexible as it primarily relies on various upper and lower tail bounds
for the input distribution and "slices" thereof.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04087" title="Abstract">arXiv:2402.04087</a> [<a href="/pdf/2402.04087" title="Download PDF">pdf</a>, <a href="/format/2402.04087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+L">Lijun Sheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T">Tieniu Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Contrastive Language-Image Pretraining (CLIP) has gained popularity for its
remarkable zero-shot capacity. Recent research has focused on developing
efficient fine-tuning methods, such as prompt learning and adapter, to enhance
CLIP's performance in downstream tasks. However, these methods still require
additional training time and computational resources, which is undesirable for
devices with limited resources. In this paper, we revisit a classical
algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream
classification of CLIP. Typically, GDA assumes that features of each class
follow Gaussian distributions with identical covariance. By leveraging Bayes'
formula, the classifier can be expressed in terms of the class means and
covariance, which can be estimated from the data without the need for training.
To integrate knowledge from both visual and textual modalities, we ensemble it
with the original zero-shot classifier within CLIP. Extensive results on 17
datasets validate that our method surpasses or achieves comparable results with
state-of-the-art methods on few-shot classification, imbalanced learning, and
out-of-distribution generalization. In addition, we extend our method to
base-to-new generalization and unsupervised learning, once again demonstrating
its superiority over competing approaches. Our code is publicly available at
\url{https://github.com/mrflogs/ICLR24}.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04088" title="Abstract">arXiv:2402.04088</a> [<a href="/pdf/2402.04088" title="Download PDF">pdf</a>, <a href="/ps/2402.04088" title="Download PostScript">ps</a>, <a href="/format/2402.04088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Use of a Large Language Model for Cyberbullying Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ogunleye%2C+B">Bayode Ogunleye</a>, 
<a href="/search/cs?searchtype=author&query=Dharmaraj%2C+B">Babitha Dharmaraj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, Journal of Analytics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Analytics 2 (2023), no. 3: 694-707
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">The dominance of social media has added to the channels of bullying for
perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent
phenomenon in todays cyber world, and is a severe threat to the mental and
physical health of citizens. This opens the need to develop a robust system to
prevent bullying content from online forums, blogs, and social media platforms
to manage the impact in our society. Several machine learning (ML) algorithms
have been proposed for this purpose. However, their performances are not
consistent due to high class imbalance and generalisation issues. In recent
years, large language models (LLMs) like BERT and RoBERTa have achieved
state-of-the-art (SOTA) results in several natural language processing (NLP)
tasks. Unfortunately, the LLMs have not been applied extensively for CB
detection. In our paper, we explored the use of these models for cyberbullying
(CB) detection. We have prepared a new dataset (D2) from existing studies
(Formspring and Twitter). Our experimental results for dataset D1 and D2 showed
that RoBERTa outperformed other models.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04090" title="Abstract">arXiv:2402.04090</a> [<a href="/pdf/2402.04090" title="Download PDF">pdf</a>, <a href="/ps/2402.04090" title="Download PostScript">ps</a>, <a href="/format/2402.04090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acceleration and energy consumption optimization in cascading  classifiers for face detection on low-cost ARM big.LITTLE asymmetric  architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corpas%2C+A">Alberto Corpas</a>, 
<a href="/search/cs?searchtype=author&query=Costero%2C+L">Luis Costero</a>, 
<a href="/search/cs?searchtype=author&query=Botella%2C+G">Guillermo Botella</a>, 
<a href="/search/cs?searchtype=author&query=Igual%2C+F+D">Francisco D. Igual</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+C">Carlos Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+M">Manuel Rodr&#xed;guez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Circuit Theory and Applications. 2018.
  46, pp 1756 1776
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">This paper proposes a mechanism to accelerate and optimize the energy
consumption of a face detection software based on Haar-like cascading
classifiers, taking advantage of the features of low-cost Asymmetric Multicore
Processors (AMPs) with limited power budget. A modelling and task
scheduling/allocation is proposed in order to efficiently make use of the
existing features on big.LITTLE ARM processors, including: (I) source-code
adaptation for parallel computing, which enables code acceleration by applying
the OmpSs programming model, a task-based programming model that handles
data-dependencies between tasks in a transparent fashion; (II) different OmpSs
task allocation policies which take into account the processor asymmetry and
can dynamically set processing resources in a more efficient way based on their
particular features. The proposed mechanism can be efficiently applied to take
advantage of the processing elements existing on low-cost and low-energy
multi-core embedded devices executing object detection algorithms based on
cascading classifiers. Although these classifiers yield the best results for
detection algorithms in the field of computer vision, their high computational
requirements prevent them from being used on these devices under real-time
requirements. Finally, we compare the energy efficiency of a heterogeneous
architecture based on asymmetric multicore processors with a suitable task
scheduling, with that of a homogeneous symmetric architecture.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04094" title="Abstract">arXiv:2402.04094</a> [<a href="/pdf/2402.04094" title="Download PDF">pdf</a>, <a href="/format/2402.04094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic theta methods for free stochastic differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Niu%2C+Y">Yuan-Ling Niu</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+J">Jia-Xin Wei</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+Z">Zhi Yin</a>, 
<a href="/search/math?searchtype=author&query=Zeng%2C+D">Dan Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We introduce free probability analogues of the stochastic theta methods for
free stochastic differential equations, which generalize the free
Euler-Maruyama method introduced by Schl\"{u}chtermann and Wibmer [27]. Under
some mild conditions, we prove the strong convergence and exponential stability
in mean square of the numerical solution. The free stochastic theta method with
$\theta=1$ can inherit the exponential stability of original equations for any
given step size. Our method can offer better stability and efficiency than the
free Euler-Maruyama method. Moreover, numerical results are reported to confirm
these theoretical findings.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04097" title="Abstract">arXiv:2402.04097</a> [<a href="/pdf/2402.04097" title="Download PDF">pdf</a>, <a href="/format/2402.04097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Deep Image Prior and Exploiting Self-Guidance for Image  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shijun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bell%2C+E">Evan Bell</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ravishankar%2C+S">Saiprasad Ravishankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The ability of deep image prior (DIP) to recover high-quality images from
incomplete or corrupted measurements has made it popular in inverse problems in
image restoration and medical imaging including magnetic resonance imaging
(MRI). However, conventional DIP suffers from severe overfitting and spectral
bias effects.In this work, we first provide an analysis of how DIP recovers
information from undersampled imaging measurements by analyzing the training
dynamics of the underlying networks in the kernel regime for different
architectures.This study sheds light on important underlying properties for
DIP-based recovery.Current research suggests that incorporating a reference
image as network input can enhance DIP's performance in image reconstruction
compared to using random inputs. However, obtaining suitable reference images
requires supervision, and raises practical difficulties. In an attempt to
overcome this obstacle, we further introduce a self-driven reconstruction
process that concurrently optimizes both the network weights and the input
while eliminating the need for training data. Our method incorporates a novel
denoiser regularization term which enables robust and stable joint estimation
of both the network input and reconstructed image.We demonstrate that our
self-guided method surpasses both the original DIP and modern supervised
methods in terms of MR image reconstruction performance and outperforms
previous DIP-based schemes for image inpainting.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04101" title="Abstract">arXiv:2402.04101</a> [<a href="/pdf/2402.04101" title="Download PDF">pdf</a>, <a href="/format/2402.04101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VRMM: A Volumetric Relightable Morphable Head Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haotian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mingwu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chongyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yu-Kun Lai</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haibin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">In this paper, we introduce the Volumetric Relightable Morphable Model
(VRMM), a novel volumetric and parametric facial prior for 3D face modeling.
While recent volumetric prior models offer improvements over traditional
methods like 3D Morphable Models (3DMMs), they face challenges in model
learning and personalized reconstructions. Our VRMM overcomes these by
employing a novel training framework that efficiently disentangles and encodes
latent spaces of identity, expression, and lighting into low-dimensional
representations. This framework, designed with self-supervised learning,
significantly reduces the constraints for training data, making it more
feasible in practice. The learned VRMM offers relighting capabilities and
encompasses a comprehensive range of expressions. We demonstrate the
versatility and effectiveness of VRMM through various applications like avatar
generation, facial reconstruction, and animation. Additionally, we address the
common issue of overfitting in generative volumetric models with a novel
prior-preserving personalization framework based on VRMM. Such an approach
enables accurate 3D face reconstruction from even a single portrait input. Our
experiments showcase the potential of VRMM to significantly enhance the field
of 3D face modeling.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04102" title="Abstract">arXiv:2402.04102</a> [<a href="/pdf/2402.04102" title="Download PDF">pdf</a>, <a href="/format/2402.04102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Use of Multi-CNNs for Section Analysis in Static Malware Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quertier%2C+T">Tony Quertier</a>, 
<a href="/search/cs?searchtype=author&query=Barru%C3%A9%2C+G">Gr&#xe9;goire Barru&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2312.12161">arXiv:2312.12161</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing research on malware detection focuses almost exclusively on the
detection rate. However, in some cases, it is also important to understand the
results of our algorithm, or to obtain more information, such as where to
investigate in the file for an analyst. In this aim, we propose a new model to
analyze Portable Executable files. Our method consists in splitting the files
in different sections, then transform each section into an image, in order to
train convolutional neural networks to treat specifically each identified
section. Then we use all these scores returned by CNNs to compute a final
detection score, using models that enable us to improve our analysis of the
importance of each section in the final score.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04103" title="Abstract">arXiv:2402.04103</a> [<a href="/pdf/2402.04103" title="Download PDF">pdf</a>, <a href="/ps/2402.04103" title="Download PostScript">ps</a>, <a href="/format/2402.04103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploration of Clustering Algorithms for Customer Segmentation in the  UK Retail Market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=John%2C+J+M">Jeen Mary John</a>, 
<a href="/search/cs?searchtype=author&query=Shobayo%2C+O">Olamilekan Shobayo</a>, 
<a href="/search/cs?searchtype=author&query=Ogunleye%2C+B">Bayode Ogunleye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, Journal of Analytics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Analytics, 2(4), 809-823 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP); Computation (stat.CO)

</div>
<p class="mathjax">Recently, peoples awareness of online purchases has significantly risen. This
has given rise to online retail platforms and the need for a better
understanding of customer purchasing behaviour. Retail companies are pressed
with the need to deal with a high volume of customer purchases, which requires
sophisticated approaches to perform more accurate and efficient customer
segmentation. Customer segmentation is a marketing analytical tool that aids
customer-centric service and thus enhances profitability. In this paper, we aim
to develop a customer segmentation model to improve decision-making processes
in the retail market industry. To achieve this, we employed a UK-based online
retail dataset obtained from the UCI machine learning repository. The retail
dataset consists of 541,909 customer records and eight features. Our study
adopted the RFM (recency, frequency, and monetary) framework to quantify
customer values. Thereafter, we compared several state-of-the-art (SOTA)
clustering algorithms, namely, K-means clustering, the Gaussian mixture model
(GMM), density-based spatial clustering of applications with noise (DBSCAN),
agglomerative clustering, and balanced iterative reducing and clustering using
hierarchies (BIRCH). The results showed the GMM outperformed other approaches,
with a Silhouette Score of 0.80.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04104" title="Abstract">arXiv:2402.04104</a> [<a href="/pdf/2402.04104" title="Download PDF">pdf</a>, <a href="/format/2402.04104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Godunov--type scheme for a scalar conservation law with space-time  flux discontinuity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gyamfi%2C+K+A">Kwame Atta Gyamfi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present and analyze a new finite volume scheme of Gudonov-type for a
nonlinear scalar conservation law whose flux function has a discontinuous
coefficient due to time-dependent changes in its sign along a Lipschitz
continuous curve.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04105" title="Abstract">arXiv:2402.04105</a> [<a href="/pdf/2402.04105" title="Download PDF">pdf</a>, <a href="/format/2402.04105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Implicit Bias in Explicitly Unbiased Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xuechunzi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Angelina Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) can pass explicit bias tests but still harbor
implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit
subtle biases. Measuring such implicit biases can be a challenge: as LLMs
become increasingly proprietary, it may not be possible to access their
embeddings and apply existing bias measures; furthermore, implicit biases are
primarily a concern if they affect the actual decisions that these systems
make. We address both of these challenges by introducing two measures of bias
inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a
prompt-based method for revealing implicit bias; and LLM Decision Bias for
detecting subtle discrimination in decision-making tasks. Using these measures,
we found pervasive human-like stereotype biases in 6 LLMs across 4 social
domains (race, gender, religion, health) and 21 categories (weapons, guilt,
science, career among others). Our prompt-based measure of implicit bias
correlates with embedding-based methods but better predicts downstream
behaviors measured by LLM Decision Bias. This measure is based on asking the
LLM to decide between individuals, motivated by psychological results
indicating that relative not absolute evaluations are more related to implicit
biases. Using prompt-based measures informed by psychology allows us to
effectively expose nuanced biases and subtle discrimination in proprietary LLMs
that do not show explicit bias on standard benchmarks.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04108" title="Abstract">arXiv:2402.04108</a> [<a href="/pdf/2402.04108" title="Download PDF">pdf</a>, <a href="/format/2402.04108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Delay Attribution Classification using Unstructured Text in  Train Management Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borg%2C+A">Anton Borg</a>, 
<a href="/search/cs?searchtype=author&query=Lingvall%2C+P">Per Lingvall</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+M">Martin Svensson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">EU directives stipulate a systematic follow-up of train delays. In Sweden,
the Swedish Transport Administration registers and assigns an appropriate delay
attribution code. However, this delay attribution code is assigned manually,
which is a complex task. In this paper, a machine learning-based decision
support for assigning delay attribution codes based on event descriptions is
investigated. The text is transformed using TF-IDF, and two models, Random
Forest and Support Vector Machine, are evaluated against a random uniform
classifier and the classification performance of the Swedish Transport
Administration. Further, the problem is modeled as both a hierarchical and flat
approach. The results indicate that a hierarchical approach performs better
than a flat approach. Both approaches perform better than the random uniform
classifier but perform worse than the manual classification.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04110" title="Abstract">arXiv:2402.04110</a> [<a href="/pdf/2402.04110" title="Download PDF">pdf</a>, <a href="/format/2402.04110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behind the Screen: Investigating ChatGPT&#x27;s Dark Personality Traits and  Conspiracy Beliefs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber%2C+E">Erik Weber</a>, 
<a href="/search/cs?searchtype=author&query=Rutinowski%2C+J">J&#xe9;r&#xf4;me Rutinowski</a>, 
<a href="/search/cs?searchtype=author&query=Pauly%2C+M">Markus Pauly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">ChatGPT is notorious for its intransparent behavior. This paper tries to shed
light on this, providing an in-depth analysis of the dark personality traits
and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and
questionnaires were employed, including the Dark Factor Test, the Mach-IV
Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.
The responses were analyzed computing average scores, standard deviations, and
significance tests to investigate differences between GPT-3.5 and GPT-4. For
traits that have shown to be interdependent in human studies, correlations were
considered. Additionally, system roles corresponding to groups that have shown
distinct answering behavior in the corresponding questionnaires were applied to
examine the models' ability to reflect characteristics associated with these
roles in their responses. Dark personality traits and conspiracy beliefs were
not particularly pronounced in either model with little differences between
GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in
information withholding. This is particularly intriguing given that GPT-4 is
trained on a significantly larger dataset than GPT-3.5. Apparently, in this
case an increased data exposure correlates with a greater belief in the control
of information. An assignment of extreme political affiliations increased the
belief in conspiracy theories. Test sequencing affected the models' responses
and the observed correlations, indicating a form of contextual memory.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04111" title="Abstract">arXiv:2402.04111</a> [<a href="/pdf/2402.04111" title="Download PDF">pdf</a>, <a href="/ps/2402.04111" title="Download PostScript">ps</a>, <a href="/format/2402.04111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akrout%2C+M">Mohamed Akrout</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tiancheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Bellili%2C+F">Faouzi Bellili</a>, 
<a href="/search/cs?searchtype=author&query=Mezghani%2C+A">Amine Mezghani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Approximate message passing (AMP) algorithms are devised under the
Gaussianity assumption of the measurement noise vector. In this work, we relax
this assumption within the vector AMP (VAMP) framework to arbitrary independent
and identically distributed (i.i.d.) noise priors. We do so by rederiving the
linear minimum mean square error (LMMSE) to accommodate both the noise and
signal estimations within the message passing steps of VAMP. Numerical results
demonstrate how our proposed algorithm handles non-Gaussian noise models as
compared to VAMP. This extension to general noise priors enables the use of AMP
algorithms in a wider range of engineering applications where non-Gaussian
noise models are more appropriate.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04119" title="Abstract">arXiv:2402.04119</a> [<a href="/pdf/2402.04119" title="Download PDF">pdf</a>, <a href="/format/2402.04119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scientific Language Modeling: A Quantitative Review of Large Language  Models in Molecular Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jun Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Efficient molecular modeling and design are crucial for the discovery and
exploration of novel molecules, and the incorporation of deep learning methods
has revolutionized this field. In particular, large language models (LLMs)
offer a fresh approach to tackle scientific problems from a natural language
processing (NLP) perspective, introducing a research paradigm called scientific
language modeling (SLM). However, two key issues remain: how to quantify the
match between model and data modalities and how to identify the
knowledge-learning preferences of models. To address these challenges, we
propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263
experiments to assess the model's compatibility with data modalities and
knowledge acquisition. Through the modal transition probability matrix, we
provide insights into the most suitable modalities for tasks. Furthermore, we
introduce a statistically interpretable approach to discover context-specific
knowledge mapping by localized feature filtering. Our pioneering analysis
offers an exploration of the learning mechanism and paves the way for advancing
SLM in molecular science.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04129" title="Abstract">arXiv:2402.04129</a> [<a href="/pdf/2402.04129" title="Download PDF">pdf</a>, <a href="/format/2402.04129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free  Class-Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei-Cheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun-Fu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+H">Hsiang Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent works have shown that by using large pre-trained models along with
learnable prompts, rehearsal-free methods for class-incremental learning (CIL)
settings can achieve superior performance to prominent rehearsal-based ones.
Rehearsal-free CIL methods struggle with distinguishing classes from different
tasks, as those are not trained together. In this work we propose a
regularization method based on virtual outliers to tighten decision boundaries
of the classifier, such that confusion of classes among different tasks is
mitigated. Recent prompt-based methods often require a pool of task-specific
prompts, in order to prevent overwriting knowledge of previous tasks with that
of the new task, leading to extra computation in querying and composing an
appropriate prompt from the pool. This additional cost can be eliminated,
without sacrificing accuracy, as we reveal in the paper. We illustrate that a
simplified prompt-based method can achieve results comparable to previous
state-of-the-art (SOTA) methods equipped with a prompt pool, using much less
learnable parameters and lower inference cost. Our regularization method has
demonstrated its compatibility with different prompt-based methods, boosting
those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and
CIFAR-100 benchmarks. Our source code is available at
https://github.com/jpmorganchase/ovor.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04134" title="Abstract">arXiv:2402.04134</a> [<a href="/pdf/2402.04134" title="Download PDF">pdf</a>, <a href="/format/2402.04134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A quasi-optimal lower bound for skew polynomial multiplication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Ke Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Symbolic Computation (cs.SC); Rings and Algebras (math.RA)

</div>
<p class="mathjax">We establish a lower bound for the complexity of multiplying two skew
polynomials. The lower bound coincides with the upper bound conjectured by
Caruso and Borgne in 2017, up to a log factor. We present algorithms for three
special cases, indicating that the aforementioned lower bound is quasi-optimal.
In fact, our lower bound is quasi-optimal in the sense of bilinear complexity.
In addition, we discuss the average bilinear complexity of simultaneous
multiplication of skew polynomials and the complexity of skew polynomial
multiplication in the case of towers of extensions.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04138" title="Abstract">arXiv:2402.04138</a> [<a href="/pdf/2402.04138" title="Download PDF">pdf</a>, <a href="/format/2402.04138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAC Method for Fitting Exponential Autoregressive Models and Others:  Applications in Economy and Finance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=S%C3%A1nchez%2C+J+C">Javier Cabello S&#xe1;nchez</a>, 
<a href="/search/math?searchtype=author&query=Torvisco%2C+J+A+F">Juan Antonio Fern&#xe1;ndez Torvisco</a>, 
<a href="/search/math?searchtype=author&query=Arias%2C+M+R">Mariano R. Arias</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics. 2021; 9(8):862
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">There are a couple of purposes in this paper: to study a problem of
approximation with exponential functions and to show its relevance for the
economic science. We present results that completely solve the problem of the
best approximation by means of exponential functions and we will be able to
determine what kind of data is suitable to be fitted. Data will be approximated
using TAC (implemented in the R-package nlstac), a numerical algorithm for
fitting data by exponential patterns without initial guess designed by the
authors. We check one more time the robustness of this algorithm by
successfully applying it to two very distant areas of economy: demand curves
and nonlinear time series. This shows TAC's utility and highlights how far this
algorithm could be used.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04139" title="Abstract">arXiv:2402.04139</a> [<a href="/pdf/2402.04139" title="Download PDF">pdf</a>, <a href="/format/2402.04139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-shaped Vision Mamba for Single Image Dehazing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Currently, Transformer is the most popular architecture for image dehazing,
but due to its large computational complexity, its ability to handle long-range
dependency is limited on resource-constrained devices. To tackle this
challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient
single-image dehazing network. Inspired by the State Space Sequence Models
(SSMs), a new deep sequence model known for its power to handle long sequences,
we design a Bi-SSM block that integrates the local feature extraction ability
of the convolutional layer with the ability of the SSM to capture long-range
dependencies. Extensive experimental results demonstrate the effectiveness of
our method. Our method provides a more highly efficient idea of long-range
dependency modeling for image dehazing as well as other image restoration
tasks. The URL of the code is \url{https://github.com/zzr-idam}.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04140" title="Abstract">arXiv:2402.04140</a> [<a href="/pdf/2402.04140" title="Download PDF">pdf</a>, <a href="/ps/2402.04140" title="Download PostScript">ps</a>, <a href="/format/2402.04140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Legal Reasoning: The Integration of AI to Navigate  Complexities and Biases in Global Jurisprudence with Semi-Automated  Arbitration Processes (SAAPs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De%27Shazer%2C+M">Michael De&#x27;Shazer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This study consists of a novel approach toward the analysis of court
judgments spanning five countries, including the United States, the United
Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the
intersection of the latest advancements in artificial intelligence (AI) and
legal analysis, emphasizing the role of AI (specifically generative AI) in
identifying human biases and facilitating automated, valid, and coherent
multisided argumentation of court judgments with the goal of ensuring
consistent application of laws in and across various jurisdictions. By
incorporating Advanced Language Models (ALMs) and a newly introduced human-AI
collaborative framework, this paper seeks to analyze Grounded Theory-based
research design with Advanced Language Models (ALMs) in the practice of law.
SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT
technology), focusing on detecting logical inconsistencies and biases across
various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a
comparison-oriented AI-based application called SAM (also an ALM) to identify
relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated
within semi-autonomous arbitration process via the ALM, SARA. A novel approach
is introduced in the utilization of an AI arbitrator to critically evaluate
biases and qualitative-in-nature nuances identified by the aforementioned AI
applications (SAM in concert with SHIRLEY), based on the Hague Rules on
Business and Human Rights Arbitration. This Semi-Automated Arbitration Process
(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring
a nuanced debate-resultant "understanding" through a hybrid system of AI and
human-based collaborative analysis.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04141" title="Abstract">arXiv:2402.04141</a> [<a href="/pdf/2402.04141" title="Download PDF">pdf</a>, <a href="/format/2402.04141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-line AI-assisted Code Authoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dunay%2C+O">Omer Dunay</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Daniel Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tait%2C+A">Adam Tait</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+P">Parth Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Rigby%2C+P+C">Peter C Rigby</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+A">Andy Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I">Imad Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+A">Arun Ganesan</a>, 
<a href="/search/cs?searchtype=author&query=Maddila%2C+C">Chandra Maddila</a>, 
<a href="/search/cs?searchtype=author&query=Murali%2C+V">Vijayaraghavan Murali</a>, 
<a href="/search/cs?searchtype=author&query=Tayyebi%2C+A">Ali Tayyebi</a>, 
<a href="/search/cs?searchtype=author&query=Nagappan%2C+N">Nachiappan Nagappan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">CodeCompose is an AI-assisted code authoring tool powered by large language
models (LLMs) that provides inline suggestions to 10's of thousands of
developers at Meta. In this paper, we present how we scaled the product from
displaying single-line suggestions to multi-line suggestions. This evolution
required us to overcome several unique challenges in improving the usability of
these suggestions for developers.
<br />First, we discuss how multi-line suggestions can have a 'jarring' effect, as
the LLM's suggestions constantly move around the developer's existing code,
which would otherwise result in decreased productivity and satisfaction.
<br />Second, multi-line suggestions take significantly longer to generate; hence
we present several innovative investments we made to reduce the perceived
latency for users. These model-hosting optimizations sped up multi-line
suggestion latency by 2.5x.
<br />Finally, we conduct experiments on 10's of thousands of engineers to
understand how multi-line suggestions impact the user experience and contrast
this with single-line suggestions. Our experiments reveal that (i) multi-line
suggestions account for 42% of total characters accepted (despite only
accounting for 16% for displayed suggestions) (ii) multi-line suggestions
almost doubled the percentage of keystrokes saved for users from 9% to 17%.
Multi-line CodeCompose has been rolled out to all engineers at Meta, and less
than 1% of engineers have opted out of multi-line suggestions.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04142" title="Abstract">arXiv:2402.04142</a> [<a href="/pdf/2402.04142" title="Download PDF">pdf</a>, <a href="/format/2402.04142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Emotions Analysis and Recognition Using EEG Signals in Response to  360$^\circ$ Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+H+u+R">Haseeb ur Rahman Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+Z">Zeeshan Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Majid%2C+M">Muhammad Majid</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+S+M">Syed Muhammad Anwar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Emotion recognition (ER) technology is an integral part for developing
innovative applications such as drowsiness detection and health monitoring that
plays a pivotal role in contemporary society. This study delves into ER using
electroencephalography (EEG), within immersive virtual reality (VR)
environments. There are four main stages in our proposed methodology including
data acquisition, pre-processing, feature extraction, and emotion
classification. Acknowledging the limitations of existing 2D datasets, we
introduce a groundbreaking 3D VR dataset to elevate the precision of emotion
elicitation. Leveraging the Interaxon Muse headband for EEG recording and
Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40
participants, prioritizing subjects without reported mental illnesses.
Pre-processing entails rigorous cleaning, uniform truncation, and the
application of a Savitzky-Golay filter to the EEG data. Feature extraction
encompasses a comprehensive analysis of metrics such as power spectral density,
correlation, rational and divisional asymmetry, and power spectrum. To ensure
the robustness of our model, we employed a 10-fold cross-validation, revealing
an average validation accuracy of 85.54\%, with a noteworthy maximum accuracy
of 90.20\% in the best fold. Subsequently, the trained model demonstrated a
commendable test accuracy of 82.03\%, promising favorable outcomes.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04149" title="Abstract">arXiv:2402.04149</a> [<a href="/pdf/2402.04149" title="Download PDF">pdf</a>, <a href="/ps/2402.04149" title="Download PostScript">ps</a>, <a href="/format/2402.04149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Realization Games in Newsvendor Inventory Centralization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dror%2C+M">M. Dror</a>, 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Puerto%2C+J">Justo Puerto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Consider a set N of n (&gt;1) stores with single-item and single-period
nondeterministic demands like in a classic newsvendor setting with holding and
penalty costs only. Assume a risk-pooling single-warehouse centralized
inventory ordering option. Allocation of costs in the centralized inventory
ordering corresponds to modelling it as a cooperative cost game whose players
are the stores. It has been shown that when holding and penalty costs are
identical for all subsets of stores, the game based on optimal expected costs
has a non empty core (Hartman et. al., 2000, Muller \textit{et. al.}, 2002). In
this paper we examine a related inventory centralization game based on demand
realizations that has, in general, an empty core even with identical penalty
and holding costs (Hartman and Dror, 2005). We propose a repeated cost
allocation scheme for dynamic realization games based on allocation processes
introduced by Lehrer (2002a). We prove that the cost subsequences of the
dynamic realization game process, based on Lehrer's rules, converge almost
surely to either a least square value or the core of the expected game. We
extend the above results to more general dynamic cost games and relax the
independence hypothesis of the sequence of players' demands at different
stages.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04154" title="Abstract">arXiv:2402.04154</a> [<a href="/pdf/2402.04154" title="Download PDF">pdf</a>, <a href="/format/2402.04154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Read to Play (R2-Play): Decision Transformer with Multimodal Game  Instruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yonggang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Liuyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+S">Shawn Yue</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing a generalist agent is a longstanding objective in artificial
intelligence. Previous efforts utilizing extensive offline datasets from
various tasks demonstrate remarkable performance in multitasking scenarios
within Reinforcement Learning.However, these works encounter challenges in
extending their capabilities to new tasks.Recent approaches integrate textual
guidance or visual trajectory into decision networks to provide task-specific
contextual cues, representing a promising direction.However, it is observed
that relying solely on textual guidance or visual trajectory is insufficient
for accurately conveying the contextual information of tasks.This paper
explores enhanced forms of task guidance for agents, enabling them to
comprehend gameplay instructions, thereby facilitating a "read-to-play"
capability.Drawing inspiration from the success of multimodal instruction
tuning in visual tasks, we treat the visual-based RL task as a long-horizon
vision task and construct a set of multimodal game instructions to incorporate
instruction tuning into a decision transformer.Experimental results demonstrate
that incorporating multimodal game instructions significantly enhances the
decision transformer's multitasking and generalization capabilities.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04157" title="Abstract">arXiv:2402.04157</a> [<a href="/pdf/2402.04157" title="Download PDF">pdf</a>, <a href="/format/2402.04157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controller synthesis for input-state data with measurement errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bisoffi%2C+A">Andrea Bisoffi</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lidong Li</a>, 
<a href="/search/eess?searchtype=author&query=De+Persis%2C+C">Claudio De Persis</a>, 
<a href="/search/eess?searchtype=author&query=Monshizadeh%2C+N">Nima Monshizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the problem of designing a state-feedback controller for a linear
system, based only on noisy input-state data. We focus on input-state data
corrupted by additive measurement errors, which, albeit less investigated, are
as relevant as process disturbances in applications. For energy and
instantaneous bounds on these measurement errors, we derive linear matrix
inequalities for controller design where the one for the energy bound is
actually equivalent to robust stabilization of all systems consistent with the
noisy data points.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04160" title="Abstract">arXiv:2402.04160</a> [<a href="/pdf/2402.04160" title="Download PDF">pdf</a>, <a href="/format/2402.04160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Plug-and-Play Controller by Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Lei Sha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Third Version of the Generation, Evaluation &amp; Metrics (GEM) Workshop in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Controllable text generation is a growing field within natural language
generation (NLG) that focuses on producing text that meets specific constraints
in real-world applications. Previous approaches, such as plug-and-play
controllers (PPCs), aimed to steer the properties of generated text in a
flexible manner. However, these methods often compromised the integrity of the
language model's decoding process, resulting in less smooth text generation.
Alternatively, other techniques utilized multiple attribute prompts to align
the generated text with desired attributes, but this approach required prompt
design for each attribute and was dependent on the size of the language model.
This paper introduces a novel method for flexible attribute control in text
generation using pre-trained language models (PLMs). The proposed approach aims
to enhance the fluency of generated text by guiding the generation process with
PPCs. The key idea is to dynamically adjust the distribution of generated text
by modifying prompts, effectively constraining the output space of the language
model and influencing the desired attribute. To enable smooth cooperation
between the PLM and the PPC, our work innovatively proposes a new model
fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback
(RLDAF).This fine-tuning process adapts a small subset of the language model's
parameters based on the generating actions taken during the PPC control
process. The resulting harmonious collaboration between the PLM and PPC leads
to improved smoothness in text generation during inference. Extensive
experiments were conducted on the SST2 dataset, and the proposed method
outperformed previous approaches in various evaluation metrics, including text
fluency and attribute consistency.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04161" title="Abstract">arXiv:2402.04161</a> [<a href="/pdf/2402.04161" title="Download PDF">pdf</a>, <a href="/format/2402.04161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention with Markov: A Framework for Principled Analysis of  Transformers via Markov Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makkuva%2C+A+V">Ashok Vardhan Makkuva</a>, 
<a href="/search/cs?searchtype=author&query=Bondaschi%2C+M">Marco Bondaschi</a>, 
<a href="/search/cs?searchtype=author&query=Girish%2C+A">Adway Girish</a>, 
<a href="/search/cs?searchtype=author&query=Nagle%2C+A">Alliot Nagle</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gastpar%2C+M">Michael Gastpar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">In recent years, attention-based transformers have achieved tremendous
success across a variety of disciplines including natural languages. A key
ingredient behind their success is the generative pretraining procedure, during
which these models are trained on a large text corpus in an auto-regressive
manner. To shed light on this phenomenon, we propose a new framework that
allows both theory and systematic experiments to study the sequential modeling
capabilities of transformers through the lens of Markov chains. Inspired by the
Markovianity of natural languages, we model the data as a Markovian source and
utilize this framework to systematically study the interplay between the
data-distributional properties, the transformer architecture, the learnt
distribution, and the final model performance. In particular, we theoretically
characterize the loss landscape of single-layer transformers and show the
existence of global minima and bad local minima contingent upon the specific
data characteristics and the transformer architecture. Backed by experiments,
we demonstrate that our theoretical findings are in congruence with the
empirical results. We further investigate these findings in the broader context
of higher order Markov chains and deeper architectures, and outline open
problems in this arena. Code is available at
\url{https://github.com/Bond1995/Markov}.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04163" title="Abstract">arXiv:2402.04163</a> [<a href="/pdf/2402.04163" title="Download PDF">pdf</a>, <a href="/ps/2402.04163" title="Download PostScript">ps</a>, <a href="/format/2402.04163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tempered Calculus for ML: Application to Hyperbolic Model Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nock%2C+R">Richard Nock</a>, 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Soen%2C+A">Alexander Soen</a>, 
<a href="/search/cs?searchtype=author&query=Warmuth%2C+M+K">Manfred K. Warmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Most mathematical distortions used in ML are fundamentally integral in
nature: $f$-divergences, Bregman divergences, (regularized) optimal transport
distances, integral probability metrics, geodesic distances, etc. In this
paper, we unveil a grounded theory and tools which can help improve these
distortions to better cope with ML requirements. We start with a generalization
of Riemann integration that also encapsulates functions that are not strictly
additive but are, more generally, $t$-additive, as in nonextensive statistical
mechanics. Notably, this recovers Volterra's product integral as a special
case. We then generalize the Fundamental Theorem of calculus using an extension
of the (Euclidean) derivative. This, along with a series of more specific
Theorems, serves as a basis for results showing how one can specifically
design, alter, or change fundamental properties of distortion measures in a
simple way, with a special emphasis on geometric- and ML-related properties
that are the metricity, hyperbolicity, and encoding. We show how to apply it to
a problem that has recently gained traction in ML: hyperbolic embeddings with a
"cheap" and accurate encoding along the hyperbolic vs Euclidean scale. We
unveil a new application for which the Poincar\'e disk model has very appealing
features, and our theory comes in handy: \textit{model} embeddings for boosted
combinations of decision trees, trained using the log-loss (trees) and logistic
loss (combinations).
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04166" title="Abstract">arXiv:2402.04166</a> [<a href="/pdf/2402.04166" title="Download PDF">pdf</a>, <a href="/ps/2402.04166" title="Download PostScript">ps</a>, <a href="/format/2402.04166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Gap: Securely modeling cyber risk based on security deviations  from a peer group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reynolds%2C+T">Taylor Reynolds</a>, 
<a href="/search/cs?searchtype=author&query=Scheffler%2C+S">Sarah Scheffler</a>, 
<a href="/search/cs?searchtype=author&query=Weitzner%2C+D+J">Daniel J. Weitzner</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Angelina Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; General Economics (econ.GN); Applications (stat.AP)

</div>
<p class="mathjax">There are two strategic and longstanding questions about cyber risk that
organizations largely have been unable to answer: What is an organization's
estimated risk exposure and how does its security compare with peers? Answering
both requires industry-wide data on security posture, incidents, and losses
that, until recently, have been too sensitive for organizations to share. Now,
privacy enhancing technologies (PETs) such as cryptographic computing can
enable the secure computation of aggregate cyber risk metrics from a peer group
of organizations while leaving sensitive input data undisclosed. As these new
aggregate data become available, analysts need ways to integrate them into
cyber risk models that can produce more reliable risk assessments and allow
comparison to a peer group. This paper proposes a new framework for
benchmarking cyber posture against peers and estimating cyber risk within
specific economic sectors using the new variables emerging from secure
computations. We introduce a new top-line variable called the Defense Gap Index
representing the weighted security gap between an organization and its peers
that can be used to forecast an organization's own security risk based on
historical industry data. We apply this approach in a specific sector using
data collected from 25 large firms, in partnership with an industry ISAO, to
build an industry risk model and provide tools back to participants to estimate
their own risk exposure and privately compare their security posture with their
peers.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04168" title="Abstract">arXiv:2402.04168</a> [<a href="/pdf/2402.04168" title="Download PDF">pdf</a>, <a href="/format/2402.04168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informed Reinforcement Learning for Situation-Aware Traffic Rule  Exceptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogdoll%2C+D">Daniel Bogdoll</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Nekolla%2C+M">Moritz Nekolla</a>, 
<a href="/search/cs?searchtype=author&query=Abouelazm%2C+A">Ahmed Abouelazm</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+T">Tim Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+J+M">J. Marius Z&#xf6;llner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Daniel Bogdoll and Jing Qin contributed equally. Accepted for publication at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement Learning is a highly active research field with promising
advancements. In the field of autonomous driving, however, often very simple
scenarios are being examined. Common approaches use non-interpretable control
commands as the action space and unstructured reward designs which lack
structure. In this work, we introduce Informed Reinforcement Learning, where a
structured rulebook is integrated as a knowledge source. We learn trajectories
and asses them with a situation-aware reward design, leading to a dynamic
reward which allows the agent to learn situations which require controlled
traffic rule exceptions. Our method is applicable to arbitrary RL models. We
successfully demonstrate high completion rates of complex scenarios with recent
model-based agents.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04173" title="Abstract">arXiv:2402.04173</a> [<a href="/pdf/2402.04173" title="Download PDF">pdf</a>, <a href="/format/2402.04173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPS: A Compact On-device Pipeline for real-time Smishing detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+H+B+S">Harichandana B S S</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sumit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Ujjinakoppa%2C+M+B">Manjunath Bhimappa Ujjinakoppa</a>, 
<a href="/search/cs?searchtype=author&query=Raja%2C+B+R+K">Barath Raj Kandur Raja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE Consumer Communications &amp; Networking Conference (CCNC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Smartphones have become indispensable in our daily lives and can do almost
everything, from communication to online shopping. However, with the increased
usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in
particular, have observed a significant upsurge in recent years. This problem
is further exacerbated by the perpetrator creating new deceptive websites
daily, with an average life cycle of under 15 hours. This renders the standard
practice of keeping a database of malicious URLs ineffective. To this end, we
propose a novel on-device pipeline: COPS that intelligently identifies features
of fraudulent messages and URLs to alert the user in real-time. COPS is a
lightweight pipeline with a detection module based on the Disentangled
Variational Autoencoder of size 3.46MB for smishing and URL phishing detection,
and we benchmark it on open datasets. We achieve an accuracy of 98.15% and
99.5%, respectively, for both tasks, with a false negative and false positive
rate of a mere 0.037 and 0.015, outperforming previous works with the added
advantage of ensuring real-time alerts on resource-constrained devices.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04177" title="Abstract">arXiv:2402.04177</a> [<a href="/pdf/2402.04177" title="Download PDF">pdf</a>, <a href="/format/2402.04177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws for Downstream Task Performance of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isik%2C+B">Berivan Isik</a>, 
<a href="/search/cs?searchtype=author&query=Ponomareva%2C+N">Natalia Ponomareva</a>, 
<a href="/search/cs?searchtype=author&query=Hazimeh%2C+H">Hussein Hazimeh</a>, 
<a href="/search/cs?searchtype=author&query=Paparas%2C+D">Dimitris Paparas</a>, 
<a href="/search/cs?searchtype=author&query=Vassilvitskii%2C+S">Sergei Vassilvitskii</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Scaling laws provide important insights that can guide the design of large
language models (LLMs). Existing work has primarily focused on studying scaling
laws for pretraining (upstream) loss. However, in transfer learning settings,
in which LLMs are pretrained on an unsupervised dataset and then finetuned on a
downstream task, we often also care about the downstream performance. In this
work, we study the scaling behavior in a transfer learning setting, where LLMs
are finetuned for machine translation tasks. Specifically, we investigate how
the choice of the pretraining data and its size affect downstream performance
(translation quality) as judged by two metrics: downstream cross-entropy and
BLEU score. Our experiments indicate that the size of the finetuning dataset
and the distribution alignment between the pretraining and downstream data
significantly influence the scaling behavior. With sufficient alignment, both
downstream cross-entropy and BLEU score improve monotonically with more
pretraining data. In such cases, we show that it is possible to predict the
downstream BLEU score with good accuracy using a log-law. However, there are
also cases where moderate misalignment causes the BLEU score to fluctuate or
get worse with more pretraining, whereas downstream cross-entropy monotonically
improves. By analyzing these observations, we provide new practical insights
for choosing appropriate pretraining data.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04178" title="Abstract">arXiv:2402.04178</a> [<a href="/pdf/2402.04178" title="Download PDF">pdf</a>, <a href="/format/2402.04178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection  with Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuhao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yingxin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jun Feng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Jun Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changsheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zitong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal large language models (MLLMs) have demonstrated remarkable
problem-solving capabilities in various vision fields (e.g., generic object
recognition and grounding) based on strong visual semantic representation and
language reasoning ability. However, whether MLLMs are sensitive to subtle
visual spoof/forged clues and how they perform in the domain of face attack
detection (e.g., face spoofing and forgery detection) is still unexplored. In
this paper, we introduce a new benchmark, namely SHIELD, to evaluate the
ability of MLLMs on face spoofing and forgery detection. Specifically, we
design true/false and multiple-choice questions to evaluate multimodal face
data in these two face security tasks. For the face anti-spoofing task, we
evaluate three different modalities (i.e., RGB, infrared, depth) under four
types of presentation attacks (i.e., print attack, replay attack, rigid mask,
paper mask). For the face forgery detection task, we evaluate GAN-based and
diffusion-based data with both visual and acoustic modalities. Each question is
subjected to both zero-shot and few-shot tests under standard and chain of
thought (COT) settings. The results indicate that MLLMs hold substantial
potential in the face security domain, offering advantages over traditional
specific models in terms of interpretability, multimodal flexible reasoning,
and joint face spoof and forgery detection. Additionally, we develop a novel
Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging
various task-specific and task-irrelevant attributes of face images, which
provides rich task-related knowledge for subtle spoof/forged clue mining.
Extensive experiments in separate face anti-spoofing, separate face forgery
detection, and joint detection tasks demonstrate the effectiveness of the
proposed MA-COT. The project is available at
https$:$//github.com/laiyingxin2/SHIELD
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04180" title="Abstract">arXiv:2402.04180</a> [<a href="/pdf/2402.04180" title="Download PDF">pdf</a>, <a href="/format/2402.04180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-Learning Estimation of Weight Distribution Using Joint Kinematics  for Lower-Limb Exoskeleton Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lhoste%2C+C">Cl&#xe9;ment Lhoste</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BC%C3%A7%C3%BCktabak%2C+E+B">Emek Bar&#x131;&#x15f; K&#xfc;&#xe7;&#xfc;ktabak</a>, 
<a href="/search/cs?searchtype=author&query=Vianello%2C+L">Lorenzo Vianello</a>, 
<a href="/search/cs?searchtype=author&query=Amato%2C+L">Lorenzo Amato</a>, 
<a href="/search/cs?searchtype=author&query=Short%2C+M+R">Matthew R. Short</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+K">Kevin Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Pons%2C+J+L">Jose L. Pons</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the control of lower-limb exoskeletons with feet, the phase in the gait
cycle can be identified by monitoring the weight distribution at the feet. This
phase information can be used in the exoskeleton's controller to compensate the
dynamics of the exoskeleton and to assign impedance parameters. Typically the
weight distribution is calculated using data from sensors such as treadmill
force plates or insole force sensors. However, these solutions increase both
the setup complexity and cost. For this reason, we propose a deep-learning
approach that uses a short time window of joint kinematics to predict the
weight distribution of an exoskeleton in real time. The model was trained on
treadmill walking data from six users wearing a four-degree-of-freedom
exoskeleton and tested in real time on three different users wearing the same
device. This test set includes two users not present in the training set to
demonstrate the model's ability to generalize across individuals. Results show
that the proposed method is able to fit the actual weight distribution with
R2=0.9 and is suitable for real-time control with prediction times less than 1
ms. Experiments in closed-loop exoskeleton control show that
deep-learning-based weight distribution estimation can be used to replace force
sensors in overground and treadmill walking.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04182" title="Abstract">arXiv:2402.04182</a> [<a href="/pdf/2402.04182" title="Download PDF">pdf</a>, <a href="/format/2402.04182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Ensemble Model Predictive Safety  Certification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gronauer%2C+S">Sven Gronauer</a>, 
<a href="/search/cs?searchtype=author&query=Haider%2C+T">Tom Haider</a>, 
<a href="/search/cs?searchtype=author&query=da+Roza%2C+F+S">Felippe Schmoeller da Roza</a>, 
<a href="/search/cs?searchtype=author&query=Diepold%2C+K">Klaus Diepold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in: Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement learning algorithms need exploration to learn. However,
unsupervised exploration prevents the deployment of such algorithms on
safety-critical tasks and limits real-world deployment. In this paper, we
propose a new algorithm called Ensemble Model Predictive Safety Certification
that combines model-based deep reinforcement learning with tube-based model
predictive control to correct the actions taken by a learning agent, keeping
safety constraint violations at a minimum through planning. Our approach aims
to reduce the amount of prior knowledge about the actual system by requiring
only offline data generated by a safe controller. Our results show that we can
achieve significantly fewer constraint violations than comparable reinforcement
learning methods.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04183" title="Abstract">arXiv:2402.04183</a> [<a href="/pdf/2402.04183" title="Download PDF">pdf</a>, <a href="/format/2402.04183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incivility in Open Source Projects: A Comprehensive Annotated Dataset of  Locked GitHub Issue Threads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehsani%2C+R">Ramtin Ehsani</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+M">Mia Mohammad Imran</a>, 
<a href="/search/cs?searchtype=author&query=Zita%2C+R">Robert Zita</a>, 
<a href="/search/cs?searchtype=author&query=Damevski%2C+K">Kostadin Damevski</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+P">Preetha Chatterjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the dynamic landscape of open source software (OSS) development,
understanding and addressing incivility within issue discussions is crucial for
fostering healthy and productive collaborations. This paper presents a curated
dataset of 404 locked GitHub issue discussion threads and 5961 individual
comments, collected from 213 OSS projects. We annotated the comments with
various categories of incivility using Tone Bearing Discussion Features
(TBDFs), and, for each issue thread, we annotated the triggers, targets, and
consequences of incivility. We observed that Bitter frustration, Impatience,
and Mocking are the most prevalent TBDFs exhibited in our dataset. The most
common triggers, targets, and consequences of incivility include Failed use of
tool/code or error messages, People, and Discontinued further discussion,
respectively. This dataset can serve as a valuable resource for analyzing
incivility in OSS and improving automated tools to detect and mitigate such
behavior.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04187" title="Abstract">arXiv:2402.04187</a> [<a href="/pdf/2402.04187" title="Download PDF">pdf</a>, <a href="/format/2402.04187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Start Stop Bit Method for Efficient Data Communication in 6G Mobile  Radio Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zirwas%2C+W">Wolfgang Zirwas</a>, 
<a href="/search/cs?searchtype=author&query=Panzner%2C+B">Berthold Panzner</a>, 
<a href="/search/cs?searchtype=author&query=Sivasivaganesan%2C+R">Rakash Sivasivaganesan</a>, 
<a href="/search/cs?searchtype=author&query=Boas%2C+B+V">Brenda Vilas Boas</a>, 
<a href="/search/cs?searchtype=author&query=Su%C3%A1rez%2C+L+A">Luis A. Su&#xe1;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this article, a novel approach for mobile radio communications is proposed
and analysed, which is promising for future 6G cooperative distributed MIMO
systems. The fundamental idea is a new mechanism namely start stop bit method,
which transmits bit sequences as the start/stop bits of a synchronized counter
instead of transmitting the full encoded bit sequence itself. In that way,
theoretically, we can transmit infinitely long data messages with only one bit
for starting and one bit for stopping the counter. The value of the counter, as
identified by the stop bit, is then used to reconstruct and remap the one and
unique transmitted bit sequence. The start stop bit method is characterized by
a high signal sparsity as only two bits are transmitted, independently of the
bit sequence length for the message. Among the benefits of the start stop bit
method are energy efficient data transmission, and effective distributed MIMO
systems, which exploit the sparse inter cooperation area interference as well
as the low processing complexity for the sparse precoder calculation. Moreover,
for the next mobile wireless generation, we propose an advanced scheme of the
start stop bit method which enhances its resource usage. We call the resulting
method a sparse dMIMO system.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04193" title="Abstract">arXiv:2402.04193</a> [<a href="/pdf/2402.04193" title="Download PDF">pdf</a>, <a href="/format/2402.04193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Coding in Decentralized Learning for Evading Stragglers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we consider a decentralized learning problem in the presence
of stragglers. Although gradient coding techniques have been developed for
distributed learning to evade stragglers, where the devices send encoded
gradients with redundant training data, it is difficult to apply those
techniques directly to decentralized learning scenarios. To deal with this
problem, we propose a new gossip-based decentralized learning method with
gradient coding (GOCO). In the proposed method, to avoid the negative impact of
stragglers, the parameter vectors are updated locally using encoded gradients
based on the framework of stochastic gradient coding and then averaged in a
gossip-based manner. We analyze the convergence performance of GOCO for
strongly convex loss functions. And we also provide simulation results to
demonstrate the superiority of the proposed method in terms of learning
performance compared with the baseline methods.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04195" title="Abstract">arXiv:2402.04195</a> [<a href="/pdf/2402.04195" title="Download PDF">pdf</a>, <a href="/format/2402.04195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance by Instance: An Iterative Framework for Multi-instance 3D  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xinyue Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuxin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhaoshuai Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaqi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-instance registration is a challenging problem in computer vision and
robotics, where multiple instances of an object need to be registered in a
standard coordinate system. In this work, we propose the first iterative
framework called instance-by-instance (IBI) for multi-instance 3D registration
(MI-3DReg). It successively registers all instances in a given scenario,
starting from the easiest and progressing to more challenging ones. Throughout
the iterative process, outliers are eliminated continuously, leading to an
increasing inlier rate for the remaining and more challenging instances. Under
the IBI framework, we further propose a sparse-to-dense-correspondence-based
multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg.
Experiments on the synthetic and real datasets have demonstrated the
effectiveness of IBI and suggested the new state-of-the-art performance of
IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing
state-of-the-art method ECC on the synthetic/real datasets.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04200" title="Abstract">arXiv:2402.04200</a> [<a href="/pdf/2402.04200" title="Download PDF">pdf</a>, <a href="/ps/2402.04200" title="Download PostScript">ps</a>, <a href="/format/2402.04200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Systems and Software Engineering: The Case for Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fitzgerald%2C+B">Brian Fitzgerald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The Information Systems (IS) and Software Engineering (SE) fields share a
remarkable number of similarities in their historical evolution to date. These
similarities are briefly outlined below. An analysis of 10 years (2001-2010) of
publications in the primary journals in both fields also reveals a good deal of
overlap in research topics. Given the challenges faced by both as young
disciplines, there is potentially much to gain from a closer interaction
between both fields than has traditionally been the case. This article seeks to
encourage such interaction, and illustrates how this might usefully occur in
the area of design. It concludes by proposing a number of practical initiatives
that could stimulate and facilitate interaction between the IS and SE fields
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04203" title="Abstract">arXiv:2402.04203</a> [<a href="/pdf/2402.04203" title="Download PDF">pdf</a>, <a href="/format/2402.04203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Like Geometric Abstraction in Large Pre-trained Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campbell%2C+D">Declan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sreejan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Giallanza%2C+T">Tyler Giallanza</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J+D">Jonathan D. Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Humans possess a remarkable capacity to recognize and manipulate abstract
structure, which is especially apparent in the domain of geometry. Recent
research in cognitive science suggests neural networks do not share this
capacity, concluding that human geometric abilities come from discrete symbolic
structure in human mental representations. However, progress in artificial
intelligence (AI) suggests that neural networks begin to demonstrate more
human-like reasoning after scaling up standard architectures in both model size
and amount of training data. In this study, we revisit empirical results in
cognitive science on geometric visual processing and identify three key biases
in geometric visual processing: a sensitivity towards complexity, regularity,
and the perception of parts and relations. We test tasks from the literature
that probe these biases in humans and find that large pre-trained neural
network models used in AI demonstrate more human-like abstract geometric
processing.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04206" title="Abstract">arXiv:2402.04206</a> [<a href="/pdf/2402.04206" title="Download PDF">pdf</a>, <a href="/format/2402.04206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Autonomy: Enhancing Human-Robot Interaction through  Explanation Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sobr%C3%ADn-Hidalgo%2C+D">David Sobr&#xed;n-Hidalgo</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Santamarta%2C+M+A">Miguel A. Gonz&#xe1;lez-Santamarta</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero-Higueras%2C+%C3%81+M">&#xc1;ngel M. Guerrero-Higueras</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-Lera%2C+F+J">Francisco J. Rodr&#xed;guez-Lera</a>, 
<a href="/search/cs?searchtype=author&query=Matell%C3%A1n-Olivera%2C+V">Vicente Matell&#xe1;n-Olivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 15 Figures, 11 Tables. This paper is a preprint of an article submitted to the International Journal of Social Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces a system designed to generate explanations for the
actions performed by an autonomous robot in Human-Robot Interaction (HRI).
Explainability in robotics, encapsulated within the concept of an eXplainable
Autonomous Robot (XAR), is a growing research area. The work described in this
paper aims to take advantage of the capabilities of Large Language Models
(LLMs) in performing natural language processing tasks. This study focuses on
the possibility of generating explanations using such models in combination
with a Retrieval Augmented Generation (RAG) method to interpret data gathered
from the logs of autonomous systems. In addition, this work also presents a
formalization of the proposed explanation system. It has been evaluated through
a navigation test from the European Robotics League (ERL), a Europe-wide social
robotics competition. Regarding the obtained results, a validation
questionnaire has been conducted to measure the quality of the explanations
from the perspective of technical users. The results obtained during the
experiment highlight the potential utility of LLMs in achieving explanatory
capabilities in robots.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04208" title="Abstract">arXiv:2402.04208</a> [<a href="/pdf/2402.04208" title="Download PDF">pdf</a>, <a href="/ps/2402.04208" title="Download PostScript">ps</a>, <a href="/format/2402.04208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Production-inventory games and pmas games: characterizations of the Owen  point
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Puerto%2C+J">Justo Puerto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Production-inventory games were introduced in Guardiola et al. (2007) as a
new class of totally balanced combinatorial optimization games. From among all
core-allocations, the Owen point was proposed as a specifically appealing
solution. In this paper we study some relationships of the class of
production-inventory games and other classes of new and known games. In
addition, we propose three axiomatic characterizations of the Owen point. We
use eight axioms for these characterizations, among those, inessentiality and
additivity of players' demands are used for the first time in this paper.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04209" title="Abstract">arXiv:2402.04209</a> [<a href="/pdf/2402.04209" title="Download PDF">pdf</a>, <a href="/ps/2402.04209" title="Download PostScript">ps</a>, <a href="/format/2402.04209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acute kidney injury prediction for non-critical care patients: a  retrospective external and internal validation study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adiyeke%2C+E">Esra Adiyeke</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuanfang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Shickel%2C+B">Benjamin Shickel</a>, 
<a href="/search/cs?searchtype=author&query=Ruppert%2C+M+M">Matthew M. Ruppert</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Ziyuan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Kane-Gill%2C+S+L">Sandra L. Kane-Gill</a>, 
<a href="/search/cs?searchtype=author&query=Murugan%2C+R">Raghavan Murugan</a>, 
<a href="/search/cs?searchtype=author&query=Amatullah%2C+N">Nabihah Amatullah</a>, 
<a href="/search/cs?searchtype=author&query=Stottlemyer%2C+B+A">Britney A. Stottlemyer</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+L">Tiffany L. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Ricketts%2C+D">Dan Ricketts</a>, 
<a href="/search/cs?searchtype=author&query=Horvat%2C+C+M">Christopher M Horvat</a>, 
<a href="/search/cs?searchtype=author&query=Rashidi%2C+P">Parisa Rashidi</a>, 
<a href="/search/cs?searchtype=author&query=Bihorac%2C+A">Azra Bihorac</a>, 
<a href="/search/cs?searchtype=author&query=Ozrazgat-Baslanti%2C+T">Tezcan Ozrazgat-Baslanti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Background: Acute kidney injury (AKI), the decline of kidney excretory
function, occurs in up to 18% of hospitalized admissions. Progression of AKI
may lead to irreversible kidney damage. Methods: This retrospective cohort
study includes adult patients admitted to a non-intensive care unit at the
University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of
Florida Health (UFH) (n = 127,202). We developed and compared deep learning and
conventional machine learning models to predict progression to Stage 2 or
higher AKI within the next 48 hours. We trained local models for each site (UFH
Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a
development cohort of patients from both sites (UFH-UPMC Model). We internally
and externally validated the models on each site and performed subgroup
analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3%
(n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under
the receiver operating curve values (AUROC) for the UFH test cohort ranged
between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged
between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort.
UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval [CI] [0.80,
0.83]) for UFH and 0.82 (95% CI [0.81,0.84]) for UPMC test cohorts; an area
under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06])
for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts. Kinetic estimated
glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen
remained the top three features with the highest influence across the models
and health centers. Conclusion: Locally developed models displayed marginally
reduced discrimination when tested on another institution, while the top set of
influencing features remained the same across the models and sites.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04210" title="Abstract">arXiv:2402.04210</a> [<a href="/pdf/2402.04210" title="Download PDF">pdf</a>, <a href="/format/2402.04210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Task Success&quot; is not Enough: Investigating the Use of Video-Language  Models as Behavior Critics for Catching Undesirable Agent Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+L">Lin Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Denis Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+Y">Yantian Zha</a>, 
<a href="/search/cs?searchtype=author&query=Amor%2C+H+B">Heni Ben Amor</a>, 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Large-scale generative models are shown to be useful for sampling meaningful
candidate solutions, yet they often overlook task constraints and user
preferences. Their full power is better harnessed when the models are coupled
with external verifiers and the final solutions are derived iteratively or
progressively according to the verification feedback. In the context of
embodied AI, verification often solely involves assessing whether goal
conditions specified in the instructions have been met. Nonetheless, for these
agents to be seamlessly integrated into daily life, it is crucial to account
for a broader range of constraints and preferences beyond bare task success
(e.g., a robot should grasp bread with care to avoid significant deformations).
However, given the unbounded scope of robot tasks, it is infeasible to
construct scripted verifiers akin to those used for explicit-knowledge tasks
like the game of Go and theorem proving. This begs the question: when no sound
verifier is available, can we use large vision and language models (VLMs),
which are approximately omniscient, as scalable Behavior Critics to catch
undesirable robot behaviors in videos? To answer this, we first construct a
benchmark that contains diverse cases of goal-reaching yet undesirable robot
policies. Then, we comprehensively evaluate VLM critics to gain a deeper
understanding of their strengths and failure modes. Based on the evaluation, we
provide guidelines on how to effectively utilize VLM critiques and showcase a
practical way to integrate the feedback into an iterative process of policy
refinement. The dataset and codebase are released at:
https://guansuns.github.io/pages/vlm-critic.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04211" title="Abstract">arXiv:2402.04211</a> [<a href="/pdf/2402.04211" title="Download PDF">pdf</a>, <a href="/format/2402.04211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Shapley Network: A Probabilistic Approach to Self-Explaining  Shapley values with Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ketenci%2C+M">Mert Ketenci</a>, 
<a href="/search/cs?searchtype=author&query=Urteaga%2C+I">I&#xf1;igo Urteaga</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+V+A">Victor Alfonso Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Elhadad%2C+N">No&#xe9;mie Elhadad</a>, 
<a href="/search/cs?searchtype=author&query=Perotte%2C+A">Adler Perotte</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Shapley values have emerged as a foundational tool in machine learning (ML)
for elucidating model decision-making processes. Despite their widespread
adoption and unique ability to satisfy essential explainability axioms,
computational challenges persist in their estimation when ($i$) evaluating a
model over all possible subset of input feature combinations, ($ii$) estimating
model marginals, and ($iii$) addressing variability in explanations. We
introduce a novel, self-explaining method that simplifies the computation of
Shapley values significantly, requiring only a single forward pass. Recognizing
the deterministic treatment of Shapley values as a limitation, we explore
incorporating a probabilistic framework to capture the inherent uncertainty in
explanations. Unlike alternatives, our technique does not rely directly on the
observed data space to estimate marginals; instead, it uses adaptable baseline
values derived from a latent, feature-specific embedding space, generated by a
novel masked neural network architecture. Evaluations on simulated and real
datasets underscore our technique's robust predictive and explanatory
performance.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04216" title="Abstract">arXiv:2402.04216</a> [<a href="/pdf/2402.04216" title="Download PDF">pdf</a>, <a href="/format/2402.04216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource-Aware Hierarchical Federated Learning in Wireless Video Caching  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pervej%2C+M+F">Md Ferdous Pervej</a>, 
<a href="/search/cs?searchtype=author&query=Molisch%2C+A+F">Andreas F. Molisch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for possible publication in IEEE TWC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Backhaul traffic congestion caused by the video traffic of a few popular
files can be alleviated by storing the to-be-requested content at various
levels in wireless video caching networks. Typically, content service providers
(CSPs) own the content, and the users request their preferred content from the
CSPs using their (wireless) internet service providers (ISPs). As these parties
do not reveal their private information and business secrets, traditional
techniques may not be readily used to predict the dynamic changes in users'
future demands. Motivated by this, we propose a novel resource-aware
hierarchical federated learning (RawHFL) solution for predicting user's future
content requests. A practical data acquisition technique is used that allows
the user to update its local training dataset based on its requested content.
Besides, since networking and other computational resources are limited,
considering that only a subset of the users participate in the model training,
we derive the convergence bound of the proposed algorithm. Based on this bound,
we minimize a weighted utility function for jointly configuring the
controllable parameters to train the RawHFL energy efficiently under practical
resource constraints. Our extensive simulation results validate the proposed
algorithm's superiority, in terms of test accuracy and energy cost, over
existing baselines.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04222" title="Abstract">arXiv:2402.04222</a> [<a href="/pdf/2402.04222" title="Download PDF">pdf</a>, <a href="/format/2402.04222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is &#x27;Typological Diversity&#x27; in NLP?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ploeger%2C+E">Esther Ploeger</a>, 
<a href="/search/cs?searchtype=author&query=Poelman%2C+W">Wessel Poelman</a>, 
<a href="/search/cs?searchtype=author&query=de+Lhoneux%2C+M">Miryam de Lhoneux</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The NLP research community has devoted increased attention to languages
beyond English, resulting in considerable improvements for multilingual NLP.
However, these improvements only apply to a small subset of the world's
languages. Aiming to extend this, an increasing number of papers aspires to
enhance generalizable multilingual performance across languages. To this end,
linguistic typology is commonly used to motivate language selection, on the
basis that a broad typological sample ought to imply generalization across a
broad range of languages. These selections are often described as being
'typologically diverse'. In this work, we systematically investigate NLP
research that includes claims regarding 'typological diversity'. We find there
are no set definitions or criteria for such claims. We introduce metrics to
approximate the diversity of language selection along several axes and find
that the results vary considerably across papers. Furthermore, we show that
skewed language selection can lead to overestimated multilingual performance.
We recommend future work to include an operationalization of 'typological
diversity' that empirically justifies the diversity of language samples.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04228" title="Abstract">arXiv:2402.04228</a> [<a href="/pdf/2402.04228" title="Download PDF">pdf</a>, <a href="/format/2402.04228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Collective Escape of Swarm Robots Based on a Novel  Fish-inspired Self-adaptive Approach with Neurodynamic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S+X">Simon X. Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is accepted for publication in a future issue of IEEE Transactions on Industrial Electronics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Fish schools present high-efficiency group behaviors through simple
individual interactions to collective migration and dynamic escape from the
predator. The school behavior of fish is usually a good inspiration to design
control architecture for swarm robots. In this paper, a novel fish-inspired
self-adaptive approach is proposed for collective escape for the swarm robots.
In addition, a bio-inspired neural network (BINN) is introduced to generate
collision-free escape robot trajectories through the combination of attractive
and repulsive forces. Furthermore, to cope with dynamic environments, a
neurodynamics-based self-adaptive mechanism is proposed to improve the
self-adaptive performance of the swarm robots in the changing environment.
Similar to fish escape maneuvers, simulation and experimental results show that
the swarm robots are capable of collectively leaving away from the threats.
Several comparison studies demonstrated that the proposed approach can
significantly improve the effectiveness and efficiency of system performance,
and the flexibility and robustness in complex environments.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04229" title="Abstract">arXiv:2402.04229</a> [<a href="/pdf/2402.04229" title="Download PDF">pdf</a>, <a href="/format/2402.04229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MusicRL: Aligning Music Generation to Human Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cideron%2C+G">Geoffrey Cideron</a>, 
<a href="/search/cs?searchtype=author&query=Girgin%2C+S">Sertan Girgin</a>, 
<a href="/search/cs?searchtype=author&query=Verzetti%2C+M">Mauro Verzetti</a>, 
<a href="/search/cs?searchtype=author&query=Vincent%2C+D">Damien Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Kastelic%2C+M">Matej Kastelic</a>, 
<a href="/search/cs?searchtype=author&query=Borsos%2C+Z">Zal&#xe1;n Borsos</a>, 
<a href="/search/cs?searchtype=author&query=McWilliams%2C+B">Brian McWilliams</a>, 
<a href="/search/cs?searchtype=author&query=Ungureanu%2C+V">Victor Ungureanu</a>, 
<a href="/search/cs?searchtype=author&query=Bachem%2C+O">Olivier Bachem</a>, 
<a href="/search/cs?searchtype=author&query=Pietquin%2C+O">Olivier Pietquin</a>, 
<a href="/search/cs?searchtype=author&query=Geist%2C+M">Matthieu Geist</a>, 
<a href="/search/cs?searchtype=author&query=Hussenot%2C+L">L&#xe9;onard Hussenot</a>, 
<a href="/search/cs?searchtype=author&query=Zeghidour%2C+N">Neil Zeghidour</a>, 
<a href="/search/cs?searchtype=author&query=Agostinelli%2C+A">Andrea Agostinelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We propose MusicRL, the first music generation system finetuned from human
feedback. Appreciation of text-to-music models is particularly subjective since
the concept of musicality as well as the specific intention behind a caption
are user-dependent (e.g. a caption such as "upbeat work-out music" can map to a
retro guitar solo or a techno pop beat). Not only this makes supervised
training of such models challenging, but it also calls for integrating
continuous human feedback in their post-deployment finetuning. MusicRL is a
pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete
audio tokens finetuned with reinforcement learning to maximise sequence-level
rewards. We design reward functions related specifically to text-adherence and
audio quality with the help from selected raters, and use those to finetune
MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial
dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning
from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model
that incorporates human feedback at scale. Human evaluations show that both
MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU
combines the two approaches and results in the best model according to human
raters. Ablation studies shed light on the musical attributes influencing human
preferences, indicating that text adherence and quality only account for a part
of it. This underscores the prevalence of subjectivity in musical appreciation
and calls for further involvement of human listeners in the finetuning of music
generation models.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04231" title="Abstract">arXiv:2402.04231</a> [<a href="/pdf/2402.04231" title="Download PDF">pdf</a>, <a href="/ps/2402.04231" title="Download PostScript">ps</a>, <a href="/format/2402.04231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Further Constructions of AMUBs for Non-prime power Composite Dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ajeet Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Subhamoy Maitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Construction of a large class of Mutually Unbiased Bases (MUBs) for non-prime
power composite dimensions ($d = k\times s$) is a long standing open problem,
which leads to different construction methods for the class Approximate MUBs
(AMUBs) by relaxing the criterion that the absolute value of the dot product
between two vectors chosen from different bases should be $\leq
\frac{\beta}{\sqrt{d}}$. In this chapter, we consider a more general class of
AMUBs (ARMUBs, considering the real ones too), compared to our earlier work in
[Cryptography and Communications, 14(3): 527--549, 2022]. We note that the
quality of AMUBs (ARMUBs) constructed using RBD$(X,A)$ with $|X|= d$,
critically depends on the parameters, $|s-k|$, $\mu$ (maximum number of
elements common between any pair of blocks), and the set of block sizes. We
present the construction of $\mathcal{O}(\sqrt{d})$ many $\beta$-AMUBs for
composite $d$ when $|s-k|&lt; \sqrt{d}$, using RBDs having block sizes
approximately $\sqrt{d}$, such that $|\braket{\psi^l_i|\psi^m_j}| \leq
\frac{\beta}{\sqrt{d}}$ where $\beta = 1 + \frac{|s-k|}{2\sqrt{d}}+
\mathcal{O}(d^{-1}) \leq 2$. Moreover, if real Hadamard matrix of order $k$ or
$s$ exists, then one can construct at least $N(k)+1$ (or $N(s)+1$) many
$\beta$-ARMUBs for dimension $d$, with $\beta \leq 2 - \frac{|s-k|}{2\sqrt{d}}+
\mathcal{O}(d^{-1})&lt; 2$, where $N(w)$ is the number of MOLS$(w)$. This improves
and generalizes some of our previous results for ARMUBs from two points, viz.,
the real cases are now extended to complex ones too. The earlier efforts use
some existing RBDs, whereas here we consider new instances of RBDs that provide
better results. Similar to the earlier cases, the AMUBs (ARMUBs) constructed
using RBDs are in general very sparse, where the sparsity $(\epsilon)$ is $1 -
\mathcal{O}(d^{-\frac{1}{2}})$.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04232" title="Abstract">arXiv:2402.04232</a> [<a href="/pdf/2402.04232" title="Download PDF">pdf</a>, <a href="/format/2402.04232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Generative Agents Predict Emotion?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Regan%2C+C">Ciaran Regan</a>, 
<a href="/search/cs?searchtype=author&query=Iwahashi%2C+N">Nanami Iwahashi</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+S">Shogo Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Oka%2C+M">Mizuki Oka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated a number of human-like
abilities, however the empathic understanding and emotional state of LLMs is
yet to be aligned to that of humans. In this work, we investigate how the
emotional state of generative LLM agents evolves as they perceive new events,
introducing a novel architecture in which new experiences are compared to past
memories. Through this comparison, the agent gains the ability to understand
new experiences in context, which according to the appraisal theory of emotion
is vital in emotion creation. First, the agent perceives new experiences as
time series text data. After perceiving each new input, the agent generates a
summary of past relevant memories, referred to as the norm, and compares the
new experience to this norm. Through this comparison we can analyse how the
agent reacts to the new experience in context. The PANAS, a test of affect, is
administered to the agent, capturing the emotional state of the agent after the
perception of the new event. Finally, the new experience is then added to the
agents memory to be used in the creation of future norms. By creating multiple
experiences in natural language from emotionally charged situations, we test
the proposed architecture on a wide range of scenarios. The mixed results
suggests that introducing context can occasionally improve the emotional
alignment of the agent, but further study and comparison with human evaluators
is necessary. We hope that this paper is another step towards the alignment of
generative agents.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04235" title="Abstract">arXiv:2402.04235</a> [<a href="/pdf/2402.04235" title="Download PDF">pdf</a>, <a href="/format/2402.04235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIPSTICK: Corruptibility-Aware and Explainable Graph Neural  Network-based Oracle-Less Attack on Logic Locking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghamohammadi%2C+Y">Yeganeh Aghamohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+A">Amin Rezaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of 29th Asia and South Pacific Design Automation Conference (ASP-DAC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In a zero-trust fabless paradigm, designers are increasingly concerned about
hardware-based attacks on the semiconductor supply chain. Logic locking is a
design-for-trust method that adds extra key-controlled gates in the circuits to
prevent hardware intellectual property theft and overproduction. While
attackers have traditionally relied on an oracle to attack logic-locked
circuits, machine learning attacks have shown the ability to retrieve the
secret key even without access to an oracle. In this paper, we first examine
the limitations of state-of-the-art machine learning attacks and argue that the
use of key hamming distance as the sole model-guiding structural metric is not
always useful. Then, we develop, train, and test a corruptibility-aware graph
neural network-based oracle-less attack on logic locking that takes into
consideration both the structure and the behavior of the circuits. Our model is
explainable in the sense that we analyze what the machine learning model has
interpreted in the training process and how it can perform a successful attack.
Chip designers may find this information beneficial in securing their designs
while avoiding incremental fixes.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04236" title="Abstract">arXiv:2402.04236</a> [<a href="/pdf/2402.04236" title="Download PDF">pdf</a>, <a href="/format/2402.04236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CogCoM: Train Large Vision-Language Models Diving into Details through  Chain of Manipulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yushi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Q">Qingsong Lv</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+W">Wenyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Vision-Language Models (VLMs) have demonstrated their widespread viability
thanks to extensive training in aligning visual instructions to answers.
However, this conclusive alignment leads models to ignore critical visual
reasoning, and further result in failures on meticulous visual problems and
unfaithful responses. In this paper, we propose Chain of Manipulations, a
mechanism that enables VLMs to solve problems with a series of manipulations,
where each manipulation refers to an operation on the visual input, either from
intrinsic abilities (e.g., grounding) acquired through prior training or from
imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs
to generate faithful responses with evidential visual reasoning, and permits
users to trace error causes in the interpretable paths. We thus train CogCoM, a
general 17B VLM with a memory-based compatible architecture endowed this
reasoning mechanism. Experiments show that our model achieves the
state-of-the-art performance across 8 benchmarks from 3 categories, and a
limited number of training steps with the data swiftly gains a competitive
performance. The code and data are publicly available at
https://github.com/THUDM/CogCoM.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04239" title="Abstract">arXiv:2402.04239</a> [<a href="/pdf/2402.04239" title="Download PDF">pdf</a>, <a href="/format/2402.04239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAST: Clustering Self-Attention using Surrogate Tokens for Efficient  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Engelenhoven%2C+A">Adjorn van Engelenhoven</a>, 
<a href="/search/cs?searchtype=author&query=Strisciuglio%2C+N">Nicola Strisciuglio</a>, 
<a href="/search/cs?searchtype=author&query=Talavera%2C+E">Estefan&#xed;a Talavera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Transformer architecture has shown to be a powerful tool for a wide range
of tasks. It is based on the self-attention mechanism, which is an inherently
computationally expensive operation with quadratic computational complexity:
memory usage and compute time increase quadratically with the length of the
input sequences, thus limiting the application of Transformers. In this work,
we propose a novel Clustering self-Attention mechanism using Surrogate Tokens
(CAST), to optimize the attention computation and achieve efficient
transformers. CAST utilizes learnable surrogate tokens to construct a cluster
affinity matrix, used to cluster the input sequence and generate novel cluster
summaries. The self-attention from within each cluster is then combined with
the cluster summaries of other clusters, enabling information flow across the
entire input sequence. CAST improves efficiency by reducing the complexity from
$O(N^2)$ to $O(\alpha N)$ where N is the sequence length, and {\alpha} is
constant according to the number of clusters and samples per cluster. We show
that CAST performs better than or comparable to the baseline Transformers on
long-range sequence modeling tasks, while also achieving higher results on time
and memory efficiency than other efficient transformers.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04243" title="Abstract">arXiv:2402.04243</a> [<a href="/pdf/2402.04243" title="Download PDF">pdf</a>, <a href="/format/2402.04243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Set Estimation for Piecewise Affine Dynamical Systems Using  Piecewise Affine Barrier Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Samanipour%2C+P">Pouya Samanipour</a>, 
<a href="/search/eess?searchtype=author&query=Poonawala%2C+H+A">Hasan A.Poonawala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to European Journal of Control, 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces an algorithm for approximating the invariant set of
closed-loop controlled dynamical systems identified using ReLU neural networks
or piecewise affine PWA functions, particularly addressing the challenge of
providing safety guarantees for ReLU networks commonly used in safety-critical
applications. The invariant set of PWA dynamical system is estimated using ReLU
networks or its equivalent PWA function. This method entails formulating the
barrier function as a PWA function and converting the search process into a
linear optimization problem using vertices. We incorporate a domain refinement
strategy to increase flexibility in case the optimization does not find a valid
barrier function. Moreover, the objective of optimization is to maximize the
invariant set based on the current partition. Our experimental results
demonstrate the effectiveness and efficiency of our approach, demonstrating its
potential for ensuring the safety of PWA dynamical systems.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04247" title="Abstract">arXiv:2402.04247</a> [<a href="/pdf/2402.04247" title="Download PDF">pdf</a>, <a href="/format/2402.04247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kunlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tongxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+M">Meng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04248" title="Abstract">arXiv:2402.04248</a> [<a href="/pdf/2402.04248" title="Download PDF">pdf</a>, <a href="/format/2402.04248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jongho Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeseung Park</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zheyang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Nayoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Oymak%2C+S">Samet Oymak</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>, 
<a href="/search/cs?searchtype=author&query=Papailiopoulos%2C+D">Dimitris Papailiopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">State-space models (SSMs), such as Mamba Gu &amp; Dao (2034), have been proposed
as alternatives to Transformer networks in language modeling, by incorporating
gating, convolutions, and input-dependent token selection to mitigate the
quadratic cost of multi-head attention. Although SSMs exhibit competitive
performance, their in-context learning (ICL) capabilities, a remarkable
emergent property of modern language models that enables task execution without
parameter optimization, remain underexplored compared to Transformers. In this
study, we evaluate the ICL performance of SSMs, focusing on Mamba, against
Transformer models across various tasks. Our results show that SSMs perform
comparably to Transformers in standard regression ICL tasks, while
outperforming them in tasks like sparse parity learning. However, SSMs fall
short in tasks involving non-standard retrieval functionality. To address these
limitations, we introduce a hybrid model, \variant, that combines Mamba with
attention blocks, surpassing individual models in tasks where they struggle
independently. Our findings suggest that hybrid architectures offer promising
avenues for enhancing ICL in language models.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04249" title="Abstract">arXiv:2402.04249</a> [<a href="/pdf/2402.04249" title="Download PDF">pdf</a>, <a href="/format/2402.04249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+N">Norman Mu</a>, 
<a href="/search/cs?searchtype=author&query=Sakhaee%2C+E">Elham Sakhaee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nathaniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Forsyth%2C+D">David Forsyth</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://www.harmbench.org">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automated red teaming holds substantial promise for uncovering and mitigating
the risks associated with the malicious use of large language models (LLMs),
yet the field lacks a standardized evaluation framework to rigorously assess
new methods. To address this issue, we introduce HarmBench, a standardized
evaluation framework for automated red teaming. We identify several desirable
properties previously unaccounted for in red teaming evaluations and
systematically design HarmBench to meet these criteria. Using HarmBench, we
conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs
and defenses, yielding novel insights. We also introduce a highly efficient
adversarial training method that greatly enhances LLM robustness across a wide
range of attacks, demonstrating how HarmBench enables codevelopment of attacks
and defenses. We open source HarmBench at
https://github.com/centerforaisafety/HarmBench.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04251" title="Abstract">arXiv:2402.04251</a> [<a href="/pdf/2402.04251" title="Download PDF">pdf</a>, <a href="/format/2402.04251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vamvas%2C+J">Jannis Vamvas</a>, 
<a href="/search/cs?searchtype=author&query=Sennrich%2C+R">Rico Sennrich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Minimum Bayes Risk (MBR) decoding is a text generation technique that has
been shown to improve the quality of machine translations, but is expensive,
even if a sampling-based approximation is used. Besides requiring a large
number of sampled sequences, it requires the pairwise calculation of a utility
metric, which has quadratic complexity. In this paper, we propose to
approximate pairwise metric scores with scores calculated against aggregated
reference representations. This changes the complexity of utility estimation
from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains
of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04252" title="Abstract">arXiv:2402.04252</a> [<a href="/pdf/2402.04252" title="Download PDF">pdf</a>, <a href="/format/2402.04252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Quan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiying Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yufeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaosong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scaling up contrastive language-image pretraining (CLIP) is critical for
empowering both vision and multimodal models. We present EVA-CLIP-18B, the
largest and most powerful open-source CLIP model to date, with 18-billion
parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an
exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized
image classification benchmarks, outperforming its forerunner EVA-CLIP
(5-billion parameters) and other open-source CLIP models by a large margin.
Remarkably, we observe a consistent performance improvement with the model size
scaling of EVA-CLIP, despite maintaining a constant training dataset of
2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly
available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)
employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the
potential of EVA-style weak-to-strong visual model scaling. With our model
weights made publicly available, we hope to facilitate future research in
vision and multimodal foundation models.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04253" title="Abstract">arXiv:2402.04253</a> [<a href="/pdf/2402.04253" title="Download PDF">pdf</a>, <a href="/format/2402.04253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yu Du</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed,  7 Feb 24</h3>
<dl>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12423" title="Abstract">arXiv:2305.12423</a> (cross-list from eess.SP) [<a href="/pdf/2305.12423" title="Download PDF">pdf</a>, <a href="/format/2305.12423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Oriented Communication with Out-of-Distribution Detection: An  Information Bottleneck Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongru Li</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wentao Yu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+H">Hengtao He</a>, 
<a href="/search/eess?searchtype=author&query=Shao%2C+J">Jiawei Shao</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code available in github, accepted by IEEE GLOBECOM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Task-oriented communication is an emerging paradigm for next-generation
communication networks, which extracts and transmits task-relevant information,
instead of raw data, for downstream applications. Most existing deep learning
(DL)-based task-oriented communication systems adopt a closed-world scenario,
assuming either the same data distribution for training and testing, or the
system could have access to a large out-of-distribution (OoD) dataset for
retraining. However, in practical open-world scenarios, task-oriented
communication systems need to handle unknown OoD data. Under such
circumstances, the powerful approximation ability of learning methods may force
the task-oriented communication systems to overfit the training data (i.e.,
in-distribution data) and provide overconfident judgments when encountering OoD
data. Based on the information bottleneck (IB) framework, we propose a class
conditional IB (CCIB) approach to address this problem in this paper, supported
by information-theoretical insights. The idea is to extract distinguishable
features from in-distribution data while keeping their compactness and
informativeness. This is achieved by imposing the class conditional latent
prior distribution and enforcing the latent of different classes to be far away
from each other. Simulation results shall demonstrate that the proposed
approach detects OoD data more efficiently than the baselines and
state-of-the-art approaches, without compromising the rate-distortion tradeoff.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02606" title="Abstract">arXiv:2402.02606</a> (cross-list from math.LO) [<a href="/pdf/2402.02606" title="Download PDF">pdf</a>, <a href="/format/2402.02606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nelson algebras, residuated lattices and rough sets: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=J%C3%A4rvinen%2C+J">Jouni J&#xe4;rvinen</a>, 
<a href="/search/math?searchtype=author&query=Radeleczki%2C+S">S&#xe1;ndor Radeleczki</a>, 
<a href="/search/math?searchtype=author&query=Rivieccio%2C+U">Umberto Rivieccio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Journal of Applied Non-Classical Logics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Over the past 50 years, Nelson algebras have been extensively studied by
distinguished scholars as the algebraic counterpart of Nelson's constructive
logic with strong negation. Despite these studies, a comprehensive survey of
the topic is currently lacking, and the theory of Nelson algebras remains
largely unknown to most logicians. This paper aims to fill this gap by
focussing on the essential developments in the field over the past two decades.
Additionally, we explore generalisations of Nelson algebras, such as
N4-lattices which correspond to the paraconsistent version of Nelson's logic,
as well as their applications to other areas of interest to logicians, such as
duality and rough set theory. A general representation theorem states that each
Nelson algebra is isomorphic to a subalgebra of a rough set-based Nelson
algebra induced by a quasiorder. Furthermore, a formula is a theorem of Nelson
logic if and only if it is valid in every finite Nelson algebra induced by a
quasiorder.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03316" title="Abstract">arXiv:2402.03316</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.03316" title="Download PDF">pdf</a>, <a href="/ps/2402.03316" title="Download PostScript">ps</a>, <a href="/format/2402.03316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence for EEG Prediction: Applied Chaos Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Syrup%2C+S">Soul Syrup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages, 14 figures, project report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">In the present research, we delve into the intricate realm of
electroencephalogram (EEG) data analysis, focusing on sequence-to-sequence
prediction of data across 32 EEG channels. The study harmoniously fuses the
principles of applied chaos theory and dynamical systems theory to engender a
novel feature set, enriching the representational capacity of our deep learning
model. The endeavour's cornerstone is a transformer-based sequence-to-sequence
architecture, calibrated meticulously to capture the non-linear and
high-dimensional temporal dependencies inherent in EEG sequences. Through
judicious architecture design, parameter initialisation strategies, and
optimisation techniques, we have navigated the intricate balance between
computational expediency and predictive performance. Our model stands as a
vanguard in EEG data sequence prediction, demonstrating remarkable
generalisability and robustness. The findings not only extend our understanding
of EEG data dynamics but also unveil a potent analytical framework that can be
adapted to diverse temporal sequence prediction tasks in neuroscience and
beyond.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03338" title="Abstract">arXiv:2402.03338</a> (cross-list from q-fin.CP) [<a href="/pdf/2402.03338" title="Download PDF">pdf</a>, <a href="/format/2402.03338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNN-DRL with Shuffled Features in Finance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Montazeri%2C+S">Sina Montazeri</a>, 
<a href="/search/q-fin?searchtype=author&query=Mirzaeinia%2C+A">Akram Mirzaeinia</a>, 
<a href="/search/q-fin?searchtype=author&query=Mirzaeinia%2C+A">Amir Mirzaeinia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10th Annual Conf. on Computational Science &amp; Computational Intelligence (CSCI'23). arXiv admin note: substantial text overlap with <a href="/abs/2401.06179">arXiv:2401.06179</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In prior methods, it was observed that the application of Convolutional
Neural Networks agent in Deep Reinforcement Learning to financial data resulted
in an enhanced reward. In this study, a specific permutation was applied to the
feature vector, thereby generating a CNN matrix that strategically positions
more pertinent features in close proximity. Our comprehensive experimental
evaluations unequivocally demonstrate a substantial enhancement in reward
attainment.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03343" title="Abstract">arXiv:2402.03343</a> (cross-list from physics.ed-ph) [<a href="/pdf/2402.03343" title="Download PDF">pdf</a>, <a href="/ps/2402.03343" title="Download PostScript">ps</a>, <a href="/format/2402.03343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Wide Area Classroom After 24,000 HPC Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Urbanic%2C+J">John Urbanic</a>, 
<a href="/search/physics?searchtype=author&query=Maiden%2C+T">Thomas Maiden</a>, 
<a href="/search/physics?searchtype=author&query=Rossi%2C+V">Valerie Rossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> HPC, training, education, supercomputing, parallel computing, MPI, OpenMP, OpenACC, Big Data, Machine Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics Education (physics.ed-ph)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">As of 2023 we have taught more than 24,000 students over the course of 106
events using the Wide Area Classroom, a novel distributed teaching platform.
This has been a successful effort gauged by several important metrics. We
describe both the technical and logistical structure of these events as well as
the specific HPC curriculums which have proven to be most popular.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03345" title="Abstract">arXiv:2402.03345</a> (cross-list from eess.SP) [<a href="/pdf/2402.03345" title="Download PDF">pdf</a>, <a href="/format/2402.03345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly supervised covariance matrices alignment through Stiefel matrices  estimation for MEG applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Collas%2C+A">Antoine Collas</a>, 
<a href="/search/eess?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/eess?searchtype=author&query=Gramfort%2C+A">Alexandre Gramfort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper introduces a novel domain adaptation technique for time series
data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the
challenge of limited labeled signals in the target dataset. Leveraging a
domain-dependent mixing model and the optimal transport domain adaptation
assumption, we exploit abundant unlabeled data in the target domain to ensure
effective prediction by establishing pairwise correspondence with equivalent
signal variances between domains. Theoretical foundations are laid for
identifying crucial Stiefel matrices, essential for recovering underlying
signal variances from a Riemannian representation of observed signal
covariances. We propose an integrated cost function that simultaneously learns
these matrices, pairwise domain relationships, and a predictor, classifier, or
regressor, depending on the task. Applied to neuroscience problems, MSA
outperforms recent methods in brain-age regression with task variations using
magnetoencephalography (MEG) signals from the Cam-CAN dataset.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03349" title="Abstract">arXiv:2402.03349</a> (cross-list from physics.geo-ph) [<a href="/pdf/2402.03349" title="Download PDF">pdf</a>, <a href="/format/2402.03349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Geoscience Meets Generative AI and Large Language Models:  Foundations, Trends, and Future Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hadid%2C+A">Abdenour Hadid</a>, 
<a href="/search/physics?searchtype=author&query=Chakraborty%2C+T">Tanujit Chakraborty</a>, 
<a href="/search/physics?searchtype=author&query=Busby%2C+D">Daniel Busby</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03352" title="Abstract">arXiv:2402.03352</a> (cross-list from math.OC) [<a href="/pdf/2402.03352" title="Download PDF">pdf</a>, <a href="/format/2402.03352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for  Nonconvex Minimax Problems with Coupled linear Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+H">Huiling Zhang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zi Xu</a>, 
<a href="/search/math?searchtype=author&query=Dai%2C+Y">Yuhong Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2212.04672">arXiv:2212.04672</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we study zeroth-order algorithms for nonconvex minimax
problems with coupled linear constraints under the deterministic and stochastic
settings, which have attracted wide attention in machine learning, signal
processing and many other fields in recent years, e.g., adversarial attacks in
resource allocation problems and network flow problems etc. We propose two
single-loop algorithms, namely the zero-order primal-dual alternating projected
gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum
primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic
and stochastic nonconvex-(strongly) concave minimax problems with coupled
linear constraints. The iteration complexity of the two proposed algorithms to
obtain an $\varepsilon$-stationary point are proved to be
$\mathcal{O}(\varepsilon ^{-2})$ (resp. $\mathcal{O}(\varepsilon ^{-4})$) for
solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems
with coupled linear constraints under deterministic settings and
$\tilde{\mathcal{O}}(\varepsilon ^{-3})$ (resp.
$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$) under stochastic settings
respectively. To the best of our knowledge, they are the first two zeroth-order
algorithms with iterative complexity guarantees for solving
nonconvex-(strongly) concave minimax problems with coupled linear constraints
under the deterministic and stochastic settings.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03353" title="Abstract">arXiv:2402.03353</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.03353" title="Download PDF">pdf</a>, <a href="/ps/2402.03353" title="Download PostScript">ps</a>, <a href="/format/2402.03353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tweet Influence on Market Trends: Analyzing the Impact of Social Media  Sentiment on Biotech Stocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Avila%2C+C+S+R">C. Sarai R. Avila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission includes 51 pages and 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA); Numerical Analysis (math.NA)

</div>
<p class="mathjax">This study investigates the relationship between tweet sentiment across
diverse categories: news, company opinions, CEO opinions, competitor opinions,
and stock market behavior in the biotechnology sector, with a focus on
understanding the impact of social media discourse on investor sentiment and
decision-making processes. We analyzed historical stock market data for ten of
the largest and most influential pharmaceutical companies alongside Twitter
data related to COVID-19, vaccines, the companies, and their respective CEOs.
Using VADER sentiment analysis, we examined the sentiment scores of tweets and
assessed their relationships with stock market performance. We employed ARIMA
(AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression)
models to forecast stock market performance, incorporating sentiment covariates
to improve predictions. Our findings revealed a complex interplay between tweet
sentiment, news, biotech companies, their CEOs, and stock market performance,
emphasizing the importance of considering diverse factors when modeling and
predicting stock prices. This study provides valuable insights into the
influence of social media on the financial sector and lays a foundation for
future research aimed at refining stock price prediction models.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03354" title="Abstract">arXiv:2402.03354</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.03354" title="Download PDF">pdf</a>, <a href="/format/2402.03354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Uncertainty in Collective Opinion Dynamics with Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mengers%2C+V">Vito Mengers</a>, 
<a href="/search/physics?searchtype=author&query=Raoufi%2C+M">Mohsen Raoufi</a>, 
<a href="/search/physics?searchtype=author&query=Brock%2C+O">Oliver Brock</a>, 
<a href="/search/physics?searchtype=author&query=Hamann%2C+H">Heiko Hamann</a>, 
<a href="/search/physics?searchtype=author&query=Romanczuk%2C+P">Pawel Romanczuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Natural and artificial collectives exhibit heterogeneities across different
dimensions, contributing to the complexity of their behavior. We investigate
the effect of two such heterogeneities on collective opinion dynamics:
heterogeneity of the quality of agents' prior information and of centrality in
the network, i.e., the number of immediate neighbors. To study these
heterogeneities, we not only consider them in our model, proposing a novel
network generator with heterogeneous centrality, but also introduce uncertainty
as an additional dimension. By quantifying the uncertainty of each agent, we
provide a mechanism for agents to adaptively weigh their individual against
social information. As uncertainties develop according to the interactions
between agents, they capture information on heterogeneities. Therefore,
uncertainty is a relevant additional observable in the study of complex
collective opinion dynamics that we use to show the bidirectional relationship
of heterogeneous centrality and information. Furthermore, we demonstrate that
uncertainty-driven adaptive weighting leads to increased accuracy and speed of
consensus, especially under heterogeneity, and provide guidelines for avoiding
performance-decreasing errors in uncertainty modeling. These opportunities for
improved performance and observability suggest the importance of uncertainty
both for the study of natural and the design of artificial heterogeneous
systems.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03363" title="Abstract">arXiv:2402.03363</a> (cross-list from math.NT) [<a href="/pdf/2402.03363" title="Download PDF">pdf</a>, <a href="/format/2402.03363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Prime Number Classification: Achieving High Recall Rate and  Rapid Convergence with Sparse Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+S">Serin Lee</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+S">S. Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a novel approach at the intersection of machine learning
and number theory, focusing on the classification of prime and non-prime
numbers. At the core of our research is the development of a highly sparse
encoding method, integrated with conventional neural network architectures.
This combination has shown promising results, achieving a recall of over 99\%
in identifying prime numbers and 79\% for non-prime numbers from an inherently
imbalanced sequential series of integers, while exhibiting rapid model
convergence before the completion of a single training epoch. We performed
training using $10^6$ integers starting from a specified integer and tested on
a different range of $2 \times 10^6$ integers extending from $10^6$ to $3
\times 10^6$, offset by the same starting integer. While constrained by the
memory capacity of our resources, which limited our analysis to a span of
$3\times10^6$, we believe that our study contribute to the application of
machine learning in prime number analysis. This work aims to demonstrate the
potential of such applications and hopes to inspire further exploration and
possibilities in diverse fields.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03369" title="Abstract">arXiv:2402.03369</a> (cross-list from eess.AS) [<a href="/pdf/2402.03369" title="Download PDF">pdf</a>, <a href="/ps/2402.03369" title="Download PostScript">ps</a>, <a href="/format/2402.03369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of Google&#x27;s Voice Recognition and Sentence Classification for  Health Care Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Uddin%2C+M">Majbah Uddin</a>, 
<a href="/search/eess?searchtype=author&query=Huynh%2C+N">Nathan Huynh</a>, 
<a href="/search/eess?searchtype=author&query=Vidal%2C+J+M">Jose M Vidal</a>, 
<a href="/search/eess?searchtype=author&query=Taaffe%2C+K+M">Kevin M Taaffe</a>, 
<a href="/search/eess?searchtype=author&query=Fredendall%2C+L+D">Lawrence D Fredendall</a>, 
<a href="/search/eess?searchtype=author&query=Greenstein%2C+J+S">Joel S Greenstein</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Engineering Management Journal, 27:3, 152-162, 2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">This study examined the use of voice recognition technology in perioperative
services (Periop) to enable Periop staff to record workflow milestones using
mobile technology. The use of mobile technology to improve patient flow and
quality of care could be facilitated if such voice recognition technology could
be made robust. The goal of this experiment was to allow the Periop staff to
provide care without being interrupted with data entry and querying tasks.
However, the results are generalizable to other situations where an engineering
manager attempts to improve communication performance using mobile technology.
This study enhanced Google's voice recognition capability by using
post-processing classifiers (i.e., bag-of-sentences, support vector machine,
and maximum entropy). The experiments investigated three factors (original
phrasing, reduced phrasing, and personalized phrasing) at three levels (zero
training repetition, 5 training repetitions, and 10 training repetitions).
Results indicated that personal phrasing yielded the highest correctness and
that training the device to recognize an individual's voice improved
correctness as well. Although simplistic, the bag-of-sentences classifier
significantly improved voice recognition correctness. The classification
efficiency of the maximum entropy and support vector machine algorithms was
found to be nearly identical. These results suggest that engineering managers
could significantly enhance Google's voice recognition technology by using
post-processing techniques, which would facilitate its use in health care and
other applications.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03383" title="Abstract">arXiv:2402.03383</a> (cross-list from eess.IV) [<a href="/pdf/2402.03383" title="Download PDF">pdf</a>, <a href="/ps/2402.03383" title="Download PostScript">ps</a>, <a href="/format/2402.03383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Collaborative Model-driven Network for MRI Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qiao%2C+X">Xiaoyu Qiao</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Weisheng Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Guofen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yuping Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Magnetic resonance imaging (MRI) is a vital medical imaging modality, but its
development has been limited by prolonged scanning time. Deep learning
(DL)-based methods, which build neural networks to reconstruct MR images from
undersampled raw data, can reliably address this problem. Among these methods,
model-driven DL methods incorporate different prior knowledge into deep
networks, thereby narrowing the solution space and achieving better results.
However, the complementarity among different prior knowledge has not been
thoroughly explored. Most of the existing model-driven networks simply stack
unrolled cascades to mimic iterative solution steps, which are inefficient and
their performances are suboptimal. To optimize the conventional network
structure, we propose a collaborative model-driven network. In the network,
each unrolled cascade comprised three parts: model-driven subnetworks,
attention modules, and correction modules. The attention modules can learn to
enhance the areas of expertise for each subnetwork, and the correction modules
can compensate for new errors caused by the attention modules. The optimized
intermediate results are fed into the next cascade for better convergence.
Experimental results on multiple sequences showed significant improvements in
the final results without additional computational complexity. Moreover, the
proposed model-driven network design strategy can be easily applied to other
model-driven methods to improve their performances.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03390" title="Abstract">arXiv:2402.03390</a> (cross-list from eess.IV) [<a href="/pdf/2402.03390" title="Download PDF">pdf</a>, <a href="/format/2402.03390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixelGen: Rethinking Embedded Camera Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Kunjun Li</a>, 
<a href="/search/eess?searchtype=author&query=Gulati%2C+M">Manoj Gulati</a>, 
<a href="/search/eess?searchtype=author&query=Waskito%2C+S">Steven Waskito</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+D">Dhairya Shah</a>, 
<a href="/search/eess?searchtype=author&query=Chakrabarty%2C+S">Shantanu Chakrabarty</a>, 
<a href="/search/eess?searchtype=author&query=Varshney%2C+A">Ambuj Varshney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Embedded camera systems are ubiquitous, representing the most widely deployed
example of a wireless embedded system. They capture a representation of the
world - the surroundings illuminated by visible or infrared light. Despite
their widespread usage, the architecture of embedded camera systems has
remained unchanged, which leads to limitations. They visualize only a tiny
portion of the world. Additionally, they are energy-intensive, leading to
limited battery lifespan. We present PixelGen, which re-imagines embedded
camera systems. Specifically, PixelGen combines sensors, transceivers, and
low-resolution image and infrared vision sensors to capture a broader world
representation. They are deliberately chosen for their simplicity, low bitrate,
and power consumption, culminating in an energy-efficient platform. We show
that despite the simplicity, the captured data can be processed using
transformer-based image and language models to generate novel representations
of the environment. For example, we demonstrate that it can allow the
generation of high-definition images, while the camera utilises low-power,
low-resolution monochrome cameras. Furthermore, the capabilities of PixelGen
extend beyond traditional photography, enabling visualization of phenomena
invisible to conventional cameras, such as sound waves. PixelGen can enable
numerous novel applications, and we demonstrate that it enables unique
visualization of the surroundings that are then projected on extended reality
headsets. We believe, PixelGen goes beyond conventional cameras and opens new
avenues for research and photography.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03392" title="Abstract">arXiv:2402.03392</a> (cross-list from math.OC) [<a href="/pdf/2402.03392" title="Download PDF">pdf</a>, <a href="/format/2402.03392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal control analysis and Practical NMPC applied to refrigeration  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/math?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>, 
<a href="/search/math?searchtype=author&query=Normey-Rico%2C+J+E">J. E. Normey-Rico</a>, 
<a href="/search/math?searchtype=author&query=Rubio%2C+F+R">F. R Rubio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 14 figures. Preprint submitted to ISA Transactions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Optimal control of vapour-compression refrigeration systems is addressed. The
\emph{moving boundary} approach is applied to derive a control-oriented model,
characterised by three states. The optimal cycle (that satisfying the cooling
demand while maximizing efficiency) is defined by three variables, but only two
inputs are available, thus a controllability analysis on the simplified model
is performed. Optimization results show that not all optimal cycles are
achieved with minimum degree of superheating at the evaporator outlet. The
Practical NMPC and other strategy from the literature are compared, both
showing trouble in reaching the optimal cycle, which agrees with the
controllability analysis conclusions.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03398" title="Abstract">arXiv:2402.03398</a> (cross-list from eess.IV) [<a href="/pdf/2402.03398" title="Download PDF">pdf</a>, <a href="/ps/2402.03398" title="Download PostScript">ps</a>, <a href="/format/2402.03398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mehrdad%2C+S">Saeid Mehrdad</a>, 
<a href="/search/eess?searchtype=author&query=Janani%2C+S+A">Seyed AmirHossein Janani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Nonlinear hyperspectral unmixing has recently received considerable
attention, as linear mixture models do not lead to an acceptable resolution in
some problems. In fact, most nonlinear unmixing methods are designed by
assuming specific assumptions on the nonlinearity model which subsequently
limits the unmixing performance. In this paper, we propose an unsupervised
nonlinear unmixing approach based on deep learning by incorporating a general
nonlinear model with no special assumptions. This model consists of two
branches. In the first branch, endmembers are learned by reconstructing the
rows of hyperspectral images using some hidden layers, and in the second
branch, abundance values are learned based on the columns of respective images.
Then, using multi-task learning, we introduce an auxiliary task to enforce the
two branches to work together. This technique can be considered as a
regularizer mitigating overfitting, which improves the performance of the total
network. Extensive experiments on synthetic and real data verify the
effectiveness of the proposed method compared to some state-of-the-art
hyperspectral unmixing methods.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03399" title="Abstract">arXiv:2402.03399</a> (cross-list from eess.IV) [<a href="/pdf/2402.03399" title="Download PDF">pdf</a>, <a href="/format/2402.03399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking RGB Color Representation for Image Restoration Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Jaerin Lee</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+J">JoonKyu Park</a>, 
<a href="/search/eess?searchtype=author&query=Baik%2C+S">Sungyong Baik</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+K+M">Kyoung Mu Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages (11 pages main manuscript + 20 pages appendices), 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image restoration models are typically trained with a pixel-wise distance
loss defined over the RGB color representation space, which is well known to be
a source of blurry and unrealistic textures in the restored images. The reason,
we believe, is that the three-channel RGB space is insufficient for supervising
the restoration models. To this end, we augment the representation to hold
structural information of local neighborhoods at each pixel while keeping the
color information and pixel-grainedness unharmed. The result is a new
representation space, dubbed augmented RGB ($a$RGB) space. Substituting the
underlying representation space for the per-pixel losses facilitates the
training of image restoration models, thereby improving the performance without
affecting the evaluation phase. Notably, when combined with auxiliary
objectives such as adversarial or perceptual losses, our $a$RGB space
consistently improves overall metrics by reconstructing both color and local
structures, overcoming the conventional perception-distortion trade-off.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03407" title="Abstract">arXiv:2402.03407</a> (cross-list from eess.AS) [<a href="/pdf/2402.03407" title="Download PDF">pdf</a>, <a href="/format/2402.03407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Stability of LLM-based Speech Generation Systems through  Self-Supervised Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mart%C3%ADn-Cortinas%2C+%C3%81">&#xc1;lvaro Mart&#xed;n-Cortinas</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A1ez-Trigueros%2C+D">Daniel S&#xe1;ez-Trigueros</a>, 
<a href="/search/eess?searchtype=author&query=Vall%C3%A9s-P%C3%A9rez%2C+I">Iv&#xe1;n Vall&#xe9;s-P&#xe9;rez</a>, 
<a href="/search/eess?searchtype=author&query=Tura-Vecino%2C+B">Biel Tura-Vecino</a>, 
<a href="/search/eess?searchtype=author&query=Bili%C5%84ski%2C+P">Piotr Bili&#x144;ski</a>, 
<a href="/search/eess?searchtype=author&query=Lajszczak%2C+M">Mateusz Lajszczak</a>, 
<a href="/search/eess?searchtype=author&query=Beringer%2C+G">Grzegorz Beringer</a>, 
<a href="/search/eess?searchtype=author&query=Barra-Chicote%2C+R">Roberto Barra-Chicote</a>, 
<a href="/search/eess?searchtype=author&query=Lorenzo-Trueba%2C+J">Jaime Lorenzo-Trueba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are one of the most promising technologies for
the next era of speech generation systems, due to their scalability and
in-context learning capabilities. Nevertheless, they suffer from multiple
stability issues at inference time, such as hallucinations, content skipping or
speech repetitions. In this work, we introduce a new self-supervised Voice
Conversion (VC) architecture which can be used to learn to encode transitory
features, such as content, separately from stationary ones, such as speaker ID
or recording conditions, creating speaker-disentangled representations. Using
speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the
LLM to generate the content and the style of the speech only from the text,
similarly to humans, while the speaker identity is provided by the decoder of
the VC model. Results show that LLMs trained over speaker-disentangled
self-supervised representations provide an improvement of 4.7pp in speaker
similarity over SOTA entangled representations, and a word error rate (WER)
5.4pp lower. Furthermore, they achieve higher naturalness than human recordings
of the LibriTTS test-other dataset. Finally, we show that using explicit
reference embedding negatively impacts intelligibility (stability), with WER
increasing by 14pp compared to the model that only uses text to infer the
style.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03412" title="Abstract">arXiv:2402.03412</a> (cross-list from eess.IV) [<a href="/pdf/2402.03412" title="Download PDF">pdf</a>, <a href="/format/2402.03412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> See More Details: Efficient Image Super-Resolution by Experts Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zamfir%2C+E">Eduard Zamfir</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zongwei Wu</a>, 
<a href="/search/eess?searchtype=author&query=Mehta%2C+N">Nancy Mehta</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Timofte%2C+R">Radu Timofte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reconstructing high-resolution (HR) images from low-resolution (LR) inputs
poses a significant challenge in image super-resolution (SR). While recent
approaches have demonstrated the efficacy of intricate operations customized
for various objectives, the straightforward stacking of these disparate
operations can result in a substantial computational burden, hampering their
practical utility. In response, we introduce SeemoRe, an efficient SR model
employing expert mining. Our approach strategically incorporates experts at
different levels, adopting a collaborative methodology. At the macro scale, our
experts address rank-wise and spatial-wise informative features, providing a
holistic understanding. Subsequently, the model delves into the subtleties of
rank choice by leveraging a mixture of low-rank experts. By tapping into
experts specialized in distinct key factors crucial for accurate SR, our model
excels in uncovering intricate intra-feature details. This collaborative
approach is reminiscent of the concept of "see more", allowing our model to
achieve an optimal performance with minimal computational costs in efficient
settings. The source will be publicly made available at
https://github.com/eduardzamfir/seemoredetails
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03414" title="Abstract">arXiv:2402.03414</a> (cross-list from eess.IV) [<a href="/pdf/2402.03414" title="Download PDF">pdf</a>, <a href="/format/2402.03414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An end-to-end deep learning pipeline to derive blood input with partial  volume corrections for automated parametric brain PET mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chavan%2C+R">Rugved Chavan</a>, 
<a href="/search/eess?searchtype=author&query=Hyman%2C+G">Gabriel Hyman</a>, 
<a href="/search/eess?searchtype=author&query=Qureshi%2C+Z">Zoraiz Qureshi</a>, 
<a href="/search/eess?searchtype=author&query=Jayakumar%2C+N">Nivetha Jayakumar</a>, 
<a href="/search/eess?searchtype=author&query=Terrell%2C+W">William Terrell</a>, 
<a href="/search/eess?searchtype=author&query=Berr%2C+S">Stuart Berr</a>, 
<a href="/search/eess?searchtype=author&query=Schiff%2C+D">David Schiff</a>, 
<a href="/search/eess?searchtype=author&query=Wardius%2C+M">Megan Wardius</a>, 
<a href="/search/eess?searchtype=author&query=Fountain%2C+N">Nathan Fountain</a>, 
<a href="/search/eess?searchtype=author&query=Muttikkal%2C+T">Thomas Muttikkal</a>, 
<a href="/search/eess?searchtype=author&query=Quigg%2C+M">Mark Quigg</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+M">Miaomiao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Kundu%2C+B">Bijoy Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography
(dFDG-PET) for human brain imaging has considerable clinical potential, yet its
utilization remains limited. A key challenge in the quantitative analysis of
dFDG-PET is characterizing a patient-specific blood input function,
traditionally reliant on invasive arterial blood sampling. This research
introduces a novel approach employing non-invasive deep learning model-based
computations from the internal carotid arteries (ICA) with partial volume (PV)
corrections, thereby eliminating the need for invasive arterial sampling. We
present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA
segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the
derivation of a model-corrected blood input function (MCIF) with PV
corrections. The developed 3D U-Net and RNN was trained and validated using a
5-fold cross-validation approach on 50 human brain FDG PET datasets. The
ICA-net achieved an average Dice score of 82.18% and an Intersection over Union
of 68.54% across all tested scans. Furthermore, the MCIF-net exhibited a
minimal root mean squared error of 0.0052. The application of this pipeline to
ground truth data for dFDG-PET brain scans resulted in the precise localization
of seizure onset regions, which contributed to a successful clinical outcome,
with the patient achieving a seizure-free state after treatment. These results
underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in
learning the ICA structure's distribution and automating MCIF computation with
PV corrections. This advancement marks a significant leap in non-invasive
neuroimaging.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03447" title="Abstract">arXiv:2402.03447</a> (cross-list from stat.ML) [<a href="/pdf/2402.03447" title="Download PDF">pdf</a>, <a href="/format/2402.03447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges in Variable Importance Ranking Under Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liang%2C+A">Annie Liang</a>, 
<a href="/search/stat?searchtype=author&query=Jemielita%2C+T">Thomas Jemielita</a>, 
<a href="/search/stat?searchtype=author&query=Liaw%2C+A">Andy Liaw</a>, 
<a href="/search/stat?searchtype=author&query=Svetnik%2C+V">Vladimir Svetnik</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+L">Lingkang Huang</a>, 
<a href="/search/stat?searchtype=author&query=Baumgartner%2C+R">Richard Baumgartner</a>, 
<a href="/search/stat?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Variable importance plays a pivotal role in interpretable machine learning as
it helps measure the impact of factors on the output of the prediction model.
Model agnostic methods based on the generation of "null" features via
permutation (or related approaches) can be applied. Such analysis is often
utilized in pharmaceutical applications due to its ability to interpret
black-box models, including tree-based ensembles. A major challenge and
significant confounder in variable importance estimation however is the
presence of between-feature correlation. Recently, several adjustments to
marginal permutation utilizing feature knockoffs were proposed to address this
issue, such as the variable importance measure known as conditional predictive
impact (CPI). Assessment and evaluation of such approaches is the focus of our
work. We first present a comprehensive simulation study investigating the
impact of feature correlation on the assessment of variable importance. We then
theoretically prove the limitation that highly correlated features pose for the
CPI through the knockoff construction. While we expect that there is always no
correlation between knockoff variables and its corresponding predictor
variables, we prove that the correlation increases linearly beyond a certain
correlation threshold between the predictor variables. Our findings emphasize
the absence of free lunch when dealing with high feature correlation, as well
as the necessity of understanding the utility and limitations behind methods in
variable importance estimation.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03460" title="Abstract">arXiv:2402.03460</a> (cross-list from stat.ML) [<a href="/pdf/2402.03460" title="Download PDF">pdf</a>, <a href="/format/2402.03460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Curse of Dimensionality with Distributed Neural Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/stat?searchtype=author&query=Furuya%2C+T">Takashi Furuya</a>, 
<a href="/search/stat?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/stat?searchtype=author&query=Law%2C+M+T">Marc T. Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Combinatorics (math.CO); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present a theoretical approach to overcome the curse of dimensionality
using a neural computation algorithm which can be distributed across several
machines. Our modular distributed deep learning paradigm, termed \textit{neural
pathways}, can achieve arbitrary accuracy while only loading a small number of
parameters into GPU VRAM. Formally, we prove that for every error level
$\varepsilon&gt;0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can
construct a neural pathways model which uniformly approximates $f$ to
$\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of
$\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and
$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the
forward pass. This improves the optimal bounds for traditional non-distributed
deep learning models, namely ReLU MLPs, which need
$\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The
only other available deep learning model that breaks the curse of
dimensionality is MLPs with super-expressive activation functions. However, we
demonstrate that these models have an infinite VC dimension, even with bounded
depth and width restrictions, unlike the neural pathways model. This implies
that only the latter generalizes. Our analysis is validated experimentally in
both regression and classification tasks, demonstrating that our model exhibits
superior performance compared to larger centralized benchmarks.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03473" title="Abstract">arXiv:2402.03473</a> (cross-list from eess.IV) [<a href="/pdf/2402.03473" title="Download PDF">pdf</a>, <a href="/format/2402.03473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Efficacy of Invisible Watermarks in AI-Generated Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xing%2C+X">Xiaodan Xing</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+H">Huiyu Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+Y">Yingying Fang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">AI-generated medical images are gaining growing popularity due to their
potential to address the data scarcity challenge in the real world. However,
the issue of accurate identification of these synthetic images, particularly
when they exhibit remarkable realism with their real copies, remains a concern.
To mitigate this challenge, image generators such as DALLE and Imagen, have
integrated digital watermarks aimed at facilitating the discernment of
synthetic images' authenticity. These watermarks are embedded within the image
pixels and are invisible to the human eye while remains their detectability.
Nevertheless, a comprehensive investigation into the potential impact of these
invisible watermarks on the utility of synthetic medical images has been
lacking. In this study, we propose the incorporation of invisible watermarks
into synthetic medical images and seek to evaluate their efficacy in the
context of downstream classification tasks. Our goal is to pave the way for
discussions on the viability of such watermarks in boosting the detectability
of synthetic medical images, fortifying ethical standards, and safeguarding
against data pollution and potential scams.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03474" title="Abstract">arXiv:2402.03474</a> (cross-list from astro-ph.SR) [<a href="/pdf/2402.03474" title="Download PDF">pdf</a>, <a href="/format/2402.03474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Region-based Flare Forecasting with Sliding Window Multivariate  Time Series Forest Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Ji%2C+A">Anli Ji</a>, 
<a href="/search/astro-ph?searchtype=author&query=Aydin%2C+B">Berkay Aydin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Over the past few decades, many applications of physics-based simulations and
data-driven techniques (including machine learning and deep learning) have
emerged to analyze and predict solar flares. These approaches are pivotal in
understanding the dynamics of solar flares, primarily aiming to forecast these
events and minimize potential risks they may pose to Earth. Although current
methods have made significant progress, there are still limitations to these
data-driven approaches. One prominent drawback is the lack of consideration for
the temporal evolution characteristics in the active regions from which these
flares originate. This oversight hinders the ability of these methods to grasp
the relationships between high-dimensional active region features, thereby
limiting their usability in operations. This study centers on the development
of interpretable classifiers for multivariate time series and the demonstration
of a novel feature ranking method with sliding window-based sub-interval
ranking. The primary contribution of our work is to bridge the gap between
complex, less understandable black-box models used for high-dimensional data
and the exploration of relevant sub-intervals from multivariate time series,
specifically in the context of solar flare forecasting. Our findings
demonstrate that our sliding-window time series forest classifier performs
effectively in solar flare prediction (with a True Skill Statistic of over
85\%) while also pinpointing the most crucial features and sub-intervals for a
given learning task.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03476" title="Abstract">arXiv:2402.03476</a> (cross-list from eess.IV) [<a href="/pdf/2402.03476" title="Download PDF">pdf</a>, <a href="/format/2402.03476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CT Material Decomposition using Spectral Diffusion Posterior Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+X">Xiao Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Gang%2C+G+J">Grace J. Gang</a>, 
<a href="/search/eess?searchtype=author&query=Stayman%2C+J+W">J. Webster Stayman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">In this work, we introduce a new deep learning approach based on diffusion
posterior sampling (DPS) to perform material decomposition from spectral CT
measurements. This approach combines sophisticated prior knowledge from
unsupervised training with a rigorous physical model of the measurements. A
faster and more stable variant is proposed that uses a jumpstarted process to
reduce the number of time steps required in the reverse process and a gradient
approximation to reduce the computational cost. Performance is investigated for
two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems,
DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10%
of iterations as used in the model-based material decomposition(MBMD).
Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and
achieves the highest accuracy, the lowest uncertainty, and the lowest
computational costs compared to classic DPS and MBMD. The results demonstrate
the potential of JSDPS for providing relatively fast and accurate material
decomposition based on spectral CT data.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03485" title="Abstract">arXiv:2402.03485</a> (cross-list from stat.ML) [<a href="/pdf/2402.03485" title="Download PDF">pdf</a>, <a href="/format/2402.03485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Meets Post-hoc Interpretability: A Mathematical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lopardo%2C+G">Gianluigi Lopardo</a>, 
<a href="/search/stat?searchtype=author&query=Precioso%2C+F">Frederic Precioso</a>, 
<a href="/search/stat?searchtype=author&query=Garreau%2C+D">Damien Garreau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Attention-based architectures, in particular transformers, are at the heart
of a technological revolution. Interestingly, in addition to helping obtain
state-of-the-art results on a wide range of applications, the attention
mechanism intrinsically provides meaningful insights on the internal behavior
of the model. Can these insights be used as explanations? Debate rages on. In
this paper, we mathematically study a simple attention-based architecture and
pinpoint the differences between post-hoc and attention-based explanations. We
show that they provide quite different results, and that, despite their
limitations, post-hoc methods are capable of capturing more useful insights
than merely examining the attention weights.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03492" title="Abstract">arXiv:2402.03492</a> (cross-list from eess.IV) [<a href="/pdf/2402.03492" title="Download PDF">pdf</a>, <a href="/format/2402.03492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Strong labels: Weakly-supervised Learning Based on Gaussian  Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in  Non-contrast CTs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+Q">Qixiang Ma</a>, 
<a href="/search/eess?searchtype=author&query=%C5%81ucas%2C+A">Antoine &#x141;ucas</a>, 
<a href="/search/eess?searchtype=author&query=Shu%2C+H">Huazhong Shu</a>, 
<a href="/search/eess?searchtype=author&query=Kaladji%2C+A">Adrien Kaladji</a>, 
<a href="/search/eess?searchtype=author&query=Haigron%2C+P">Pascal Haigron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep-learning-based automated segmentation of vascular structures in
preoperative CT scans contributes to computer-assisted diagnosis and
intervention procedure in vascular diseases. While CT angiography (CTA) is the
common standard, non-contrast CT imaging is significant as a contrast-risk-free
alternative, avoiding complications associated with contrast agents. However,
the challenges of labor-intensive labeling and high labeling variability due to
the ambiguity of vascular boundaries hinder conventional strong-label-based,
fully-supervised learning in non-contrast CTs. This paper introduces a
weakly-supervised framework using ellipses' topology in slices, including 1) an
efficient annotation process based on predefined standards, 2) ellipse-fitting
processing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels,
4) a training process through a combination of voxel reconstruction loss and
distribution loss with the pseudo labels. We assess the effectiveness of the
proposed method on one local and two public datasets comprising non-contrast CT
scans, particularly focusing on the abdominal aorta. On the local dataset, our
weakly-supervised learning approach based on pseudo labels outperforms
strong-label-based fully-supervised learning (1.54\% of Dice score on average),
reducing labeling time by around 82.0\%. The efficiency in generating pseudo
labels allows the inclusion of label-agnostic external data in the training
set, leading to an additional improvement in performance (2.74\% of Dice score
on average) with a reduction of 66.3\% labeling time, where the labeling time
remains considerably less than that of strong labels. On the public dataset,
the pseudo labels achieve an overall improvement of 1.95\% in Dice score for 2D
models while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D
model.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03500" title="Abstract">arXiv:2402.03500</a> (cross-list from quant-ph) [<a href="/pdf/2402.03500" title="Download PDF">pdf</a>, <a href="/format/2402.03500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curriculum reinforcement learning for quantum architecture search under  hardware errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Patel%2C+Y+J">Yash J. Patel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+A">Akash Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ostaszewski%2C+M">Mateusz Ostaszewski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bonet-Monroig%2C+X">Xavier Bonet-Monroig</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dunjko%2C+V">Vedran Dunjko</a>, 
<a href="/search/quant-ph?searchtype=author&query=Danaci%2C+O">Onur Danaci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 11 figures, 6 tables. Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The key challenge in the noisy intermediate-scale quantum era is finding
useful circuits compatible with current device limitations. Variational quantum
algorithms (VQAs) offer a potential solution by fixing the circuit architecture
and optimizing individual gate parameters in an external loop. However,
parameter optimization can become intractable, and the overall performance of
the algorithm depends heavily on the initially chosen circuit architecture.
Several quantum architecture search (QAS) algorithms have been developed to
design useful circuit architectures automatically. In the case of parameter
optimization alone, noise effects have been observed to dramatically influence
the performance of the optimizer and final outcomes, which is a key line of
study. However, the effects of noise on the architecture search, which could be
just as critical, are poorly understood. This work addresses this gap by
introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm
designed to tackle challenges in realistic VQA deployment. The algorithm
incorporates (i) a 3D architecture encoding and restrictions on environment
dynamics to explore the search space of possible circuits efficiently, (ii) an
episode halting scheme to steer the agent to find shorter circuits, and (iii) a
novel variant of simultaneous perturbation stochastic approximation as an
optimizer for faster convergence. To facilitate studies, we developed an
optimized simulator for our algorithm, significantly improving computational
efficiency in simulating noisy quantum circuits by employing the Pauli-transfer
matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing
on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS
algorithms across several metrics in both noiseless and noisy environments.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03520" title="Abstract">arXiv:2402.03520</a> (cross-list from math.CO) [<a href="/pdf/2402.03520" title="Download PDF">pdf</a>, <a href="/format/2402.03520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling List Packings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Camrud%2C+E">Evan Camrud</a>, 
<a href="/search/math?searchtype=author&query=Davies%2C+E">Ewan Davies</a>, 
<a href="/search/math?searchtype=author&query=Karduna%2C+A">Alex Karduna</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+H">Holden Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the problem of approximately counting the number of list packings of
a graph. The analogous problem for usual vertex coloring and list coloring has
attracted a lot of attention. For list packing the setup is similar but we seek
a full decomposition of the lists of colors into pairwise-disjoint proper list
colorings. In particular, the existence of a list packing implies the existence
of a list coloring. Recent works on list packing have focused on existence or
extremal results of on the number of list packings, but here we turn to the
algorithmic aspects of counting.
<br />In graphs of maximum degree $\Delta$ and when the number of colors is at
least $\Omega(\Delta^2)$, we give an FPRAS based on rapid mixing of a natural
Markov chain (the Glauber dynamics) which we analyze with the path coupling
technique. Some motivation for our work is the investigation of an atypical
spin system, one where the number of spins for each vertex is much larger than
the graph degree.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03523" title="Abstract">arXiv:2402.03523</a> (cross-list from math.AT) [<a href="/pdf/2402.03523" title="Download PDF">pdf</a>, <a href="/ps/2402.03523" title="Download PostScript">ps</a>, <a href="/format/2402.03523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Monoidal Smash Products in Homotopy Type Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ljungstr%C3%B6m%2C+A">Axel Ljungstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">In Homotopy Type Theory, few constructions have proved as troublesome as the
smash product. While its definition is just as direct as in classical
mathematics, one quickly realises that in order to define and reason about
functions over iterations of it, one has to verify an exponentially growing
number of coherences. This has led to crucial results concerning smash products
remaining open. One particularly important such result is the fact that smash
products form a (1-coherent) symmetric monoidal product on the universe of
pointed types. This fact was used, without a complete proof, by e.g. Brunerie
in his PhD thesis to construct the cup product on integral cohomology and is,
more generally, a fundamental result in traditional algebraic topology. In this
paper, we salvage the situation by introducing a simple informal heuristic for
reasoning about functions defined over iterated smash products. We then use the
heuristic to verify e.g. the hexagon and pentagon identities, thereby obtaining
a proof of symmetric monoidality. We also provide a formal statement of the
heuristic in terms of an induction principle concerning the construction of
homotopies of functions defined over iterated smash products. The key results
presented here have been formalised in the proof assistant Cubical Agda.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03527" title="Abstract">arXiv:2402.03527</a> (cross-list from stat.ML) [<a href="/pdf/2402.03527" title="Download PDF">pdf</a>, <a href="/format/2402.03527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Validation for Predictive Methods in Spatial Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Burt%2C+D+R">David R. Burt</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+Y">Yunyi Shen</a>, 
<a href="/search/stat?searchtype=author&query=Broderick%2C+T">Tamara Broderick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Spatial prediction tasks are key to weather forecasting, studying air
pollution, and other scientific endeavors. Determining how much to trust
predictions made by statistical or physical methods is essential for the
credibility of scientific conclusions. Unfortunately, classical approaches for
validation fail to handle mismatch between locations available for validation
and (test) locations where we want to make predictions. This mismatch is often
not an instance of covariate shift (as commonly formalized) because the
validation and test locations are fixed (e.g., on a grid or at select points)
rather than i.i.d. from two distributions. In the present work, we formalize a
check on validation methods: that they become arbitrarily accurate as
validation data becomes arbitrarily dense. We show that classical and
covariate-shift methods can fail this check. We instead propose a method that
builds from existing ideas in the covariate-shift literature, but adapts them
to the validation data at hand. We prove that our proposal passes our check.
And we demonstrate its advantages empirically on simulated and real data.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03535" title="Abstract">arXiv:2402.03535</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.03535" title="Download PDF">pdf</a>, <a href="/format/2402.03535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision  Photometric Redshift Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/astro-ph?searchtype=author&query=Narayan%2C+G">Gautham Narayan</a>, 
<a href="/search/astro-ph?searchtype=author&query=Byler%2C+N">Nell Byler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure, 1 table. Submitted to AI4Differential Equations in Science Workshop at ICLR24. Public repository unavailable while under institutional review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The availability of large, public, multi-modal astronomical datasets presents
an opportunity to execute novel research that straddles the line between
science of AI and science of astronomy. Photometric redshift estimation is a
well-established subfield of astronomy. Prior works show that computer vision
models typically outperform catalog-based models, but these models face
additional complexities when incorporating images from more than one instrument
or sensor. In this report, we detail our progress creating Mantis Shrimp, a
multi-survey computer vision model for photometric redshift estimation that
fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery.
We use deep learning interpretability diagnostics to measure how the model
leverages information from the different inputs. We reason about the behavior
of the CNNs from the interpretability metrics, specifically framing the result
in terms of physically-grounded knowledge of galaxy properties.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03547" title="Abstract">arXiv:2402.03547</a> (cross-list from eess.IV) [<a href="/pdf/2402.03547" title="Download PDF">pdf</a>, <a href="/ps/2402.03547" title="Download PostScript">ps</a>, <a href="/format/2402.03547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype  Identification Using a Novel AUROC Loss Function for Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Namdar%2C+K">Khashayar Namdar</a>, 
<a href="/search/eess?searchtype=author&query=Wagner%2C+M+W">Matthias W. Wagner</a>, 
<a href="/search/eess?searchtype=author&query=Hawkins%2C+C">Cynthia Hawkins</a>, 
<a href="/search/eess?searchtype=author&query=Tabori%2C+U">Uri Tabori</a>, 
<a href="/search/eess?searchtype=author&query=Ertl-Wagner%2C+B+B">Birgit B. Ertl-Wagner</a>, 
<a href="/search/eess?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common
pediatric cancer type, accounting for 40% of brain tumors in children, and
identifying PLGNT molecular subtype is crucial for treatment planning. However,
the gold standard to determine the PLGNT subtype is biopsy, which can be
impractical or dangerous for patients. This research improves the performance
of Convolutional Neural Networks (CNNs) in classifying PLGNT subtypes through
MRI scans by introducing a loss function that specifically improves the model's
Area Under the Receiver Operating Characteristic (ROC) Curve (AUROC), offering
a non-invasive diagnostic alternative. In this study, a retrospective dataset
of 339 children with PLGNT (143 BRAF fusion, 71 with BRAF V600E mutation, and
125 non-BRAF) was curated. We employed a CNN model with Monte Carlo random data
splitting. The baseline model was trained using binary cross entropy (BCE), and
achieved an AUROC of 86.11% for differentiating BRAF fusion and BRAF V600E
mutations, which was improved to 87.71% using our proposed AUROC loss function
(p-value 0.045). With multiclass classification, the AUROC improved from 74.42%
to 76. 59% (p-value 0.0016).
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03580" title="Abstract">arXiv:2402.03580</a> (cross-list from math.OC) [<a href="/pdf/2402.03580" title="Download PDF">pdf</a>, <a href="/format/2402.03580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MINLP-based hybrid strategy for operating mode selection of  TES-backed-up refrigeration systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/math?searchtype=author&query=Rodr%C3%ADguez%2C+D">D. Rodr&#xed;guez</a>, 
<a href="/search/math?searchtype=author&query=Lemos%2C+J+M">J. M. Lemos</a>, 
<a href="/search/math?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/math?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Robust and Nonlinear Control (2020), 30,
  6091-6111
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This brief deals with the satisfaction of the daily cooling demand by a
hybrid system that consists of a vapour-compression refrigeration cycle and a
thermal energy storage (TES) unit, based on phase change materials. The
addition of the TES tank to the original refrigeration plant allows to schedule
the cooling production regardless of the instantaneous demand, given that the
TES tank can store cold energy and release it whenever deemed appropriate. The
scheduling problem is posed as an optimization problem based on mixed-integer
non-linear programming (MINLP), since it includes both discrete and continuous
variables. The latter corresponds to the references on the main cooling powers
involved in the problem (cooling production at the evaporator and TES
charging/discharging), whereas the discrete variables define the operating mode
scheduling. Therefore, in addition to the hybrid features of the physical
plant, a hybrid optimal control strategy is also proposed. A receding horizon
approach is applied, similar to model predictive control (MPC) strategies,
while economic criteria are imposed in the objective function, as well as
feasibility issues. The TES state estimation is also addressed, since its
instantaneous \emph{charge ratio} is not measurable. The proposed strategy is
applied in simulation to a challenging cooling demand profile and the main
advantages of the MINLP-based strategy over a non-linear MPC-based scheduling
strategy previously developed are highlighted, regarding operating cost, ease
of tuning, and ability to adapt to cooling demand variations.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03648" title="Abstract">arXiv:2402.03648</a> (cross-list from eess.SP) [<a href="/pdf/2402.03648" title="Download PDF">pdf</a>, <a href="/format/2402.03648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilinear Kernel Regression and Imputation via Manifold Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+D+T">Duc Thien Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Slavakis%2C+K">Konstantinos Slavakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel nonparametric framework for data imputation,
coined multilinear kernel regression and imputation via the manifold assumption
(MultiL-KRIM). Motivated by manifold learning, MultiL-KRIM models data features
as a point cloud located in or close to a user-unknown smooth manifold embedded
in a reproducing kernel Hilbert space. Unlike typical manifold-learning routes,
which seek low-dimensional patterns via regularizers based on graph-Laplacian
matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces
to manifolds and incorporates collaboration among point-cloud neighbors
(regressors) directly into the data-modeling term of the loss function.
Multiple kernel functions are allowed to offer robustness and rich
approximation properties, while multiple matrix factors offer low-rank
modeling, integrate dimensionality reduction, and streamline computations with
no need of training data. Two important application domains showcase the
functionality of MultiL-KRIM: time-varying-graph-signal (TVGS) recovery, and
reconstruction of highly accelerated dynamic-magnetic-resonance-imaging (dMRI)
data. Extensive numerical tests on real and synthetic data demonstrate
MultiL-KRIM's remarkable speedups over its predecessors, and outperformance
over prevalent "shallow" data-imputation techniques, with a more intuitive and
explainable pipeline than deep-image-prior methods.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03675" title="Abstract">arXiv:2402.03675</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.03675" title="Download PDF">pdf</a>, <a href="/format/2402.03675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Protein-Protein Interaction Exploration with PPIretrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hua%2C+C">Chenqing Hua</a>, 
<a href="/search/q-bio?searchtype=author&query=Coley%2C+C">Connor Coley</a>, 
<a href="/search/q-bio?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/q-bio?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/q-bio?searchtype=author&query=Zheng%2C+S">Shuangjia Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Protein-protein interactions (PPIs) are crucial in regulating numerous
cellular functions, including signal transduction, transportation, and immune
defense. As the accuracy of multi-chain protein complex structure prediction
improves, the challenge has shifted towards effectively navigating the vast
complex universe to identify potential PPIs. Herein, we propose PPIretrieval,
the first deep learning-based model for protein-protein interaction
exploration, which leverages existing PPI data to effectively search for
potential PPIs in an embedding space, capturing rich geometric and chemical
information of protein surfaces. When provided with an unseen query protein
with its associated binding site, PPIretrieval effectively identifies a
potential binding partner along with its corresponding binding site in an
embedding space, facilitating the formation of protein-protein complexes.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03683" title="Abstract">arXiv:2402.03683</a> (cross-list from stat.ME) [<a href="/pdf/2402.03683" title="Download PDF">pdf</a>, <a href="/format/2402.03683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gambling-Based Confidence Sequences for Bounded Random Vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ryu%2C+J+J">J. Jon Ryu</a>, 
<a href="/search/stat?searchtype=author&query=Wornell%2C+G+W">Gregory W. Wornell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Information Theory (cs.IT); Statistics Theory (math.ST)

</div>
<p class="mathjax">A confidence sequence (CS) is a sequence of confidence sets that contains a
target parameter of an underlying stochastic process at any time step with high
probability. This paper proposes a new approach to constructing CSs for means
of bounded multivariate stochastic processes using a general gambling
framework, extending the recently established coin toss framework for bounded
random processes. The proposed gambling framework provides a general recipe for
constructing CSs for categorical and probability-vector-valued observations, as
well as for general bounded multidimensional observations through a simple
reduction. This paper specifically explores the use of the mixture portfolio,
akin to Cover's universal portfolio, in the proposed framework and investigates
the properties of the resulting CSs. Simulations demonstrate the tightness of
these confidence sequences compared to existing methods. When applied to the
sampling without-replacement setting for finite categorical data, it is shown
that the resulting CS based on a universal gambling strategy is provably
tighter than that of the posterior-prior ratio martingale proposed by
Waudby-Smith and Ramdas.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03685" title="Abstract">arXiv:2402.03685</a> (cross-list from math.CO) [<a href="/pdf/2402.03685" title="Download PDF">pdf</a>, <a href="/ps/2402.03685" title="Download PostScript">ps</a>, <a href="/format/2402.03685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Friends-and-strangers is PSPACE-complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhujun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In this paper, we show that the friends-and-strangers problem is
PSPACE-complete by reduction from the Ncl (non-deterministic constraint logic)
problem.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03695" title="Abstract">arXiv:2402.03695</a> (cross-list from eess.IV) [<a href="/pdf/2402.03695" title="Download PDF">pdf</a>, <a href="/format/2402.03695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic  Cartilage Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sapkota%2C+N">Nishchal Sapkota</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yejia Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Perrine%2C+S+M+M">Susan M. Motch Perrine</a>, 
<a href="/search/eess?searchtype=author&query=Hsi%2C+Y">Yuhan Hsi</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Sirui Li</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+M">Meng Wu</a>, 
<a href="/search/eess?searchtype=author&query=Holmes%2C+G">Greg Holmes</a>, 
<a href="/search/eess?searchtype=author&query=Abdulai%2C+A+R">Abdul R. Abdulai</a>, 
<a href="/search/eess?searchtype=author&query=Jabs%2C+E+W">Ethylin W. Jabs</a>, 
<a href="/search/eess?searchtype=author&query=Richtsmeier%2C+J+T">Joan T. Richtsmeier</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+D+Z">Danny Z Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Studying the morphological development of cartilaginous and osseous
structures is critical to the early detection of life-threatening skeletal
dysmorphology. Embryonic cartilage undergoes rapid structural changes within
hours, introducing biological variations and morphological shifts that limit
the generalization of deep learning-based segmentation models that infer across
multiple embryonic age groups. Obtaining individual models for each age group
is expensive and less effective, while direct transfer (predicting an age
unseen during training) suffers a potential performance drop due to
morphological shifts. We propose a novel Transformer-based segmentation model
with improved biological priors that better distills morphologically diverse
information through conditional mechanisms. This enables a single model to
accurately predict cartilage across multiple age groups. Experiments on the
mice cartilage dataset show the superiority of our new model compared to other
competitive segmentation models. Additional studies on a separate mice
cartilage dataset with a distinct mutation show that our model generalizes well
and effectively captures age-based cartilage morphology patterns.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03710" title="Abstract">arXiv:2402.03710</a> (cross-list from eess.AS) [<a href="/pdf/2402.03710" title="Download PDF">pdf</a>, <a href="/format/2402.03710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced  Auditory Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+X">Xilin Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+C">Cong Han</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y+A">Yinghao Aaron Li</a>, 
<a href="/search/eess?searchtype=author&query=Mesgarani%2C+N">Nima Mesgarani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">In daily life, we encounter a variety of sounds, both desirable and
undesirable, with limited control over their presence and volume. Our work
introduces "Listen, Chat, and Edit" (LCE), a novel multimodal sound mixture
editor that modifies each sound source in a mixture based on user-provided text
instructions. LCE distinguishes itself with a user-friendly chat interface and
its unique ability to edit multiple sound sources simultaneously within a
mixture, without needing to separate them. Users input open-vocabulary text
prompts, which are interpreted by a large language model to create a semantic
filter for editing the sound mixture. The system then decomposes the mixture
into its components, applies the semantic filter, and reassembles it into the
desired output. We developed a 160-hour dataset with over 100k mixtures,
including speech and various audio sources, along with text prompts for diverse
editing tasks like extraction, removal, and volume control. Our experiments
demonstrate significant improvements in signal quality across all editing tasks
and robust performance in zero-shot scenarios with varying numbers and types of
sound sources.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03724" title="Abstract">arXiv:2402.03724</a> (cross-list from stat.ML) [<a href="/pdf/2402.03724" title="Download PDF">pdf</a>, <a href="/format/2402.03724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Test for Anomaly Detections by Variational Auto-Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Miwa%2C+D">Daiki Miwa</a>, 
<a href="/search/stat?searchtype=author&query=Shiraishi%2C+T">Tomohiro Shiraishi</a>, 
<a href="/search/stat?searchtype=author&query=Duy%2C+V+N+L">Vo Nguyen Le Duy</a>, 
<a href="/search/stat?searchtype=author&query=Katsuoka%2C+T">Teruyuki Katsuoka</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we consider the reliability assessment of anomaly detection
(AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD
has been actively studied in various perspective, from method development to
applied research. However, when the results of ADs are used in high-stakes
decision-making, such as in medical diagnosis, it is necessary to ensure the
reliability of the detected anomalies. In this study, we propose the VAE-AD
Test as a method for quantifying the statistical reliability of VAE-based AD
within the framework of statistical testing. Using the VAE-AD Test, the
reliability of the anomaly regions detected by a VAE can be quantified in the
form of p-values. This means that if an anomaly is declared when the p-value is
below a certain threshold, it is possible to control the probability of false
detection to a desired level. Since the VAE-AD Test is constructed based on a
new statistical inference framework called selective inference, its validity is
theoretically guaranteed in finite samples. To demonstrate the validity and
effectiveness of the proposed VAE-AD Test, numerical experiments on artificial
data and applications to brain image analysis are conducted.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03756" title="Abstract">arXiv:2402.03756</a> (cross-list from math.ST) [<a href="/pdf/2402.03756" title="Download PDF">pdf</a>, <a href="/ps/2402.03756" title="Download PostScript">ps</a>, <a href="/format/2402.03756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform error bounds of the ensemble transform Kalman filter for  infinite-dimensional dynamics with multiplicative covariance inflation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Takeda%2C+K">Kota Takeda</a>, 
<a href="/search/math?searchtype=author&query=Sakajo%2C+T">Takashi Sakajo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Data assimilation is a method of uncertainty quantification to estimate the
hidden true state by updating the prediction owing to model dynamics with
observation data. As a prediction model, we consider a class of nonlinear
dynamical systems on Hilbert spaces including the two-dimensional Navier-Stokes
equations and the Lorenz '63 and '96 equations. For nonlinear model dynamics,
the ensemble Kalman filter (EnKF) is often used to approximate the mean and
covariance of the probability distribution with a set of particles called an
ensemble. In this paper, we consider a deterministic version of the EnKF known
as the ensemble transform Kalman filter (ETKF), performing well even with
limited ensemble sizes in comparison to other stochastic implementations of the
EnKF. When the ETKF is applied to large-scale systems, an ad-hoc numerical
technique called a covariance inflation is often employed to reduce
approximation errors. Despite the practical effectiveness of the ETKF, little
is theoretically known. The present study aims to establish the theoretical
analysis of the ETKF. We obtain that the estimation error of the ETKF with and
without the covariance inflation is bounded for any finite time. In particular,
the uniform-in-time error bound is obtained when an inflation parameter is
chosen appropriately, justifying the effectiveness of the covariance inflation
in the ETKF.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03761" title="Abstract">arXiv:2402.03761</a> (cross-list from eess.IV) [<a href="/pdf/2402.03761" title="Download PDF">pdf</a>, <a href="/ps/2402.03761" title="Download PostScript">ps</a>, <a href="/format/2402.03761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Correction and Unmixing of Hyperspectral Images for  Brain Tumor Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Black%2C+D">David Black</a>, 
<a href="/search/eess?searchtype=author&query=Gill%2C+J">Jaidev Gill</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+A">Andrew Xie</a>, 
<a href="/search/eess?searchtype=author&query=Liquet%2C+B">Benoit Liquet</a>, 
<a href="/search/eess?searchtype=author&query=Di+leva%2C+A">Antonio Di leva</a>, 
<a href="/search/eess?searchtype=author&query=Stummer%2C+W">Walter Stummer</a>, 
<a href="/search/eess?searchtype=author&query=Molina%2C+E+S">Eric Suero Molina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures, 3 tables - Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Tissues and Organs (q-bio.TO)

</div>
<p class="mathjax">Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection
enables visualization of differences between tissues that are not
distinguishable to humans. This augmentation can maximize brain tumor
resection, improving patient outcomes. However, much of the processing in HSI
uses simplified linear methods that are unable to capture the non-linear,
wavelength-dependent phenomena that must be modeled for accurate recovery of
fluorophore abundances. We therefore propose two deep learning models for
correction and unmixing, which can account for the nonlinear effects and
produce more accurate estimates of abundances. Both models use an
autoencoder-like architecture to process the captured spectra. One is trained
with protoporphyrin IX (PpIX) concentration labels. The other undergoes
semi-supervised training, first learning hyperspectral unmixing self-supervised
and then learning to correct fluorescence emission spectra for heterogeneous
optical and geometric properties using a reference white-light reflectance
spectrum in a few-shot manner. The models were evaluated against phantom and
pig brain data with known PpIX concentration; the supervised model achieved
Pearson correlation coefficients (R values) between the known and computed PpIX
concentrations of 0.997 and 0.990, respectively, whereas the classical approach
achieved only 0.93 and 0.82. The semi-supervised approach's R values were 0.98
and 0.91, respectively. On human data, the semi-supervised model gives
qualitatively more realistic results than the classical method, better removing
bright spots of specular reflectance and reducing the variance in PpIX
abundance over biopsies that should be relatively homogeneous. These results
show promise for using deep learning to improve HSI in fluorescence-guided
neurosurgery.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03779" title="Abstract">arXiv:2402.03779</a> (cross-list from stat.ML) [<a href="/pdf/2402.03779" title="Download PDF">pdf</a>, <a href="/format/2402.03779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EERO: Early Exit with Reject Option for Efficient Classification with  limited budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Valade%2C+F">Florian Valade</a> (LAMA), 
<a href="/search/stat?searchtype=author&query=Hebiri%2C+M">Mohamed Hebiri</a> (LAMA), 
<a href="/search/stat?searchtype=author&query=Gay%2C+P">Paul Gay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The increasing complexity of advanced machine learning models requires
innovative approaches to manage computational resources effectively. One such
method is the Early Exit strategy, which allows for adaptive computation by
providing a mechanism to shorten the processing path for simpler data
instances. In this paper, we propose EERO, a new methodology to translate the
problem of early exiting to a problem of using multiple classifiers with reject
option in order to better select the exiting head for each instance. We
calibrate the probabilities of exiting at the different heads using aggregation
with exponential weights to guarantee a fixed budget .We consider factors such
as Bayesian risk, budget constraints, and head-specific budget consumption.
Experimental results, conducted using a ResNet-18 model and a ConvNext
architecture on Cifar and ImageNet datasets, demonstrate that our method not
only effectively manages budget allocation but also enhances accuracy in
overthinking scenarios.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03781" title="Abstract">arXiv:2402.03781</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.03781" title="Download PDF">pdf</a>, <a href="/format/2402.03781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MolTC: Towards Molecular Relational Modeling In Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fang%2C+J">Junfeng Fang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+C">Chang Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Du%2C+W">Wenjie Du</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Molecular Relational Learning (MRL), aiming to understand interactions
between molecular pairs, plays a pivotal role in advancing biochemical
research. Recently, the adoption of large language models (LLMs), known for
their vast knowledge repositories and advanced logical inference capabilities,
has emerged as a promising way for efficient and effective MRL. Despite their
potential, these methods predominantly rely on the textual data, thus not fully
harnessing the wealth of structural information inherent in molecular graphs.
Moreover, the absence of a unified framework exacerbates the information
underutilization, as it hinders the sharing of interaction rationale learned
across diverse datasets. To address these challenges, this work proposes a
novel LLM-based multi-modal framework for Molecular inTeraction prediction
following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently
integrate rich graphical information of molecular pairs. For achieving a
unified MRL, MolTC innovatively develops a dynamic parameter-sharing strategy
for cross-dataset information exchange, and introduces a Multi-hierarchical CoT
principle to refine training paradigm. Our experiments, conducted across twelve
varied datasets involving over 4,000,000 molecular pairs, demonstrate the
superiority of our method over current GNN and LLM-based baselines. On the top
of that, a comprehensive Molecular Interactive Instructions dataset is
constructed for the development of biochemical LLM, including our MolTC. Code
is available at https://github.com/MangoKiller/MolTC.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03806" title="Abstract">arXiv:2402.03806</a> (cross-list from q-fin.RM) [<a href="/pdf/2402.03806" title="Download PDF">pdf</a>, <a href="/ps/2402.03806" title="Download PostScript">ps</a>, <a href="/format/2402.03806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Automated Machine Learning for Credit Decisions: Enhancing  Human Artificial Intelligence Collaboration in Financial Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Schmitt%2C+M">Marc Schmitt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
<p class="mathjax">This paper explores the integration of Explainable Automated Machine Learning
(AutoML) in the realm of financial engineering, specifically focusing on its
application in credit decision-making. The rapid evolution of Artificial
Intelligence (AI) in finance has necessitated a balance between sophisticated
algorithmic decision-making and the need for transparency in these systems. The
focus is on how AutoML can streamline the development of robust machine
learning models for credit scoring, while Explainable AI (XAI) methods,
particularly SHapley Additive exPlanations (SHAP), provide insights into the
models' decision-making processes. This study demonstrates how the combination
of AutoML and XAI not only enhances the efficiency and accuracy of credit
decisions but also fosters trust and collaboration between humans and AI
systems. The findings underscore the potential of explainable AutoML in
improving the transparency and accountability of AI-driven financial decisions,
aligning with regulatory requirements and ethical considerations.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03808" title="Abstract">arXiv:2402.03808</a> (cross-list from eess.SP) [<a href="/pdf/2402.03808" title="Download PDF">pdf</a>, <a href="/format/2402.03808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal  Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu-Tung Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kuan-Chen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+K">Kai-Chun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+S">Sheng-Yu Peng</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Surface electromyography (sEMG) recordings can be influenced by
electrocardiogram (ECG) signals when the muscle being monitored is close to the
heart. Several existing methods use signal-processing-based approaches, such as
high-pass filter and template subtraction, while some derive mapping functions
to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference).
Recently, the score-based diffusion model, a renowned generative model, has
been introduced to generate high-quality and accurate samples with noisy input
data. In this study, we proposed a novel approach, termed SDEMG, as a
score-based diffusion model for sEMG signal denoising. To evaluate the proposed
SDEMG approach, we conduct experiments to reduce noise in sEMG signals,
employing data from an openly accessible source, the Non-Invasive Adaptive
Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus
Rhythm Database. The experiment result indicates that SDEMG outperformed
comparative methods and produced high-quality sEMG samples. The source code of
SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03819" title="Abstract">arXiv:2402.03819</a> (cross-list from stat.ML) [<a href="/pdf/2402.03819" title="Download PDF">pdf</a>, <a href="/format/2402.03819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical and experimental study of SMOTE: limitations and comparisons  of rebalancing strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sakho%2C+A">Abdoulaye Sakho</a> (LPSM), 
<a href="/search/stat?searchtype=author&query=Scornet%2C+E">Erwan Scornet</a> (LPSM), 
<a href="/search/stat?searchtype=author&query=Malherbe%2C+E">Emmanuel Malherbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing
strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE
(with default parameter) regenerates the original distribution by simply
copying the original minority samples. We also prove that SMOTE density
vanishes near the boundary of the support of the minority distribution,
therefore justifying the common BorderLine SMOTE strategy. Then we introduce
two new SMOTE-related strategies, and compare them with state-of-the-art
rebalancing procedures. We show that rebalancing strategies are only required
when the data set is highly imbalanced. For such data sets, SMOTE, our
proposals, or undersampling procedures are the best strategies.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03838" title="Abstract">arXiv:2402.03838</a> (cross-list from stat.ML) [<a href="/pdf/2402.03838" title="Download PDF">pdf</a>, <a href="/format/2402.03838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman  graph kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Perez%2C+R+C">Rapha&#xeb;l Carpintero Perez</a> (CMAP), 
<a href="/search/stat?searchtype=author&query=da+Veiga%2C+S">S&#xe9;bastien da Veiga</a> (ENSAI, CREST), 
<a href="/search/stat?searchtype=author&query=Garnier%2C+J">Josselin Garnier</a> (CMAP), 
<a href="/search/stat?searchtype=author&query=Staber%2C+B">Brian Staber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Supervised learning has recently garnered significant attention in the field
of computational physics due to its ability to effectively extract complex
patterns for tasks like solving partial differential equations, or predicting
material properties. Traditionally, such datasets consist of inputs given as
meshes with a large number of nodes representing the problem geometry (seen as
graphs), and corresponding outputs obtained with a numerical solver. This means
the supervised learning model must be able to handle large and sparse graphs
with continuous node attributes. In this work, we focus on Gaussian process
regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman
(SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL
kernel enjoys positive definiteness and a drastic complexity reduction, which
makes it possible to process datasets that were previously impossible to
handle. The new kernel is first validated on graph classification for molecular
datasets, where the input graphs have a few tens of nodes. The efficiency of
the SWWL kernel is then illustrated on graph regression in computational fluid
dynamics and solid mechanics, where the input graphs are made up of tens of
thousands of nodes.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03859" title="Abstract">arXiv:2402.03859</a> (cross-list from math.AP) [<a href="/pdf/2402.03859" title="Download PDF">pdf</a>, <a href="/format/2402.03859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General boundary conditions for a Boussinesq model with varying  bathymetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lannes%2C+D">David Lannes</a>, 
<a href="/search/math?searchtype=author&query=Rigal%2C+M">Mathieu Rigal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Mathematical Physics (math-ph); Numerical Analysis (math.NA); Atmospheric and Oceanic Physics (physics.ao-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">This paper is devoted to the theoretical and numerical investigation of the
initial boundary value problem for a system of equations used for the
description of waves in coastal areas, namely, the Boussinesq-Abbott system in
the presence of topography. We propose a procedure that allows one to handle
very general linear or nonlinear boundary conditions. It consists in reducing
the problem to a system of conservation laws with nonlocal fluxes and coupled
to an ODE. This reformulation is used to propose two hybrid finite
volumes/finite differences schemes of first and second order respectively. The
possibility to use many kinds of boundary conditions is used to investigate
numerically the asymptotic stability of the boundary conditions, which is an
issue of practical relevance in coastal oceanography since asymptotically
stable boundary conditions would allow one to reconstruct a wave field from the
knowledge of the boundary data only, even if the initial data is not known.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03871" title="Abstract">arXiv:2402.03871</a> (cross-list from quant-ph) [<a href="/pdf/2402.03871" title="Download PDF">pdf</a>, <a href="/format/2402.03871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric quantum machine learning of BQP$^A$ protocols and latent graph  classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Umeano%2C+C">Chukwudubem Umeano</a>, 
<a href="/search/quant-ph?searchtype=author&query=Elfving%2C+V+E">Vincent E. Elfving</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kyriienko%2C+O">Oleksandr Kyriienko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">Geometric quantum machine learning (GQML) aims to embed problem symmetries
for learning efficient solving protocols. However, the question remains if
(G)QML can be routinely used for constructing protocols with an exponential
separation from classical analogs. In this Letter we consider Simon's problem
for learning properties of Boolean functions, and show that this can be related
to an unsupervised circuit classification problem. Using the workflow of
geometric QML, we learn from first principles Simon's algorithm, thus
discovering an example of BQP$^A\neq$BPP protocol with respect to some dataset
(oracle $A$). Our key findings include the development of an equivariant
feature map for embedding Boolean functions, based on twirling with respect to
identified bitflip and permutational symmetries, and measurement based on
invariant observables with a sampling advantage. The proposed workflow points
to the importance of data embeddings and classical post-processing, while
keeping the variational circuit as a trivial identity operator. Next,
developing the intuition for the function learning, we visualize instances as
directed computational hypergraphs, and observe that the GQML protocol can
access their global topological features for distinguishing bijective and
surjective functions. Finally, we discuss the prospects for learning other
BQP$^A$-type protocols, and conjecture that this depends on the ability of
simplifying embeddings-based oracles $A$ applied as a linear combination of
unitaries.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03883" title="Abstract">arXiv:2402.03883</a> (cross-list from math.OC) [<a href="/pdf/2402.03883" title="Download PDF">pdf</a>, <a href="/format/2402.03883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Bilevel Optimization on Riemannian Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Han%2C+A">Andi Han</a>, 
<a href="/search/math?searchtype=author&query=Mishra%2C+B">Bamdev Mishra</a>, 
<a href="/search/math?searchtype=author&query=Jawanpuria%2C+P">Pratik Jawanpuria</a>, 
<a href="/search/math?searchtype=author&query=Takeda%2C+A">Akiko Takeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Bilevel optimization has seen an increasing presence in various domains of
applications. In this work, we propose a framework for solving bilevel
optimization problems where variables of both lower and upper level problems
are constrained on Riemannian manifolds. We provide several hypergradient
estimation strategies on manifolds and study their estimation error. We provide
convergence and complexity analysis for the proposed hypergradient descent
algorithm on manifolds. We also extend the developments to stochastic bilevel
optimization and to the use of general retraction. We showcase the utility of
the proposed framework on various applications.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03931" title="Abstract">arXiv:2402.03931</a> (cross-list from cond-mat.mes-hall) [<a href="/pdf/2402.03931" title="Download PDF">pdf</a>, <a href="/format/2402.03931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully autonomous tuning of a spin qubit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Schuff%2C+J">Jonas Schuff</a>, 
<a href="/search/cond-mat?searchtype=author&query=Carballido%2C+M+J">Miguel J. Carballido</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kotzagiannidis%2C+M">Madeleine Kotzagiannidis</a>, 
<a href="/search/cond-mat?searchtype=author&query=Calvo%2C+J+C">Juan Carlos Calvo</a>, 
<a href="/search/cond-mat?searchtype=author&query=Caselli%2C+M">Marco Caselli</a>, 
<a href="/search/cond-mat?searchtype=author&query=Rawling%2C+J">Jacob Rawling</a>, 
<a href="/search/cond-mat?searchtype=author&query=Craig%2C+D+L">David L. Craig</a>, 
<a href="/search/cond-mat?searchtype=author&query=van+Straaten%2C+B">Barnaby van Straaten</a>, 
<a href="/search/cond-mat?searchtype=author&query=Severin%2C+B">Brandon Severin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fedele%2C+F">Federico Fedele</a>, 
<a href="/search/cond-mat?searchtype=author&query=Svab%2C+S">Simon Svab</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kwon%2C+P+C">Pierre Chevalier Kwon</a>, 
<a href="/search/cond-mat?searchtype=author&query=Eggli%2C+R+S">Rafael S. Eggli</a>, 
<a href="/search/cond-mat?searchtype=author&query=Patlatiuk%2C+T">Taras Patlatiuk</a>, 
<a href="/search/cond-mat?searchtype=author&query=Korda%2C+N">Nathan Korda</a>, 
<a href="/search/cond-mat?searchtype=author&query=Zumb%C3%BChl%2C+D">Dominik Zumb&#xfc;hl</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ares%2C+N">Natalia Ares</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mesoscale and Nanoscale Physics (cond-mat.mes-hall)</span>; Machine Learning (cs.LG); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Spanning over two decades, the study of qubits in semiconductors for quantum
computing has yielded significant breakthroughs. However, the development of
large-scale semiconductor quantum circuits is still limited by challenges in
efficiently tuning and operating these circuits. Identifying optimal operating
conditions for these qubits is complex, involving the exploration of vast
parameter spaces. This presents a real 'needle in the haystack' problem, which,
until now, has resisted complete automation due to device variability and
fabrication imperfections. In this study, we present the first fully autonomous
tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a
clear indication of successful qubit operation. We demonstrate this automation,
achieved without human intervention, in a Ge/Si core/shell nanowire device. Our
approach integrates deep learning, Bayesian optimization, and computer vision
techniques. We expect this automation algorithm to apply to a wide range of
semiconductor qubit devices, allowing for statistical studies of qubit quality
metrics. As a demonstration of the potential of full automation, we
characterise how the Rabi frequency and g-factor depend on barrier gate
voltages for one of the qubits found by the algorithm. Twenty years after the
initial demonstrations of spin qubit operation, this significant advancement is
poised to finally catalyze the operation of large, previously unexplored
quantum circuits.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03961" title="Abstract">arXiv:2402.03961</a> (cross-list from nlin.CG) [<a href="/pdf/2402.03961" title="Download PDF">pdf</a>, <a href="/format/2402.03961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Reproduction and Evolution in Cellular Automata: 25 Years after  Evoloops
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Sayama%2C+H">Hiroki Sayama</a>, 
<a href="/search/nlin?searchtype=author&query=Nehaniv%2C+C+L">Chrystopher L. Nehaniv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cellular Automata and Lattice Gases (nlin.CG)</span>; Neural and Evolutionary Computing (cs.NE); Pattern Formation and Solitons (nlin.PS); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">The year of 2024 marks the 25th anniversary of the publication of evoloops,
an evolutionary variant of Chris Langton's self-reproducing loops which proved
that Darwinian evolution of self-reproducing organisms by variation and natural
selection is possible within deterministic cellular automata. Over the last few
decades, this line of Artificial Life research has since undergone several
important developments. Although it experienced a relative dormancy of
activities for a while, the recent rise of interest in open-ended evolution and
the success of continuous cellular automata models have brought researchers'
attention back to how to make spatio-temporal patterns self-reproduce and
evolve within spatially distributed computational media. This article provides
a review of the relevant literature on this topic over the past 25 years and
highlights the major accomplishments made so far, the challenges being faced,
and promising future research directions.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03982" title="Abstract">arXiv:2402.03982</a> (cross-list from math.OC) [<a href="/pdf/2402.03982" title="Download PDF">pdf</a>, <a href="/ps/2402.03982" title="Download PostScript">ps</a>, <a href="/format/2402.03982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Convergence of Adam for Stochastic Optimization under Relaxed  Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hong%2C+Y">Yusu Hong</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+J">Junhong Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Adaptive Momentum Estimation (Adam) algorithm is highly effective in
training various deep learning tasks. Despite this, there's limited theoretical
understanding for Adam, especially when focusing on its vanilla form in
non-convex smooth scenarios with potential unbounded gradients and affine
variance noise. In this paper, we study vanilla Adam under these challenging
conditions. We introduce a comprehensive noise model which governs affine
variance noise, bounded noise and sub-Gaussian noise. We show that Adam can
find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate
in high probability under this general noise model where $T$ denotes total
number iterations, matching the lower rate of stochastic first-order algorithms
up to logarithm factors. More importantly, we reveal that Adam is free of
tuning step-sizes with any problem-parameters, yielding a better adaptation
property than the Stochastic Gradient Descent under the same conditions. We
also provide a probabilistic convergence result for Adam under a generalized
smooth condition which allows unbounded smoothness parameters and has been
illustrated empirically to more accurately capture the smooth property of many
practical objective functions.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03988" title="Abstract">arXiv:2402.03988</a> (cross-list from eess.AS) [<a href="/pdf/2402.03988" title="Download PDF">pdf</a>, <a href="/format/2402.03988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tseng%2C+L">Liang-Hsuan Tseng</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+E">En-Pei Hu</a>, 
<a href="/search/eess?searchtype=author&query=Chiang%2C+C">Cheng-Han Chiang</a>, 
<a href="/search/eess?searchtype=author&query=Tseng%2C+Y">Yuan Tseng</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+L">Lin-shan Lee</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+S">Shao-Hua Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Unsupervised automatic speech recognition (ASR) aims to learn the mapping
between the speech signal and its corresponding textual transcription without
the supervision of paired speech-text data. A word/phoneme in the speech signal
is represented by a segment of speech signal with variable length and unknown
boundary, and this segmental structure makes learning the mapping between
speech and text challenging, especially without paired data. In this paper, we
propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative
Training for Unsupervised ASR. REBORN alternates between (1) training a
segmentation model that predicts the boundaries of the segmental structures in
speech signals and (2) training the phoneme prediction model, whose input is a
segmental structure segmented by the segmentation model, to predict a phoneme
transcription. Since supervised data for training the segmentation model is not
available, we use reinforcement learning to train the segmentation model to
favor segmentations that yield phoneme sequence predictions with a lower
perplexity. We conduct extensive experiments and find that under the same
setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech,
TIMIT, and five non-English languages in Multilingual LibriSpeech. We
comprehensively analyze why the boundaries learned by REBORN improve the
unsupervised ASR performance.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03990" title="Abstract">arXiv:2402.03990</a> (cross-list from stat.ML) [<a href="/pdf/2402.03990" title="Download PDF">pdf</a>, <a href="/format/2402.03990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subsampling is not Magic: Why Large Batch Sizes Work for Differentially  Private Stochastic Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=R%C3%A4is%C3%A4%2C+O">Ossi R&#xe4;is&#xe4;</a>, 
<a href="/search/stat?searchtype=author&query=J%C3%A4lk%C3%B6%2C+J">Joonas J&#xe4;lk&#xf6;</a>, 
<a href="/search/stat?searchtype=author&query=Honkela%2C+A">Antti Honkela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the effect of the batch size to the total gradient variance in
differentially private stochastic gradient descent (DP-SGD), seeking a
theoretical explanation for the usefulness of large batch sizes. As DP-SGD is
the basis of modern DP deep learning, its properties have been widely studied,
and recent works have empirically found large batch sizes to be beneficial.
However, theoretical explanations of this benefit are currently heuristic at
best. We first observe that the total gradient variance in DP-SGD can be
decomposed into subsampling-induced and noise-induced variances. We then prove
that in the limit of an infinite number of iterations, the effective
noise-induced variance is invariant to the batch size. The remaining
subsampling-induced variance decreases with larger batch sizes, so large
batches reduce the effective total gradient variance. We confirm numerically
that the asymptotic regime is relevant in practical settings when the batch
size is not small, and find that outside the asymptotic regime, the total
gradient variance decreases even more with large batch sizes. We also find a
sufficient condition that implies that large batch sizes similarly reduce
effective DP noise variance for one iteration of DP-SGD.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04022" title="Abstract">arXiv:2402.04022</a> (cross-list from stat.ML) [<a href="/pdf/2402.04022" title="Download PDF">pdf</a>, <a href="/format/2402.04022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory for Kernel Packets: from state space model to compactly  supported basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/stat?searchtype=author&query=Rui%2C+T">Tuo Rui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It is well known that the state space (SS) model formulation of a Gaussian
process (GP) can lower its training and prediction time both to O(n) for n data
points. We prove that an $m$-dimensional SS model formulation of GP is
equivalent to a concept we introduce as the general right Kernel Packet (KP): a
transformation for the GP covariance function $K$ such that
$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j
\leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes
$j$-th order derivative acting on $t$. We extend this idea to the backward SS
model formulation of the GP, leading to the concept of the left KP for next $m$
consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq
t_{2m}$. By combining both left and right KPs, we can prove that a suitable
linear combination of these covariance functions yields $m$ compactly supported
KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and
$j=0,\cdots,m-1$. KPs further reduces the prediction time of GP to O(log n) or
even O(1) and can be applied to more general problems involving the derivative
of GPs.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04038" title="Abstract">arXiv:2402.04038</a> (cross-list from stat.ML) [<a href="/pdf/2402.04038" title="Download PDF">pdf</a>, <a href="/ps/2402.04038" title="Download PostScript">ps</a>, <a href="/format/2402.04038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sun%2C+T">Tan Sun</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+J">Junhong Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph neural networks (GNNs) have gained popularity for various graph-related
tasks. However, similar to deep neural networks, GNNs are also vulnerable to
adversarial attacks. Empirical studies have shown that adversarially robust
generalization has a pivotal role in establishing effective defense algorithms
against adversarial attacks. In this paper, we contribute by providing
adversarially robust generalization bounds for two kinds of popular GNNs, graph
convolutional network (GCN) and message passing graph neural network, using the
PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion
matrix on the graph and spectral norm of the weights as well as the
perturbation factor govern the robust generalization bounds of both models. Our
bounds are nontrivial generalizations of the results developed in (Liao et al.,
2020) from the standard setting to adversarial setting while avoiding
exponential dependence of the maximum node degree. As corollaries, we derive
better PAC-Bayesian robust generalization bounds for GCN in the standard
setting, which improve the bounds in (Liao et al., 2020) by avoiding
exponential dependence on the maximum node degree.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04100" title="Abstract">arXiv:2402.04100</a> (cross-list from physics.optics) [<a href="/pdf/2402.04100" title="Download PDF">pdf</a>, <a href="/format/2402.04100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A programmable photonic memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ashtiani%2C+F">Farshid Ashtiani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The significant advancements in integrated photonics have enabled high-speed
and energy efficient systems for various applications from data communications
and high-performance computing, to medical diagnosis, sensing and ranging.
However, data storage in these systems has been dominated by electronic
memories which necessitates signal conversion between optical and electrical as
well as analog and digital domains, and data movement between processor and
memory that reduce the speed and energy efficiency. To date, a scalable optical
memory with optical control has remained an open problem. Here we report an
integrated photonic set-reset latch as a fundamental optical static memory unit
based on universal optical logic gates. While the proposed memory is compatible
with different photonic platforms, its functionality is demonstrated on a
programmable silicon photonic chip as a proof of concept. Optical set, reset,
and complementary outputs, scalability to a large number of memory units via
the independent latch supply light, and compatibility with different photonic
platforms enable more efficient and lower latency optical processing systems.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04114" title="Abstract">arXiv:2402.04114</a> (cross-list from stat.ML) [<a href="/pdf/2402.04114" title="Download PDF">pdf</a>, <a href="/format/2402.04114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated  Linear Stochastic Approximation and Temporal Difference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mangold%2C+P">Paul Mangold</a>, 
<a href="/search/stat?searchtype=author&query=Samsonov%2C+S">Sergey Samsonov</a>, 
<a href="/search/stat?searchtype=author&query=Labbi%2C+S">Safwan Labbi</a>, 
<a href="/search/stat?searchtype=author&query=Levin%2C+I">Ilya Levin</a>, 
<a href="/search/stat?searchtype=author&query=Alami%2C+R">Reda Alami</a>, 
<a href="/search/stat?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we perform a non-asymptotic analysis of the federated linear
stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias
introduced by local training with heterogeneous agents, and investigate the
sample complexity of the algorithm. We show that the communication complexity
of FedLSA scales polynomially with the desired precision $\epsilon$, which
limits the benefits of federation. To overcome this, we propose SCAFFLSA, a
novel variant of FedLSA, that uses control variates to correct the bias of
local training, and prove its convergence without assumptions on statistical
heterogeneity. We apply the proposed methodology to federated temporal
difference learning with linear function approximation, and analyze the
corresponding complexity improvements.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04118" title="Abstract">arXiv:2402.04118</a> (cross-list from math.AP) [<a href="/pdf/2402.04118" title="Download PDF">pdf</a>, <a href="/ps/2402.04118" title="Download PostScript">ps</a>, <a href="/format/2402.04118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An explicit Euler method for the continuity equation with Sobolev  velocity fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cortopassi%2C+T">Tommaso Cortopassi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages. The note on <a href="/abs/2310.03871">arXiv:2310.03871</a> has been included to keep the work self-contained
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We prove a stability estimate, in a suitable expected value, of the
$1$-Wasserstein distance between the solution of the continuity equation under
a Sobolev velocity field and a measure obtained by pushing forward Dirac deltas
whose centers belong to a partition of the domain by a (sort of) explicit
forward Euler method. The main tool is a $L^\infty_t (L^p_x)$ estimate on the
difference between the regular Lagrangian flow of the velocity field and an
explicitly constructed approximation of such flow. Although our result only
gives estimates in expected value, it has the advantage of being easily
parallelizable and of not relying on any particular structure on the mesh. At
the end, we also provide estimates with a logarithmic Wasserstein distance,
already used in other works on this particular problem.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04146" title="Abstract">arXiv:2402.04146</a> (cross-list from stat.ML) [<a href="/pdf/2402.04146" title="Download PDF">pdf</a>, <a href="/format/2402.04146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian  Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ravi%2C+S+K">Sandipp Krishnan Ravi</a>, 
<a href="/search/stat?searchtype=author&query=Comlek%2C+Y">Yigitcan Comlek</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/stat?searchtype=author&query=Pathak%2C+A">Arjun Pathak</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+V">Vipul Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Umretiya%2C+R">Rajnikant Umretiya</a>, 
<a href="/search/stat?searchtype=author&query=Hoffman%2C+A">Andrew Hoffman</a>, 
<a href="/search/stat?searchtype=author&query=Pilania%2C+G">Ghanshyam Pilania</a>, 
<a href="/search/stat?searchtype=author&query=Pandita%2C+P">Piyush Pandita</a>, 
<a href="/search/stat?searchtype=author&query=Ghosh%2C+S">Sayan Ghosh</a>, 
<a href="/search/stat?searchtype=author&query=Mckeever%2C+N">Nathaniel Mckeever</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+L">Liping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 Pages,9 Figures, 3 Supplementary Figures, 2 Supplementary Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the advent of artificial intelligence (AI) and machine learning (ML),
various domains of science and engineering communites has leveraged data-driven
surrogates to model complex systems from numerous sources of information
(data). The proliferation has led to significant reduction in cost and time
involved in development of superior systems designed to perform specific
functionalities. A high proposition of such surrogates are built extensively
fusing multiple sources of data, may it be published papers, patents, open
repositories, or other resources. However, not much attention has been paid to
the differences in quality and comprehensiveness of the known and unknown
underlying physical parameters of the information sources that could have
downstream implications during system optimization. Towards resolving this
issue, a multi-source data fusion framework based on Latent Variable Gaussian
Process (LVGP) is proposed. The individual data sources are tagged as a
characteristic categorical variable that are mapped into a physically
interpretable latent space, allowing the development of source-aware data
fusion modeling. Additionally, a dissimilarity metric based on the latent
variables of LVGP is introduced to study and understand the differences in the
sources of data. The proposed approach is demonstrated on and analyzed through
two mathematical (representative parabola problem, 2D Ackley function) and two
materials science (design of FeCrAl and SmCoFe alloys) case studies. From the
case studies, it is observed that compared to using single-source and source
unaware ML models, the proposed multi-source data fusion framework can provide
better predictions for sparse-data problems, interpretability regarding the
sources, and enhanced modeling capabilities by taking advantage of the
correlations and relationships among different sources.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04171" title="Abstract">arXiv:2402.04171</a> (cross-list from eess.IV) [<a href="/pdf/2402.04171" title="Download PDF">pdf</a>, <a href="/format/2402.04171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ha%2C+J">Juhyung Ha</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+N">Nian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Maharjan%2C+S">Surendra Maharjan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xuhong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This study introduces the 3D Residual-in-Residual Dense Block GAN (3D
RRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D
RRDB-GAN is the integration of a 2.5D perceptual loss function, which
contributes to improved volumetric image quality and realism. The effectiveness
of our model was evaluated through 4x super-resolution experiments across
diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6.
These evaluations, encompassing both quantitative metrics like LPIPS and FID
and qualitative assessments through sample visualizations, demonstrate the
models effectiveness in detailed image analysis. The 3D RRDB-GAN offers a
significant contribution to medical imaging, particularly by enriching the
depth, clarity, and volumetric detail of medical images. Its application shows
promise in enhancing the interpretation and analysis of complex medical imagery
from a comprehensive 3D perspective.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04241" title="Abstract">arXiv:2402.04241</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.04241" title="Download PDF">pdf</a>, <a href="/ps/2402.04241" title="Download PostScript">ps</a>, <a href="/format/2402.04241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic identifiability of partial differential equation models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Byrne%2C+H">Helen Byrne</a>, 
<a href="/search/q-bio?searchtype=author&query=Harrington%2C+H">Heather Harrington</a>, 
<a href="/search/q-bio?searchtype=author&query=Ovchinnikov%2C+A">Alexey Ovchinnikov</a>, 
<a href="/search/q-bio?searchtype=author&query=Pogudin%2C+G">Gleb Pogudin</a>, 
<a href="/search/q-bio?searchtype=author&query=Rahkooy%2C+H">Hamid Rahkooy</a>, 
<a href="/search/q-bio?searchtype=author&query=Soto%2C+P">Pedro Soto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Symbolic Computation (cs.SC); Systems and Control (eess.SY); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">Differential equation models are crucial to scientific processes. The values
of model parameters are important for analyzing the behaviour of solutions. A
parameter is called globally identifiable if its value can be uniquely
determined from the input and output functions. To determine if a parameter
estimation problem is well-posed for a given model, one must check if the model
parameters are globally identifiable. This problem has been intensively studied
for ordinary differential equation models, with theory and several efficient
algorithms and software packages developed. A comprehensive theory of algebraic
identifiability for PDEs has hitherto not been developed due to the complexity
of initial and boundary conditions. Here, we provide theory and algorithms,
based on differential algebra, for testing identifiability of polynomial PDE
models. We showcase this approach on PDE models arising in the sciences.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed,  7 Feb 24</h3>
<dl>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.05386" title="Abstract">arXiv:1904.05386</a> (replaced) [<a href="/pdf/1904.05386" title="Download PDF">pdf</a>, <a href="/format/1904.05386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fake News, Disinformation, and Deepfakes: Leveraging Distributed Ledger  Technologies and Blockchain to Combat Digital Deception and Counterfeit  Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Caram%C3%A9s%2C+T+M">Tiago M. Fern&#xe1;ndez-Caram&#xe9;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version of IT Professional journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in: IT Professional ( Volume: 22, Issue: 2, 01
  March-April 2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.06486" title="Abstract">arXiv:1908.06486</a> (replaced) [<a href="/pdf/1908.06486" title="Download PDF">pdf</a>, <a href="/format/1908.06486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Independence Testing for Temporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shen%2C+C">Cencheng Shen</a>, 
<a href="/search/stat?searchtype=author&query=Chung%2C+J">Jaewon Chung</a>, 
<a href="/search/stat?searchtype=author&query=Mehta%2C+R">Ronak Mehta</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+T">Ting Xu</a>, 
<a href="/search/stat?searchtype=author&query=Vogelstein%2C+J+T">Joshua T. Vogelstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages main + 10 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.01095" title="Abstract">arXiv:2001.01095</a> (replaced) [<a href="/pdf/2001.01095" title="Download PDF">pdf</a>, <a href="/format/2001.01095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Dimensional Independence Testing via Maximum and Average Distance  Correlations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shen%2C+C">Cencheng Shen</a>, 
<a href="/search/stat?searchtype=author&query=Dong%2C+Y">Yuexiao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages main + 5 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.09218" title="Abstract">arXiv:2005.09218</a> (replaced) [<a href="/pdf/2005.09218" title="Download PDF">pdf</a>, <a href="/format/2005.09218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+J">Jia-Fong Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hsin-Ying Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+B">Bing-Chen Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-Rong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Ping-Chia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W+H">Winston H. Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of the CDFSL competition report (in CVPRW'20), archived
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.09311" title="Abstract">arXiv:2009.09311</a> (replaced) [<a href="/pdf/2009.09311" title="Download PDF">pdf</a>, <a href="/ps/2009.09311" title="Download PostScript">ps</a>, <a href="/format/2009.09311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Codes on Higher Dimensional Varieties Via Grothendieck&#x27;s  Residue Symbol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grant%2C+D">David Grant</a>, 
<a href="/search/math?searchtype=author&query=Massman%2C+J+D">John D. Massman, III</a>, 
<a href="/search/math?searchtype=author&query=Srimathy%2C+S">S. Srimathy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> major changes; new contents; final version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Pure and Applied Algebra, Volume 228, Issue 4, April
  2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.05784" title="Abstract">arXiv:2010.05784</a> (replaced) [<a href="/pdf/2010.05784" title="Download PDF">pdf</a>, <a href="/format/2010.05784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Calibrated Uncertainties for Domain Shift: A Distributionally  Robust Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiding Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yisong Yue</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.05530" title="Abstract">arXiv:2011.05530</a> (replaced) [<a href="/pdf/2011.05530" title="Download PDF">pdf</a>, <a href="/format/2011.05530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+R+E">Ramy E. Ali</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+J">Jinhyun So</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+A+S">A. Salman Avestimehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.11877" title="Abstract">arXiv:2011.11877</a> (replaced) [<a href="/pdf/2011.11877" title="Download PDF">pdf</a>, <a href="/format/2011.11877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstaHide&#x27;s Sample Complexity When Mixing Two Private Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baihe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Runzhou Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.09894" title="Abstract">arXiv:2101.09894</a> (replaced) [<a href="/pdf/2101.09894" title="Download PDF">pdf</a>, <a href="/format/2101.09894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alternating Direction Method of Multipliers-Based Parallel Optimization  for Multi-Agent Collision-Free Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zilong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zicheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=de+Silva%2C+C+W">Clarence W. de Silva</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+H">Tong Heng Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02926" title="Abstract">arXiv:2106.02926</a> (replaced) [<a href="/pdf/2106.02926" title="Download PDF">pdf</a>, <a href="/ps/2106.02926" title="Download PostScript">ps</a>, <a href="/format/2106.02926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IM-META: Influence Maximization Using Node Metadata in Networks With  Unknown Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+C">Cong Tran</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Won-Yong Shin</a>, 
<a href="/search/cs?searchtype=author&query=Spitz%2C+A">Andreas Spitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures, 4 tables, to appear in the IEEE Transactions on Network Science and Engineering (Please cite our journal version that will appear in an upcoming issue.)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07106" title="Abstract">arXiv:2106.07106</a> (replaced) [<a href="/pdf/2106.07106" title="Download PDF">pdf</a>, <a href="/format/2106.07106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alignment and Comparison of Directed Networks via Transition Couplings  of Random Walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+B">Bongsoo Yi</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+K">Kevin O&#x27;Connor</a>, 
<a href="/search/cs?searchtype=author&query=McGoff%2C+K">Kevin McGoff</a>, 
<a href="/search/cs?searchtype=author&query=Nobel%2C+A+B">Andrew B. Nobel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.10901" title="Abstract">arXiv:2106.10901</a> (replaced) [<a href="/pdf/2106.10901" title="Download PDF">pdf</a>, <a href="/format/2106.10901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software-Based Dialogue Systems: Survey, Taxonomy and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motger%2C+Q">Quim Motger</a>, 
<a href="/search/cs?searchtype=author&query=Franch%2C+X">Xavier Franch</a>, 
<a href="/search/cs?searchtype=author&query=Marco%2C+J">Jordi Marco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.04567" title="Abstract">arXiv:2108.04567</a> (replaced) [<a href="/pdf/2108.04567" title="Download PDF">pdf</a>, <a href="/format/2108.04567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babarahmati%2C+K+K">Keyhan Kouhkiloui Babarahmati</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+M">Mohammadreza Kasaei</a>, 
<a href="/search/cs?searchtype=author&query=Tiseo%2C+C">Carlo Tiseo</a>, 
<a href="/search/cs?searchtype=author&query=Mistry%2C+M">Michael Mistry</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+S">Sethu Vijayakumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.08677" title="Abstract">arXiv:2108.08677</a> (replaced) [<a href="/pdf/2108.08677" title="Download PDF">pdf</a>, <a href="/format/2108.08677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order Optimal Bounds for One-Shot Federated Learning over non-Convex  Loss Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharifnassab%2C+A">Arsalan Sharifnassab</a>, 
<a href="/search/cs?searchtype=author&query=Salehkaleybar%2C+S">Saber Salehkaleybar</a>, 
<a href="/search/cs?searchtype=author&query=Golestani%2C+S+J">S. Jamaloddin Golestani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03854" title="Abstract">arXiv:2110.03854</a> (replaced) [<a href="/pdf/2110.03854" title="Download PDF">pdf</a>, <a href="/format/2110.03854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning 3D Shape Segmentation Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuaihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.04234" title="Abstract">arXiv:2110.04234</a> (replaced) [<a href="/pdf/2110.04234" title="Download PDF">pdf</a>, <a href="/format/2110.04234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extremum Seeking Tracking for Derivative-free Distributed Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mimmo%2C+N">Nicola Mimmo</a>, 
<a href="/search/math?searchtype=author&query=Carnevale%2C+G">Guido Carnevale</a>, 
<a href="/search/math?searchtype=author&query=Testa%2C+A">Andrea Testa</a>, 
<a href="/search/math?searchtype=author&query=Notarstefano%2C+G">Giuseppe Notarstefano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.08973" title="Abstract">arXiv:2110.08973</a> (replaced) [<a href="/pdf/2110.08973" title="Download PDF">pdf</a>, <a href="/format/2110.08973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Bounds on the Spooky Pebble Game: Recycling Qubits with  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kornerup%2C+N">Niels Kornerup</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sadun%2C+J">Jonathan Sadun</a>, 
<a href="/search/quant-ph?searchtype=author&query=Soloveichik%2C+D">David Soloveichik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 5 figures, presented as a poster at QIP 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.10307" title="Abstract">arXiv:2110.10307</a> (replaced) [<a href="/pdf/2110.10307" title="Download PDF">pdf</a>, <a href="/format/2110.10307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Secret Sharing over a Public Channel from Correlated Random  Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+R+A">Remi A. Chou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 6 figures, two-column, accepted to IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.15842" title="Abstract">arXiv:2110.15842</a> (replaced) [<a href="/pdf/2110.15842" title="Download PDF">pdf</a>, <a href="/ps/2110.15842" title="Download PostScript">ps</a>, <a href="/format/2110.15842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equiangular lines via matrix projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balla%2C+I">Igor Balla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, LaTeX; Some more improvements and new references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT); Metric Geometry (math.MG); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.07913" title="Abstract">arXiv:2111.07913</a> (replaced) [<a href="/pdf/2111.07913" title="Download PDF">pdf</a>, <a href="/ps/2111.07913" title="Download PostScript">ps</a>, <a href="/format/2111.07913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Circuit Diameter Bounds via Circuit Imbalances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dadush%2C+D">Daniel Dadush</a>, 
<a href="/search/math?searchtype=author&query=Koh%2C+Z+K">Zhuan Khye Koh</a>, 
<a href="/search/math?searchtype=author&query=Natura%2C+B">Bento Natura</a>, 
<a href="/search/math?searchtype=author&query=V%C3%A9gh%2C+L+A">L&#xe1;szl&#xf3; A. V&#xe9;gh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08748" title="Abstract">arXiv:2111.08748</a> (replaced) [<a href="/pdf/2111.08748" title="Download PDF">pdf</a>, <a href="/format/2111.08748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-based Diffusion Approximated Markov Decision Processes for  Off-Road Autonomous Navigation and Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junhong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Kai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+J+M">Jason M. Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Stump%2C+E+A">Ethan A. Stump</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lantao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Journal of Robotics Research. arXiv admin note: text overlap with <a href="/abs/2006.02008">arXiv:2006.02008</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.10735" title="Abstract">arXiv:2112.10735</a> (replaced) [<a href="/pdf/2112.10735" title="Download PDF">pdf</a>, <a href="/ps/2112.10735" title="Download PostScript">ps</a>, <a href="/format/2112.10735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successive Cancellation Ordered Search Decoding of Modified  $\boldsymbol{G}_N$-Coset Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+P">Peihong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Co%C5%9Fkun%2C+M+C">Mustafa Cemil Co&#x15f;kun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, 4 tables. To appear in IEEE TCOM. arXiv admin note: text overlap with <a href="/abs/2105.04048">arXiv:2105.04048</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03457" title="Abstract">arXiv:2202.03457</a> (replaced) [<a href="/pdf/2202.03457" title="Download PDF">pdf</a>, <a href="/format/2202.03457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selecting Seed Words for Wordle using Character Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Silva%2C+N">Nisansa de Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09724" title="Abstract">arXiv:2202.09724</a> (replaced) [<a href="/pdf/2202.09724" title="Download PDF">pdf</a>, <a href="/format/2202.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes-Optimal Classifiers under Group Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zeng%2C+X">Xianli Zeng</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This technical report has been largely superseded by our later paper: "Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing'' (<a href="/abs/2402.02817">arXiv:2402.02817</a>). Please cite that one instead of this technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12329" title="Abstract">arXiv:2202.12329</a> (replaced) [<a href="/pdf/2202.12329" title="Download PDF">pdf</a>, <a href="/format/2202.12329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dynamic Low-Rank Fast Gaussian Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baihe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Weinstein%2C+O">Omri Weinstein</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruizhe Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13059" title="Abstract">arXiv:2202.13059</a> (replaced) [<a href="/pdf/2202.13059" title="Download PDF">pdf</a>, <a href="/format/2202.13059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Error Analysis of Entropy Approximation for Gaussian Mixture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Furuya%2C+T">Takashi Furuya</a>, 
<a href="/search/stat?searchtype=author&query=Kusumoto%2C+H">Hiroyuki Kusumoto</a>, 
<a href="/search/stat?searchtype=author&query=Taniguchi%2C+K">Koichi Taniguchi</a>, 
<a href="/search/stat?searchtype=author&query=Kanno%2C+N">Naoya Kanno</a>, 
<a href="/search/stat?searchtype=author&query=Suetake%2C+K">Kazuma Suetake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.12604" title="Abstract">arXiv:2204.12604</a> (replaced) [<a href="/pdf/2204.12604" title="Download PDF">pdf</a>, <a href="/format/2204.12604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-theoretic multi-time-scale partially observable systems with  inspiration from leukemia treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chapman%2C+M+P">Margaret P. Chapman</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+E">Emily Jensen</a>, 
<a href="/search/eess?searchtype=author&query=Chan%2C+S+M">Steven M. Chan</a>, 
<a href="/search/eess?searchtype=author&query=Lessard%2C+L">Laurent Lessard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07707" title="Abstract">arXiv:2205.07707</a> (replaced) [<a href="/pdf/2205.07707" title="Download PDF">pdf</a>, <a href="/ps/2205.07707" title="Download PostScript">ps</a>, <a href="/format/2205.07707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the rigidity of Arnoux-Rauzy words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berth%C3%A9%2C+V">Val&#xe9;rie Berth&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Puzynina%2C+S">Svetlana Puzynina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15059" title="Abstract">arXiv:2205.15059</a> (replaced) [<a href="/pdf/2205.15059" title="Download PDF">pdf</a>, <a href="/format/2205.15059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hilbert Curve Projection Distance for Distribution Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Cheng Meng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06294" title="Abstract">arXiv:2207.06294</a> (replaced) [<a href="/pdf/2207.06294" title="Download PDF">pdf</a>, <a href="/format/2207.06294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning Assisted Recursive QAOA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Patel%2C+Y+J">Yash J. Patel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jerbi%2C+S">Sofiene Jerbi</a>, 
<a href="/search/quant-ph?searchtype=author&query=B%C3%A4ck%2C+T">Thomas B&#xe4;ck</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dunjko%2C+V">Vedran Dunjko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures. EPJ Quantum Technology journal version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPJ Quantum Technol. 11, 6 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13684" title="Abstract">arXiv:2207.13684</a> (replaced) [<a href="/pdf/2207.13684" title="Download PDF">pdf</a>, <a href="/format/2207.13684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Surface Edge Explorer (SEE): A measurement-direct approach to next  best view planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Border%2C+R">Rowan Border</a>, 
<a href="/search/cs?searchtype=author&query=Gammell%2C+J+D">Jonathan D. Gammell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The International Journal of Robotics Research (IJRR) 2024, Vol. 0(0) 1-27. 25 pages, 17 figures, 6 tables. Videos available at <a href="https://www.youtube.com/watch?v=dqppqRlaGEA">this https URL</a> and <a href="https://www.youtube.com/playlist?list=PLbaQBz4TuPcyNh4COoaCtC1ZGhpbEkFEo">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The International Journal of Robotics Research (IJRR) 2024, Vol.
  0(0) 1-27
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.06956" title="Abstract">arXiv:2208.06956</a> (replaced) [<a href="/pdf/2208.06956" title="Download PDF">pdf</a>, <a href="/format/2208.06956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARIEL: Adversarial Graph Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shengyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+B">Baoyu Jing</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yada Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2202.06491">arXiv:2202.06491</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10373" title="Abstract">arXiv:2208.10373</a> (replaced) [<a href="/pdf/2208.10373" title="Download PDF">pdf</a>, <a href="/format/2208.10373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reversing Skin Cancer Adversarial Examples by Multiscale Diffusive and  Denoising Aggregation Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuhui Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Computers in Biology and Medicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.14966" title="Abstract">arXiv:2208.14966</a> (replaced) [<a href="/pdf/2208.14966" title="Download PDF">pdf</a>, <a href="/format/2208.14966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept Gradient: Concept-based Interpretation Without Linear Assumption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+A">Andrew Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chih-Kuan Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+N+Y+C">Neil Y. C. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures, published in ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02614" title="Abstract">arXiv:2209.02614</a> (replaced) [<a href="/pdf/2209.02614" title="Download PDF">pdf</a>, <a href="/format/2209.02614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variable binding and substitution for (nameless) dummies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirschowitz%2C+A">Andr&#xe9; Hirschowitz</a>, 
<a href="/search/cs?searchtype=author&query=Hirschowitz%2C+T">Tom Hirschowitz</a>, 
<a href="/search/cs?searchtype=author&query=Lafont%2C+A">Ambroise Lafont</a>, 
<a href="/search/cs?searchtype=author&query=Maggesi%2C+M">Marco Maggesi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Expanded version of the FoSSaCS 2022 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05653" title="Abstract">arXiv:2209.05653</a> (replaced) [<a href="/pdf/2209.05653" title="Download PDF">pdf</a>, <a href="/ps/2209.05653" title="Download PostScript">ps</a>, <a href="/format/2209.05653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action  Segmentation in Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junbin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+P">Pei-Hsuan Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+M">Meng-Hsun Tsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, 9 tables. Published on Applied Intelligence
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Intelligence(2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07481" title="Abstract">arXiv:2209.07481</a> (replaced) [<a href="/pdf/2209.07481" title="Download PDF">pdf</a>, <a href="/format/2209.07481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Representations of Annealing Paths: Bregman Information  under Monotonic Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brekelmans%2C+R">Rob Brekelmans</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Information Geometry (Info. Geo. 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09135" title="Abstract">arXiv:2209.09135</a> (replaced) [<a href="/pdf/2209.09135" title="Download PDF">pdf</a>, <a href="/format/2209.09135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $S$-Packing Coloring of Cubic Halin Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tarhini%2C+B">Batoul Tarhini</a>, 
<a href="/search/math?searchtype=author&query=Togni%2C+O">Olivier Togni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09178" title="Abstract">arXiv:2209.09178</a> (replaced) [<a href="/pdf/2209.09178" title="Download PDF">pdf</a>, <a href="/format/2209.09178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01283" title="Abstract">arXiv:2210.01283</a> (replaced) [<a href="/pdf/2210.01283" title="Download PDF">pdf</a>, <a href="/format/2210.01283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Robust Non-Prehensile Manipulation via Persistent Homology  Guided Monte-Carlo Tree Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vieira%2C+E+R">Ewerton R. Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Nakhimovich%2C+D">Daniel Nakhimovich</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K+E">Kostas E. Bekris</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09138" title="Abstract">arXiv:2210.09138</a> (replaced) [<a href="/pdf/2210.09138" title="Download PDF">pdf</a>, <a href="/format/2210.09138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Open-source Benchmark of Deep Learning Models for Audio-visual  Apparent and Self-reported Personality Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Rongfan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Siyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Gunes%2C+H">Hatice Gunes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Affective Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10718" title="Abstract">arXiv:2210.10718</a> (replaced) [<a href="/pdf/2210.10718" title="Download PDF">pdf</a>, <a href="/format/2210.10718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whole Page Unbiased Learning to Rank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lixin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yujia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaokai Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiashu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00323" title="Abstract">arXiv:2211.00323</a> (replaced) [<a href="/pdf/2211.00323" title="Download PDF">pdf</a>, <a href="/format/2211.00323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface: Power Consumption Modeling and  Practical Measurement Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinghe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wankai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+C">Jing Cheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J+Y">Jun Yan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+T+J">Tie Jun Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05939" title="Abstract">arXiv:2211.05939</a> (replaced) [<a href="/pdf/2211.05939" title="Download PDF">pdf</a>, <a href="/format/2211.05939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pyRDDLGym: From RDDL to Gym Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taitler%2C+A">Ayal Taitler</a>, 
<a href="/search/cs?searchtype=author&query=Gimelfarb%2C+M">Michael Gimelfarb</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jihwan Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+S">Sriram Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Mladenov%2C+M">Martin Mladenov</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaotian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sanner%2C+S">Scott Sanner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06108" title="Abstract">arXiv:2211.06108</a> (replaced) [<a href="/pdf/2211.06108" title="Download PDF">pdf</a>, <a href="/format/2211.06108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object  Detection Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yanlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qing-Long Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Gang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10254" title="Abstract">arXiv:2211.10254</a> (replaced) [<a href="/pdf/2211.10254" title="Download PDF">pdf</a>, <a href="/format/2211.10254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Being Simple on Complex Issues&quot; -- Accounts on Visual Data  Communication about Climate Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schuster%2C+R">Regina Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+K">Kathleen Gregory</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Torsten M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Koesten%2C+L">Laura Koesten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16290" title="Abstract">arXiv:2211.16290</a> (replaced) [<a href="/pdf/2211.16290" title="Download PDF">pdf</a>, <a href="/format/2211.16290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yinlin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 3DV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03128" title="Abstract">arXiv:2212.03128</a> (replaced) [<a href="/pdf/2212.03128" title="Download PDF">pdf</a>, <a href="/format/2212.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persistent Homology of Chromatic Alpha Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=di+Montesano%2C+S+C">Sebastiano Cultrera di Montesano</a>, 
<a href="/search/math?searchtype=author&query=Draganov%2C+O">Ond&#x159;ej Draganov</a>, 
<a href="/search/math?searchtype=author&query=Edelsbrunner%2C+H">Herbert Edelsbrunner</a>, 
<a href="/search/math?searchtype=author&query=Saghafian%2C+M">Morteza Saghafian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13410" title="Abstract">arXiv:2212.13410</a> (replaced) [<a href="/pdf/2212.13410" title="Download PDF">pdf</a>, <a href="/format/2212.13410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A parallel solver for fluid structure interaction problems with Lagrange  multiplier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boffi%2C+D">Daniele Boffi</a>, 
<a href="/search/math?searchtype=author&query=Credali%2C+F">Fabio Credali</a>, 
<a href="/search/math?searchtype=author&query=Gastaldi%2C+L">Lucia Gastaldi</a>, 
<a href="/search/math?searchtype=author&query=Scacchi%2C+S">Simone Scacchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 8 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13619" title="Abstract">arXiv:2212.13619</a> (replaced) [<a href="/pdf/2212.13619" title="Download PDF">pdf</a>, <a href="/ps/2212.13619" title="Download PostScript">ps</a>, <a href="/format/2212.13619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost-Bayesian Quadratic Persuasion (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Massicot%2C+O">Olivier Massicot</a>, 
<a href="/search/cs?searchtype=author&query=Langbort%2C+C">C&#xe9;dric Langbort</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version extends the article submitted to the IEEE Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01968" title="Abstract">arXiv:2301.01968</a> (replaced) [<a href="/pdf/2301.01968" title="Download PDF">pdf</a>, <a href="/format/2301.01968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Neural Force Manifolds for Sim2Real Robotic Symmetrical Paper  Folding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+A">Andrew Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+D">Dezhong Tong</a>, 
<a href="/search/cs?searchtype=author&query=Terzopoulos%2C+D">Demetri Terzopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+J">Jungseock Joo</a>, 
<a href="/search/cs?searchtype=author&query=Jawed%2C+M+K">M. Khalid Jawed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary video is available on YouTube: <a href="https://youtu.be/k0nexYGy-P4">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06156" title="Abstract">arXiv:2301.06156</a> (replaced) [<a href="/pdf/2301.06156" title="Download PDF">pdf</a>, <a href="/format/2301.06156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Least-Squares Neural Network (LSNN) Method For Linear Advection-Reaction  Equation: Discontinuity Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+Z">Zhiqiang Cai</a>, 
<a href="/search/math?searchtype=author&query=Choi%2C+J">Junpyo Choi</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+M">Min Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13557" title="Abstract">arXiv:2301.13557</a> (replaced) [<a href="/pdf/2301.13557" title="Download PDF">pdf</a>, <a href="/ps/2301.13557" title="Download PostScript">ps</a>, <a href="/format/2301.13557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On locating and neighbor-locating colorings of sparse graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakraborty%2C+D">Dipayan Chakraborty</a>, 
<a href="/search/math?searchtype=author&query=Foucaud%2C+F">Florent Foucaud</a>, 
<a href="/search/math?searchtype=author&query=Nandi%2C+S">Soumen Nandi</a>, 
<a href="/search/math?searchtype=author&query=Sen%2C+S">Sagnik Sen</a>, 
<a href="/search/math?searchtype=author&query=Supraja%2C+D+K">D K Supraja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00487" title="Abstract">arXiv:2302.00487</a> (replaced) [<a href="/pdf/2302.00487" title="Download PDF">pdf</a>, <a href="/format/2302.00487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Continual Learning: Theory, Method and  Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The concise version is in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01034" title="Abstract">arXiv:2302.01034</a> (replaced) [<a href="/pdf/2302.01034" title="Download PDF">pdf</a>, <a href="/format/2302.01034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D  LiDAR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ningning Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03781" title="Abstract">arXiv:2302.03781</a> (replaced) [<a href="/pdf/2302.03781" title="Download PDF">pdf</a>, <a href="/format/2302.03781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cardinality-Constrained Continuous Knapsack Problem with Concave  Piecewise-Linear Utilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+M">Miao Bai</a>, 
<a href="/search/cs?searchtype=author&query=Cardonha%2C+C">Carlos Cardonha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06014" title="Abstract">arXiv:2302.06014</a> (replaced) [<a href="/pdf/2302.06014" title="Download PDF">pdf</a>, <a href="/ps/2302.06014" title="Download PostScript">ps</a>, <a href="/format/2302.06014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Recommendations for Agents with Discounted Adaptive Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Arpit Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+W">William Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates for camera-ready version (ALT 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07200" title="Abstract">arXiv:2302.07200</a> (replaced) [<a href="/pdf/2302.07200" title="Download PDF">pdf</a>, <a href="/ps/2302.07200" title="Download PostScript">ps</a>, <a href="/format/2302.07200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeLong%2C+L+N">Lauren Nicole DeLong</a>, 
<a href="/search/cs?searchtype=author&query=Mir%2C+R+F">Ramon Fern&#xe1;ndez Mir</a>, 
<a href="/search/cs?searchtype=author&query=Fleuriot%2C+J+D">Jacques D. Fleuriot</a> (The University of Edinburgh School of Informatics, Artificial Intelligence and its Applications Institute)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures, 2 table, currently under review. Corresponding GitHub page here: <a href="https://github.com/NeSymGraphs.">this https URL</a> Revised in February 2024 according to major revisions by peer reviewers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11290" title="Abstract">arXiv:2302.11290</a> (replaced) [<a href="/pdf/2302.11290" title="Download PDF">pdf</a>, <a href="/ps/2302.11290" title="Download PostScript">ps</a>, <a href="/format/2302.11290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Equivalences, Homomorphism Indistinguishability, and Forbidden  Minors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Seppelt%2C+T">Tim Seppelt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 1 figure, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 48th International Symposium on Mathematical Foundations of
  Computer Science (MFCS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11861" title="Abstract">arXiv:2302.11861</a> (replaced) [<a href="/pdf/2302.11861" title="Download PDF">pdf</a>, <a href="/format/2302.11861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Domain Robustness via Targeted Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+I">Irena Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sagawa%2C+S">Shiori Sagawa</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+P+W">Pang Wei Koh</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00477" title="Abstract">arXiv:2303.00477</a> (replaced) [<a href="/pdf/2303.00477" title="Download PDF">pdf</a>, <a href="/format/2303.00477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ORCHNet: A Robust Global Feature Aggregation approach for 3D LiDAR-based  Place recognition in Orchards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barros%2C+T">T. Barros</a>, 
<a href="/search/cs?searchtype=author&query=Garrote%2C+L">L. Garrote</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+P">P. Conde</a>, 
<a href="/search/cs?searchtype=author&query=Coombes%2C+M+J">M.J. Coombes</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">C. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Premebida%2C+C">C. Premebida</a>, 
<a href="/search/cs?searchtype=author&query=Nunes%2C+U+J">U.J. Nunes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07937" title="Abstract">arXiv:2303.07937</a> (replaced) [<a href="/pdf/2303.07937" title="Download PDF">pdf</a>, <a href="/format/2303.07937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Junyoung Seo</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+W">Wooseok Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+M">Min-Seop Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeonsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jaehoon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin-Hwa Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jiyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page <a href="https://ku-cvlab.github.io/3DFuse/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09197" title="Abstract">arXiv:2303.09197</a> (replaced) [<a href="/pdf/2303.09197" title="Download PDF">pdf</a>, <a href="/ps/2303.09197" title="Download PostScript">ps</a>, <a href="/format/2303.09197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Temporality and Causality into Acyclic Argumentation  Frameworks using a Transition System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munro%2C+Y">Y. Munro</a> (1), 
<a href="/search/cs?searchtype=author&query=Sarmiento%2C+C">C. Sarmiento</a> (1), 
<a href="/search/cs?searchtype=author&query=Bloch%2C+I">I. Bloch</a> (1), 
<a href="/search/cs?searchtype=author&query=Bourgne%2C+G">G. Bourgne</a> (1), 
<a href="/search/cs?searchtype=author&query=Lesot%2C+M+-">M.-J. Lesot</a> (1) ((1) Sorbonne Universit&#xe9;, CNRS, LIP6, Paris, France)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10931" title="Abstract">arXiv:2303.10931</a> (replaced) [<a href="/pdf/2303.10931" title="Download PDF">pdf</a>, <a href="/format/2303.10931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approaching an unknown communication system by latent space exploration  and causal inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Begu%C5%A1%2C+G">Ga&#x161;per Begu&#x161;</a>, 
<a href="/search/stat?searchtype=author&query=Leban%2C+A">Andrej Leban</a>, 
<a href="/search/stat?searchtype=author&query=Gero%2C+S">Shane Gero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 23 figures; new format and section layout (moved some sections to the appendix), added replication experiments, updated references: to a subsequent experimental validation of the work, as well as to related methodological work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12649" title="Abstract">arXiv:2303.12649</a> (replaced) [<a href="/pdf/2303.12649" title="Download PDF">pdf</a>, <a href="/format/2303.12649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bi%2C+Y">Yuan Bi</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Zhongliang Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Clarenbach%2C+R">Ricarda Clarenbach</a>, 
<a href="/search/eess?searchtype=author&query=Ghotbi%2C+R">Reza Ghotbi</a>, 
<a href="/search/eess?searchtype=author&query=Karlas%2C+A">Angelos Karlas</a>, 
<a href="/search/eess?searchtype=author&query=Navab%2C+N">Nassir Navab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15414" title="Abstract">arXiv:2303.15414</a> (replaced) [<a href="/pdf/2303.15414" title="Download PDF">pdf</a>, <a href="/format/2303.15414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnable Graph Matching: A Practical Paradigm for Data Association
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiawei He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zehao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Naiyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TPAMI 2024. arXiv admin note: substantial text overlap with <a href="/abs/2103.16178">arXiv:2103.16178</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17000" title="Abstract">arXiv:2303.17000</a> (replaced) [<a href="/pdf/2303.17000" title="Download PDF">pdf</a>, <a href="/format/2303.17000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilizer Codes with Exotic Local-dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gunderman%2C+L+G">Lane G. Gunderman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17201" title="Abstract">arXiv:2303.17201</a> (replaced) [<a href="/pdf/2303.17201" title="Download PDF">pdf</a>, <a href="/format/2303.17201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Academic Quality of Children&#x27;s Videos using Machine  Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sumeet Kumar</a>, 
<a href="/search/cs?searchtype=author&query=T.%2C+M">Mallikarjuna T.</a>, 
<a href="/search/cs?searchtype=author&query=Khudabukhsh%2C+A">Ashiqur Khudabukhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18013" title="Abstract">arXiv:2303.18013</a> (replaced) [<a href="/pdf/2303.18013" title="Download PDF">pdf</a>, <a href="/format/2303.18013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaCViT: A Label-aware Contrastive Fine-tuning Framework for Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Camarasa%2C+G+A">Gerardo Aragon Camarasa</a>, 
<a href="/search/cs?searchtype=author&query=McCreadie%2C+R">Richard McCreadie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05099" title="Abstract">arXiv:2304.05099</a> (replaced) [<a href="/pdf/2304.05099" title="Download PDF">pdf</a>, <a href="/format/2304.05099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feudal Graph Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marzi%2C+T">Tommaso Marzi</a>, 
<a href="/search/cs?searchtype=author&query=Khehra%2C+A">Arshjot Khehra</a>, 
<a href="/search/cs?searchtype=author&query=Cini%2C+A">Andrea Cini</a>, 
<a href="/search/cs?searchtype=author&query=Alippi%2C+C">Cesare Alippi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07248" title="Abstract">arXiv:2304.07248</a> (replaced) [<a href="/pdf/2304.07248" title="Download PDF">pdf</a>, <a href="/ps/2304.07248" title="Download PostScript">ps</a>, <a href="/format/2304.07248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The University of California San Francisco, Brain Metastases  Stereotactic Radiosurgery (UCSF-BMSR) MRI Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rudie%2C+J+D">Jeffrey D. Rudie</a>, 
<a href="/search/eess?searchtype=author&query=Saluja%2C+R">Rachit Saluja</a>, 
<a href="/search/eess?searchtype=author&query=Weiss%2C+D+A">David A. Weiss</a>, 
<a href="/search/eess?searchtype=author&query=Nedelec%2C+P">Pierre Nedelec</a>, 
<a href="/search/eess?searchtype=author&query=Calabrese%2C+E">Evan Calabrese</a>, 
<a href="/search/eess?searchtype=author&query=Colby%2C+J+B">John B. Colby</a>, 
<a href="/search/eess?searchtype=author&query=Laguna%2C+B">Benjamin Laguna</a>, 
<a href="/search/eess?searchtype=author&query=Mongan%2C+J">John Mongan</a>, 
<a href="/search/eess?searchtype=author&query=Braunstein%2C+S">Steve Braunstein</a>, 
<a href="/search/eess?searchtype=author&query=Hess%2C+C+P">Christopher P. Hess</a>, 
<a href="/search/eess?searchtype=author&query=Rauschecker%2C+A+M">Andreas M. Rauschecker</a>, 
<a href="/search/eess?searchtype=author&query=Sugrue%2C+L+P">Leo P. Sugrue</a>, 
<a href="/search/eess?searchtype=author&query=Villanueva-Meyer%2C+J+E">Javier E. Villanueva-Meyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07944" title="Abstract">arXiv:2304.07944</a> (replaced) [<a href="/pdf/2304.07944" title="Download PDF">pdf</a>, <a href="/format/2304.07944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-depth Investigation of User Response Simulation for Conversational  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenduo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in The Web Conference 2024, 8 pages with Appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08882" title="Abstract">arXiv:2304.08882</a> (replaced) [<a href="/pdf/2304.08882" title="Download PDF">pdf</a>, <a href="/format/2304.08882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Along the Margins: Marginalized Communities&#x27; Ethical Concerns about  Social Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olson%2C+L">Lauren Olson</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n%2C+E">Emitz&#xe1; Guzm&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Kunneman%2C+F">Florian Kunneman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09986" title="Abstract">arXiv:2304.09986</a> (replaced) [<a href="/pdf/2304.09986" title="Download PDF">pdf</a>, <a href="/ps/2304.09986" title="Download PostScript">ps</a>, <a href="/format/2304.09986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on Stone-&#x10c;ech compactification in ZFA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Przyby%C5%82ek%2C+M+R">Micha&#x142; R. Przyby&#x142;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10750" title="Abstract">arXiv:2304.10750</a> (replaced) [<a href="/pdf/2304.10750" title="Download PDF">pdf</a>, <a href="/format/2304.10750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Grounded Language Understanding in a Collaborative Environment  by Interacting with Agents Through Help Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehta%2C+N">Nikhil Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Teruel%2C+M">Milagro Teruel</a>, 
<a href="/search/cs?searchtype=author&query=Sanz%2C+P+F">Patricio Figueroa Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Kiseleva%2C+J">Julia Kiseleva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11062" title="Abstract">arXiv:2304.11062</a> (replaced) [<a href="/pdf/2304.11062" title="Download PDF">pdf</a>, <a href="/format/2304.11062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Transformer to 1M tokens and beyond with RMT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bulatov%2C+A">Aydar Bulatov</a>, 
<a href="/search/cs?searchtype=author&query=Kuratov%2C+Y">Yuri Kuratov</a>, 
<a href="/search/cs?searchtype=author&query=Kapushev%2C+Y">Yermek Kapushev</a>, 
<a href="/search/cs?searchtype=author&query=Burtsev%2C+M+S">Mikhail S. Burtsev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11860" title="Abstract">arXiv:2304.11860</a> (replaced) [<a href="/pdf/2304.11860" title="Download PDF">pdf</a>, <a href="/format/2304.11860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the lifting and reconstruction of nonlinear systems with multiple  invariant sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pan%2C+S">Shaowu Pan</a>, 
<a href="/search/math?searchtype=author&query=Duraisamy%2C+K">Karthik Duraisamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12794" title="Abstract">arXiv:2304.12794</a> (replaced) [<a href="/pdf/2304.12794" title="Download PDF">pdf</a>, <a href="/format/2304.12794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expand-and-Cluster: Parameter Recovery of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinelli%2C+F">Flavio Martinelli</a>, 
<a href="/search/cs?searchtype=author&query=Simsek%2C+B">Berfin Simsek</a>, 
<a href="/search/cs?searchtype=author&query=Gerstner%2C+W">Wulfram Gerstner</a>, 
<a href="/search/cs?searchtype=author&query=Brea%2C+J">Johanni Brea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint: 11 pages, 6 figures. Appendix: 9 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14621" title="Abstract">arXiv:2304.14621</a> (replaced) [<a href="/pdf/2304.14621" title="Download PDF">pdf</a>, <a href="/format/2304.14621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUDiff: Unified Diffusion for Complete Molecule Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+C">Chenqing Hua</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+S">Sitao Luan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14989" title="Abstract">arXiv:2304.14989</a> (replaced) [<a href="/pdf/2304.14989" title="Download PDF">pdf</a>, <a href="/format/2304.14989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded  Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Hao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+K">Kwang-Sung Jun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chicheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05465" title="Abstract">arXiv:2305.05465</a> (replaced) [<a href="/pdf/2305.05465" title="Download PDF">pdf</a>, <a href="/format/2305.05465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The emergence of clusters in self-attention dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geshkovski%2C+B">Borjan Geshkovski</a>, 
<a href="/search/cs?searchtype=author&query=Letrouit%2C+C">Cyril Letrouit</a>, 
<a href="/search/cs?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>, 
<a href="/search/cs?searchtype=author&query=Rigollet%2C+P">Philippe Rigollet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Analysis of PDEs (math.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06841" title="Abstract">arXiv:2305.06841</a> (replaced) [<a href="/pdf/2305.06841" title="Download PDF">pdf</a>, <a href="/format/2305.06841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think Twice: Measuring the Efficiency of Eliminating Prediction  Shortcuts of Question Answering Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mikula%2C+L">Luk&#xe1;&#x161; Mikula</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0tef%C3%A1nik%2C+M">Michal &#x160;tef&#xe1;nik</a>, 
<a href="/search/cs?searchtype=author&query=Petrovi%C4%8D%2C+M">Marek Petrovi&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Sojka%2C+P">Petr Sojka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper in Proceedings of EACL 2024: Main track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08926" title="Abstract">arXiv:2305.08926</a> (replaced) [<a href="/pdf/2305.08926" title="Download PDF">pdf</a>, <a href="/format/2305.08926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptive Locomotion through Whole-Body MPC and Optimal Region  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corb%C3%A8res%2C+T">Thomas Corb&#xe8;res</a>, 
<a href="/search/cs?searchtype=author&query=Mastalli%2C+C">Carlos Mastalli</a>, 
<a href="/search/cs?searchtype=author&query=Merkt%2C+W">Wolfgang Merkt</a>, 
<a href="/search/cs?searchtype=author&query=Havoutis%2C+I">Ioannis Havoutis</a>, 
<a href="/search/cs?searchtype=author&query=Fallon%2C+M">Maurice Fallon</a>, 
<a href="/search/cs?searchtype=author&query=Mansard%2C+N">Nicolas Mansard</a>, 
<a href="/search/cs?searchtype=author&query=Flayols%2C+T">Thomas Flayols</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+S">Sethu Vijayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Tonneau%2C+S">Steve Tonneau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13865" title="Abstract">arXiv:2305.13865</a> (replaced) [<a href="/pdf/2305.13865" title="Download PDF">pdf</a>, <a href="/format/2305.13865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Pre-training for Private Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Da Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gopi%2C+S">Sivakanth Gopi</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+J">Janardhan Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zinan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+S">Saurabh Naik</a>, 
<a href="/search/cs?searchtype=author&query=Religa%2C+T+L">Tomasz Lukasz Religa</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14330" title="Abstract">arXiv:2305.14330</a> (replaced) [<a href="/pdf/2305.14330" title="Download PDF">pdf</a>, <a href="/format/2305.14330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot  Text-to-Video Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Susung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Junyoung Seo</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Heeseong Shin</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Sunghwan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code and demo will be available at <a href="https://github.com/KU-CVLAB/DirecT2V">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14777" title="Abstract">arXiv:2305.14777</a> (replaced) [<a href="/pdf/2305.14777" title="Download PDF">pdf</a>, <a href="/format/2305.14777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling through the Semi-dual Formulation of Unbalanced  Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaemoo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14998" title="Abstract">arXiv:2305.14998</a> (replaced) [<a href="/pdf/2305.14998" title="Download PDF">pdf</a>, <a href="/format/2305.14998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Examination of the Robustness of Reference-Free Image Captioning  Evaluation Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+S">Saba Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aishwarya Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15611" title="Abstract">arXiv:2305.15611</a> (replaced) [<a href="/pdf/2305.15611" title="Download PDF">pdf</a>, <a href="/format/2305.15611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Size Generalization of Graph Neural Networks on Biological Data:  Insights and Practices from the Spectral Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gaotang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yujun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Koutra%2C+D">Danai Koutra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, including appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17028" title="Abstract">arXiv:2305.17028</a> (replaced) [<a href="/pdf/2305.17028" title="Download PDF">pdf</a>, <a href="/format/2305.17028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Batch for Deep Probabilistic Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zheng%2C+V+Z">Vincent Zhihao Zheng</a>, 
<a href="/search/stat?searchtype=author&query=Choi%2C+S">Seongjin Choi</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+L">Lijun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, modified peer-review version, accepted to The 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17326" title="Abstract">arXiv:2305.17326</a> (replaced) [<a href="/pdf/2305.17326" title="Download PDF">pdf</a>, <a href="/format/2305.17326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Information Theory for Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19190" title="Abstract">arXiv:2305.19190</a> (replaced) [<a href="/pdf/2305.19190" title="Download PDF">pdf</a>, <a href="/format/2305.19190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Approximation Theory for Nonlinear Recurrent Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qianxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19302" title="Abstract">arXiv:2305.19302</a> (replaced) [<a href="/pdf/2305.19302" title="Download PDF">pdf</a>, <a href="/format/2305.19302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth, exact rotational symmetrization for deep learning on point  clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pozdnyakov%2C+S+N">Sergey N. Pozdnyakov</a>, 
<a href="/search/cs?searchtype=author&query=Ceriotti%2C+M">Michele Ceriotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Enhancing figures; minor polishing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19600" title="Abstract">arXiv:2305.19600</a> (replaced) [<a href="/pdf/2305.19600" title="Download PDF">pdf</a>, <a href="/format/2305.19600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Self-Distillation for Minimizing Client Drift in Heterogeneous  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yashwanth%2C+M">M.Yashwanth</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+G+K">Gaurav Kumar Nayak</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Arya Singh</a>, 
<a href="/search/cs?searchtype=author&query=Simmhan%2C+Y">Yogesh Simmhan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Anirban Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01015" title="Abstract">arXiv:2306.01015</a> (replaced) [<a href="/pdf/2306.01015" title="Download PDF">pdf</a>, <a href="/format/2306.01015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Estimate Model Transferability of Pre-Trained Speech Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zih-Ching Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nanxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shuo-Yiin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sainath%2C+T+N">Tara N. Sainath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech. Code is available at: <a href="https://github.com/virginiakm1988/LogME-CTC.">this https URL</a> Fixed a typo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Neural and Evolutionary Computing (cs.NE); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01474" title="Abstract">arXiv:2306.01474</a> (replaced) [<a href="/pdf/2306.01474" title="Download PDF">pdf</a>, <a href="/format/2306.01474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalist Equivariant Transformer Towards 3D Molecular Interaction  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xiangzhe Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenbing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 AI4D3 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03933" title="Abstract">arXiv:2306.03933</a> (replaced) [<a href="/pdf/2306.03933" title="Download PDF">pdf</a>, <a href="/format/2306.03933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional and Permutation Invariant Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Mikuni%2C+V">Vinicius Mikuni</a>, 
<a href="/search/hep-ph?searchtype=author&query=Nachman%2C+B">Benjamin Nachman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04877" title="Abstract">arXiv:2306.04877</a> (replaced) [<a href="/pdf/2306.04877" title="Download PDF">pdf</a>, <a href="/format/2306.04877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trojan Model Detection Using Activation Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussein%2C+M+E">Mohamed E. Hussein</a>, 
<a href="/search/cs?searchtype=author&query=Janakiraman%2C+S+S">Sudharshan Subramaniam Janakiraman</a>, 
<a href="/search/cs?searchtype=author&query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05418" title="Abstract">arXiv:2306.05418</a> (replaced) [<a href="/pdf/2306.05418" title="Download PDF">pdf</a>, <a href="/format/2306.05418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised 3D Object Detection with Multi-Stage Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiawei He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuntao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://ba2det.site">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07445" title="Abstract">arXiv:2306.07445</a> (replaced) [<a href="/pdf/2306.07445" title="Download PDF">pdf</a>, <a href="/format/2306.07445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Least-Squares Neural Network (LSNN) Method For Linear Advection-Reaction  Equation: Non-constant Jumps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+Z">Zhiqiang Cai</a>, 
<a href="/search/math?searchtype=author&query=Choi%2C+J">Junpyo Choi</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+M">Min Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages. arXiv admin note: substantial text overlap with <a href="/abs/2301.06156">arXiv:2301.06156</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14670" title="Abstract">arXiv:2306.14670</a> (replaced) [<a href="/pdf/2306.14670" title="Download PDF">pdf</a>, <a href="/format/2306.14670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jagadeesan%2C+M">Meena Jagadeesan</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>, 
<a href="/search/cs?searchtype=author&query=Haghtalab%2C+N">Nika Haghtalab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at NeurIPS 2023; this is the full version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00405" title="Abstract">arXiv:2307.00405</a> (replaced) [<a href="/pdf/2307.00405" title="Download PDF">pdf</a>, <a href="/ps/2307.00405" title="Download PostScript">ps</a>, <a href="/format/2307.00405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Efficient UCB-type Algorithms For Learning Predictive State  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruiquan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jing Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01898" title="Abstract">arXiv:2307.01898</a> (replaced) [<a href="/pdf/2307.01898" title="Download PDF">pdf</a>, <a href="/format/2307.01898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Artificial Intelligence Reproducibility and Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Edward Kim</a>, 
<a href="/search/cs?searchtype=author&query=Isozaki%2C+I">Isamu Isozaki</a>, 
<a href="/search/cs?searchtype=author&query=Sirkin%2C+N">Naomi Sirkin</a>, 
<a href="/search/cs?searchtype=author&query=Robson%2C+M">Michael Robson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03898" title="Abstract">arXiv:2307.03898</a> (replaced) [<a href="/e-print/2307.03898" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StyleGAN3: Generative Networks for Improving the Equivariance of  Translation and Rotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianlei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Renzhe Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gaurav Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> But now we feel we haven't fully studied our work and have found some new great results. So after careful consideration, we're going to rework this manuscript and try to give a more accurate model
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05591" title="Abstract">arXiv:2307.05591</a> (replaced) [<a href="/pdf/2307.05591" title="Download PDF">pdf</a>, <a href="/format/2307.05591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Alignment of Vision-language Models for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paischer%2C+F">Fabian Paischer</a>, 
<a href="/search/cs?searchtype=author&query=Hofmarcher%2C+M">Markus Hofmarcher</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>, 
<a href="/search/cs?searchtype=author&query=Adler%2C+T">Thomas Adler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (+ references and appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07286" title="Abstract">arXiv:2307.07286</a> (replaced) [<a href="/pdf/2307.07286" title="Download PDF">pdf</a>, <a href="/format/2307.07286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Siyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hwa%2C+E+M">Er Meng Hwa</a>, 
<a href="/search/cs?searchtype=author&query=Kot%2C+A+C">Alex C. Kot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 6 tables. Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07889" title="Abstract">arXiv:2307.07889</a> (replaced) [<a href="/pdf/2307.07889" title="Download PDF">pdf</a>, <a href="/format/2307.07889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise  Comparisons using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liusie%2C+A">Adian Liusie</a>, 
<a href="/search/cs?searchtype=author&query=Manakul%2C+P">Potsawee Manakul</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08540" title="Abstract">arXiv:2307.08540</a> (replaced) [<a href="/pdf/2307.08540" title="Download PDF">pdf</a>, <a href="/format/2307.08540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilization of Pre-trained Language Model for Adapter-based Knowledge  Transfer in Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saberi%2C+I">Iman Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Fard%2C+F">Fatemeh Fard</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fuxiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMSE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15641" title="Abstract">arXiv:2307.15641</a> (replaced) [<a href="/pdf/2307.15641" title="Download PDF">pdf</a>, <a href="/format/2307.15641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QbC: Quantum Correctness by Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Peduri%2C+A">Anurudh Peduri</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schaefer%2C+I">Ina Schaefer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Walter%2C+M">Michael Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2. 24 pages. generalized while rule, proved completeness, improved exposition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16806" title="Abstract">arXiv:2307.16806</a> (replaced) [<a href="/pdf/2307.16806" title="Download PDF">pdf</a>, <a href="/format/2307.16806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing the Depth of ChatGPT&#x27;s Comprehension via Cross-Modal Tasks Based  on ASCII-Art: GPT3.5&#x27;s Abilities in Regard to Recognizing and Generating  ASCII-Art Are Not Totally Lacking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayani%2C+D">David Bayani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EACL 2024 as a long paper. See <a href="https://2024.eacl.org/program/findings-accepted/#long-papers">this https URL</a> . Note: this paper's ArXiv version includes additional discussion, analysis, and types of experiments compared to the EACL version. Changes introduced in V2 of ArXiv paper: only this comment metadata. V1 was initially submission on July 26th, 2023 - release was delayed by ArXiv for a few days
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16847" title="Abstract">arXiv:2307.16847</a> (replaced) [<a href="/pdf/2307.16847" title="Download PDF">pdf</a>, <a href="/format/2307.16847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CroSSL: Cross-modal Self-Supervised Learning for Time-series through  Latent Masking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deldari%2C+S">Shohreh Deldari</a>, 
<a href="/search/cs?searchtype=author&query=Spathis%2C+D">Dimitris Spathis</a>, 
<a href="/search/cs?searchtype=author&query=Malekzadeh%2C+M">Mohammad Malekzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kawsar%2C+F">Fahim Kawsar</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F">Flora Salim</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Akhil Mathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WSDM24. Short version presented in ML4MHD @ICML23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01285" title="Abstract">arXiv:2308.01285</a> (replaced) [<a href="/pdf/2308.01285" title="Download PDF">pdf</a>, <a href="/format/2308.01285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flows: Building Blocks of Reasoning and Collaborating AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+L">Lars Klein</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+N">Nicolas Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Saibo Geng</a>, 
<a href="/search/cs?searchtype=author&query=Schnitzler%2C+J+P">Julian Paul Schnitzler</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01722" title="Abstract">arXiv:2308.01722</a> (replaced) [<a href="/pdf/2308.01722" title="Download PDF">pdf</a>, <a href="/format/2308.01722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relational hyperevent models for the coevolution of coauthoring and  citation networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lerner%2C+J">J&#xfc;rgen Lerner</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A2ncean%2C+M">Marian-Gabriel H&#xe2;ncean</a>, 
<a href="/search/cs?searchtype=author&query=Lomi%2C+A">Alessandro Lomi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of a submitted manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02135" title="Abstract">arXiv:2308.02135</a> (replaced) [<a href="/pdf/2308.02135" title="Download PDF">pdf</a>, <a href="/ps/2308.02135" title="Download PostScript">ps</a>, <a href="/format/2308.02135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Beam Management for mmWave and THz Communications Towards 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Q">Qing Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+C">Chengwang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shaodan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiajia Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Communications Surveys &amp; Tutorials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06680" title="Abstract">arXiv:2308.06680</a> (replaced) [<a href="/pdf/2308.06680" title="Download PDF">pdf</a>, <a href="/format/2308.06680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untangling Carbon-free Energy Attribution and Carbon Intensity  Estimation for Carbon-aware Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maji%2C+D">Diptyaroop Maji</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+D">David Irwin</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>, 
<a href="/search/cs?searchtype=author&query=Sitaraman%2C+R+K">Ramesh K. Sitaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06911" title="Abstract">arXiv:2308.06911</a> (replaced) [<a href="/pdf/2308.06911" title="Download PDF">pdf</a>, <a href="/format/2308.06911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIT-Mol: A Multi-modal Large Language Model for Molecular Science with  Graph, Image, and Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yiming Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jun Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The article has been accepted by Computers in Biology and Medicine, with 14 pages and 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers in Biology and Medicine, 108073, 2024, ISSN 0010-4825
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07134" title="Abstract">arXiv:2308.07134</a> (replaced) [<a href="/pdf/2308.07134" title="Download PDF">pdf</a>, <a href="/format/2308.07134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language is All a Graph Needs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Ruosong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Caiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09687" title="Abstract">arXiv:2308.09687</a> (replaced) [<a href="/pdf/2308.09687" title="Download PDF">pdf</a>, <a href="/format/2308.09687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph of Thoughts: Solving Elaborate Problems with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Besta%2C+M">Maciej Besta</a>, 
<a href="/search/cs?searchtype=author&query=Blach%2C+N">Nils Blach</a>, 
<a href="/search/cs?searchtype=author&query=Kubicek%2C+A">Ales Kubicek</a>, 
<a href="/search/cs?searchtype=author&query=Gerstenberger%2C+R">Robert Gerstenberger</a>, 
<a href="/search/cs?searchtype=author&query=Podstawski%2C+M">Michal Podstawski</a>, 
<a href="/search/cs?searchtype=author&query=Gianinazzi%2C+L">Lukas Gianinazzi</a>, 
<a href="/search/cs?searchtype=author&query=Gajda%2C+J">Joanna Gajda</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+T">Tomasz Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Niewiadomski%2C+H">Hubert Niewiadomski</a>, 
<a href="/search/cs?searchtype=author&query=Nyczyk%2C+P">Piotr Nyczyk</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11129" title="Abstract">arXiv:2308.11129</a> (replaced) [<a href="/pdf/2308.11129" title="Download PDF">pdf</a>, <a href="/format/2308.11129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Graph Transformers with Hierarchical Distance Structural  Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuankai Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11774" title="Abstract">arXiv:2308.11774</a> (replaced) [<a href="/pdf/2308.11774" title="Download PDF">pdf</a>, <a href="/ps/2308.11774" title="Download PostScript">ps</a>, <a href="/format/2308.11774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene  Reconstruction by Neural Radiance Field (NeRF)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+A">Ange Lou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yamin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yike Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J">Jack Noble</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SPIE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11776" title="Abstract">arXiv:2308.11776</a> (replaced) [<a href="/pdf/2308.11776" title="Download PDF">pdf</a>, <a href="/ps/2308.11776" title="Download PostScript">ps</a>, <a href="/format/2308.11776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation  on Surgical Videos with Unknown Camera Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+A">Ange Lou</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J">Jack Noble</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SPIE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13111" title="Abstract">arXiv:2308.13111</a> (replaced) [<a href="/pdf/2308.13111" title="Download PDF">pdf</a>, <a href="/format/2308.13111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Low-rank Adaptation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+X">Adam X. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Robeyns%2C+M">Maxime Robeyns</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15812" title="Abstract">arXiv:2308.15812</a> (replaced) [<a href="/pdf/2308.15812" title="Download PDF">pdf</a>, <a href="/format/2308.15812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peering Through Preferences: Unraveling Feedback Acquisition for  Aligning Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+H">Hritik Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+J">John Dang</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16697" title="Abstract">arXiv:2308.16697</a> (replaced) [<a href="/pdf/2308.16697" title="Download PDF">pdf</a>, <a href="/ps/2308.16697" title="Download PostScript">ps</a>, <a href="/format/2308.16697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game semantics for the constructive $&#x3bc;$-calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pacheco%2C+L">Leonardo Pacheco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01516" title="Abstract">arXiv:2309.01516</a> (replaced) [<a href="/pdf/2309.01516" title="Download PDF">pdf</a>, <a href="/format/2309.01516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiWay-Adapater: Adapting large-scale multi-modal models for scalable  image-text retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Killick%2C+G">George Killick</a>, 
<a href="/search/cs?searchtype=author&query=McCreadie%2C+R">Richard McCreadie</a>, 
<a href="/search/cs?searchtype=author&query=Camarasa%2C+G+A">Gerardo Aragon Camarasa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01945" title="Abstract">arXiv:2309.01945</a> (replaced) [<a href="/pdf/2309.01945" title="Download PDF">pdf</a>, <a href="/format/2309.01945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OHQ: On-chip Hardware-aware Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yangdong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jingzhuo Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02908" title="Abstract">arXiv:2309.02908</a> (replaced) [<a href="/pdf/2309.02908" title="Download PDF">pdf</a>, <a href="/format/2309.02908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DECODE: Data-driven Energy Consumption Prediction leveraging Historical  Data and Environmental Factors in Buildings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Aditya Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Lone%2C+H+R">Haroon R. Lone</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Aayush Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04146" title="Abstract">arXiv:2309.04146</a> (replaced) [<a href="/pdf/2309.04146" title="Download PDF">pdf</a>, <a href="/format/2309.04146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyoungyeon Cho</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Seungkum Han</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y+R">Young Rok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+W">Wonseok Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 System Demonstration Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04148" title="Abstract">arXiv:2309.04148</a> (replaced) [<a href="/pdf/2309.04148" title="Download PDF">pdf</a>, <a href="/format/2309.04148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Synthesis by Probabilistic Many-Valued Logic Operation in  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+H">Hiroki Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Okada%2C+M">Masashi Okada</a>, 
<a href="/search/cs?searchtype=author&query=Taniguchi%2C+T">Tadahiro Taniguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07460" title="Abstract">arXiv:2309.07460</a> (replaced) [<a href="/pdf/2309.07460" title="Download PDF">pdf</a>, <a href="/format/2309.07460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tutorial on Environment-Aware Communications via Channel Knowledge Map  for 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gesbert%2C+D">David Gesbert</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07683" title="Abstract">arXiv:2309.07683</a> (replaced) [<a href="/pdf/2309.07683" title="Download PDF">pdf</a>, <a href="/ps/2309.07683" title="Download PostScript">ps</a>, <a href="/format/2309.07683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the nature of large language models: A caution against  anthropocentrism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Speed%2C+A">Ann Speed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08249" title="Abstract">arXiv:2309.08249</a> (replaced) [<a href="/pdf/2309.08249" title="Download PDF">pdf</a>, <a href="/format/2309.08249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Nonnegative Matrix Factorization with Beta Divergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leplat%2C+V">Valentin Leplat</a>, 
<a href="/search/cs?searchtype=author&query=Hien%2C+L+T+K">Le Thi Khanh Hien</a>, 
<a href="/search/cs?searchtype=author&query=Onwunta%2C+A">Akwum Onwunta</a>, 
<a href="/search/cs?searchtype=author&query=Gillis%2C+N">Nicolas Gillis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages. We have improved the presentation of the paper, and added numerical experiments for beta=3/2 with 4 layers on the CBCL data set
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08499" title="Abstract">arXiv:2309.08499</a> (replaced) [<a href="/pdf/2309.08499" title="Download PDF">pdf</a>, <a href="/format/2309.08499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> P-ROCKET: Pruning Random Convolution Kernels for Time Series  Classification from a Feature Selection Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shaowu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weize Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+D">Deepu John</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09844" title="Abstract">arXiv:2309.09844</a> (replaced) [<a href="/pdf/2309.09844" title="Download PDF">pdf</a>, <a href="/format/2309.09844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drayson%2C+G">George Drayson</a>, 
<a href="/search/cs?searchtype=author&query=Panagiotaki%2C+E">Efimia Panagiotaki</a>, 
<a href="/search/cs?searchtype=author&query=Omeiza%2C+D">Daniel Omeiza</a>, 
<a href="/search/cs?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10852" title="Abstract">arXiv:2309.10852</a> (replaced) [<a href="/pdf/2309.10852" title="Download PDF">pdf</a>, <a href="/format/2309.10852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using AI Uncertainty Quantification to Improve Human Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marusich%2C+L+R">Laura R. Marusich</a>, 
<a href="/search/cs?searchtype=author&query=Bakdash%2C+J+Z">Jonathan Z. Bakdash</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kantarcioglu%2C+M">Murat Kantarcioglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages and 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10889" title="Abstract">arXiv:2309.10889</a> (replaced) [<a href="/pdf/2309.10889" title="Download PDF">pdf</a>, <a href="/format/2309.10889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Orthogonal Time-Frequency Space Modulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsi%2C+M">Mahdi Shamsi</a>, 
<a href="/search/cs?searchtype=author&query=Marvasti%2C+F">Farokh Marvasti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11674" title="Abstract">arXiv:2309.11674</a> (replaced) [<a href="/pdf/2309.11674" title="Download PDF">pdf</a>, <a href="/format/2309.11674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Paradigm Shift in Machine Translation: Boosting Translation  Performance of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J">Young Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sharaf%2C+A">Amr Sharaf</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+H+H">Hany Hassan Awadalla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16883" title="Abstract">arXiv:2309.16883</a> (replaced) [<a href="/pdf/2309.16883" title="Download PDF">pdf</a>, <a href="/format/2309.16883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delattre%2C+B">Blaise Delattre</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+A">Alexandre Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Barth%C3%A9lemy%2C+Q">Quentin Barth&#xe9;lemy</a>, 
<a href="/search/cs?searchtype=author&query=Allauzen%2C+A">Alexandre Allauzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00344" title="Abstract">arXiv:2310.00344</a> (replaced) [<a href="/pdf/2310.00344" title="Download PDF">pdf</a>, <a href="/format/2310.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HarmonyDream: Task Harmonization Inside World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+N">Ningya Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenjun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00526" title="Abstract">arXiv:2310.00526</a> (replaced) [<a href="/pdf/2310.00526" title="Download PDF">pdf</a>, <a href="/format/2310.00526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Graph Neural Networks Optimal Approximation Algorithms?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yau%2C+M">Morris Yau</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Eric Lu</a>, 
<a href="/search/cs?searchtype=author&query=Karalias%2C+N">Nikolaos Karalias</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jessica Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated references, fixed more typos and wording issues
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00793" title="Abstract">arXiv:2310.00793</a> (replaced) [<a href="/pdf/2310.00793" title="Download PDF">pdf</a>, <a href="/format/2310.00793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Link Prediction: A Data Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Shomer%2C+H">Harry Shomer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00854" title="Abstract">arXiv:2310.00854</a> (replaced) [<a href="/pdf/2310.00854" title="Download PDF">pdf</a>, <a href="/format/2310.00854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulating CPU Temperature With Thermal-Aware Scheduling Using a Reduced  Order Learning Thermal Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dowling%2C+A">Anthony Dowling</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+L">Lin Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+M">Ming-Cheng Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version includes revisions to the previous version to improve the clarity and presentation of the work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01105" title="Abstract">arXiv:2310.01105</a> (replaced) [<a href="/pdf/2310.01105" title="Download PDF">pdf</a>, <a href="/format/2310.01105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Guided Continuous Entropic Barycenter Estimation for General  Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Udovichenko%2C+I">Igor Udovichenko</a>, 
<a href="/search/cs?searchtype=author&query=Gazdieva%2C+M">Milena Gazdieva</a>, 
<a href="/search/cs?searchtype=author&query=Pammer%2C+G">Gudmund Pammer</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01689" title="Abstract">arXiv:2310.01689</a> (replaced) [<a href="/pdf/2310.01689" title="Download PDF">pdf</a>, <a href="/format/2310.01689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Threat Modelling in Internet of Things (IoT) Environment Using Dynamic  Attack Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salayma%2C+M">Marwa Salayma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates: this article has only one auther, i.e. Marwa Salayma, also 1st paragraph of the Conclusion (Section IX) is modified. This work was supported by PETRAS National Centre of Excellence for IoT Systems Cybersecurity (PETRAS 2/RACE Project). Grant number is EP/S035362/1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02031" title="Abstract">arXiv:2310.02031</a> (replaced) [<a href="/pdf/2310.02031" title="Download PDF">pdf</a>, <a href="/format/2310.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanGPT: A Large Language Model for Ocean Science Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Daxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project Website: <a href="https://zjunlp.github.io/project/OceanGPT/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02505" title="Abstract">arXiv:2310.02505</a> (replaced) [<a href="/pdf/2310.02505" title="Download PDF">pdf</a>, <a href="/format/2310.02505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Reach Goals via Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vineet Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03253" title="Abstract">arXiv:2310.03253</a> (replaced) [<a href="/pdf/2310.03253" title="Download PDF">pdf</a>, <a href="/format/2310.03253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecule Design by Latent Prompt Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deqian Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jianwen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03841" title="Abstract">arXiv:2310.03841</a> (replaced) [<a href="/pdf/2310.03841" title="Download PDF">pdf</a>, <a href="/format/2310.03841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALBERTA: ALgorithm-Based Error Resilience in Transformer Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+V">Vasu Singh</a>, 
<a href="/search/cs?searchtype=author&query=Filipiuk%2C+M">Micha&#x142; Filipiuk</a>, 
<a href="/search/cs?searchtype=author&query=Hari%2C+S+K+S">Siva Kumar Sastry Hari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04055" title="Abstract">arXiv:2310.04055</a> (replaced) [<a href="/pdf/2310.04055" title="Download PDF">pdf</a>, <a href="/format/2310.04055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Buyukates%2C+B">Baturalp Buyukates</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weizhao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuhang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoyang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04655" title="Abstract">arXiv:2310.04655</a> (replaced) [<a href="/pdf/2310.04655" title="Download PDF">pdf</a>, <a href="/format/2310.04655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Muchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+T">Tianyu Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinguo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05453" title="Abstract">arXiv:2310.05453</a> (replaced) [<a href="/pdf/2310.05453" title="Download PDF">pdf</a>, <a href="/format/2310.05453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuxiang Lai</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinghong Liu</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a> (3) ((1) School of Computer Science and Engineering, Southeast University, China (2) Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China (3) School of Computer Science and Engineering, Nanjing University of Science and Technology, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07779" title="Abstract">arXiv:2310.07779</a> (replaced) [<a href="/pdf/2310.07779" title="Download PDF">pdf</a>, <a href="/format/2310.07779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Approval and Network Homophily as Motivators of Online Toxicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Julie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Walther%2C+J+B">Joseph B. Walther</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07818" title="Abstract">arXiv:2310.07818</a> (replaced) [<a href="/pdf/2310.07818" title="Download PDF">pdf</a>, <a href="/format/2310.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relationship between Sentence Analogy Identification and Sentence  Structure Encoding in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijesiriwardene%2C+T">Thilini Wijesiriwardene</a>, 
<a href="/search/cs?searchtype=author&query=Wickramarachchi%2C+R">Ruwan Wickramarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Reganti%2C+A+N">Aishwarya Naresh Reganti</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Findings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07852" title="Abstract">arXiv:2310.07852</a> (replaced) [<a href="/pdf/2310.07852" title="Download PDF">pdf</a>, <a href="/format/2310.07852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Computational Complexity of Private High-dimensional Model  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Roy%2C+S">Saptarshi Roy</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Zehua Wang</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07975" title="Abstract">arXiv:2310.07975</a> (replaced) [<a href="/pdf/2310.07975" title="Download PDF">pdf</a>, <a href="/format/2310.07975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised visual learning for analyzing firearms trafficking  activities on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konstantakos%2C+S">Sotirios Konstantakos</a>, 
<a href="/search/cs?searchtype=author&query=Chalkiadaki%2C+D+I">Despina Ioanna Chalkiadaki</a>, 
<a href="/search/cs?searchtype=author&query=Mademlis%2C+I">Ioannis Mademlis</a>, 
<a href="/search/cs?searchtype=author&query=Chrysochoou%2C+A+A+R">Adamantia Anna Rebolledo Chrysochoou</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+G+T">Georgios Th. Papadopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10143" title="Abstract">arXiv:2310.10143</a> (replaced) [<a href="/pdf/2310.10143" title="Download PDF">pdf</a>, <a href="/format/2310.10143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Self-supervised Learning with Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>, 
<a href="/search/stat?searchtype=author&query=Takezawa%2C+Y">Yuki Takezawa</a>, 
<a href="/search/stat?searchtype=author&query=Houry%2C+G">Guillaume Houry</a>, 
<a href="/search/stat?searchtype=author&query=Dusterwald%2C+K+M">Kira Michaela Dusterwald</a>, 
<a href="/search/stat?searchtype=author&query=Sulem%2C+D">Deborah Sulem</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Tsai%2C+Y+H">Yao-Hung Hubert Tsai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10301" title="Abstract">arXiv:2310.10301</a> (replaced) [<a href="/pdf/2310.10301" title="Download PDF">pdf</a>, <a href="/format/2310.10301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Body Neural Scene Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidanapathirana%2C+K">Kavisha Vidanapathirana</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+S">Shin-Fang Chng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xueqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for 3DV'2024 (oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10410" title="Abstract">arXiv:2310.10410</a> (replaced) [<a href="/pdf/2310.10410" title="Download PDF">pdf</a>, <a href="/format/2310.10410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loci-Segmented: Improving Scene Segmentation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Traub%2C+M">Manuel Traub</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+F">Frederic Becker</a>, 
<a href="/search/cs?searchtype=author&query=Sauter%2C+A">Adrian Sauter</a>, 
<a href="/search/cs?searchtype=author&query=Otte%2C+S">Sebastian Otte</a>, 
<a href="/search/cs?searchtype=author&query=Butz%2C+M+V">Martin V. Butz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10858" title="Abstract">arXiv:2310.10858</a> (replaced) [<a href="/pdf/2310.10858" title="Download PDF">pdf</a>, <a href="/format/2310.10858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Shared Information Displays for Agents of Varying Strategic  Sophistication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hartline%2C+J">Jason Hartline</a>, 
<a href="/search/cs?searchtype=author&query=Hullman%2C+J">Jessica Hullman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 11 figures, 7 tables. Accepted by ACM CSCW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13033" title="Abstract">arXiv:2310.13033</a> (replaced) [<a href="/pdf/2310.13033" title="Download PDF">pdf</a>, <a href="/format/2310.13033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LASER: Linear Compression in Wireless Distributed Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makkuva%2C+A+V">Ashok Vardhan Makkuva</a>, 
<a href="/search/cs?searchtype=author&query=Bondaschi%2C+M">Marco Bondaschi</a>, 
<a href="/search/cs?searchtype=author&query=Vogels%2C+T">Thijs Vogels</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gastpar%2C+M+C">Michael C. Gastpar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13341" title="Abstract">arXiv:2310.13341</a> (replaced) [<a href="/pdf/2310.13341" title="Download PDF">pdf</a>, <a href="/ps/2310.13341" title="Download PostScript">ps</a>, <a href="/format/2310.13341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Packing forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hoppenot%2C+P">Pierre Hoppenot</a>, 
<a href="/search/math?searchtype=author&query=Martin%2C+M">Mathis Martin</a>, 
<a href="/search/math?searchtype=author&query=Szigeti%2C+Z">Zolt&#xe1;n Szigeti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14403" title="Abstract">arXiv:2310.14403</a> (replaced) [<a href="/pdf/2310.14403" title="Download PDF">pdf</a>, <a href="/format/2310.14403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> O3D: Offline Data-driven Discovery and Distillation for Sequential  Decision-Making with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yuchen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Madhushani%2C+U">Udari Madhushani</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+J">Jared Vann</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+D">Deepeka Garg</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+S">Sumitra Ganesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14907" title="Abstract">arXiv:2310.14907</a> (replaced) [<a href="/pdf/2310.14907" title="Download PDF">pdf</a>, <a href="/format/2310.14907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orientation-Aware Leg Movement Learning for Action-Driven Human Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Chunzhi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kuriyama%2C+S">Shigeru Kuriyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18168" title="Abstract">arXiv:2310.18168</a> (replaced) [<a href="/pdf/2310.18168" title="Download PDF">pdf</a>, <a href="/format/2310.18168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personas as a Way to Model Truthfulness in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+N">Nitish Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Rando%2C+J">Javier Rando</a>, 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Najoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18948" title="Abstract">arXiv:2310.18948</a> (replaced) [<a href="/pdf/2310.18948" title="Download PDF">pdf</a>, <a href="/format/2310.18948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building a Safer Maritime Environment Through Multi-Path Long-Term  Vessel Trajectory Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spadon%2C+G">Gabriel Spadon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+J">Jay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+M">Matthew Smith</a>, 
<a href="/search/cs?searchtype=author&query=Vela%2C+S">Sarah Vela</a>, 
<a href="/search/cs?searchtype=author&query=Gehrmann%2C+R">Romina Gehrmann</a>, 
<a href="/search/cs?searchtype=author&query=Eden%2C+D">Derek Eden</a>, 
<a href="/search/cs?searchtype=author&query=van+Berkel%2C+J">Joshua van Berkel</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+A">Amilcar Soares</a>, 
<a href="/search/cs?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a>, 
<a href="/search/cs?searchtype=author&query=Pelot%2C+R">Ronald Pelot</a>, 
<a href="/search/cs?searchtype=author&query=Matwin%2C+S">Stan Matwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18961" title="Abstract">arXiv:2310.18961</a> (replaced) [<a href="/pdf/2310.18961" title="Download PDF">pdf</a>, <a href="/format/2310.18961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shibo He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19218" title="Abstract">arXiv:2310.19218</a> (replaced) [<a href="/pdf/2310.19218" title="Download PDF">pdf</a>, <a href="/format/2310.19218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Federated Unlearning: A Taxonomy, Challenges and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yiling Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19391" title="Abstract">arXiv:2310.19391</a> (replaced) [<a href="/pdf/2310.19391" title="Download PDF">pdf</a>, <a href="/format/2310.19391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Fair Metric: Bridging Causality, Individual Fairness, and  Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehyaei%2C+A">Ahmad-Reza Ehyaei</a>, 
<a href="/search/cs?searchtype=author&query=Farnadi%2C+G">Golnoosh Farnadi</a>, 
<a href="/search/cs?searchtype=author&query=Samadi%2C+S">Samira Samadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19917" title="Abstract">arXiv:2310.19917</a> (replaced) [<a href="/pdf/2310.19917" title="Download PDF">pdf</a>, <a href="/ps/2310.19917" title="Download PostScript">ps</a>, <a href="/format/2310.19917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Bias in AI: A Systematic Review of Bias Detection and  Mitigation Strategies in Electronic Health Record-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Julie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiaqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 2 figures, 4 tables, 2 supplementary files, 69 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20192" title="Abstract">arXiv:2310.20192</a> (replaced) [<a href="/pdf/2310.20192" title="Download PDF">pdf</a>, <a href="/format/2310.20192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaping Opinions in Social Networks with Shadow Banning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Shao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+T">Tauhid Zaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00768" title="Abstract">arXiv:2311.00768</a> (replaced) [<a href="/pdf/2311.00768" title="Download PDF">pdf</a>, <a href="/format/2311.00768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Training Paradigms for Clinical Feature Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yurong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+M">Manuel Burger</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A4tsch%2C+G">Gunnar R&#xe4;tsch</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+R">Rita Kuznetsova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Poster at "NeurIPS 2023 Workshop: Self-Supervised Learning - Theory and Practice"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01344" title="Abstract">arXiv:2311.01344</a> (replaced) [<a href="/pdf/2311.01344" title="Download PDF">pdf</a>, <a href="/format/2311.01344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Like an Open Book? Read Neural Network Architecture with Simple Power  Analysis on 32-bit Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joud%2C+R">Raphael Joud</a>, 
<a href="/search/cs?searchtype=author&query=Moellic%2C+P">Pierre-Alain Moellic</a>, 
<a href="/search/cs?searchtype=author&query=Pontie%2C+S">Simon Pontie</a>, 
<a href="/search/cs?searchtype=author&query=Rigaud%2C+J">Jean-Baptiste Rigaud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted CARDIS 2023; ANR PICTURE PROJECT (ANR-20-CE39-0013)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02427" title="Abstract">arXiv:2311.02427</a> (replaced) [<a href="/pdf/2311.02427" title="Download PDF">pdf</a>, <a href="/format/2311.02427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Succinct Data Structure for Graphs with $d$-Dimensional  $t$-Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balakrishnan%2C+G">Girish Balakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Sankardeep Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+S">Seungbum Jo</a>, 
<a href="/search/cs?searchtype=author&query=Narayanaswamy%2C+N+S">N S Narayanaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Sadakane%2C+K">Kunihiko Sadakane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03191" title="Abstract">arXiv:2311.03191</a> (replaced) [<a href="/pdf/2311.03191" title="Download PDF">pdf</a>, <a href="/format/2311.03191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepInception: Hypnotize Large Language Model to Be Jailbreaker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanke Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03300" title="Abstract">arXiv:2311.03300</a> (replaced) [<a href="/pdf/2311.03300" title="Download PDF">pdf</a>, <a href="/format/2311.03300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Run-to-Run Feedforward Control of Electromechanical Switching  Devices: a Sensitivity-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ramirez-Laboreo%2C+E">Edgar Ramirez-Laboreo</a> (1), 
<a href="/search/eess?searchtype=author&query=Moya-Lasheras%2C+E">Eduardo Moya-Lasheras</a> (1), 
<a href="/search/eess?searchtype=author&query=Serrano-Seco%2C+E">Eloy Serrano-Seco</a> (1) ((1) Universidad de Zaragoza)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures. Typo corrected in Table I. Figure 2 updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03615" title="Abstract">arXiv:2311.03615</a> (replaced) [<a href="/pdf/2311.03615" title="Download PDF">pdf</a>, <a href="/format/2311.03615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data  Centers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jieming Bian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, Experiments Updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04076" title="Abstract">arXiv:2311.04076</a> (replaced) [<a href="/pdf/2311.04076" title="Download PDF">pdf</a>, <a href="/format/2311.04076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do LLMs exhibit human-like response biases? A case study in survey  design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tjuatja%2C+L">Lindia Tjuatja</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Valerie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S+T">Sherry Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Talwalkar%2C+A">Ameet Talwalkar</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08313" title="Abstract">arXiv:2311.08313</a> (replaced) [<a href="/pdf/2311.08313" title="Download PDF">pdf</a>, <a href="/ps/2311.08313" title="Download PostScript">ps</a>, <a href="/format/2311.08313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fast Track to Full Gold Open Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudeli%C4%87%2C+R">Robert Kudeli&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08588" title="Abstract">arXiv:2311.08588</a> (replaced) [<a href="/pdf/2311.08588" title="Download PDF">pdf</a>, <a href="/format/2311.08588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeScope: An Execution-based Multilingual Multitask Multidimensional  Benchmark for Evaluating LLMs on Code Understanding and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Weixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haitian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weishan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Li Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuiguang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+H">Hari Sundaram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09386" title="Abstract">arXiv:2311.09386</a> (replaced) [<a href="/pdf/2311.09386" title="Download PDF">pdf</a>, <a href="/format/2311.09386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaghooti%2C+B">Bahram Yaghooti</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+N">Netanel Raviv</a>, 
<a href="/search/cs?searchtype=author&query=Sinopoli%2C+B">Bruno Sinopoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12997" title="Abstract">arXiv:2311.12997</a> (replaced) [<a href="/pdf/2311.12997" title="Download PDF">pdf</a>, <a href="/format/2311.12997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Capabilities of Autoregressive Transformers: A Study on  Synthetic, Interpretable Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+R">Rahul Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Khona%2C+M">Mikail Khona</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R+P">Robert P. Dick</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13871" title="Abstract">arXiv:2311.13871</a> (replaced) [<a href="/pdf/2311.13871" title="Download PDF">pdf</a>, <a href="/format/2311.13871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legal Requirements Analysis: A Regulatory Compliance Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abualhaija%2C+S">Sallam Abualhaija</a>, 
<a href="/search/cs?searchtype=author&query=Ceci%2C+M">Marcello Ceci</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L">Lionel Briand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15607" title="Abstract">arXiv:2311.15607</a> (replaced) [<a href="/pdf/2311.15607" title="Download PDF">pdf</a>, <a href="/format/2311.15607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially Covariant Image Registration with Text Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+M">Min Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+R">Rongguang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+R">Renjiu Hu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Dongdong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+G">Gaolei Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Hang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15843" title="Abstract">arXiv:2311.15843</a> (replaced) [<a href="/e-print/2311.15843" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust observer-based control with modularity and exponential stability  for interconnected systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Heydarishahna%2C+M">Mehdi Heydarishahna</a>, 
<a href="/search/eess?searchtype=author&query=Bahari%2C+M">Mohammad Bahari</a>, 
<a href="/search/eess?searchtype=author&query=Mattila%2C+J">Jouni Mattila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We are developing the paper and will submit it later
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15852" title="Abstract">arXiv:2311.15852</a> (replaced) [<a href="/pdf/2311.15852" title="Download PDF">pdf</a>, <a href="/format/2311.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Auto-Tuning Fault-Tolerant Control of N Degrees-of-Freedom  Manipulators Subject to Torque Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahna%2C+M+H">Mehdi Heydari Shahna</a>, 
<a href="/search/cs?searchtype=author&query=Mattila%2C+J">Jouni Mattila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted for possible publication in the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16839" title="Abstract">arXiv:2311.16839</a> (replaced) [<a href="/pdf/2311.16839" title="Download PDF">pdf</a>, <a href="/format/2311.16839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware  Direct Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+L">Linke Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://opendatalab.github.io/HA-DPO">this https URL</a>, Code: <a href="https://github.com/opendatalab/HA-DPO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17536" title="Abstract">arXiv:2311.17536</a> (replaced) [<a href="/pdf/2311.17536" title="Download PDF">pdf</a>, <a href="/format/2311.17536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmoothVideo: Smooth Video Synthesis with Noise Constraints on Diffusion  Models for One-shot Video Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Haoran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruisi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Linxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chaotian Song</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qinglin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01037" title="Abstract">arXiv:2312.01037</a> (replaced) [<a href="/pdf/2312.01037" title="Download PDF">pdf</a>, <a href="/format/2312.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting Latent Knowledge from Quirky Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Belrose%2C+N">Nora Belrose</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01549" title="Abstract">arXiv:2312.01549</a> (replaced) [<a href="/pdf/2312.01549" title="Download PDF">pdf</a>, <a href="/ps/2312.01549" title="Download PostScript">ps</a>, <a href="/format/2312.01549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive Non-Compatibility of Optimistic Rollups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Landis%2C+D">Daji Landis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02409" title="Abstract">arXiv:2312.02409</a> (replaced) [<a href="/pdf/2312.02409" title="Download PDF">pdf</a>, <a href="/format/2312.02409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yiqian Gan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Hao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yizhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Ethan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+L">Lingting Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02650" title="Abstract">arXiv:2312.02650</a> (replaced) [<a href="/pdf/2312.02650" title="Download PDF">pdf</a>, <a href="/format/2312.02650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a convex cost-to-go for single step model predictive control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Turan%2C+E+M">E. M. Turan</a>, 
<a href="/search/eess?searchtype=author&query=Mdoe%2C+Z">Z. Mdoe</a>, 
<a href="/search/eess?searchtype=author&query=J%C3%A4schke%2C+J">J. J&#xe4;schke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03414" title="Abstract">arXiv:2312.03414</a> (replaced) [<a href="/pdf/2312.03414" title="Download PDF">pdf</a>, <a href="/format/2312.03414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed Context Memory For Online Language Model Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jang-Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yeom%2C+J">Junyoung Yeom</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Add streaming setting results and training set analyses
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03523" title="Abstract">arXiv:2312.03523</a> (replaced) [<a href="/pdf/2312.03523" title="Download PDF">pdf</a>, <a href="/format/2312.03523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sig-Networks Toolkit: Signature Networks for Longitudinal Language  Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseriotou%2C+T">Talia Tseriotou</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R+S">Ryan Sze-Yin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Tsakalidis%2C+A">Adam Tsakalidis</a>, 
<a href="/search/cs?searchtype=author&query=Bilal%2C+I+M">Iman Munire Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Kochkina%2C+E">Elena Kochkina</a>, 
<a href="/search/cs?searchtype=author&query=Lyons%2C+T">Terry Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EACL 2024: System Demonstrations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04511" title="Abstract">arXiv:2312.04511</a> (replaced) [<a href="/pdf/2312.04511" title="Download PDF">pdf</a>, <a href="/format/2312.04511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An LLM Compiler for Parallel Function Calling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Suhong Moon</a>, 
<a href="/search/cs?searchtype=author&query=Tabrizi%2C+R">Ryan Tabrizi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Nicholas Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05462" title="Abstract">arXiv:2312.05462</a> (replaced) [<a href="/pdf/2312.05462" title="Download PDF">pdf</a>, <a href="/format/2312.05462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumanReg: Self-supervised Non-rigid Registration of Human Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhiyu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhicheng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenxuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jianjiang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07335" title="Abstract">arXiv:2312.07335</a> (replaced) [<a href="/pdf/2312.07335" title="Download PDF">pdf</a>, <a href="/format/2312.07335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Particle Maximum Likelihood
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+J+N">Jen Ning Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kuntz%2C+J">Juan Kuntz</a>, 
<a href="/search/cs?searchtype=author&query=Power%2C+S">Samuel Power</a>, 
<a href="/search/cs?searchtype=author&query=Johansen%2C+A+M">Adam M. Johansen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07348" title="Abstract">arXiv:2312.07348</a> (replaced) [<a href="/pdf/2312.07348" title="Download PDF">pdf</a>, <a href="/format/2312.07348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;It doesn&#x27;t tell me anything about how my data is used&#x27;&#x27;: User  Perceptions of Data Collection Purposes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kyi%2C+L">Lin Kyi</a>, 
<a href="/search/cs?searchtype=author&query=Mhaidli%2C+A">Abraham Mhaidli</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+C">Cristiana Santos</a>, 
<a href="/search/cs?searchtype=author&query=Roesner%2C+F">Franziska Roesner</a>, 
<a href="/search/cs?searchtype=author&query=Biega%2C+A">Asia Biega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 2024 ACM Conference on Human Factors in Computing Systems (CHI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07541" title="Abstract">arXiv:2312.07541</a> (replaced) [<a href="/pdf/2312.07541" title="Download PDF">pdf</a>, <a href="/format/2312.07541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMERF: Streamable Memory Efficient Radiance Fields for Real-Time  Large-Scene Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duckworth%2C+D">Daniel Duckworth</a>, 
<a href="/search/cs?searchtype=author&query=Hedman%2C+P">Peter Hedman</a>, 
<a href="/search/cs?searchtype=author&query=Reiser%2C+C">Christian Reiser</a>, 
<a href="/search/cs?searchtype=author&query=Zhizhin%2C+P">Peter Zhizhin</a>, 
<a href="/search/cs?searchtype=author&query=Thibert%2C+J">Jean-Fran&#xe7;ois Thibert</a>, 
<a href="/search/cs?searchtype=author&query=Lu%C4%8Di%C4%87%2C+M">Mario Lu&#x10d;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Szeliski%2C+R">Richard Szeliski</a>, 
<a href="/search/cs?searchtype=author&query=Barron%2C+J+T">Jonathan T. Barron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added appendix. Changed LaTeX template. Project website: <a href="https://smerf-3d.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08768" title="Abstract">arXiv:2312.08768</a> (replaced) [<a href="/pdf/2312.08768" title="Download PDF">pdf</a>, <a href="/format/2312.08768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Conditional Controlling for Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yibo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zekai Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hengjia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=lu%2C+q">qinglin lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08846" title="Abstract">arXiv:2312.08846</a> (replaced) [<a href="/pdf/2312.08846" title="Download PDF">pdf</a>, <a href="/format/2312.08846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TiMix: Text-aware Image Mixing for Effective Vision-Language  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=ye%2C+W">Wei ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08927" title="Abstract">arXiv:2312.08927</a> (replaced) [<a href="/pdf/2312.08927" title="Download PDF">pdf</a>, <a href="/format/2312.08927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes  Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Jain%2C+K">Konark Jain</a>, 
<a href="/search/q-fin?searchtype=author&query=Firoozye%2C+N">Nick Firoozye</a>, 
<a href="/search/q-fin?searchtype=author&query=Kochems%2C+J">Jonathan Kochems</a>, 
<a href="/search/q-fin?searchtype=author&query=Treleaven%2C+P">Philip Treleaven</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended abstract for the poster presented at the Market Microstructure workshop. The full paper will follow in the future
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Computational Engineering, Finance, and Science (cs.CE); Computational Finance (q-fin.CP); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10794" title="Abstract">arXiv:2312.10794</a> (replaced) [<a href="/pdf/2312.10794" title="Download PDF">pdf</a>, <a href="/format/2312.10794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mathematical perspective on Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geshkovski%2C+B">Borjan Geshkovski</a>, 
<a href="/search/cs?searchtype=author&query=Letrouit%2C+C">Cyril Letrouit</a>, 
<a href="/search/cs?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>, 
<a href="/search/cs?searchtype=author&query=Rigollet%2C+P">Philippe Rigollet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Analysis of PDEs (math.AP); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11509" title="Abstract">arXiv:2312.11509</a> (replaced) [<a href="/pdf/2312.11509" title="Download PDF">pdf</a>, <a href="/format/2312.11509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Reinforcement-Learning-Based System for Adjusting Medication to  Minimize Speech Disfluency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Constas%2C+P">Pavlos Constas</a>, 
<a href="/search/cs?searchtype=author&query=Rawal%2C+V">Vikram Rawal</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+M+H">Matthew Honorio Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Constas%2C+A">Andreas Constas</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Aditya Khan</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+K">Kaison Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Sultani%2C+N">Najma Sultani</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Carrie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Altomare%2C+M">Micol Altomare</a>, 
<a href="/search/cs?searchtype=author&query=Akzam%2C+M">Michael Akzam</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+V">Vhea He</a>, 
<a href="/search/cs?searchtype=author&query=Altomare%2C+L">Lauren Altomare</a>, 
<a href="/search/cs?searchtype=author&query=Murqi%2C+H">Heraa Murqi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Asad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Bhanshali%2C+N+A">Nimit Amikumar Bhanshali</a>, 
<a href="/search/cs?searchtype=author&query=Rachad%2C+Y">Youssef Rachad</a>, 
<a href="/search/cs?searchtype=author&query=Guerzhoy%2C+M">Michael Guerzhoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proc. Machine Learning for Cognitive and Mental Health Workshop (ML4CMH) at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12044" title="Abstract">arXiv:2312.12044</a> (replaced) [<a href="/pdf/2312.12044" title="Download PDF">pdf</a>, <a href="/format/2312.12044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Zisman%2C+I">Ilya Zisman</a>, 
<a href="/search/cs?searchtype=author&query=Agarkov%2C+A">Artem Agarkov</a>, 
<a href="/search/cs?searchtype=author&query=Sinii%2C+V">Viacheslav Sinii</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, Workshop, Source code: <a href="https://github.com/corl-team/xland-minigrid">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14215" title="Abstract">arXiv:2312.14215</a> (replaced) [<a href="/pdf/2312.14215" title="Download PDF">pdf</a>, <a href="/format/2312.14215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimLM: Can Language Models Infer Parameters of Physical Systems?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Memery%2C+S">Sean Memery</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Subr%2C+K">Kartic Subr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15122" title="Abstract">arXiv:2312.15122</a> (replaced) [<a href="/pdf/2312.15122" title="Download PDF">pdf</a>, <a href="/format/2312.15122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Is All You Need: Autonomous Driving with JAX-Accelerated  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harmel%2C+M">Moritz Harmel</a>, 
<a href="/search/cs?searchtype=author&query=Paras%2C+A">Anubhav Paras</a>, 
<a href="/search/cs?searchtype=author&query=Pasternak%2C+A">Andreas Pasternak</a>, 
<a href="/search/cs?searchtype=author&query=Linscott%2C+G">Gary Linscott</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15220" title="Abstract">arXiv:2312.15220</a> (replaced) [<a href="/pdf/2312.15220" title="Download PDF">pdf</a>, <a href="/format/2312.15220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look-ahead Search on Top of Policy Networks in Imperfect Information  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubicek%2C+O">Ondrej Kubicek</a>, 
<a href="/search/cs?searchtype=author&query=Burch%2C+N">Neil Burch</a>, 
<a href="/search/cs?searchtype=author&query=Lisy%2C+V">Viliam Lisy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15574" title="Abstract">arXiv:2312.15574</a> (replaced) [<a href="/pdf/2312.15574" title="Download PDF">pdf</a>, <a href="/format/2312.15574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustered Switchback Experiments: Near-Optimal Rates Under  Spatiotemporal Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jia%2C+S">Su Jia</a>, 
<a href="/search/math?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+C+L">Christina Lee Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16624" title="Abstract">arXiv:2312.16624</a> (replaced) [<a href="/pdf/2312.16624" title="Download PDF">pdf</a>, <a href="/format/2312.16624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-stage optimizer for systematic overestimation adjustment applied to  multi-objective genetic algorithms for biomarker selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Cattelani%2C+L">Luca Cattelani</a>, 
<a href="/search/q-bio?searchtype=author&query=Fortino%2C+V">Vittorio Fortino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added a picture with the algorithm steps and a supplementary section with disambiguation of the technical terms. Moved sections in the supplementary to shorten the main text. Fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17058" title="Abstract">arXiv:2312.17058</a> (replaced) [<a href="/pdf/2312.17058" title="Download PDF">pdf</a>, <a href="/ps/2312.17058" title="Download PostScript">ps</a>, <a href="/format/2312.17058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Sharing mechanisms for DAOs: On the optimality of Shapley mechanism  for funding public excludable goods under Sybil strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazorra%2C+B">Bruno Mazorra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17080" title="Abstract">arXiv:2312.17080</a> (replaced) [<a href="/pdf/2312.17080" title="Download PDF">pdf</a>, <a href="/format/2312.17080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhongshen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jiaya Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/dvlab-research/MR-GSM8K">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17748" title="Abstract">arXiv:2312.17748</a> (replaced) [<a href="/pdf/2312.17748" title="Download PDF">pdf</a>, <a href="/format/2312.17748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-PERM: Personalized Response Generation Using Dynamic Knowledge  Retrieval and Persona-Adaptive Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raj%2C+K">Kanak Raj</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>, 
<a href="/search/cs?searchtype=author&query=Bonagiri%2C+V">Vamshi Bonagiri</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+P">Priyanshul Govil</a>, 
<a href="/search/cs?searchtype=author&query=Thirunarayanan%2C+K">Krishnaprasad Thirunarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+M">Manas Gaur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024 Spring Symposium Series
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00006" title="Abstract">arXiv:2401.00006</a> (replaced) [<a href="/pdf/2401.00006" title="Download PDF">pdf</a>, <a href="/format/2401.00006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Open-Ended Embodied Agent via Language-Policy Bidirectional  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shaopeng Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fuxian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Ming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jing Hou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00736" title="Abstract">arXiv:2401.00736</a> (replaced) [<a href="/pdf/2401.00736" title="Download PDF">pdf</a>, <a href="/format/2401.00736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models, Image Super-Resolution And Everything: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+B">Brian B. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Shanbhag%2C+A+S">Arundhati S. Shanbhag</a>, 
<a href="/search/cs?searchtype=author&query=Raue%2C+F">Federico Raue</a>, 
<a href="/search/cs?searchtype=author&query=Frolov%2C+S">Stanislav Frolov</a>, 
<a href="/search/cs?searchtype=author&query=Palacio%2C+S">Sebastian Palacio</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01148" title="Abstract">arXiv:2401.01148</a> (replaced) [<a href="/pdf/2401.01148" title="Download PDF">pdf</a>, <a href="/ps/2401.01148" title="Download PostScript">ps</a>, <a href="/format/2401.01148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Bayes-Chernoff bounds for unbounded losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Casado%2C+I">Ioar Casado</a>, 
<a href="/search/stat?searchtype=author&query=Ortega%2C+L+A">Luis A. Ortega</a>, 
<a href="/search/stat?searchtype=author&query=Masegosa%2C+A+R">Andr&#xe9;s R. Masegosa</a>, 
<a href="/search/stat?searchtype=author&query=P%C3%A9rez%2C+A">Aritz P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated Section 5
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02225" title="Abstract">arXiv:2401.02225</a> (replaced) [<a href="/pdf/2401.02225" title="Download PDF">pdf</a>, <a href="/format/2401.02225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory-Oriented Policy Optimization with Sparse Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guojian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Faguo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02695" title="Abstract">arXiv:2401.02695</a> (replaced) [<a href="/pdf/2401.02695" title="Download PDF">pdf</a>, <a href="/format/2401.02695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingxian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yi Hou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Ji Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03301" title="Abstract">arXiv:2401.03301</a> (replaced) [<a href="/pdf/2401.03301" title="Download PDF">pdf</a>, <a href="/format/2401.03301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Sample-Efficient Offline Reinforcement Learning: Data Diversity,  Posterior Sampling, and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen-Tang%2C+T">Thanh Nguyen-Tang</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+R">Raman Arora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23; Arxiv is the authors' preferred version; v2: add a missing related work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03892" title="Abstract">arXiv:2401.03892</a> (replaced) [<a href="/pdf/2401.03892" title="Download PDF">pdf</a>, <a href="/format/2401.03892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling in Unit Time with Kernel Fisher-Rao Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Maurais%2C+A">Aimee Maurais</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated with additional numerical examples and a stochastic variant of the approach
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04242" title="Abstract">arXiv:2401.04242</a> (replaced) [<a href="/pdf/2401.04242" title="Download PDF">pdf</a>, <a href="/ps/2401.04242" title="Download PostScript">ps</a>, <a href="/format/2401.04242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automata and coalgebras in categories of species
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Loregian%2C+F">Fosco Loregian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Hom. Il. 18.371-376
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04795" title="Abstract">arXiv:2401.04795</a> (replaced) [<a href="/pdf/2401.04795" title="Download PDF">pdf</a>, <a href="/format/2401.04795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First 100 days of pandemic; an interplay of pharmaceutical, behavioral  and digital interventions -- A study using agent based modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gauri Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kapila%2C+R">Ritvik Kapila</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+A">Ayush Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Raskar%2C+R">Ramesh Raskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, In Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024), Auckland, New Zealand, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05074" title="Abstract">arXiv:2401.05074</a> (replaced) [<a href="/pdf/2401.05074" title="Download PDF">pdf</a>, <a href="/format/2401.05074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Occupancy Prediction for Building Energy Systems with Latent Force  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wietzke%2C+T">Thore Wietzke</a>, 
<a href="/search/eess?searchtype=author&query=Gall%2C+J">Jan Gall</a>, 
<a href="/search/eess?searchtype=author&query=Graichen%2C+K">Knut Graichen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to Energy and Buildings, data and code available at <a href="https://github.com/ThoreWietzke/occupancy-benchmark-dataset">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Energy and Buildings, Volume 307 (2024) 113968
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06118" title="Abstract">arXiv:2401.06118</a> (replaced) [<a href="/pdf/2401.06118" title="Download PDF">pdf</a>, <a href="/format/2401.06118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Compression of Large Language Models via Additive Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egiazarian%2C+V">Vage Egiazarian</a>, 
<a href="/search/cs?searchtype=author&query=Panferov%2C+A">Andrei Panferov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznedelev%2C+D">Denis Kuznedelev</a>, 
<a href="/search/cs?searchtype=author&query=Frantar%2C+E">Elias Frantar</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07237" title="Abstract">arXiv:2401.07237</a> (replaced) [<a href="/pdf/2401.07237" title="Download PDF">pdf</a>, <a href="/format/2401.07237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Event Sequence Knowledge From Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wadhwa%2C+S">Somin Wadhwa</a>, 
<a href="/search/cs?searchtype=author&query=Hassanzadeh%2C+O">Oktie Hassanzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjya%2C+D">Debarun Bhattacharjya</a>, 
<a href="/search/cs?searchtype=author&query=Barker%2C+K">Ken Barker</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jian Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07844" title="Abstract">arXiv:2401.07844</a> (replaced) [<a href="/pdf/2401.07844" title="Download PDF">pdf</a>, <a href="/ps/2401.07844" title="Download PostScript">ps</a>, <a href="/format/2401.07844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ODE Method for Stochastic Approximation and Reinforcement Learning  with Markovian Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shangtong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09885" title="Abstract">arXiv:2401.09885</a> (replaced) [<a href="/pdf/2401.09885" title="Download PDF">pdf</a>, <a href="/format/2401.09885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source Code Clone Detection Using Unsupervised Similarity Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez-Gil%2C+J">Jorge Martinez-Gil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication as Full Paper in the Software Quality Days 2024, Vienna, Austria
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09902" title="Abstract">arXiv:2401.09902</a> (replaced) [<a href="/pdf/2401.09902" title="Download PDF">pdf</a>, <a href="/format/2401.09902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interplay between depth and width for interpolation in neural ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=%C3%81lvarez-L%C3%B3pez%2C+A">Antonio &#xc1;lvarez-L&#xf3;pez</a>, 
<a href="/search/math?searchtype=author&query=Slimane%2C+A+H">Arselane Hadj Slimane</a>, 
<a href="/search/math?searchtype=author&query=Zuazua%2C+E">Enrique Zuazua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures, double column
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10119" title="Abstract">arXiv:2401.10119</a> (replaced) [<a href="/pdf/2401.10119" title="Download PDF">pdf</a>, <a href="/format/2401.10119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Principled Graph Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+L">Luis M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Kusuma%2C+D">Daniel Kusuma</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10463" title="Abstract">arXiv:2401.10463</a> (replaced) [<a href="/pdf/2401.10463" title="Download PDF">pdf</a>, <a href="/format/2401.10463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical Data Size of Language Models from a Grokking Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xuekai Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bowen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11063" title="Abstract">arXiv:2401.11063</a> (replaced) [<a href="/pdf/2401.11063" title="Download PDF">pdf</a>, <a href="/format/2401.11063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Best Ends by the Best Means: Ethical Concerns in App Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olson%2C+L">Lauren Olson</a>, 
<a href="/search/cs?searchtype=author&query=Tjikhoeri%2C+N">Neelam Tjikhoeri</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n%2C+E">Emitz&#xe1; Guzm&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11708" title="Abstract">arXiv:2401.11708</a> (replaced) [<a href="/pdf/2401.11708" title="Download PDF">pdf</a>, <a href="/format/2401.11708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mastering Text-to-Image Diffusion: Recaptioning, Planning, and  Generating with Multimodal LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Ling Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+B">Bin Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13227" title="Abstract">arXiv:2401.13227</a> (replaced) [<a href="/pdf/2401.13227" title="Download PDF">pdf</a>, <a href="/format/2401.13227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LPNL: Scalable Link Prediction with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Baolong Bi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13967" title="Abstract">arXiv:2401.13967</a> (replaced) [<a href="/pdf/2401.13967" title="Download PDF">pdf</a>, <a href="/format/2401.13967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual-oriented Learned Image Compression with Dynamic Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+N">Nianxiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huairui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenzhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14199" title="Abstract">arXiv:2401.14199</a> (replaced) [<a href="/pdf/2401.14199" title="Download PDF">pdf</a>, <a href="/format/2401.14199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MTRGL:Effective Temporal Correlation Discerning through Multi-modal  Temporal Relational Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinhui Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; General Economics (econ.GN); Trading and Market Microstructure (q-fin.TR)

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14375" title="Abstract">arXiv:2401.14375</a> (replaced) [<a href="/pdf/2401.14375" title="Download PDF">pdf</a>, <a href="/format/2401.14375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The GraphTempo Framework for Exploring the Evolution of a Graph through  Pattern Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsoukanara%2C+E">Evangelia Tsoukanara</a>, 
<a href="/search/cs?searchtype=author&query=Koloniari%2C+G">Georgia Koloniari</a>, 
<a href="/search/cs?searchtype=author&query=Pitoura%2C+E">Evaggelia Pitoura</a>, 
<a href="/search/cs?searchtype=author&query=Triantafillou%2C+P">Peter Triantafillou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14422" title="Abstract">arXiv:2401.14422</a> (replaced) [<a href="/pdf/2401.14422" title="Download PDF">pdf</a>, <a href="/format/2401.14422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar  Power Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Shazid Islam</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+A+S+M+J">A S M Jahid Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+S">Md Saydur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Yusuf%2C+J">Jubair Yusuf</a>, 
<a href="/search/cs?searchtype=author&query=Sajol%2C+M+S+I">Md Saiful Islam Sajol</a>, 
<a href="/search/cs?searchtype=author&query=Tumpa%2C+F+A">Farhana Akter Tumpa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15270" title="Abstract">arXiv:2401.15270</a> (replaced) [<a href="/pdf/2401.15270" title="Download PDF">pdf</a>, <a href="/format/2401.15270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiqun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhili Li</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaowei Jia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+A">Aolin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024 (preprint)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15497" title="Abstract">arXiv:2401.15497</a> (replaced) [<a href="/pdf/2401.15497" title="Download PDF">pdf</a>, <a href="/format/2401.15497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foregrounding Artist Opinions: A Survey Study on Transparency,  Ownership, and Fairness in AI Generative Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lovato%2C+J">Juniper Lovato</a>, 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+J">Julia Zimmerman</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+I">Isabelle Smith</a>, 
<a href="/search/cs?searchtype=author&query=Dodds%2C+P">Peter Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Karson%2C+J">Jennifer Karson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15628" title="Abstract">arXiv:2401.15628</a> (replaced) [<a href="/pdf/2401.15628" title="Download PDF">pdf</a>, <a href="/format/2401.15628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WetSpongeCake: a Surface Appearance Model Considering Porosity and  Saturation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gaole Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yuang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beibei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15656" title="Abstract">arXiv:2401.15656</a> (replaced) [<a href="/pdf/2401.15656" title="Download PDF">pdf</a>, <a href="/format/2401.15656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLsM: Generative Linguistic Steganography with Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ruiqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingxiao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16355" title="Abstract">arXiv:2401.16355</a> (replaced) [<a href="/e-print/2401.16355" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding  and Reasoning in Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenglu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sunyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qizi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunlong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiaoxiao Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mengyue Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingxiong Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xinheng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> make source and method updates before resubmission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16979" title="Abstract">arXiv:2401.16979</a> (replaced) [<a href="/pdf/2401.16979" title="Download PDF">pdf</a>, <a href="/format/2401.16979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re3val: Reinforced and Reranked Generative Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+E">EuiYul Song</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangryul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Haeju Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, Findings of the Association for Computational Linguistics: EACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17819" title="Abstract">arXiv:2401.17819</a> (replaced) [<a href="/pdf/2401.17819" title="Download PDF">pdf</a>, <a href="/ps/2401.17819" title="Download PostScript">ps</a>, <a href="/format/2401.17819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QTFlow: Quantitative Timing-Sensitive Information Flow for  Security-Aware Hardware Design on RTL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reimann%2C+L+M">Lennart M. Reimann</a>, 
<a href="/search/cs?searchtype=author&query=Prashar%2C+A">Anshul Prashar</a>, 
<a href="/search/cs?searchtype=author&query=Ghinami%2C+C">Chiara Ghinami</a>, 
<a href="/search/cs?searchtype=author&query=Pelke%2C+R">Rebecca Pelke</a>, 
<a href="/search/cs?searchtype=author&query=Sisejkovic%2C+D">Dominik Sisejkovic</a>, 
<a href="/search/cs?searchtype=author&query=Merchant%2C+F">Farhad Merchant</a>, 
<a href="/search/cs?searchtype=author&query=Leupers%2C+R">Rainer Leupers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at IEEE VLSI-DAT 2024, Taiwan; 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17826" title="Abstract">arXiv:2401.17826</a> (replaced) [<a href="/pdf/2401.17826" title="Download PDF">pdf</a>, <a href="/format/2401.17826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory  Generation and Uncertainty Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangcheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Linwei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+R">Ruoyu Geng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hexiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lujia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jianhao Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures. Accepted by 2024 IEEE/ASME Transactions on Mechatronics (TMECH)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17907" title="Abstract">arXiv:2401.17907</a> (replaced) [<a href="/pdf/2401.17907" title="Download PDF">pdf</a>, <a href="/format/2401.17907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and  Visual-inertial Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez-Tu%C3%B1%C3%B3n%2C+O">Olaya &#xc1;lvarez-Tu&#xf1;&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Marnet%2C+L+R">Luiza Ribeiro Marnet</a>, 
<a href="/search/cs?searchtype=author&query=Antal%2C+L">L&#xe1;szl&#xf3; Antal</a>, 
<a href="/search/cs?searchtype=author&query=Aubard%2C+M">Martin Aubard</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Maria Costa</a>, 
<a href="/search/cs?searchtype=author&query=Brodskiy%2C+Y">Yury Brodskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00326" title="Abstract">arXiv:2402.00326</a> (replaced) [<a href="/pdf/2402.00326" title="Download PDF">pdf</a>, <a href="/format/2402.00326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PirateNets: Physics-informed Deep Learning with Residual Adaptive  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Perdikaris%2C+P">Paris Perdikaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 Pages, 15 Figures, 8 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00522" title="Abstract">arXiv:2402.00522</a> (replaced) [<a href="/pdf/2402.00522" title="Download PDF">pdf</a>, <a href="/ps/2402.00522" title="Download PostScript">ps</a>, <a href="/format/2402.00522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Expressive Power and Mechanisms of Transformer for  Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=E%2C+W">Weinan E</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00809" title="Abstract">arXiv:2402.00809</a> (replaced) [<a href="/pdf/2402.00809" title="Download PDF">pdf</a>, <a href="/format/2402.00809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Skoularidou%2C+M">Maria Skoularidou</a>, 
<a href="/search/cs?searchtype=author&query=Palla%2C+K">Konstantina Palla</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+J">Julyan Arbel</a>, 
<a href="/search/cs?searchtype=author&query=Dunson%2C+D">David Dunson</a>, 
<a href="/search/cs?searchtype=author&query=Filippone%2C+M">Maurizio Filippone</a>, 
<a href="/search/cs?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>, 
<a href="/search/cs?searchtype=author&query=Lobato%2C+J+M+H">Jose Miguel Hernandez Lobato</a>, 
<a href="/search/cs?searchtype=author&query=Hubin%2C+A">Aliaksandr Hubin</a>, 
<a href="/search/cs?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/cs?searchtype=author&query=Karaletsos%2C+T">Theofanis Karaletsos</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+E">Mohammad Emtiyaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kristiadi%2C+A">Agustinus Kristiadi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>, 
<a href="/search/cs?searchtype=author&query=Nemeth%2C+C">Christopher Nemeth</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>, 
<a href="/search/cs?searchtype=author&query=Rudner%2C+T+G+J">Tim G. J. Rudner</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCgamer%2C+D">David R&#xfc;gamer</a>, 
<a href="/search/cs?searchtype=author&query=Teh%2C+Y+W">Yee Whye Teh</a>, 
<a href="/search/cs?searchtype=author&query=Welling%2C+M">Max Welling</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00854" title="Abstract">arXiv:2402.00854</a> (replaced) [<a href="/pdf/2402.00854" title="Download PDF">pdf</a>, <a href="/format/2402.00854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SymbolicAI: A framework for logic-based approaches combining generative  models and solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinu%2C+M">Marius-Constantin Dinu</a>, 
<a href="/search/cs?searchtype=author&query=Leoveanu-Condrei%2C+C">Claudiu Leoveanu-Condrei</a>, 
<a href="/search/cs?searchtype=author&query=Holzleitner%2C+M">Markus Holzleitner</a>, 
<a href="/search/cs?searchtype=author&query=Zellinger%2C+W">Werner Zellinger</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 12 figures, external resources: framework is available at <a href="https://github.com/ExtensityAI/symbolicai">this https URL</a> and benchmark at <a href="https://github.com/ExtensityAI/benchmark">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Symbolic Computation (cs.SC); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00868" title="Abstract">arXiv:2402.00868</a> (replaced) [<a href="/pdf/2402.00868" title="Download PDF">pdf</a>, <a href="/format/2402.00868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We&#x27;re Not Using Videos Effectively: An Updated Domain Adaptive Video  Segmentation Baseline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kareer%2C+S">Simar Kareer</a>, 
<a href="/search/cs?searchtype=author&query=Vijaykumar%2C+V">Vivek Vijaykumar</a>, 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+H">Harsh Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+P">Prithvijit Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+J">Judy Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+V">Viraj Prabhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TMLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00899" title="Abstract">arXiv:2402.00899</a> (replaced) [<a href="/pdf/2402.00899" title="Download PDF">pdf</a>, <a href="/format/2402.00899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Learners for Correction of AI Errors with Provable  Performance Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyukin%2C+I+Y">Ivan Y. Tyukin</a>, 
<a href="/search/cs?searchtype=author&query=Tyukina%2C+T">Tatiana Tyukina</a>, 
<a href="/search/cs?searchtype=author&query=van+Helden%2C+D">Daniel van Helden</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zedong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mirkes%2C+E+M">Evgeny M. Mirkes</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+O+J">Oliver J. Sutton</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qinghua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gorban%2C+A+N">Alexander N. Gorban</a>, 
<a href="/search/cs?searchtype=author&query=Allison%2C+P">Penelope Allison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00909" title="Abstract">arXiv:2402.00909</a> (replaced) [<a href="/pdf/2402.00909" title="Download PDF">pdf</a>, <a href="/format/2402.00909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing GradCAM for Embedding Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bachhawat%2C+M">Mudit Bachhawat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00966" title="Abstract">arXiv:2402.00966</a> (replaced) [<a href="/pdf/2402.00966" title="Download PDF">pdf</a>, <a href="/ps/2402.00966" title="Download PostScript">ps</a>, <a href="/format/2402.00966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the specification of modal systems: A comparison of three frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos+Escrig%2C+D">David de Frutos Escrig</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Replaced conference version with the extended paper published in a journal (This update includes a change in the title of the paper)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Science of Computer Programming 78. 2013
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00970" title="Abstract">arXiv:2402.00970</a> (replaced) [<a href="/pdf/2402.00970" title="Download PDF">pdf</a>, <a href="/ps/2402.00970" title="Download PostScript">ps</a>, <a href="/format/2402.00970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Are Prime Formulae Characteristic?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=Della+Monica%2C+D">Dario Della Monica</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated conference version with the extended version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Theoretical Computer Science vol 777. 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00973" title="Abstract">arXiv:2402.00973</a> (replaced) [<a href="/pdf/2402.00973" title="Download PDF">pdf</a>, <a href="/ps/2402.00973" title="Download PostScript">ps</a>, <a href="/format/2402.00973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical characterisations, rule formats and compositionality for  input-output conformance simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Gregorio-Rodr%C3%ADguez%2C+C">Carlos Gregorio-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Replaced conference version for extended version (this includes a change in the name of the paper plus additional sections and results)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01217" title="Abstract">arXiv:2402.01217</a> (replaced) [<a href="/pdf/2402.01217" title="Download PDF">pdf</a>, <a href="/format/2402.01217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaokun Li</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+C">Chao Gou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+G">Guang Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01230" title="Abstract">arXiv:2402.01230</a> (replaced) [<a href="/pdf/2402.01230" title="Download PDF">pdf</a>, <a href="/format/2402.01230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trees and co-trees in planar 3-connected planar graphs An easier proof  via Schnyder woods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortlieb%2C+C">Christian Ortlieb</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J+M">Jens M. Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01253" title="Abstract">arXiv:2402.01253</a> (replaced) [<a href="/pdf/2402.01253" title="Download PDF">pdf</a>, <a href="/format/2402.01253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+H">Haolei Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yangping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yuan Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01303" title="Abstract">arXiv:2402.01303</a> (replaced) [<a href="/pdf/2402.01303" title="Download PDF">pdf</a>, <a href="/format/2402.01303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AGILE: Approach-based Grasp Inference Learned from Element Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koosheshi%2C+M">MohammadHossein Koosheshi</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+H">Hamed Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Masouleh%2C+M+T">Mehdi Tale Masouleh</a>, 
<a href="/search/cs?searchtype=author&query=Kalhor%2C+A">Ahmad Kalhor</a>, 
<a href="/search/cs?searchtype=author&query=Yazdi%2C+M+R+H">Mohammad Reza Hairi Yazdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference Paper, ICROM 2023, 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01345" title="Abstract">arXiv:2402.01345</a> (replaced) [<a href="/pdf/2402.01345" title="Download PDF">pdf</a>, <a href="/format/2402.01345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip \n: A simple method to reduce hallucination in Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zongbo Han</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zechen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01370" title="Abstract">arXiv:2402.01370</a> (replaced) [<a href="/pdf/2402.01370" title="Download PDF">pdf</a>, <a href="/format/2402.01370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CC-VPSTO: Chance-Constrained Via-Point-based Stochastic Trajectory  Optimisation for Safe and Efficient Online Robot Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruderm%C3%BCller%2C+L">Lara Bruderm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+G">Guillaume Berger</a>, 
<a href="/search/cs?searchtype=author&query=Jankowski%2C+J">Julius Jankowski</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+R">Raunak Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Hawes%2C+N">Nick Hawes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures, submitted to IEEE Transactions on Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01459" title="Abstract">arXiv:2402.01459</a> (replaced) [<a href="/pdf/2402.01459" title="Download PDF">pdf</a>, <a href="/format/2402.01459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waczy%C5%84ska%2C+J">Joanna Waczy&#x144;ska</a>, 
<a href="/search/cs?searchtype=author&query=Borycki%2C+P">Piotr Borycki</a>, 
<a href="/search/cs?searchtype=author&query=Tadeja%2C+S">S&#x142;awomir Tadeja</a>, 
<a href="/search/cs?searchtype=author&query=Tabor%2C+J">Jacek Tabor</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01645" title="Abstract">arXiv:2402.01645</a> (replaced) [<a href="/pdf/2402.01645" title="Download PDF">pdf</a>, <a href="/ps/2402.01645" title="Download PostScript">ps</a>, <a href="/format/2402.01645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Innovations in Footwear Sensors: Role of Smart Footwear in  Healthcare -- A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R.%2C+P+G">Pradyumna G. R.</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+R+B">Roopa B. Hegde</a>, 
<a href="/search/cs?searchtype=author&query=B.%2C+B+K">Bommegowda K. B.</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+A+K">Anil Kumar Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+G+R">Ganesh R. Naik</a>, 
<a href="/search/cs?searchtype=author&query=Pujari%2C+A+N">Amit N. Pujari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01685" title="Abstract">arXiv:2402.01685</a> (replaced) [<a href="/pdf/2402.01685" title="Download PDF">pdf</a>, <a href="/format/2402.01685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMUTF: Schema Matching Using Generative Tags and Hybrid Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+M">Mei Di</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haozheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+R+T">Richard Tzong-Han Tsai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01704" title="Abstract">arXiv:2402.01704</a> (replaced) [<a href="/pdf/2402.01704" title="Download PDF">pdf</a>, <a href="/format/2402.01704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gemp%2C+I">Ian Gemp</a>, 
<a href="/search/cs?searchtype=author&query=Bachrach%2C+Y">Yoram Bachrach</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+R">Roma Patel</a>, 
<a href="/search/cs?searchtype=author&query=Dasagi%2C+V">Vibhavari Dasagi</a>, 
<a href="/search/cs?searchtype=author&query=Marris%2C+L">Luke Marris</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tuyls%2C+K">Karl Tuyls</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 8 figures, code available @ <a href="https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01763" title="Abstract">arXiv:2402.01763</a> (replaced) [<a href="/pdf/2402.01763" title="Download PDF">pdf</a>, <a href="/format/2402.01763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Large Language Models Meet Vector Databases: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+Z">Zhi Jing</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yongye Su</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yikun Han</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Bo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunjiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kehai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01801" title="Abstract">arXiv:2402.01801</a> (replaced) [<a href="/pdf/2402.01801" title="Download PDF">pdf</a>, <a href="/format/2402.01801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Time Series: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+R+R">Ranak Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R+K">Rajesh K. Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub repository: <a href="https://github.com/xiyuanzh/awesome-llm-time-series">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01817" title="Abstract">arXiv:2402.01817</a> (replaced) [<a href="/pdf/2402.01817" title="Download PDF">pdf</a>, <a href="/format/2402.01817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Can&#x27;t Plan, But Can Help Planning in LLM-Modulo Frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>, 
<a href="/search/cs?searchtype=author&query=Valmeekam%2C+K">Karthik Valmeekam</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+L">Lin Guan</a>, 
<a href="/search/cs?searchtype=author&query=Stechly%2C+K">Kaya Stechly</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+M">Mudit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Bhambri%2C+S">Siddhant Bhambri</a>, 
<a href="/search/cs?searchtype=author&query=Saldyt%2C+L">Lucas Saldyt</a>, 
<a href="/search/cs?searchtype=author&query=Murthy%2C+A">Anil Murthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01852" title="Abstract">arXiv:2402.01852</a> (replaced) [<a href="/pdf/2402.01852" title="Download PDF">pdf</a>, <a href="/ps/2402.01852" title="Download PostScript">ps</a>, <a href="/format/2402.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QPP and HPPK: Unifying Non-Commutativity for Quantum-Secure Cryptography  with Galois Permutation Group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+R">Randy Kuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01881" title="Abstract">arXiv:2402.01881</a> (replaced) [<a href="/pdf/2402.01881" title="Download PDF">pdf</a>, <a href="/format/2402.01881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Agent for Hyper-Parameter Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01905" title="Abstract">arXiv:2402.01905</a> (replaced) [<a href="/pdf/2402.01905" title="Download PDF">pdf</a>, <a href="/format/2402.01905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carthago Delenda Est: Co-opetitive Indirect Information Diffusion Model  for Influence Operations on Online Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Low%2C+J+F">Jwen Fai Low</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+B+C+M">Benjamin C. M. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+F">Farkhund Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Fachkha%2C+C">Claude Fachkha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01960" title="Abstract">arXiv:2402.01960</a> (replaced) [<a href="/pdf/2402.01960" title="Download PDF">pdf</a>, <a href="/format/2402.01960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrated Uncertainty Quantification for Operator Learning via  Conformal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01965" title="Abstract">arXiv:2402.01965</a> (replaced) [<a href="/pdf/2402.01965" title="Download PDF">pdf</a>, <a href="/format/2402.01965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Neural Network-Based Generative Diffusion Models through  Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fangzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01969" title="Abstract">arXiv:2402.01969</a> (replaced) [<a href="/pdf/2402.01969" title="Download PDF">pdf</a>, <a href="/format/2402.01969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-Enhanced Data Augmentation for Machine Learning Pathloss  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A+P">Ahmed P. Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byunghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaguang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hollingsworth%2C+M">Max Hollingsworth</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+C+R">C. Robert Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Krogmeier%2C+J+V">James V. Krogmeier</a>, 
<a href="/search/cs?searchtype=author&query=Love%2C+D+J">David J. Love</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, Accepted at ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02018" title="Abstract">arXiv:2402.02018</a> (replaced) [<a href="/pdf/2402.02018" title="Download PDF">pdf</a>, <a href="/format/2402.02018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Landscape and Challenges of HPC Research and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Le Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Akash Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Arijit Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+Q+I">Quazi Ishtiaque Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Abebe%2C+W">Waqwoya Abebe</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hung Phan</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Aishwarya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+B">Branden Butler</a>, 
<a href="/search/cs?searchtype=author&query=Hasabnis%2C+N">Niranjan Hasabnis</a>, 
<a href="/search/cs?searchtype=author&query=Oren%2C+G">Gal Oren</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+V+A">Vy A. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Munoz%2C+J+P">Juan Pablo Munoz</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T+L">Theodore L. Willke</a>, 
<a href="/search/cs?searchtype=author&query=Mattson%2C+T">Tim Mattson</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02085" title="Abstract">arXiv:2402.02085</a> (replaced) [<a href="/pdf/2402.02085" title="Download PDF">pdf</a>, <a href="/format/2402.02085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeCoF: Generated Video Detection via Frame Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Long Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hongping Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02193" title="Abstract">arXiv:2402.02193</a> (replaced) [<a href="/pdf/2402.02193" title="Download PDF">pdf</a>, <a href="/ps/2402.02193" title="Download PostScript">ps</a>, <a href="/format/2402.02193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Extended ADMM for Three-block Nonconvex Nonseparable Problem with  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Z">Zekun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02205" title="Abstract">arXiv:2402.02205</a> (replaced) [<a href="/pdf/2402.02205" title="Download PDF">pdf</a>, <a href="/format/2402.02205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model  on Complex Traffic Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingcheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A+C">Alois C. Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02229" title="Abstract">arXiv:2402.02229</a> (replaced) [<a href="/pdf/2402.02229" title="Download PDF">pdf</a>, <a href="/format/2402.02229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vanilla Bayesian Optimization Performs Great in High Dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E+O">Erik Orm Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02277" title="Abstract">arXiv:2402.02277</a> (replaced) [<a href="/pdf/2402.02277" title="Download PDF">pdf</a>, <a href="/format/2402.02277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Bayesian Optimization via Exogenous Distribution Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaogang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xiaoning Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02322" title="Abstract">arXiv:2402.02322</a> (replaced) [<a href="/pdf/2402.02322" title="Download PDF">pdf</a>, <a href="/format/2402.02322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Incremental Optimization for Best Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaogang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xiaoning Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2207.02058">arXiv:2207.02058</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02327" title="Abstract">arXiv:2402.02327</a> (replaced) [<a href="/pdf/2402.02327" title="Download PDF">pdf</a>, <a href="/format/2402.02327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhentao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+T">Tao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Q">Qi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jieping Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02380" title="Abstract">arXiv:2402.02380</a> (replaced) [<a href="/pdf/2402.02380" title="Download PDF">pdf</a>, <a href="/ps/2402.02380" title="Download PostScript">ps</a>, <a href="/format/2402.02380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models in Analysing Classroom Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yun Long</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haifeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02416" title="Abstract">arXiv:2402.02416</a> (replaced) [<a href="/pdf/2402.02416" title="Download PDF">pdf</a>, <a href="/format/2402.02416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+H">Hantao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Donghai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02430" title="Abstract">arXiv:2402.02430</a> (replaced) [<a href="/pdf/2402.02430" title="Download PDF">pdf</a>, <a href="/format/2402.02430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Low-level Representations for Ultra-Fast Road Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Feng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yucong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiqun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, IEEE TITS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02441" title="Abstract">arXiv:2402.02441</a> (replaced) [<a href="/pdf/2402.02441" title="Download PDF">pdf</a>, <a href="/format/2402.02441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Agerberg%2C+J">Jens Agerberg</a>, 
<a href="/search/cs?searchtype=author&query=AlJabea%2C+I">Ibrahem AlJabea</a>, 
<a href="/search/cs?searchtype=author&query=Ballester%2C+R">Ruben Ballester</a>, 
<a href="/search/cs?searchtype=author&query=Battiloro%2C+C">Claudio Battiloro</a>, 
<a href="/search/cs?searchtype=author&query=Bern%C3%A1rdez%2C+G">Guillermo Bern&#xe1;rdez</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Brent%2C+A">Aiden Brent</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Gardaa%2C+O+H">Odin Hoff Gardaa</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+G">Gurusankar Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+D">Devendra Govil</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+J">Josef Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M+R">Maneel Reddy Karri</a>, 
<a href="/search/cs?searchtype=author&query=Khouja%2C+J">Jude Khouja</a>, 
<a href="/search/cs?searchtype=author&query=Lecha%2C+M">Manuel Lecha</a>, 
<a href="/search/cs?searchtype=author&query=Livesay%2C+N">Neal Livesay</a>, 
<a href="/search/cs?searchtype=author&query=Mei%C3%9Fner%2C+J">Jan Mei&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Soham Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Nikitin%2C+A">Alexander Nikitin</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Pr%C3%ADlepok%2C+J">Jaro Pr&#xed;lepok</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n-S%C3%A1enz%2C+A">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, 
<a href="/search/cs?searchtype=author&query=Salatiello%2C+A">Alessandro Salatiello</a>, 
<a href="/search/cs?searchtype=author&query=Samaga%2C+S+N">Shreyas N. Samaga</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Scofano%2C+L">Luca Scofano</a>, 
<a href="/search/cs?searchtype=author&query=Spinelli%2C+I">Indro Spinelli</a>, 
<a href="/search/cs?searchtype=author&query=Telyatnikov%2C+L">Lev Telyatnikov</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zaghen%2C+O">Olga Zaghen</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+A">Ali Zia</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02474" title="Abstract">arXiv:2402.02474</a> (replaced) [<a href="/pdf/2402.02474" title="Download PDF">pdf</a>, <a href="/format/2402.02474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Spectral Improvement for Unsupervised Image Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arefi%2C+F">Farnoosh Arefi</a>, 
<a href="/search/cs?searchtype=author&query=Mansourian%2C+A+M">Amir M. Mansourian</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+S">Shohreh Kasaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 13 figures and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02551" title="Abstract">arXiv:2402.02551</a> (replaced) [<a href="/pdf/2402.02551" title="Download PDF">pdf</a>, <a href="/format/2402.02551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner  with Robust Low-Level Control for Robotic Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahna%2C+M+H">Mehdi Heydari Shahna</a>, 
<a href="/search/cs?searchtype=author&query=Kolagar%2C+S+A+A">Seyed Adel Alizadeh Kolagar</a>, 
<a href="/search/cs?searchtype=author&query=Mattila%2C+J">Jouni Mattila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted for possible publication in the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02597" title="Abstract">arXiv:2402.02597</a> (replaced) [<a href="/pdf/2402.02597" title="Download PDF">pdf</a>, <a href="/ps/2402.02597" title="Download PostScript">ps</a>, <a href="/format/2402.02597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient simulation strategy for PCM-based cold-energy storage systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Casta%C3%B1o%2C+F">F.Casta&#xf1;o</a>, 
<a href="/search/eess?searchtype=author&query=Normey-Rico%2C+J+E">J. E. Normey-Rico</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 14 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Thermal Engineering (2018), 139, 419-431
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02599" title="Abstract">arXiv:2402.02599</a> (replaced) [<a href="/pdf/2402.02599" title="Download PDF">pdf</a>, <a href="/format/2402.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling and cooling power control of a TES-backed-up  vapour-compression refrigeration system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez%2C+D">D. Rodr&#xed;guez</a>, 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Lemos%2C+J+M">J. M. Lemos</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 20 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Thermal Engineering (2020), 164, 114415
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02639" title="Abstract">arXiv:2402.02639</a> (replaced) [<a href="/pdf/2402.02639" title="Download PDF">pdf</a>, <a href="/format/2402.02639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;It&#x27;s how you do things that matters&quot;: Attending to Process to Better  Serve Indigenous Communities with Language Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cooper%2C+N">Ned Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Heldreth%2C+C">Courtney Heldreth</a>, 
<a href="/search/cs?searchtype=author&query=Hutchinson%2C+B">Ben Hutchinson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02772" title="Abstract">arXiv:2402.02772</a> (replaced) [<a href="/pdf/2402.02772" title="Download PDF">pdf</a>, <a href="/format/2402.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Yixiang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Ting Long</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qifan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Liang Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages with appendix and references, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02791" title="Abstract">arXiv:2402.02791</a> (replaced) [<a href="/pdf/2402.02791" title="Download PDF">pdf</a>, <a href="/format/2402.02791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Optimization and Architecture for Tiny Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yunsheng Ni</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zheyuan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yi-Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sichao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jui%2C+S">Shangling Jui</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02817" title="Abstract">arXiv:2402.02817</a> (replaced) [<a href="/pdf/2402.02817" title="Download PDF">pdf</a>, <a href="/format/2402.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes-Optimal Fair Classification with Linear Disparity Constraints via  Pre-, In-, and Post-processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zeng%2C+X">Xianli Zeng</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper replaces the preprint "Bayes-optimal classifiers under group fairness" by Xianli Zeng, Edgar Dobriban, and Guang Cheng (<a href="/abs/2202.09724">arXiv:2202.09724</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02961" title="Abstract">arXiv:2402.02961</a> (replaced) [<a href="/pdf/2402.02961" title="Download PDF">pdf</a>, <a href="/format/2402.02961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GitBug-Java: A Reproducible Benchmark of Recent Java Bugs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Andr&#xe9; Silva</a>, 
<a href="/search/cs?searchtype=author&query=Saavedra%2C+N">Nuno Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MSR '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03021" title="Abstract">arXiv:2402.03021</a> (replaced) [<a href="/pdf/2402.03021" title="Download PDF">pdf</a>, <a href="/format/2402.03021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-induced multiscale losses and efficient multirate gradient descent  schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Juncai He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y+R">Yen-Hsi Richard Tsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 4 figures, submitted under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03049" title="Abstract">arXiv:2402.03049</a> (replaced) [<a href="/pdf/2402.03049" title="Download PDF">pdf</a>, <a href="/format/2402.03049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EasyInstruct: An Easy-to-use Instruction Processing Framework for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Runnan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; the project website is at <a href="https://zjunlp.github.io/project/EasyInstruct">this https URL</a>, code is at <a href="https://github.com/zjunlp/EasyInstruct">this https URL</a>, demo is at <a href="https://huggingface.co/spaces/zjunlp/EasyInstruct">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03161" title="Abstract">arXiv:2402.03161</a> (replaced) [<a href="/pdf/2402.03161" title="Download PDF">pdf</a>, <a href="/format/2402.03161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-LaVIT: Unified Video-Language Pre-training with Decoupled  Visual-Motional Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhicheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chengru Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yadong Mu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03179" title="Abstract">arXiv:2402.03179</a> (replaced) [<a href="/pdf/2402.03179" title="Download PDF">pdf</a>, <a href="/format/2402.03179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cool-chic video: Learned video coding with 800 parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leguay%2C+T">Thomas Leguay</a>, 
<a href="/search/eess?searchtype=author&query=Ladune%2C+T">Th&#xe9;o Ladune</a>, 
<a href="/search/eess?searchtype=author&query=Philippe%2C+P">Pierrick Philippe</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%A9forges%2C+O">Olivier D&#xe9;forges</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, published in Data Compression Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03300" title="Abstract">arXiv:2402.03300</a> (replaced) [<a href="/pdf/2402.03300" title="Download PDF">pdf</a>, <a href="/format/2402.03300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Junxiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+K">Y.K. Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Y. Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daya Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item437">Cross-lists</a></li>
<li><a href="#item502">Replacements</a></li>
</ul>
<small>[ total of 800 entries:  <b>1-800</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
