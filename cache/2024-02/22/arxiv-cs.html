<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 20 Feb 24  to  Wed 21 Feb 24, announced Thu, 22 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item396">Replacements</a></li>
</ul>
<small>[ total of 692 entries:  <b>1-692</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu, 22 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13264" title="Abstract">arXiv:2402.13264</a> [<a href="/pdf/2402.13264" title="Download PDF">pdf</a>, <a href="/format/2402.13264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KGroot: Enhancing Root Cause Analysis through Knowledge Graphs and Graph  Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tingting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianxing Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Fault localization is challenging in online micro-service due to the wide
variety of monitoring data volume, types, events and complex interdependencies
in service and components. Faults events in services are propagative and can
trigger a cascade of alerts in a short period of time. In the industry, fault
localization is typically conducted manually by experienced personnel. This
reliance on experience is unreliable and lacks automation. Different modules
present information barriers during manual localization, making it difficult to
quickly align during urgent faults. This inefficiency lags stability assurance
to minimize fault detection and repair time. Though actionable methods aimed to
automatic the process, the accuracy and efficiency are less than satisfactory.
The precision of fault localization results is of paramount importance as it
underpins engineers trust in the diagnostic conclusions, which are derived from
multiple perspectives and offer comprehensive insights. Therefore, a more
reliable method is required to automatically identify the associative
relationships among fault events and propagation path. To achieve this, KGroot
uses event knowledge and the correlation between events to perform root cause
reasoning by integrating knowledge graphs and GCNs for RCA. FEKG is built based
on historical data, an online graph is constructed in real-time when a failure
event occurs, and the similarity between each knowledge graph and online graph
is compared using GCNs to pinpoint the fault type through a ranking strategy.
Comprehensive experiments demonstrate KGroot can locate the root cause with
accuracy of 93.5% top 3 potential causes in second-level. This performance
matches the level of real-time fault diagnosis in the industrial environment
and significantly surpasses state-of-the-art baselines in RCA in terms of
effectiveness and efficiency.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13272" title="Abstract">arXiv:2402.13272</a> [<a href="/pdf/2402.13272" title="Download PDF">pdf</a>, <a href="/ps/2402.13272" title="Download PostScript">ps</a>, <a href="/format/2402.13272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spontaneous Theory of Mind for Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gurney%2C+N">Nikolos Gurney</a>, 
<a href="/search/cs?searchtype=author&query=Pynadath%2C+D+V">David V. Pynadath</a>, 
<a href="/search/cs?searchtype=author&query=Ustun%2C+V">Volkan Ustun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI)
overemphasize prompted, or cue-based, ToM, which may limit our collective
ability to develop Artificial Social Intelligence (ASI). Drawing from research
in computer science, cognitive science, and related disciplines, we contrast
prompted ToM with what we call spontaneous ToM -- reasoning about others'
mental states that is grounded in unintentional, possibly uncontrollable
cognitive functions. We argue for a principled approach to studying and
developing AI ToM and suggest that a robust, or general, ASI will respond to
prompts \textit{and} spontaneously engage in social reasoning.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13273" title="Abstract">arXiv:2402.13273</a> [<a href="/pdf/2402.13273" title="Download PDF">pdf</a>, <a href="/ps/2402.13273" title="Download PostScript">ps</a>, <a href="/format/2402.13273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operational Collective Intelligence of Humans and Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gurney%2C+N">Nikolos Gurney</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>, 
<a href="/search/cs?searchtype=author&query=Pynadath%2C+D+V">David V. Pynadath</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+A">Adam Russell</a>, 
<a href="/search/cs?searchtype=author&query=Satyukov%2C+G">Gleb Satyukov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We explore the use of aggregative crowdsourced forecasting (ACF) as a
mechanism to help operationalize ``collective intelligence'' of human-machine
teams for coordinated actions. We adopt the definition for Collective
Intelligence as: ``A property of groups that emerges from synergies among
data-information-knowledge, software-hardware, and individuals (those with new
insights as well as recognized authorities) that enables just-in-time knowledge
for better decisions than these three elements acting alone.'' Collective
Intelligence emerges from new ways of connecting humans and AI to enable
decision-advantage, in part by creating and leveraging additional sources of
information that might otherwise not be included. Aggregative crowdsourced
forecasting (ACF) is a recent key advancement towards Collective Intelligence
wherein predictions (X\% probability that Y will happen) and rationales (why I
believe it is this probability that X will happen) are elicited independently
from a diverse crowd, aggregated, and then used to inform higher-level
decision-making. This research asks whether ACF, as a key way to enable
Operational Collective Intelligence, could be brought to bear on operational
scenarios (i.e., sequences of events with defined agents, components, and
interactions) and decision-making, and considers whether such a capability
could provide novel operational capabilities to enable new forms of
decision-advantage.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13277" title="Abstract">arXiv:2402.13277</a> [<a href="/pdf/2402.13277" title="Download PDF">pdf</a>, <a href="/format/2402.13277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek  in WSNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Talukder%2C+M+A">Md. Alamin Talukder</a>, 
<a href="/search/cs?searchtype=author&query=Sharmin%2C+S">Selina Sharmin</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+A">Md Ashraf Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Manowarul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Aryal%2C+S">Sunil Aryal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Q1, Scopus, ISI, SCIE, IF: 3.2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures,
encompassing both stationary and mobile sensors. These sensors self-organize
and establish multi-hop connections for communication, collectively sensing,
gathering, processing, and transmitting data about their surroundings. Despite
their significance, WSNs face rapid and detrimental attacks that can disrupt
functionality. Existing intrusion detection methods for WSNs encounter
challenges such as low detection rates, computational overhead, and false
alarms. These issues stem from sensor node resource constraints, data
redundancy, and high correlation within the network. To address these
challenges, we propose an innovative intrusion detection approach that
integrates Machine Learning (ML) techniques with the Synthetic Minority
Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend
synthesizes minority instances and eliminates Tomek links, resulting in a
balanced dataset that significantly enhances detection accuracy in WSNs.
Additionally, we incorporate feature scaling through standardization to render
input features consistent and scalable, facilitating more precise training and
detection. To counteract imbalanced WSN datasets, we employ the SMOTE-Tomek
resampling technique, mitigating overfitting and underfitting issues. Our
comprehensive evaluation, using the WSN Dataset (WSN-DS) containing 374,661
records, identifies the optimal model for intrusion detection in WSNs. The
standout outcome of our research is the remarkable performance of our model. In
binary, it achieves an accuracy rate of 99.78% and in multiclass, it attains an
exceptional accuracy rate of 99.92%. These findings underscore the efficiency
and superiority of our proposal in the context of WSN intrusion detection,
showcasing its effectiveness in detecting and mitigating intrusions in WSNs.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13281" title="Abstract">arXiv:2402.13281</a> [<a href="/pdf/2402.13281" title="Download PDF">pdf</a>, <a href="/ps/2402.13281" title="Download PostScript">ps</a>, <a href="/format/2402.13281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fight Hardware with Hardware: System-wide Detection and Mitigation of  Side-Channel Attacks using Performance Counters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carn%C3%A0%2C+S">Stefano Carn&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Ferracci%2C+S">Serena Ferracci</a>, 
<a href="/search/cs?searchtype=author&query=Quaglia%2C+F">Francesco Quaglia</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrini%2C+A">Alessandro Pellegrini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">We present a kernel-level infrastructure that allows system-wide detection of
malicious applications attempting to exploit cache-based side-channel attacks
to break the process confinement enforced by standard operating systems. This
infrastructure relies on hardware performance counters to collect information
at runtime from all applications running on the machine. High-level detection
metrics are derived from these measurements to maximize the likelihood of
promptly detecting a malicious application. Our experimental assessment shows
that we can catch a large family of side-channel attacks with a significantly
reduced overhead. We also discuss countermeasures that can be enacted once a
process is suspected of carrying out a side-channel attack to increase the
overall tradeoff between the system's security level and the delivered
performance under non-suspected process executions.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13284" title="Abstract">arXiv:2402.13284</a> [<a href="/pdf/2402.13284" title="Download PDF">pdf</a>, <a href="/format/2402.13284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure Guided Large Language Model for SQL Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junnan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wentao Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13287" title="Abstract">arXiv:2402.13287</a> [<a href="/pdf/2402.13287" title="Download PDF">pdf</a>, <a href="/format/2402.13287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manipulating hidden-Markov-model inferences by corrupting batch data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caballero%2C+W+N">William N. Caballero</a>, 
<a href="/search/cs?searchtype=author&query=Camacho%2C+J+M">Jose Manuel Camacho</a>, 
<a href="/search/cs?searchtype=author&query=Ekin%2C+T">Tahir Ekin</a>, 
<a href="/search/cs?searchtype=author&query=Naveiro%2C+R">Roi Naveiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 8 figures, 11 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Caballero, W. N., Camacho, J. M., Ekin, T., &amp; Naveiro, R. (2024).
  Manipulating hidden-Markov-model inferences by corrupting batch data.
  Computers &amp; Operations Research, 162, 106478
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Time-series models typically assume untainted and legitimate streams of data.
However, a self-interested adversary may have incentive to corrupt this data,
thereby altering a decision maker's inference. Within the broader field of
adversarial machine learning, this research provides a novel, probabilistic
perspective toward the manipulation of hidden Markov model inferences via
corrupted data. In particular, we provision a suite of corruption problems for
filtering, smoothing, and decoding inferences leveraging an adversarial risk
analysis approach. Multiple stochastic programming models are set forth that
incorporate realistic uncertainties and varied attacker objectives. Three
general solution methods are developed by alternatively viewing the problem
from frequentist and Bayesian perspectives. The efficacy of each method is
illustrated via extensive, empirical testing. The developed methods are
characterized by their solution quality and computational effort, resulting in
a stratification of techniques across varying problem-instance architectures.
This research highlights the weaknesses of hidden Markov models under
adversarial activity, thereby motivating the need for robustification
techniques to ensure their security.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13288" title="Abstract">arXiv:2402.13288</a> [<a href="/pdf/2402.13288" title="Download PDF">pdf</a>, <a href="/format/2402.13288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Table Question Answering via SQL Query Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mouravieff%2C+R">Rapha&#xeb;l Mouravieff</a>, 
<a href="/search/cs?searchtype=author&query=Piwowarski%2C+B">Benjamin Piwowarski</a>, 
<a href="/search/cs?searchtype=author&query=Lamprier%2C+S">Sylvain Lamprier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Table Question-Answering involves both understanding the natural language
query and grounding it in the context of the input table to extract the
relevant information. In this context, many methods have highlighted the
benefits of intermediate pre-training from SQL queries. However, while most
approaches aim at generating final answers from inputs directly, we claim that
there is better to do with SQL queries during training. By learning to imitate
a restricted portion of SQL-like algebraic operations, we show that their
execution flow provides intermediate supervision steps that allow increased
generalization and structural reasoning compared with classical approaches of
the field. Our study bridges the gap between semantic parsing and direct
answering methods and provides useful insights regarding what types of
operations should be predicted by a generative architecture or be preferably
executed by an external algorithm.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13290" title="Abstract">arXiv:2402.13290</a> [<a href="/pdf/2402.13290" title="Download PDF">pdf</a>, <a href="/format/2402.13290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding from an AI and Cognitive Science Lens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bajaj%2C+G">Goonmeet Bajaj</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+S">Srinivasan Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Shalin%2C+V+L">Valerie L. Shalin</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Intelligent Systems, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Grounding is a challenging problem, requiring a formal definition and
different levels of abstraction. This article explores grounding from both
cognitive science and machine learning perspectives. It identifies the
subtleties of grounding, its significance for collaborative agents, and
similarities and differences in grounding approaches in both communities. The
article examines the potential of neuro-symbolic approaches tailored for
grounding tasks, showcasing how they can more comprehensively address
grounding. Finally, we discuss areas for further exploration and development in
grounding.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13291" title="Abstract">arXiv:2402.13291</a> [<a href="/pdf/2402.13291" title="Download PDF">pdf</a>, <a href="/format/2402.13291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berabi%2C+B">Berkay Berabi</a>, 
<a href="/search/cs?searchtype=author&query=Gronskiy%2C+A">Alexey Gronskiy</a>, 
<a href="/search/cs?searchtype=author&query=Raychev%2C+V">Veselin Raychev</a>, 
<a href="/search/cs?searchtype=author&query=Sivanrupan%2C+G">Gishor Sivanrupan</a>, 
<a href="/search/cs?searchtype=author&query=Chibotaru%2C+V">Victor Chibotaru</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
<p class="mathjax">The automated program repair field has attracted substantial interest over
the years, but despite significant research efforts, creating a system that
works well for complex semantic bugs such as security vulnerabilities has
proven difficult. A promising direction to solve this challenge is by
leveraging large language models (LLMs), which are increasingly used to solve
various programming tasks. In this paper, we investigate the effectiveness of
LLMs for solving code-repair task. We show that the task is difficult as it
requires the model to learn long-range code relationships, a task that
inherently relies on extensive amounts of training data. At the same time,
creating a large, clean dataset for complex program bugs and their
corresponding fixes is non-trivial. We propose a technique to address these
challenges with a new approach for querying and fine-tuning LLMs. The idea is
to use program analysis to limit the LLM's attention mechanism on the portions
of code needed to perform the fix, drastically reducing the amount of required
training data. Concretely, for training and inference, rather than feeding the
entire program to the LLM, we reduce its code to a much shorter snippet that
contains the reported defect together with the necessary context - and use that
instead. Our evaluation shows that this code reduction approach substantially
improves available models such as GPT-4 using few-shot learning, as well as
fine-tuning models. To train and evaluate our system, we created a
comprehensive code fixing dataset by extensively labeling 156 bug patterns
(including 40 security rules), requiring complex interprocedural dataflow to
discover. Our best system with Mixtral-8x7B can remove more than 80% of the
reported defects while exactly matching the human fix in between 10 and 50% of
cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on
window-based models like TFix.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13292" title="Abstract">arXiv:2402.13292</a> [<a href="/pdf/2402.13292" title="Download PDF">pdf</a>, <a href="/format/2402.13292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aakash">Aakash</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+I">Indranil Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The fundamental goal assignment problem for a multi-robot application aims to
assign a unique goal to each robot while ensuring collision-free paths,
minimizing the total movement cost. A plausible algorithmic solution to this
NP-hard problem involves an iterative process that integrates a task planner to
compute the goal assignment while ignoring the collision possibilities among
the robots and a multi-agent path-finding algorithm to find the collision-free
trajectories for a given assignment. This procedure involves a method for
computing the next best assignment given the current best assignment. A naive
way of computing the next best assignment, as done in the state-of-the-art
solutions, becomes a roadblock to achieving scalability in solving the overall
problem. To obviate this bottleneck, we propose an efficient conflict-guided
method to compute the next best assignment. Additionally, we introduce two more
optimizations to the algorithm -- first for avoiding the unconstrained path
computations between robot-goal pairs wherever possible, and the second to
prevent duplicate constrained path computations for multiple robot-goal pairs.
We extensively evaluate our algorithm for up to a hundred robots on several
benchmark workspaces. The results demonstrate that the proposed algorithm
achieves nearly an order of magnitude speedup over the state-of-the-art
algorithm, showcasing its efficacy in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13296" title="Abstract">arXiv:2402.13296</a> [<a href="/pdf/2402.13296" title="Download PDF">pdf</a>, <a href="/format/2402.13296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Reinforcement Learning: A Systematic Review and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuanguo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+G">Guorong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lixin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengcheng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">In response to the limitations of reinforcement learning and evolutionary
algorithms (EAs) in complex problem-solving, Evolutionary Reinforcement
Learning (EvoRL) has emerged as a synergistic solution. EvoRL integrates EAs
and reinforcement learning, presenting a promising avenue for training
intelligent agents. This systematic review firstly navigates through the
technological background of EvoRL, examining the symbiotic relationship between
EAs and reinforcement learning algorithms. We then delve into the challenges
faced by both EAs and reinforcement learning, exploring their interplay and
impact on the efficacy of EvoRL. Furthermore, the review underscores the need
for addressing open issues related to scalability, adaptability, sample
efficiency, adversarial robustness, ethic and fairness within the current
landscape of EvoRL. Finally, we propose future directions for EvoRL,
emphasizing research avenues that strive to enhance self-adaptation and
self-improvement, generalization, interpretability, explainability, and so on.
Serving as a comprehensive resource for researchers and practitioners, this
systematic review provides insights into the current state of EvoRL and offers
a guide for advancing its capabilities in the ever-evolving landscape of
artificial intelligence.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13301" title="Abstract">arXiv:2402.13301</a> [<a href="/pdf/2402.13301" title="Download PDF">pdf</a>, <a href="/format/2402.13301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-informed Positional Encoding for Music Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+M">Manvi Agarwal</a> (S2A_IDS), 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changhong Wang</a> (S2A_IDS), 
<a href="/search/cs?searchtype=author&query=Richard%2C+G">Ga&#xeb;l Richard</a> (S2A_IDS)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Music generated by deep learning methods often suffers from a lack of
coherence and long-term organization. Yet, multi-scale hierarchical structure
is a distinctive feature of music signals. To leverage this information, we
propose a structure-informed positional encoding framework for music generation
with Transformers. We design three variants in terms of absolute, relative and
non-stationary positional information. We comprehensively test them on two
symbolic music generation tasks: next-timestep prediction and accompaniment
generation. As a comparison, we choose multiple baselines from the literature
and demonstrate the merits of our methods using several musically-motivated
evaluation metrics. In particular, our methods improve the melodic and
structural consistency of the generated pieces.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13302" title="Abstract">arXiv:2402.13302</a> [<a href="/pdf/2402.13302" title="Download PDF">pdf</a>, <a href="/ps/2402.13302" title="Download PostScript">ps</a>, <a href="/format/2402.13302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic  Lexical Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melacci%2C+S">Stefano Melacci</a>, 
<a href="/search/cs?searchtype=author&query=Globo%2C+A">Achille Globo</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 11th International Conference on Language Resources and Evaluation (LREC 2018)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of The 11th International Conference on Language
  Resources and Evaluation (LREC 2018)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Supervised models for Word Sense Disambiguation (WSD) currently yield to
state-of-the-art results in the most popular benchmarks. Despite the recent
introduction of Word Embeddings and Recurrent Neural Networks to design
powerful context-related features, the interest in improving WSD models using
Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based
approaches. In this paper, we enhance "modern" supervised WSD models exploiting
two popular SLRs: WordNet and WordNet Domains. We propose an effective way to
introduce semantic features into the classifiers, and we consider using the SLR
structure to augment the training data. We study the effect of different types
of semantic features, investigating their interaction with local contexts
encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks,
and we extend the proposed model into a novel multi-layer architecture for WSD.
A detailed experimental comparison in the recent Unified Evaluation Framework
(Raganato et al., 2017) shows that the proposed approach leads to supervised
models that compare favourably with the state-of-the art.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13304" title="Abstract">arXiv:2402.13304</a> [<a href="/pdf/2402.13304" title="Download PDF">pdf</a>, <a href="/format/2402.13304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmful algal bloom forecasting. A comparison between stream and batch  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Molares-Ulloa%2C+A">Andres Molares-Ulloa</a>, 
<a href="/search/cs?searchtype=author&query=Rocruz%2C+E">Elisabet Rocruz</a>, 
<a href="/search/cs?searchtype=author&query=Rivero%2C+D">Daniel Rivero</a>, 
<a href="/search/cs?searchtype=author&query=Padin%2C+X+A">Xos&#xe9; A. Padin</a>, 
<a href="/search/cs?searchtype=author&query=Nolasco%2C+R">Rita Nolasco</a>, 
<a href="/search/cs?searchtype=author&query=Dubert%2C+J">Jes&#xfa;s Dubert</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Blanco%2C+E">Enrique Fernandez-Blanco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from
shellfish contaminated with toxins produced by dinoflagellates. The condition,
with its widespread incidence, high morbidity rate, and persistent shellfish
toxicity, poses risks to public health and the shellfish industry. High biomass
of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs).
Monitoring and forecasting systems are crucial for mitigating HABs impact.
Predicting harmful algal blooms involves a time-series-based problem with a
strong historical seasonal component, however, recent anomalies due to changes
in meteorological and oceanographic events have been observed. Stream Learning
stands out as one of the most promising approaches for addressing
time-series-based problems with concept drifts. However, its efficacy in
predicting HABs remains unproven and needs to be tested in comparison with
Batch Learning. Historical data availability is a critical point in developing
predictive systems. In oceanography, the available data collection can have
some constrains and limitations, which has led to exploring new tools to obtain
more exhaustive time series. In this study, a machine learning workflow for
predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata,
was developed with several key advancements. Seven machine learning algorithms
were compared within two learning paradigms. Notably, the output data from
CROCO, the ocean hydrodynamic model, was employed as the primary dataset,
palliating the limitation of time-continuous historical data. This study
highlights the value of models interpretability, fair models comparison
methodology, and the incorporation of Stream Learning models. The model DoME,
with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most
effective and interpretable predictor, outperforming the other algorithms.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13306" title="Abstract">arXiv:2402.13306</a> [<a href="/pdf/2402.13306" title="Download PDF">pdf</a>, <a href="/ps/2402.13306" title="Download PostScript">ps</a>, <a href="/format/2402.13306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision System Prototype for Inspection and Monitoring with a Smart  Camera
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Molina%2C+E">Efren Hern&#xe1;ndez-Molina</a>, 
<a href="/search/cs?searchtype=author&query=Ojeda-Maga%C3%B1a%2C+B">Benjamin Ojeda-Maga&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Robledo-Hern%C3%A1ndez%2C+J+G">Jose Guadalupe Robledo-Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Ruelas%2C+R">Ruben Ruelas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 16 figures, in Spanish language
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Latin America Transactions, vol. 18, no. 09, pp. 1614-1622,
  September 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper presents the design of an artificial vision system prototype for
automatic inspection and monitoring of objects over a conveyor belt and using a
Smart camera 2D BOA-INS. The prototype consists of a conveyor belt and an
embedded system based on an Arduino Mega card for system control, and it has as
main peripherals the smart camera, a direct current motor, a photoelectric
sensor, LED illumination and LEDs indicating the status (good or defect) of
each evaluated object. The application of the prototype is for educational
purposes, so that undergraduate, master and diploma students can simulate a
continuous production line, controlled by an embedded system, and perform
quality control by monitoring through a visual system and a personal computer.
This allows implementing the topics of embedded systems, artificial vision,
artificial intelligence, pattern recognition, automatic control, as well as
automation of real processes.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13329" title="Abstract">arXiv:2402.13329</a> [<a href="/pdf/2402.13329" title="Download PDF">pdf</a>, <a href="/format/2402.13329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Disruptive Research Playbook for Studying Disruptive Innovations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Storey%2C+M">Margaret-Anne Storey</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>, 
<a href="/search/cs?searchtype=author&query=Novielli%2C+N">Nicole Novielli</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+T">Takashi Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">As researchers, we are now witnessing a fundamental change in our
technologically-enabled world due to the advent and diffusion of highly
disruptive technologies such as generative AI, Augmented Reality (AR) and
Virtual Reality (VR). In particular, software engineering has been profoundly
affected by the transformative power of disruptive innovations for decades,
with a significant impact of technical advancements on social dynamics due to
its the socio-technical nature. In this paper, we reflect on the importance of
formulating and addressing research in software engineering through a
socio-technical lens, thus ensuring a holistic understanding of the complex
phenomena in this field. We propose a research playbook with the goal of
providing a guide to formulate compelling and socially relevant research
questions and to identify the appropriate research strategies for empirical
investigations, with an eye on the long-term implications of technologies or
their use. We showcase how to apply the research playbook. Firstly, we show how
it can be used retrospectively to reflect on a prior disruptive technology,
Stack Overflow, and its impact on software development. Secondly, we show it
can be used to question the impact of two current disruptive technologies: AI
and AR/VR. Finally, we introduce a specialized GPT model to support the
researcher in framing future investigations. We conclude by discussing the
broader implications of adopting the playbook for both researchers and
practitioners in software engineering and beyond.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13331" title="Abstract">arXiv:2402.13331</a> [<a href="/pdf/2402.13331" title="Download PDF">pdf</a>, <a href="/format/2402.13331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Hallucination Detection in Neural Machine Translation through  Simple Detector Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Himmi%2C+A">Anas Himmi</a>, 
<a href="/search/cs?searchtype=author&query=Staerman%2C+G">Guillaume Staerman</a>, 
<a href="/search/cs?searchtype=author&query=Picot%2C+M">Marine Picot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Guerreiro%2C+N+M">Nuno M. Guerreiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hallucinated translations pose significant threats and safety concerns when
it comes to the practical deployment of machine translation systems. Previous
research works have identified that detectors exhibit complementary performance
different detectors excel at detecting different types of hallucinations. In
this paper, we propose to address the limitations of individual detectors by
combining them and introducing a straightforward method for aggregating
multiple detectors. Our results demonstrate the efficacy of our aggregated
detector, providing a promising step towards evermore reliable machine
translation systems.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13332" title="Abstract">arXiv:2402.13332</a> [<a href="/pdf/2402.13332" title="Download PDF">pdf</a>, <a href="/format/2402.13332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double machine learning for causal hybrid modeling -- applications in  the Earth sciences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohrs%2C+K">Kai-Hendrik Cohrs</a>, 
<a href="/search/cs?searchtype=author&query=Varando%2C+G">Gherardo Varando</a>, 
<a href="/search/cs?searchtype=author&query=Carvalhais%2C+N">Nuno Carvalhais</a>, 
<a href="/search/cs?searchtype=author&query=Reichstein%2C+M">Markus Reichstein</a>, 
<a href="/search/cs?searchtype=author&query=Camps-Valls%2C+G">Gustau Camps-Valls</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Hybrid modeling integrates machine learning with scientific knowledge with
the goal of enhancing interpretability, generalization, and adherence to
natural laws. Nevertheless, equifinality and regularization biases pose
challenges in hybrid modeling to achieve these purposes. This paper introduces
a novel approach to estimating hybrid models via a causal inference framework,
specifically employing Double Machine Learning (DML) to estimate causal
effects. We showcase its use for the Earth sciences on two problems related to
carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based
hybrid modeling is superior in estimating causal parameters over end-to-end
deep neural network (DNN) approaches, proving efficiency, robustness to bias
from regularization methods, and circumventing equifinality. Our approach,
applied to carbon flux partitioning, exhibits flexibility in accommodating
heterogeneous causal effects. The study emphasizes the necessity of explicitly
defining causal graphs and relationships, advocating for this as a general best
practice. We encourage the continued exploration of causality in hybrid models
for more interpretable and trustworthy results in knowledge-guided machine
learning.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13338" title="Abstract">arXiv:2402.13338</a> [<a href="/pdf/2402.13338" title="Download PDF">pdf</a>, <a href="/ps/2402.13338" title="Download PostScript">ps</a>, <a href="/format/2402.13338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivized Exploration via Filtered Posterior Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalvit%2C+A">Anand Kalvit</a>, 
<a href="/search/cs?searchtype=author&query=Slivkins%2C+A">Aleksandrs Slivkins</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+Y">Yonatan Gur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">We study "incentivized exploration" (IE) in social learning problems where
the principal (a recommendation algorithm) can leverage information asymmetry
to incentivize sequentially-arriving agents to take exploratory actions. We
identify posterior sampling, an algorithmic approach that is well known in the
multi-armed bandits literature, as a general-purpose solution for IE. In
particular, we expand the existing scope of IE in several practically-relevant
dimensions, from private agent types to informative recommendations to
correlated Bayesian priors. We obtain a general analysis of posterior sampling
in IE which allows us to subsume these extended settings as corollaries, while
also recovering existing results as special cases.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13340" title="Abstract">arXiv:2402.13340</a> [<a href="/pdf/2402.13340" title="Download PDF">pdf</a>, <a href="/format/2402.13340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Greedy Monochromatic Island Partitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+den+Broek%2C+S">Steven van den Broek</a>, 
<a href="/search/cs?searchtype=author&query=Meulemans%2C+W">Wouter Meulemans</a>, 
<a href="/search/cs?searchtype=author&query=Speckmann%2C+B">Bettina Speckmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Constructing partitions of colored points is a well-studied problem in
discrete and computational geometry. We study the problem of creating a
minimum-cardinality partition into monochromatic islands. Our input is a set
$S$ of $n$ points in the plane where each point has one of $k \geq 2$ colors. A
set of points is monochromatic if it contains points of only one color. An
island $I$ is a subset of $S$ such that $\mathcal{CH}(I) \cap S = I$, where
$\mathcal{CH}(I)$ denotes the convex hull of $I$. We identify an island with
its convex hull; therefore, a partition into islands has the additional
requirement that the convex hulls of the islands are pairwise-disjoint. We
present three greedy algorithms for constructing island partitions and analyze
their approximation ratios.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13343" title="Abstract">arXiv:2402.13343</a> [<a href="/pdf/2402.13343" title="Download PDF">pdf</a>, <a href="/format/2402.13343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EOSC CZ: Towards the development of Czech national ecosystem for FAIR  research data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antol%2C+M">Matej Antol</a>, 
<a href="/search/cs?searchtype=author&query=Marek%2C+J">Jiri Marek</a>, 
<a href="/search/cs?searchtype=author&query=Capandova%2C+M">Michaela Capandova</a>, 
<a href="/search/cs?searchtype=author&query=Juracek%2C+J">Jaroslav Juracek</a>, 
<a href="/search/cs?searchtype=author&query=Matyska%2C+L">Ludek Matyska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 1 image
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This short paper presents a compact overview of the Czech approach to
implementing the European Open Science Cloud and plans for developing a Czech
national infrastructure for FAIR research data. Its purpose is to provide an
all-encompassing summary of the near future of research data management in
Czechia. As such, we deliberately attempt to explain complicated concepts in
minimum words, sacrificing the precision of expression for compactness.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13347" title="Abstract">arXiv:2402.13347</a> [<a href="/pdf/2402.13347" title="Download PDF">pdf</a>, <a href="/format/2402.13347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-averaged virtual element methods for convection-diffusion and  convection-dominated problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cao%2C+S">Shuhao Cao</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+S">Seulip Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This manuscript develops edge-averaged virtual element (EAVE) methodologies
to address convection-diffusion problems effectively in the
convection-dominated regime. It introduces a variant of EAVE that ensures
monotonicity (producing an $M$-matrix) on Voronoi polygonal meshes, provided
their duals are Delaunay triangulations with acute angles. Furthermore, the
study outlines a comprehensive framework for EAVE methodologies, introducing
another variant that integrates with the stiffness matrix derived from the
lowest-order virtual element method for the Poisson equation. Numerical
experiments confirm the theoretical advantages of the monotonicity property and
demonstrate an optimal convergence rate across various mesh configurations.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13349" title="Abstract">arXiv:2402.13349</a> [<a href="/pdf/2402.13349" title="Download PDF">pdf</a>, <a href="/format/2402.13349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aria Everyday Activities Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhaoyang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Charron%2C+N">Nickolas Charron</a>, 
<a href="/search/cs?searchtype=author&query=Moulon%2C+P">Pierre Moulon</a>, 
<a href="/search/cs?searchtype=author&query=Gamino%2C+A">Alexander Gamino</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Cheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sweeney%2C+C">Chris Sweeney</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+E">Edward Miller</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+J">Jeff Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+K">Kiran Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Pesqueira%2C+L">Luis Pesqueira</a>, 
<a href="/search/cs?searchtype=author&query=Schwesinger%2C+M">Mark Schwesinger</a>, 
<a href="/search/cs?searchtype=author&query=Parkhi%2C+O">Omkar Parkhi</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qiao Gu</a>, 
<a href="/search/cs?searchtype=author&query=De+Nardi%2C+R">Renzo De Nardi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shangyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Saarinen%2C+S">Steve Saarinen</a>, 
<a href="/search/cs?searchtype=author&query=Baiyya%2C+V">Vijay Baiyya</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuyang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Newcombe%2C+R">Richard Newcombe</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+J+J">Jakob Julian Engel</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaqing Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Carl Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset website: <a href="https://www.projectaria.com/datasets/aea/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal
open dataset recorded using Project Aria glasses. AEA contains 143 daily
activity sequences recorded by multiple wearers in five geographically diverse
indoor locations. Each of the recording contains multimodal sensor data
recorded through the Project Aria glasses. In addition, AEA provides machine
perception data including high frequency globally aligned 3D trajectories,
scene point cloud, per-frame 3D eye gaze vector and time aligned speech
transcription. In this paper, we demonstrate a few exemplar research
applications enabled by this dataset, including neural scene reconstruction and
prompted segmentation. AEA is an open source dataset that can be downloaded
from projectaria.com. We are also providing open-source implementations and
examples of how to use the dataset in Project Aria Tools.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13350" title="Abstract">arXiv:2402.13350</a> [<a href="/pdf/2402.13350" title="Download PDF">pdf</a>, <a href="/format/2402.13350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text  Retrieval Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadas%2C+S">S&#x142;awomir Dadas</a>, 
<a href="/search/cs?searchtype=author&query=Pere%C5%82kiewicz%2C+M">Micha&#x142; Pere&#x142;kiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Po%C5%9Bwiata%2C+R">Rafa&#x142; Po&#x15b;wiata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present Polish Information Retrieval Benchmark (PIRB), a comprehensive
evaluation framework encompassing 41 text information retrieval tasks for
Polish. The benchmark incorporates existing datasets as well as 10 new,
previously unpublished datasets covering diverse topics such as medicine, law,
business, physics, and linguistics. We conduct an extensive evaluation of over
20 dense and sparse retrieval models, including the baseline models trained by
us as well as other available Polish and multilingual methods. Finally, we
introduce a three-step process for training highly effective language-specific
retrievers, consisting of knowledge distillation, supervised fine-tuning, and
building sparse-dense hybrid retrievers using a lightweight rescoring model. In
order to validate our approach, we train new text encoders for Polish and
compare their results with previously evaluated methods. Our dense models
outperform the best solutions available to date, and the use of hybrid methods
further improves their performance.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13353" title="Abstract">arXiv:2402.13353</a> [<a href="/pdf/2402.13353" title="Download PDF">pdf</a>, <a href="/format/2402.13353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining unsupervised and supervised learning in microscopy enables  defect analysis of a full 4H-SiC wafer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B+D">Binh Duong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+J">Johannes Steiner</a>, 
<a href="/search/cs?searchtype=author&query=Wellmann%2C+P">Peter Wellmann</a>, 
<a href="/search/cs?searchtype=author&query=Sandfeld%2C+S">Stefan Sandfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)

</div>
<p class="mathjax">Detecting and analyzing various defect types in semiconductor materials is an
important prerequisite for understanding the underlying mechanisms as well as
tailoring the production processes. Analysis of microscopy images that reveal
defects typically requires image analysis tasks such as segmentation and object
detection. With the permanently increasing amount of data that is produced by
experiments, handling these tasks manually becomes more and more impossible. In
this work, we combine various image analysis and data mining techniques for
creating a robust and accurate, automated image analysis pipeline. This allows
for extracting the type and position of all defects in a microscopy image of a
KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000
individual images.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13357" title="Abstract">arXiv:2402.13357</a> [<a href="/pdf/2402.13357" title="Download PDF">pdf</a>, <a href="/format/2402.13357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing Tardy Processing Time on a Single Machine in Near-Linear Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Wennmann%2C+L">Leo Wennmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this work we revisit the elementary scheduling problem $1||\sum p_j U_j$.
The goal is to select, among $n$ jobs with processing times and due dates, a
subset of jobs with maximum total processing time that can be scheduled in
sequence without violating their due dates. This problem is NP-hard, but a
classical algorithm by Lawler and Moore from the 60s solves this problem in
pseudo-polynomial time $O(nP)$, where $P$ is the total processing time of all
jobs. With the aim to develop best-possible pseudo-polynomial-time algorithms,
a recent wave of results has improved Lawler and Moore's algorithm for $1||\sum
p_j U_j$: First to time $\tilde O(P^{7/4})$ [Bringmann, Fischer, Hermelin,
Shabtay, Wellnitz; ICALP'20], then to time $\tilde O(P^{5/3})$ [Klein, Polak,
Rohwedder; SODA'23], and finally to time $\tilde O(P^{7/5})$ [Schieber,
Sitaraman; WADS'23]. It remained an exciting open question whether these works
can be improved further.
<br />In this work we develop an algorithm in near-linear time $\tilde O(P)$ for
the $1||\sum p_j U_j$ problem. This running time not only significantly
improves upon the previous results, but also matches conditional lower bounds
based on the Strong Exponential Time Hypothesis or the Set Cover Hypothesis and
is therefore likely optimal (up to subpolynomial factors). Our new algorithm
also extends to the case of $m$ machines in time $\tilde O(P^m)$. In contrast
to the previous improvements, we take a different, more direct approach
inspired by the recent reductions from Modular Subset Sum to dynamic string
problems. We thereby arrive at a satisfyingly simple algorithm.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13364" title="Abstract">arXiv:2402.13364</a> [<a href="/pdf/2402.13364" title="Download PDF">pdf</a>, <a href="/format/2402.13364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ramprasad%2C+R">Rampi Ramprasad</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive abilities in
generating unstructured natural language according to instructions. However,
their performance can be inconsistent when tasked with producing text that
adheres to specific structured formats, which is crucial in applications like
named entity recognition (NER) or relation extraction (RE). To address this
issue, this paper introduces an efficient method, G&amp;O, to enhance their
structured text generation capabilities. It breaks the generation into a
two-step pipeline: initially, LLMs generate answers in natural language as
intermediate responses. Subsequently, LLMs are asked to organize the output
into the desired structure, using the intermediate responses as context. G&amp;O
effectively separates the generation of content from the structuring process,
reducing the pressure of completing two orthogonal tasks simultaneously. Tested
on zero-shot NER and RE, the results indicate a significant improvement in LLM
performance with minimal additional efforts. This straightforward and adaptable
prompting technique can also be combined with other strategies, like
self-consistency, to further elevate LLM capabilities in various structured
text generation tasks.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13366" title="Abstract">arXiv:2402.13366</a> [<a href="/pdf/2402.13366" title="Download PDF">pdf</a>, <a href="/format/2402.13366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical curriculum learning: An elimination algorithm achieving an  oracle risk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+O">Omer Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Meir%2C+R">Ron Meir</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+N">Nir Weinberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider a statistical version of curriculum learning (CL) in a parametric
prediction setting. The learner is required to estimate a target parameter
vector, and can adaptively collect samples from either the target model, or
other source models that are similar to the target model, but less noisy. We
consider three types of learners, depending on the level of side-information
they receive. The first two, referred to as strong/weak-oracle learners,
receive high/low degrees of information about the models, and use these to
learn. The third, a fully adaptive learner, estimates the target parameter
vector without any prior information. In the single source case, we propose an
elimination learning method, whose risk matches that of a strong-oracle
learner. In the multiple source case, we advocate that the risk of the
weak-oracle learner is a realistic benchmark for the risk of adaptive learners.
We develop an adaptive multiple elimination-rounds CL algorithm, and
characterize instance-dependent conditions for its risk to match that of the
weak-oracle learner. We consider instance-dependent minimax lower bounds, and
discuss the challenges associated with defining the class of instances for the
bound. We derive two minimax lower bounds, and determine the conditions under
which the performance weak-oracle learner is minimax optimal.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13368" title="Abstract">arXiv:2402.13368</a> [<a href="/pdf/2402.13368" title="Download PDF">pdf</a>, <a href="/format/2402.13368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Concept Discovery Mitigates Spurious Correlations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arefin%2C+M+R">Md Rifat Arefin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Baratin%2C+A">Aristide Baratin</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Models prone to spurious correlations in training data often produce brittle
predictions and introduce unintended biases. Addressing this challenge
typically involves methods relying on prior knowledge and group annotation to
remove spurious correlations, which may not be readily available in many
applications. In this paper, we establish a novel connection between
unsupervised object-centric learning and mitigation of spurious correlations.
Instead of directly inferring sub-groups with varying correlations with labels,
our approach focuses on discovering concepts: discrete ideas that are shared
across input samples. Leveraging existing object-centric representation
learning, we introduce CoBalT: a concept balancing technique that effectively
mitigates spurious correlations without requiring human labeling of subgroups.
Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for
subpopulation shifts demonstrate superior or competitive performance compared
state-of-the-art baselines, without the need for group annotation.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13369" title="Abstract">arXiv:2402.13369</a> [<a href="/pdf/2402.13369" title="Download PDF">pdf</a>, <a href="/format/2402.13369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Uncanny Valley: A Comprehensive Analysis of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+K">Karam Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Bzdok%2C+D">Danilo Bzdok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Through Diffusion Models (DMs), we have made significant advances in
generating high-quality images. Our exploration of these models delves deeply
into their core operational principles by systematically investigating key
aspects across various DM architectures: i) noise schedules, ii) samplers, and
iii) guidance. Our comprehensive examination of these models sheds light on
their hidden fundamental mechanisms, revealing the concealed foundational
elements that are essential for their effectiveness. Our analyses emphasize the
hidden key factors that determine model performance, offering insights that
contribute to the advancement of DMs. Past findings show that the configuration
of noise schedules, samplers, and guidance is vital to the quality of generated
images; however, models reach a stable level of quality across different
configurations at a remarkably similar point, revealing that the decisive
factors for optimal performance predominantly reside in the diffusion process
dynamics and the structural design of the model's network, rather than the
specifics of configuration details. Our comparative analysis reveals that
Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics
consistently outperform the Noise Conditioned Score Network (NCSN)-based ones,
not only when evaluated in their original forms but also when continuous
through Stochastic Differential Equation (SDE)-based implementations.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13371" title="Abstract">arXiv:2402.13371</a> [<a href="/pdf/2402.13371" title="Download PDF">pdf</a>, <a href="/format/2402.13371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FIDLAR: Forecast-Informed Deep Learning Architecture for Flood  Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jimeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zeda Yin</a>, 
<a href="/search/cs?searchtype=author&query=Leon%2C+A">Arturo Leon</a>, 
<a href="/search/cs?searchtype=author&query=Obeysekera%2C+J">Jayantha Obeysekera</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+G">Giri Narasimhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In coastal river systems, frequent floods, often occurring during major
storms or king tides, pose a severe threat to lives and property. However,
these floods can be mitigated or even prevented by strategically releasing
water before extreme weather events with hydraulic structures such as dams,
gates, pumps, and reservoirs. A standard approach used by local water
management agencies is the "rule-based" method, which specifies predetermined
pre-releases of water based on historical and time-tested human experience, but
which tends to result in excess or inadequate water release. The model
predictive control (MPC), a physics-based model for prediction, is an
alternative approach, albeit involving computationally intensive calculations.
In this paper, we propose a Forecast Informed Deep Learning Architecture,
FIDLAR, to achieve rapid and optimal flood management with precise water
pre-releases. FIDLAR seamlessly integrates two neural network modules: one
called the Flood Manager, which is responsible for generating water pre-release
schedules, and another called the Flood Evaluator, which assesses these
generated schedules. The Evaluator module is pre-trained separately, and its
gradient-based feedback is used to train the Manager model, ensuring optimal
water pre-releases. We have conducted experiments using FIDLAR with data from a
flood-prone coastal area in South Florida, particularly susceptible to frequent
storms. Results show that FIDLAR is several orders of magnitude faster than
currently used physics-based approaches while outperforming baseline methods
with improved water pre-release schedules. Our code is at
https://github.com/JimengShi/FIDLAR/.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13372" title="Abstract">arXiv:2402.13372</a> [<a href="/pdf/2402.13372" title="Download PDF">pdf</a>, <a href="/format/2402.13372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J+H">Jing Han Sun</a>, 
<a href="/search/cs?searchtype=author&query=Emami%2C+A">Ali Emami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in main proceedings of LREC-COLING 2024, 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While Large Language Models (LLMs) excel at the Winograd Schema Challenge
(WSC), a coreference resolution task testing common-sense reasoning through
pronoun disambiguation, they struggle with instances that feature minor
alterations or rewording. To address this, we introduce EvoGrad, an open-source
platform that harnesses a human-in-the-loop approach to create a dynamic
dataset tailored to such altered WSC instances. Leveraging ChatGPT's
capabilities, we expand our task instances from 182 to 3,691, setting a new
benchmark for diverse common-sense reasoning datasets. Additionally, we
introduce the error depth metric, assessing model stability in dynamic tasks.
Our results emphasize the challenge posed by EvoGrad: Even the best performing
LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,
a stark contrast to human performance of 92. 8% accuracy without perturbation
errors. This highlights ongoing model limitations and the value of dynamic
datasets in uncovering them.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13373" title="Abstract">arXiv:2402.13373</a> [<a href="/pdf/2402.13373" title="Download PDF">pdf</a>, <a href="/ps/2402.13373" title="Download PostScript">ps</a>, <a href="/format/2402.13373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New preconditioner strategy for solving block four-by-four linear  systems: An application to the saddle-point problem from 3D Stokes equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Badahmane%2C+A">Achraf Badahmane</a>, 
<a href="/search/math?searchtype=author&query=Ratnani%2C+A">Ahmed Ratnani</a>, 
<a href="/search/math?searchtype=author&query=Sadok%2C+H">Hassane Sadok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We have presented a fast method for solving a specific type of block
four-by-four saddlepoint problem arising from the finite element discretization
of the generalized 3D Stokes problem. We analyze the eigenvalue distribution
and the eigenvectors of the preconditioned matrix. Furthermore, we suggested
utilizing the preconditioned global conjugate gradient method (PGCG) as a block
iterative solver for handling multiple right-hand sides within the sub-system
and give some new convergence results. Numerical experiments have shown that
our preconditioned iterative approach is very efficient for solving the 3D
Stokes problem
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13374" title="Abstract">arXiv:2402.13374</a> [<a href="/pdf/2402.13374" title="Download PDF">pdf</a>, <a href="/format/2402.13374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sekuli%C4%87%2C+I">Ivan Sekuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Terragni%2C+S">Silvia Terragni</a>, 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+V">Victor Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Khau%2C+N">Nghia Khau</a>, 
<a href="/search/cs?searchtype=author&query=Guedes%2C+B">Bruna Guedes</a>, 
<a href="/search/cs?searchtype=author&query=Filipavicius%2C+M">Modestas Filipavicius</a>, 
<a href="/search/cs?searchtype=author&query=Manso%2C+A+F">Andr&#xe9; Ferreira Manso</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+R">Roland Mathis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the realm of dialogue systems, user simulation techniques have emerged as
a game-changer, redefining the evaluation and enhancement of task-oriented
dialogue (TOD) systems. These methods are crucial for replicating real user
interactions, enabling applications like synthetic data augmentation, error
detection, and robust evaluation. However, existing approaches often rely on
rigid rule-based methods or on annotated data. This paper introduces DAUS, a
Domain-Aware User Simulator. Leveraging large language models, we fine-tune
DAUS on real examples of task-oriented dialogues. Results on two relevant
benchmarks showcase significant improvements in terms of user goal fulfillment.
Notably, we have observed that fine-tuning enhances the simulator's coherence
with user goals, effectively mitigating hallucinations -- a major source of
inconsistencies in simulator responses.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13376" title="Abstract">arXiv:2402.13376</a> [<a href="/pdf/2402.13376" title="Download PDF">pdf</a>, <a href="/ps/2402.13376" title="Download PostScript">ps</a>, <a href="/format/2402.13376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic automatic complexity of finite strings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gill%2C+K">Kenneth Gill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 5 figures. This work extends Chapter 2 of the author's PhD dissertation at Penn State
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic (math.LO)

</div>
<p class="mathjax">We introduce a new complexity measure for finite strings using probabilistic
finite-state automata (PFAs), in the same spirit as existing notions employing
DFAs and NFAs, and explore its properties. The PFA complexity $A_P(x)$ is the
least number of states of a PFA for which $x$ is the most likely string of its
length to be accepted. The variant $A_{P,\delta}(x)$ adds a real-valued
parameter $\delta$ specifying a lower bound on the gap in acceptance
probabilities between $x$ and other strings. We relate $A_P$ to the DFA and NFA
complexities, obtain a complete classification of binary strings with $A_P=2$,
and prove $A_{P,\delta}(x)$ is computable for every $x$ and for cofinitely many
$\delta$ (depending on $x$). Finally, we discuss several other variations on
$A_P$ with a view to obtaining additional desirable properties.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13379" title="Abstract">arXiv:2402.13379</a> [<a href="/pdf/2402.13379" title="Download PDF">pdf</a>, <a href="/format/2402.13379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Referee-Meta-Learning for Fast Adaptation of Locational Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weiye Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiqun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaowei Jia</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+E">Erhu He</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Han Bao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">When dealing with data from distinct locations, machine learning algorithms
tend to demonstrate an implicit preference of some locations over the others,
which constitutes biases that sabotage the spatial fairness of the algorithm.
This unfairness can easily introduce biases in subsequent decision-making given
broad adoptions of learning-based solutions in practice. However, locational
biases in AI are largely understudied. To mitigate biases over locations, we
propose a locational meta-referee (Meta-Ref) to oversee the few-shot
meta-training and meta-testing of a deep neural network. Meta-Ref dynamically
adjusts the learning rates for training samples of given locations to advocate
a fair performance across locations, through an explicit consideration of
locational biases and the characteristics of input data. We present a
three-phase training framework to learn both a meta-learning-based predictor
and an integrated Meta-Ref that governs the fairness of the model. Once trained
with a distribution of spatial tasks, Meta-Ref is applied to samples from new
spatial tasks (i.e., regions outside the training area) to promote fairness
during the fine-tune step. We carried out experiments with two case studies on
crop monitoring and transportation safety, which show Meta-Ref can improve
locational fairness while keeping the overall prediction quality at a similar
level.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13380" title="Abstract">arXiv:2402.13380</a> [<a href="/pdf/2402.13380" title="Download PDF">pdf</a>, <a href="/format/2402.13380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward TransfORmers: Revolutionizing the Solution of Mixed Integer  Programs with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cooper%2C+J+F">Joshua F. Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S+J">Seung Jin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Buyuktahtakin%2C+I+E">I. Esra Buyuktahtakin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Combinatorics (math.CO); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this study, we introduce an innovative deep learning framework that
employs a transformer model to address the challenges of mixed-integer
programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP).
Our approach, to our knowledge, is the first to utilize transformers to predict
the binary variables of a mixed-integer programming (MIP) problem.
Specifically, our approach harnesses the encoder decoder transformer's ability
to process sequential data, making it well-suited for predicting binary
variables indicating production setup decisions in each period of the CLSP.
This problem is inherently dynamic, and we need to handle sequential decision
making under constraints. We present an efficient algorithm in which CLSP
solutions are learned through a transformer neural network. The proposed
post-processed transformer algorithm surpasses the state-of-the-art solver,
CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and
percent infeasibility over 240K benchmark CLSP instances tested. After the ML
model is trained, conducting inference on the model, including post-processing,
reduces the MIP into a linear program (LP). This transforms the ML-based
algorithm, combined with an LP solver, into a polynomial-time approximation
algorithm to solve a well-known NP-Hard problem, with almost perfect solution
quality.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13381" title="Abstract">arXiv:2402.13381</a> [<a href="/pdf/2402.13381" title="Download PDF">pdf</a>, <a href="/format/2402.13381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree semi-separable matrices: a simultaneous generalization of  sequentially and hierarchically semi-separable representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Govindarajan%2C+N">Nithin Govindarajan</a>, 
<a href="/search/math?searchtype=author&query=Chandrasekaran%2C+S">Shivkumar Chandrasekaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a unification and generalization of sequentially and
hierarchically semi-separable (SSS and HSS) matrices called tree semi-separable
(TSS) matrices. Our main result is to show that any dense matrix can be
expressed in a TSS format. Here, the dimensions of the generators are specified
by the ranks of the Hankel blocks of the matrix. TSS matrices satisfy a
graph-induced rank structure (GIRS) property. It is shown that TSS matrices
generalize the algebraic properties of SSS and HSS matrices under addition,
products, and inversion. Subsequently, TSS matrices admit linear time
matrix-vector multiply, matrix-matrix multiply, matrix-matrix addition,
inversion, and solvers.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13385" title="Abstract">arXiv:2402.13385</a> [<a href="/pdf/2402.13385" title="Download PDF">pdf</a>, <a href="/ps/2402.13385" title="Download PostScript">ps</a>, <a href="/format/2402.13385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regular Languages in the Sliding Window Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganardi%2C+M">Moses Ganardi</a>, 
<a href="/search/cs?searchtype=author&query=Hucke%2C+D">Danny Hucke</a>, 
<a href="/search/cs?searchtype=author&query=Lohrey%2C+M">Markus Lohrey</a>, 
<a href="/search/cs?searchtype=author&query=Mamouras%2C+K">Konstantinos Mamouras</a>, 
<a href="/search/cs?searchtype=author&query=Starikovskaya%2C+T">Tatiana Starikovskaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1909.10261">arXiv:1909.10261</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">We study the space complexity of the following problem: For a fixed regular
language $L$, test membership of a sliding window of length $n$ to $L$ over a
given stream of symbols. For deterministic streaming algorithms we prove a
trichotomy theorem, namely that the (optimal) space complexity is either
constant, logarithmic or linear, measured in the window length $n$.
Additionally, we provide natural language-theoretic characterizations of the
space classes. We then extend the results to randomized streaming algorithms
and we show that in this setting, the space complexity of any regular language
is either constant, doubly logarithmic, logarithmic or linear. Finally, we
introduce sliding window testers, which can distinguish whether a window
belongs to the language $L$ or has Hamming distance $&gt; \epsilon n$ to $L$. We
prove that every regular language has a deterministic (resp., randomized)
sliding window tester that requires only logarithmic (resp., constant) space.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13387" title="Abstract">arXiv:2402.13387</a> [<a href="/pdf/2402.13387" title="Download PDF">pdf</a>, <a href="/ps/2402.13387" title="Download PostScript">ps</a>, <a href="/format/2402.13387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistriFS: A Platform and User Agnostic Approach to File Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boesch%2C+J">Julian Boesch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In an age where the distribution of information is crucial, current file
sharing solutions suffer significant deficiencies. Popular systems such as
Google Drive, torrenting and IPFS suffer issues with compatibility,
accessibility and censorship. This paper introduces DistriFS, a novel
decentralized approach tailored for efficient and large-scale distribution of
files. The architecture of DistriFS is grounded in three foundational pillars:
scalability, security, and seamless integration. The proposed server
implementation harnesses the power of Golang, ensuring near-universal
interoperability across operating systems and hardware. Moreover, the use of
the HTTP protocol eliminates the need for additional software to access the
network, ensuring compatibility across all major operating systems and
facilitating effortless downloads. The design and efficacy of DistriFS
represent a significant advancement in the realm of file distribution systems,
offering a scalable and secure alternative to current centralized and
decentralized models.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13388" title="Abstract">arXiv:2402.13388</a> [<a href="/pdf/2402.13388" title="Download PDF">pdf</a>, <a href="/format/2402.13388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer tricks: Precomputing the first layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graef%2C+N">Nils Graef</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This short paper describes a trick to speed up inference of transformers with
RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of
the first transformer layer can be precomputed, which results in slightly lower
latency and lower cost-per-token. Because this trick optimizes only one layer,
the relative savings depend on the total number of layers. For example, the
maximum savings for a model with only 4 layers (such as Whisper tiny) is
limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3%
savings.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13393" title="Abstract">arXiv:2402.13393</a> [<a href="/pdf/2402.13393" title="Download PDF">pdf</a>, <a href="/format/2402.13393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness Risks for Group-conditionally Missing Demographics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kaiqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenzhe Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinhua Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Fairness-aware classification models have gained increasing attention in
recent years as concerns grow on discrimination against some demographic
groups. Most existing models require full knowledge of the sensitive features,
which can be impractical due to privacy, legal issues, and an individual's fear
of discrimination. The key challenge we will address is the group dependency of
the unavailability, e.g., people of some age range may be more reluctant to
reveal their age. Our solution augments general fairness risks with
probabilistic imputations of the sensitive features, while jointly learning the
group-conditionally missing probabilities in a variational auto-encoder. Our
model is demonstrated effective on both image and tabular datasets, achieving
an improved balance between accuracy and fairness.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13397" title="Abstract">arXiv:2402.13397</a> [<a href="/pdf/2402.13397" title="Download PDF">pdf</a>, <a href="/format/2402.13397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Xling: A Learned Filter Framework for Accelerating High-Dimensional  Approximate Similarity Join
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+V">Vyom Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D+Z">Daisy Zhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Similarity join finds all pairs of close points within a given distance
threshold. Many similarity join methods have been proposed, but they are
usually not efficient on high-dimensional space due to the curse of
dimensionality and data-unawareness. We investigate the possibility of using
metric space Bloom filter (MSBF), a family of data structures checking if a
query point has neighbors in a multi-dimensional space, to speed up similarity
join. However, there are several challenges when applying MSBF to similarity
join, including excessive information loss, data-unawareness and hard
constraint on the distance metric. In this paper, we propose Xling, a generic
framework to build a learning-based metric space filter with any existing
regression model, aiming at accurately predicting whether a query point has
enough number of neighbors. The framework provides a suite of optimization
strategies to further improve the prediction quality based on the learning
model, which has demonstrated significantly higher prediction quality than
existing MSBF. We also propose XJoin, one of the first filter-based similarity
join methods, based on Xling. By predicting and skipping those queries without
enough neighbors, XJoin can effectively reduce unnecessary neighbor searching
and therefore it achieves a remarkable acceleration. Benefiting from the
generalization capability of deep learning models, XJoin can be easily
transferred onto new dataset (in similar distribution) without re-training.
Furthermore, Xling is not limited to being applied in XJoin, instead, it acts
as a flexible plugin that can be inserted to any loop-based similarity join
methods for a speedup.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13399" title="Abstract">arXiv:2402.13399</a> [<a href="/pdf/2402.13399" title="Download PDF">pdf</a>, <a href="/format/2402.13399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning and Sustaining Shared Normative Systems via Bayesian Rule  Induction in Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oldenburg%2C+N">Ninell Oldenburg</a>, 
<a href="/search/cs?searchtype=author&query=Zhi-Xuan%2C+T">Tan Zhi-Xuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, The 23rd International Conference on Autonomous Agents and Multi-Agent Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A universal feature of human societies is the adoption of systems of rules
and norms in the service of cooperative ends. How can we build learning agents
that do the same, so that they may flexibly cooperate with the human
institutions they are embedded in? We hypothesize that agents can achieve this
by assuming there exists a shared set of norms that most others comply with
while pursuing their individual desires, even if they do not know the exact
content of those norms. By assuming shared norms, a newly introduced agent can
infer the norms of an existing population from observations of compliance and
violation. Furthermore, groups of agents can converge to a shared set of norms,
even if they initially diverge in their beliefs about what the norms are. This
in turn enables the stability of the normative system: since agents can
bootstrap common knowledge of the norms, this leads the norms to be widely
adhered to, enabling new entrants to rapidly learn those norms. We formalize
this framework in the context of Markov games and demonstrate its operation in
a multi-agent environment via approximately Bayesian rule induction of
obligative and prohibitive norms. Using our approach, agents are able to
rapidly learn and sustain a variety of cooperative institutions, including
resource management norms and compensation for pro-social labor, promoting
collective welfare while still allowing agents to act in their own interests.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13402" title="Abstract">arXiv:2402.13402</a> [<a href="/pdf/2402.13402" title="Download PDF">pdf</a>, <a href="/ps/2402.13402" title="Download PostScript">ps</a>, <a href="/format/2402.13402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards accelerating physical discovery via non-interactive and  interactive multi-fidelity Bayesian Optimization: Current challenges and  future opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+A">Arpan Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Valleti%2C+S+M+P">Sai Mani Prudhvi Valleti</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+R">Rama Vasudevan</a>, 
<a href="/search/cs?searchtype=author&query=Ziatdinov%2C+M">Maxim Ziatdinov</a>, 
<a href="/search/cs?searchtype=author&query=Kalinin%2C+S+V">Sergei V. Kalinin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text includes 29 pages and 10 figures, Supplementary mat. includes 4 pages and 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Both computational and experimental material discovery bring forth the
challenge of exploring multidimensional and often non-differentiable parameter
spaces, such as phase diagrams of Hamiltonians with multiple interactions,
composition spaces of combinatorial libraries, processing spaces, and molecular
embedding spaces. Often these systems are expensive or time-consuming to
evaluate a single instance, and hence classical approaches based on exhaustive
grid or random search are too data intensive. This resulted in strong interest
towards active learning methods such as Bayesian optimization (BO) where the
adaptive exploration occurs based on human learning (discovery) objective.
However, classical BO is based on a predefined optimization target, and
policies balancing exploration and exploitation are purely data driven. In
practical settings, the domain expert can pose prior knowledge on the system in
form of partially known physics laws and often varies exploration policies
during the experiment. Here, we explore interactive workflows building on
multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then
structured (physics-driven) sMFBO, and extending it to allow human in the loop
interactive iMFBO workflows for adaptive and domain expert aligned exploration.
These approaches are demonstrated over highly non-smooth multi-fidelity
simulation data generated from an Ising model, considering spin-spin
interaction as parameter space, lattice sizes as fidelity spaces, and the
objective as maximizing heat capacity. Detailed analysis and comparison show
the impact of physics knowledge injection and on-the-fly human decisions for
improved exploration, current challenges, and potential opportunities for
algorithm development with combining data, physics and real time human
decisions.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13404" title="Abstract">arXiv:2402.13404</a> [<a href="/pdf/2402.13404" title="Download PDF">pdf</a>, <a href="/format/2402.13404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Layout-to-Image Generation with Localized Descriptions using ControlNet  with Cross-Attention Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lukovnikov%2C+D">Denis Lukovnikov</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Asja Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While text-to-image diffusion models can generate highquality images from
textual descriptions, they generally lack fine-grained control over the visual
composition of the generated images. Some recent works tackle this problem by
training the model to condition the generation process on additional input
describing the desired image layout. Arguably the most popular among such
methods, ControlNet, enables a high degree of control over the generated image
using various types of conditioning inputs (e.g. segmentation maps). However,
it still lacks the ability to take into account localized textual descriptions
that indicate which image region is described by which phrase in the prompt. In
this work, we show the limitations of ControlNet for the layout-to-image task
and enable it to use localized descriptions using a training-free approach that
modifies the crossattention scores during generation. We adapt and investigate
several existing cross-attention control methods in the context of ControlNet
and identify shortcomings that cause failure (concept bleeding) or image
degradation under specific conditions. To address these shortcomings, we
develop a novel cross-attention manipulation method in order to maintain image
quality while improving control. Qualitative and quantitative experimental
studies focusing on challenging cases are presented, demonstrating the
effectiveness of the investigated general approach, and showing the
improvements obtained by the proposed cross-attention control method.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13405" title="Abstract">arXiv:2402.13405</a> [<a href="/pdf/2402.13405" title="Download PDF">pdf</a>, <a href="/format/2402.13405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set  Expansion and Taxonomy Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanzhen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy
Construction are three representative tasks that can be used to automatically
populate an existing taxonomy with new entities. However, previous approaches
often address these tasks separately with heterogeneous techniques, lacking a
unified perspective. To tackle this issue, in this paper, we identify the
common key skills needed for these tasks from the view of taxonomy structures
-- finding 'siblings' and finding 'parents' -- and propose a unified
taxonomy-guided instruction tuning framework to jointly solve the three tasks.
To be specific, by leveraging the existing taxonomy as a rich source of entity
relationships, we utilize instruction tuning to fine-tune a large language
model to generate parent and sibling entities. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct,
which outperforms task-specific baselines across all three tasks.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13408" title="Abstract">arXiv:2402.13408</a> [<a href="/pdf/2402.13408" title="Download PDF">pdf</a>, <a href="/format/2402.13408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Healthcare Copilot: Eliciting the Power of General LLMs for Medical  Consultation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhiyao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Baosheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The copilot framework, which aims to enhance and tailor large language models
(LLMs) for specific complex tasks without requiring fine-tuning, is gaining
increasing attention from the community. In this paper, we introduce the
construction of a Healthcare Copilot designed for medical consultation. The
proposed Healthcare Copilot comprises three main components: 1) the Dialogue
component, responsible for effective and safe patient interactions; 2) the
Memory component, storing both current conversation data and historical patient
information; and 3) the Processing component, summarizing the entire dialogue
and generating reports. To evaluate the proposed Healthcare Copilot, we
implement an auto-evaluation scheme using ChatGPT for two roles: as a virtual
patient engaging in dialogue with the copilot, and as an evaluator to assess
the quality of the dialogue. Extensive results demonstrate that the proposed
Healthcare Copilot significantly enhances the capabilities of general LLMs for
medical consultations in terms of inquiry capability, conversational fluency,
response accuracy, and safety. Furthermore, we conduct ablation studies to
highlight the contribution of each individual module in the Healthcare Copilot.
Code will be made publicly available on GitHub.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13409" title="Abstract">arXiv:2402.13409</a> [<a href="/pdf/2402.13409" title="Download PDF">pdf</a>, <a href="/format/2402.13409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient finite element method for computing the response of a  strain-limiting elastic solid containing a v-notch and inclusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=G.%2C+S">Shylaja G.</a>, 
<a href="/search/math?searchtype=author&query=V.%2C+K+N">Kesavulu Naidu V.</a>, 
<a href="/search/math?searchtype=author&query=B.%2C+V">Venkatesh B.</a>, 
<a href="/search/math?searchtype=author&query=Mallikarjunaiah%2C+S+M">S. M. Mallikarjunaiah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Accurate triangulation of the domain plays a pivotal role in computing the
numerical approximation of the differential operators. A good triangulation is
the one which aids in reducing discretization errors. In a standard collocation
technique, the smooth curved domain is typically triangulated with a mesh by
taking points on the boundary to approximate them by polygons. However, such an
approach often leads to geometrical errors which directly affect the accuracy
of the numerical approximation. To restrict such geometrical errors,
\textit{isoparametric}, \textit{subparametric}, and \textit{iso-geometric}
methods were introduced which allow the approximation of the curved surfaces
(or curved line segments). In this paper, we present an efficient finite
element method to approximate the solution to the elliptic boundary value
problem (BVP), which governs the response of an elastic solid containing a
v-notch and inclusions. The algebraically nonlinear constitutive equation along
with the balance of linear momentum reduces to second-order quasi-linear
elliptic partial differential equation. Our approach allows us to represent the
complex curved boundaries by smooth \textit{one-of-its-kind} point
transformation. The main idea is to obtain higher-order shape functions which
enable us to accurately compute the entries in the finite element matrices and
vectors. A Picard-type linearization is utilized to handle the nonlinearities
in the governing differential equation. The numerical results for the test
cases show considerable improvement in the accuracy.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13410" title="Abstract">arXiv:2402.13410</a> [<a href="/pdf/2402.13410" title="Download PDF">pdf</a>, <a href="/format/2402.13410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Networks with Domain Knowledge Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sam%2C+D">Dylan Sam</a>, 
<a href="/search/cs?searchtype=author&query=Pukdee%2C+R">Rattana Pukdee</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+D+P">Daniel P. Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+Y">Yewon Byun</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian neural networks (BNNs) have recently gained popularity due to their
ability to quantify model uncertainty. However, specifying a prior for BNNs
that captures relevant domain knowledge is often extremely challenging. In this
work, we propose a framework for integrating general forms of domain knowledge
(i.e., any knowledge that can be represented by a loss function) into a BNN
prior through variational inference, while enabling computationally efficient
posterior inference and sampling. Specifically, our approach results in a prior
over neural network weights that assigns high probability mass to models that
better align with our domain knowledge, leading to posterior samples that also
exhibit this behavior. We show that BNNs using our proposed domain knowledge
priors outperform those with standard priors (e.g., isotropic Gaussian,
Gaussian process), successfully incorporating diverse types of prior
information such as fairness, physics rules, and healthcare knowledge and
achieving better predictive performance. We also present techniques for
transferring the learned priors across different model architectures,
demonstrating their broad utility across various settings.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13412" title="Abstract">arXiv:2402.13412</a> [<a href="/pdf/2402.13412" title="Download PDF">pdf</a>, <a href="/format/2402.13412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling physics-informed hard constraints with mixture-of-experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chalapathi%2C+N">Nithin Chalapathi</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yiheng Du</a>, 
<a href="/search/cs?searchtype=author&query=Krishnapriyan%2C+A">Aditi Krishnapriyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">Imposing known physical constraints, such as conservation laws, during neural
network training introduces an inductive bias that can improve accuracy,
reliability, convergence, and data efficiency for modeling physical dynamics.
While such constraints can be softly imposed via loss function penalties,
recent advancements in differentiable physics and optimization improve
performance by incorporating PDE-constrained optimization as individual layers
in neural networks. This enables a stricter adherence to physical constraints.
However, imposing hard constraints significantly increases computational and
memory costs, especially for complex dynamical systems. This is because it
requires solving an optimization problem over a large number of points in a
mesh, representing spatial and temporal discretizations, which greatly
increases the complexity of the constraint. To address this challenge, we
develop a scalable approach to enforce hard physical constraints using
Mixture-of-Experts (MoE), which can be used with any neural network
architecture. Our approach imposes the constraint over smaller decomposed
domains, each of which is solved by an "expert" through differentiable
optimization. During training, each expert independently performs a localized
backpropagation step by leveraging the implicit function theorem; the
independence of each expert allows for parallelization across multiple GPUs.
Compared to standard differentiable optimization, our scalable approach
achieves greater accuracy in the neural PDE solver setting for predicting the
dynamics of challenging non-linear systems. We also improve training stability
and require significantly less computation time during both training and
inference stages.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13414" title="Abstract">arXiv:2402.13414</a> [<a href="/pdf/2402.13414" title="Download PDF">pdf</a>, <a href="/format/2402.13414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Large Language Models as Post-hoc Correctors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhiqiang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kuangyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mottin%2C+D">Davide Mottin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">As Machine Learning (ML) models grow in size and demand higher-quality
training data, the expenses associated with re-training and fine-tuning these
models are escalating rapidly. Inspired by recent impressive achievements of
Large Language Models (LLMs) in different fields, this paper delves into the
question: can LLMs efficiently improve an ML's performance at a minimal cost?
We show that, through our proposed training-free framework LlmCorr, an LLM can
work as a post-hoc corrector to propose corrections for the predictions of an
arbitrary ML model. In particular, we form a contextual knowledge database by
incorporating the dataset's label information and the ML model's predictions on
the validation dataset. Leveraging the in-context learning capability of LLMs,
we ask the LLM to summarise the instances in which the ML model makes mistakes
and the correlation between primary predictions and true labels. Following
this, the LLM can transfer its acquired knowledge to suggest corrections for
the ML model's predictions. Our experimental results on the challenging
molecular predictions show that LlmCorr improves the performance of a number of
models by up to 39%.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13415" title="Abstract">arXiv:2402.13415</a> [<a href="/pdf/2402.13415" title="Download PDF">pdf</a>, <a href="/format/2402.13415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure Guided Prompt: Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kewei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T">Theodore Willke</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although Large Language Models (LLMs) excel at addressing straightforward
reasoning tasks, they frequently struggle with difficulties when confronted by
more complex multi-step reasoning due to a range of factors. Firstly, natural
language often encompasses complex relationships among entities, making it
challenging to maintain a clear reasoning chain over longer spans. Secondly,
the abundance of linguistic diversity means that the same entities and
relationships can be expressed using different terminologies and structures,
complicating the task of identifying and establishing connections between
multiple pieces of information. Graphs provide an effective solution to
represent data rich in relational information and capture long-term
dependencies among entities. To harness the potential of graphs, our paper
introduces Structure Guided Prompt, an innovative three-stage task-agnostic
prompting framework designed to improve the multi-step reasoning capabilities
of LLMs in a zero-shot setting. This framework explicitly converts unstructured
text into a graph via LLMs and instructs them to navigate this graph using
task-specific strategies to formulate responses. By effectively organizing
information and guiding navigation, it enables LLMs to provide more accurate
and context-aware responses. Our experiments show that this framework
significantly enhances the reasoning capabilities of LLMs, enabling them to
excel in a broader spectrum of natural language scenarios.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13417" title="Abstract">arXiv:2402.13417</a> [<a href="/pdf/2402.13417" title="Download PDF">pdf</a>, <a href="/format/2402.13417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the `Why&#x27; of Buying: Introducing a New Dataset and Benchmark  for Purchase Reason and Post-Purchase Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+S">Siqi Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Explanations are crucial for enhancing user trust and understanding within
modern recommendation systems. To build truly explainable systems, we need
high-quality datasets that elucidate why users make choices. While previous
efforts have focused on extracting users' post-purchase sentiment in reviews,
they ignore the reasons behind the decision to buy.
<br />In our work, we propose a novel purchase reason explanation task. To this
end, we introduce an LLM-based approach to generate a dataset that consists of
textual explanations of why real users make certain purchase decisions. We
induce LLMs to explicitly distinguish between the reasons behind purchasing a
product and the experience after the purchase in a user review. An automated,
LLM-driven evaluation, as well as a small scale human evaluation, confirms the
effectiveness of our approach to obtaining high-quality, personalized
explanations. We benchmark this dataset on two personalized explanation
generation tasks. We release the code and prompts to spur further research.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13418" title="Abstract">arXiv:2402.13418</a> [<a href="/pdf/2402.13418" title="Download PDF">pdf</a>, <a href="/format/2402.13418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvolMPNN: Predicting Mutational Effect on Homologous Proteins by  Evolution Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhiqiang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Mottin%2C+D">Davide Mottin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Predicting protein properties is paramount for biological and medical
advancements. Current protein engineering mutates on a typical protein, called
the wild-type, to construct a family of homologous proteins and study their
properties. Yet, existing methods easily neglect subtle mutations, failing to
capture the effect on the protein properties. To this end, we propose EvolMPNN,
Evolution-aware Message Passing Neural Network, to learn evolution-aware
protein embeddings. EvolMPNN samples sets of anchor proteins, computes
evolutionary information by means of residues and employs a differentiable
evolution-aware aggregation scheme over these sampled anchors. This way
EvolMPNNcan capture the mutation effect on proteins with respect to the anchor
proteins. Afterwards, the aggregated evolution-aware embeddings are integrated
with sequence embeddings to generate final comprehensive protein embeddings.
Our model shows up to 6.4% better than state-of-the-art methods and attains 36x
inference speedup in comparison with large pre-trained models.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13419" title="Abstract">arXiv:2402.13419</a> [<a href="/pdf/2402.13419" title="Download PDF">pdf</a>, <a href="/ps/2402.13419" title="Download PostScript">ps</a>, <a href="/format/2402.13419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Bound for Behavioral Guarantee of Model-based Planning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+Z">Zhiyu An</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xianzhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ICLR 24 tiny paper track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Recent years have seen an emerging interest in the trustworthiness of machine
learning-based agents in the wild, especially in robotics, to provide safety
assurance for the industry. Obtaining behavioral guarantees for these agents
remains an important problem. In this work, we focus on guaranteeing a
model-based planning agent reaches a goal state within a specific future time
step. We show that there exists a lower bound for the reward at the goal state,
such that if the said reward is below that bound, it is impossible to obtain
such a guarantee. By extension, we show how to enforce preferences over
multiple goals.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13421" title="Abstract">arXiv:2402.13421</a> [<a href="/pdf/2402.13421" title="Download PDF">pdf</a>, <a href="/format/2402.13421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Quantitative Risk Assessment Machine Learning Model for  Drivers Distraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fasanmade%2C+A">Adebamigbe Fasanmade</a>, 
<a href="/search/cs?searchtype=author&query=Al-Bayatti%2C+A+H">Ali H. Al-Bayatti</a>, 
<a href="/search/cs?searchtype=author&query=Morden%2C+J+N">Jarrad Neil Morden</a>, 
<a href="/search/cs?searchtype=author&query=Caraffini%2C+F">Fabio Caraffini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Risk mitigation techniques are critical to avoiding accidents associated with
driving behaviour. We provide a novel Multi-Class Driver Distraction Risk
Assessment (MDDRA) model that considers the vehicle, driver, and environmental
data during a journey. MDDRA categorises the driver on a risk matrix as safe,
careless, or dangerous. It offers flexibility in adjusting the parameters and
weights to consider each event on a specific severity level. We collect
real-world data using the Field Operation Test (TeleFOT), covering drivers
using the same routes in the East Midlands, United Kingdom (UK). The results
show that reducing road accidents caused by driver distraction is possible. We
also study the correlation between distraction (driver, vehicle, and
environment) and the classification severity based on a continuous distraction
severity score. Furthermore, we apply machine learning techniques to classify
and predict driver distraction according to severity levels to aid the
transition of control from the driver to the vehicle (vehicle takeover) when a
situation is deemed risky. The Ensemble Bagged Trees algorithm performed best,
with an accuracy of 96.2%.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13425" title="Abstract">arXiv:2402.13425</a> [<a href="/pdf/2402.13425" title="Download PDF">pdf</a>, <a href="/format/2402.13425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Histogram Loss in Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imani%2C+E">Ehsan Imani</a>, 
<a href="/search/cs?searchtype=author&query=Luedemann%2C+K">Kai Luedemann</a>, 
<a href="/search/cs?searchtype=author&query=Scholnick-Hughes%2C+S">Sam Scholnick-Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Elelimy%2C+E">Esraa Elelimy</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Martha White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">It is becoming increasingly common in regression to train neural networks
that model the entire distribution even if only the mean is required for
prediction. This additional modeling often comes with performance gain and the
reasons behind the improvement are not fully known. This paper investigates a
recent approach to regression, the Histogram Loss, which involves learning the
conditional distribution of the target variable by minimizing the cross-entropy
between a target distribution and a flexible histogram prediction. We design
theoretical and empirical analyses to determine why and when this performance
gain appears, and how different components of the loss contribute to it. Our
results suggest that the benefits of learning distributions in this setup come
from improvements in optimization rather than learning a better representation.
We then demonstrate the viability of the Histogram Loss in common deep learning
applications without a need for costly hyperparameter tuning.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13426" title="Abstract">arXiv:2402.13426</a> [<a href="/pdf/2402.13426" title="Download PDF">pdf</a>, <a href="/format/2402.13426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Relationships Among Research Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangci Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+J">Jessica Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the rapid pace of research publications, keeping up to date with all
the latest related papers is very time-consuming, even with daily feed tools.
There is a need for automatically generated, short, customized literature
reviews of sets of papers to help researchers decide what to read. While
several works in the last decade have addressed the task of explaining a single
research paper, usually in the context of another paper citing it, the
relationship among multiple papers has been ignored; prior works have focused
on generating a single citation sentence in isolation, without addressing the
expository and transition sentences needed to connect multiple papers in a
coherent story. In this work, we explore a feature-based, LLM-prompting
approach to generate richer citation texts, as well as generating multiple
citations at once to capture the complex relationships among research papers.
We perform an expert evaluation to investigate the impact of our proposed
features on the quality of the generated paragraphs and find a strong
correlation between human preference and integrative writing style, suggesting
that humans prefer high-level, abstract citations, with transition sentences
between them to provide an overall story.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13427" title="Abstract">arXiv:2402.13427</a> [<a href="/pdf/2402.13427" title="Download PDF">pdf</a>, <a href="/ps/2402.13427" title="Download PostScript">ps</a>, <a href="/format/2402.13427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative causality, causality-guided scientific discovery, and  causal machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X+S">X. San Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dake Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renhe Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures. To appear in Ocean-Land-Atmosphere Research. arXiv admin note: substantial text overlap with <a href="/abs/2112.14839">arXiv:2112.14839</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">It has been said, arguably, that causality analysis should pave a promising
way to interpretable deep learning and generalization. Incorporation of
causality into artificial intelligence (AI) algorithms, however, is challenged
with its vagueness, non-quantitiveness, computational inefficiency, etc. During
the past 18 years, these challenges have been essentially resolved, with the
establishment of a rigorous formalism of causality analysis initially motivated
from atmospheric predictability. This not only opens a new field in the
atmosphere-ocean science, namely, information flow, but also has led to
scientific discoveries in other disciplines, such as quantum mechanics,
neuroscience, financial economics, etc., through various applications. This
note provides a brief review of the decade-long effort, including a list of
major theoretical results, a sketch of the causal deep learning framework, and
some representative real-world applications in geoscience pertaining to this
journal, such as those on the anthropogenic cause of global warming, the
decadal prediction of El Ni\~no Modoki, the forecasting of an extreme drought
in China, among others.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13429" title="Abstract">arXiv:2402.13429</a> [<a href="/pdf/2402.13429" title="Download PDF">pdf</a>, <a href="/ps/2402.13429" title="Download PostScript">ps</a>, <a href="/format/2402.13429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everything You Always Wanted to Know About Storage Compressibility of  Pre-Trained ML Models but Were Afraid to Ask
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhaoyuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Ammar Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Ali Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yue Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">As the number of pre-trained machine learning (ML) models is growing
exponentially, data reduction tools are not catching up. Existing data
reduction techniques are not specifically designed for pre-trained model (PTM)
dataset files. This is largely due to a lack of understanding of the patterns
and characteristics of these datasets, especially those relevant to data
reduction and compressibility.
<br />This paper presents the first, exhaustive analysis to date of PTM datasets on
storage compressibility. Our analysis spans different types of data reduction
and compression techniques, from hash-based data deduplication, data similarity
detection, to dictionary-coding compression. Our analysis explores these
techniques at three data granularity levels, from model layers, model chunks,
to model parameters. We draw new observations that indicate that modern data
reduction tools are not effective when handling PTM datasets. There is a
pressing need for new compression methods that take into account PTMs' data
characteristics for effective storage reduction.
<br />Motivated by our findings, we design ELF, a simple yet effective,
error-bounded, lossy floating-point compression method. ELF transforms
floating-point parameters in such a way that the common exponent field of the
transformed parameters can be completely eliminated to save storage space. We
develop Elves, a compression framework that integrates ELF along with several
other data reduction methods. Elves uses the most effective method to compress
PTMs that exhibit different patterns. Evaluation shows that Elves achieves an
overall compression ratio of $1.52\times$, which is $1.31\times$, $1.32\times$
and $1.29\times$ higher than a general-purpose compressor (zstd), an
error-bounded lossy compressor (SZ3), and the uniform model quantization,
respectively, with negligible model accuracy loss.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13430" title="Abstract">arXiv:2402.13430</a> [<a href="/pdf/2402.13430" title="Download PDF">pdf</a>, <a href="/format/2402.13430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinkSAGE: Optimizing Job Matching Using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haichao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+X">Xiaochen Hou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shihai He</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+K+Q">Kay Qianqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhujun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Borisyuk%2C+F">Fedor Borisyuk</a>, 
<a href="/search/cs?searchtype=author&query=Hewlett%2C+D">Daniel Hewlett</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Veeraraghavan%2C+S">Srikant Veeraraghavan</a>, 
<a href="/search/cs?searchtype=author&query=Tsun%2C+A">Alex Tsun</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chengming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjing Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We present LinkSAGE, an innovative framework that integrates Graph Neural
Networks (GNNs) into large-scale personalized job matching systems, designed to
address the complex dynamics of LinkedIns extensive professional network. Our
approach capitalizes on a novel job marketplace graph, the largest and most
intricate of its kind in industry, with billions of nodes and edges. This graph
is not merely extensive but also richly detailed, encompassing member and job
nodes along with key attributes, thus creating an expansive and interwoven
network. A key innovation in LinkSAGE is its training and serving methodology,
which effectively combines inductive graph learning on a heterogeneous,
evolving graph with an encoder-decoder GNN model. This methodology decouples
the training of the GNN model from that of existing Deep Neural Nets (DNN)
models, eliminating the need for frequent GNN retraining while maintaining
up-to-date graph signals in near realtime, allowing for the effective
integration of GNN insights through transfer learning. The subsequent nearline
inference system serves the GNN encoder within a real-world setting,
significantly reducing online latency and obviating the need for costly
real-time GNN infrastructure. Validated across multiple online A/B tests in
diverse product scenarios, LinkSAGE demonstrates marked improvements in member
engagement, relevance matching, and member retention, confirming its
generalizability and practical impact.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13432" title="Abstract">arXiv:2402.13432</a> [<a href="/pdf/2402.13432" title="Download PDF">pdf</a>, <a href="/format/2402.13432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrBenchmark: A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labrak%2C+Y">Yanis Labrak</a>, 
<a href="/search/cs?searchtype=author&query=Bazoge%2C+A">Adrien Bazoge</a>, 
<a href="/search/cs?searchtype=author&query=Khettari%2C+O+E">Oumaima El Khettari</a>, 
<a href="/search/cs?searchtype=author&query=Rouvier%2C+M">Mickael Rouvier</a>, 
<a href="/search/cs?searchtype=author&query=Beaufils%2C+P+C+d">Pacome Constant dit Beaufils</a>, 
<a href="/search/cs?searchtype=author&query=Grabar%2C+N">Natalia Grabar</a>, 
<a href="/search/cs?searchtype=author&query=Daille%2C+B">Beatrice Daille</a>, 
<a href="/search/cs?searchtype=author&query=Quiniou%2C+S">Solen Quiniou</a>, 
<a href="/search/cs?searchtype=author&query=Morin%2C+E">Emmanuel Morin</a>, 
<a href="/search/cs?searchtype=author&query=Gourraud%2C+P">Pierre-Antoine Gourraud</a>, 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-Coling 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The biomedical domain has sparked a significant interest in the field of
Natural Language Processing (NLP), which has seen substantial advancements with
pre-trained language models (PLMs). However, comparing these models has proven
challenging due to variations in evaluation protocols across different models.
A fair solution is to aggregate diverse downstream tasks into a benchmark,
allowing for the assessment of intrinsic PLMs qualities from various
perspectives. Although still limited to few languages, this initiative has been
undertaken in the biomedical field, notably English and Chinese. This
limitation hampers the evaluation of the latest French biomedical models, as
they are either assessed on a minimal number of tasks with non-standardized
protocols or evaluated using general downstream tasks. To bridge this research
gap and account for the unique sensitivities of French, we present the
first-ever publicly available French biomedical language understanding
benchmark called DrBenchmark. It encompasses 20 diversified tasks, including
named-entity recognition, part-of-speech tagging, question-answering, semantic
textual similarity, and classification. We evaluate 8 state-of-the-art
pre-trained masked language models (MLMs) on general and biomedical-specific
data, as well as English specific MLMs to assess their cross-lingual
capabilities. Our experiments reveal that no single model excels across all
tasks, while generalist models are sometimes still competitive.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13433" title="Abstract">arXiv:2402.13433</a> [<a href="/pdf/2402.13433" title="Download PDF">pdf</a>, <a href="/format/2402.13433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Tree Alignment for Evaluation of (Speech) Constituency  Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+F">Freda Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gimpel%2C+K">Kevin Gimpel</a>, 
<a href="/search/cs?searchtype=author&query=Livescu%2C+K">Karen Livescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We present the structured average intersection-over-union ratio (STRUCT-IOU),
a similarity metric between constituency parse trees motivated by the problem
of evaluating speech parsers. STRUCT-IOU enables comparison between a
constituency parse tree (over automatically recognized spoken word boundaries)
with the ground-truth parse (over written words). To compute the metric, we
project the ground-truth parse tree to the speech domain by forced alignment,
align the projected ground-truth constituents with the predicted ones under
certain structured constraints, and calculate the average IOU score across all
aligned constituent pairs. STRUCT-IOU takes word boundaries into account and
overcomes the challenge that the predicted words and ground truth may not have
perfect one-to-one correspondence. Extending to the evaluation of text
constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to
syntactically plausible parses than PARSEVAL (Black et al., 1991).
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13435" title="Abstract">arXiv:2402.13435</a> [<a href="/pdf/2402.13435" title="Download PDF">pdf</a>, <a href="/format/2402.13435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Retrieve for Job Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Juan%2C+Y">Yuchin Juan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaobo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+W">Wen Pu</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+S">Sriram Vasudevan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qingquan Song</a>, 
<a href="/search/cs?searchtype=author&query=Borisyuk%2C+F">Fedor Borisyuk</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+K+Q">Kay Qianqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haichao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yunxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chiou%2C+Y+S">Yeou S. Chiou</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+S">Sicong Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Ben Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Muchen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gharghabi%2C+S">Shaghayegh Gharghabi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Huichao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hewlett%2C+D">Daniel Hewlett</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+L">Luke Simon</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Liangjie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjing Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Web-scale search systems typically tackle the scalability challenge with a
two-step paradigm: retrieval and ranking. The retrieval step, also known as
candidate selection, often involves extracting standardized entities, creating
an inverted index, and performing term matching for retrieval. Such traditional
methods require manual and time-consuming development of query models. In this
paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns
job search and recommendation systems. In the realm of promoted jobs, the key
objective is to improve the quality of applicants, thereby delivering value to
recruiter customers. To achieve this, we leverage confirmed hire data to
construct a graph that evaluates a seeker's qualification for a job, and
utilize learned links for retrieval. Our learned model is easy to explain,
debug, and adjust. On the other hand, the focus for organic jobs is to optimize
seeker engagement. We accomplished this by training embeddings for personalized
retrieval, fortified by a set of rules derived from the categorization of
member feedback. In addition to a solution based on a conventional inverted
index, we developed an on-GPU solution capable of supporting both KNN and term
matching efficiently.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13437" title="Abstract">arXiv:2402.13437</a> [<a href="/pdf/2402.13437" title="Download PDF">pdf</a>, <a href="/format/2402.13437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sketching AI Concepts with Capabilities and Examples: AI Innovation in  the Intensive Care Unit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+N">Nur Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=Zlotnikov%2C+S">Susanna Zlotnikov</a>, 
<a href="/search/cs?searchtype=author&query=Sayar%2C+D">Deniz Sayar</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+J+M">Jeremy M. Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Bukowski%2C+L+A">Leigh A. Bukowski</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+S+S">Sher Shah Amin</a>, 
<a href="/search/cs?searchtype=author&query=Riman%2C+K+A">Kathryn A. Riman</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+B+S">Billie S. Davis</a>, 
<a href="/search/cs?searchtype=author&query=Minturn%2C+J+S">John S. Minturn</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+A+J">Andrew J. King</a>, 
<a href="/search/cs?searchtype=author&query=Ricketts%2C+D">Dan Ricketts</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Lu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+V">Venkatesh Sivaraman</a>, 
<a href="/search/cs?searchtype=author&query=Perer%2C+A">Adam Perer</a>, 
<a href="/search/cs?searchtype=author&query=Preum%2C+S+M">Sarah M. Preum</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+J">James McCann</a>, 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+J">John Zimmerman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Advances in artificial intelligence (AI) have enabled unprecedented
capabilities, yet innovation teams struggle when envisioning AI concepts. Data
science teams think of innovations users do not want, while domain experts
think of innovations that cannot be built. A lack of effective ideation seems
to be a breakdown point. How might multidisciplinary teams identify buildable
and desirable use cases? This paper presents a first hand account of ideating
AI concepts to improve critical care medicine. As a team of data scientists,
clinicians, and HCI researchers, we conducted a series of design workshops to
explore more effective approaches to AI concept ideation and problem
formulation. We detail our process, the challenges we encountered, and
practices and artifacts that proved effective. We discuss the research
implications for improved collaboration and stakeholder engagement, and discuss
the role HCI might play in reducing the high failure rate experienced in AI
innovation.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13440" title="Abstract">arXiv:2402.13440</a> [<a href="/pdf/2402.13440" title="Download PDF">pdf</a>, <a href="/format/2402.13440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and  Probabilistic Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+C">Chitra Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Miao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+N">Naweed Khan</a>, 
<a href="/search/cs?searchtype=author&query=Lenchner%2C+J">Jonathan Lenchner</a>, 
<a href="/search/cs?searchtype=author&query=Amarnath%2C+A">Aporva Amarnath</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+S">Sarathkrishna Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Riegel%2C+R">Ryan Riegel</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+A">Alexander Gray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Multi-agent reinforcement learning (MARL) is well-suited for runtime
decision-making in optimizing the performance of systems where multiple agents
coexist and compete for shared resources. However, applying common deep
learning-based MARL solutions to real-world problems suffers from issues of
interpretability, sample efficiency, partial observability, etc. To address
these challenges, we present an event-driven formulation, where decision-making
is handled by distributed co-operative MARL agents using neuro-symbolic
methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN)
framework serves as a function approximator for the RL, to train a rules-based
policy that is both logical and interpretable by construction. To enable
decision-making under uncertainty and partial observability, we developed a
novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural
Networks (PLNN), which combines the capabilities of logical reasoning with
probabilistic graphical models. In PLNN, the upward/downward inference
strategy, inherited from LNN, is coupled with belief bounds by setting the
activation function for the logical operator associated with each neural
network node to a probability-respecting generalization of the Fr\'echet
inequalities. These PLNN nodes form the unifying element that combines
probabilistic logic and Bayes Nets, permitting inference for variables with
unobserved states. We demonstrate our contributions by addressing key MARL
challenges for power sharing in a system-on-chip application.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13441" title="Abstract">arXiv:2402.13441</a> [<a href="/pdf/2402.13441" title="Download PDF">pdf</a>, <a href="/format/2402.13441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory  Access Prediction Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Neelesh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengmiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Rajgopal Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, HPEC '23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE High Performance Extreme Computing Conference (HPEC),
  2023, pp. 1-7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Deep neural networks (DNNs) have proven to be effective models for accurate
Memory Access Prediction (MAP), a critical task in mitigating memory latency
through data prefetching. However, existing DNN-based MAP models suffer from
the challenges such as significant physical storage space and poor inference
latency, primarily due to their large number of parameters. These limitations
render them impractical for deployment in real-world scenarios. In this paper,
we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to
compress MAP models while maintaining the prediction performance. The PaCKD
approach encompasses three steps: clustering memory access sequences into
distinct partitions involving similar patterns, training large pattern-specific
teacher models for memory access prediction for each partition, and training a
single lightweight student model by distilling the knowledge from the trained
pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and
ResNet models, as they exhibit diverse structures and are widely used for image
classification tasks in order to test their effectiveness in four widely used
graph applications. Compared to the teacher models with 5.406M parameters and
an F1-score of 0.4626, our student models achieve a 552$\times$ model size
compression while maintaining an F1-score of 0.4538 (with a 1.92% performance
drop). Our approach yields an 8.70% higher result compared to student models
trained with standard knowledge distillation and an 8.88% higher result
compared to student models trained without any form of knowledge distillation.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13442" title="Abstract">arXiv:2402.13442</a> [<a href="/pdf/2402.13442" title="Download PDF">pdf</a>, <a href="/format/2402.13442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaldenbrand%2C+P">Peter Schaldenbrand</a>, 
<a href="/search/cs?searchtype=author&query=Parmar%2C+G">Gaurav Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+J">James McCann</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jean Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Prior robot painting and drawing work, such as FRIDA, has focused on
decreasing the sim-to-real gap and expanding input modalities for users, but
the interaction with these systems generally exists only in the input stages.
To support interactive, human-robot collaborative painting, we introduce the
Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by
modifying and engaging with content already painted by a human collaborator. To
improve text-image alignment, FRIDA's major weakness, our system uses
pre-trained text-to-image models; however, pre-trained models in the context of
real-world co-painting do not perform well because they (1) do not understand
the constraints and abilities of the robot and (2) cannot perform co-painting
without making unrealistic edits to the canvas and overwriting content. We
propose a self-supervised fine-tuning procedure that can tackle both issues,
allowing the use of pre-trained state-of-the-art text-image alignment models
with robots to enable co-painting in the physical world. Our open-source
approach, CoFRIDA, creates paintings and drawings that match the input text
prompt more clearly than FRIDA, both from a blank canvas and one with human
created work. More generally, our fine-tuning procedure successfully encodes
the robot's constraints and abilities into a foundation model, showcasing
promising results as an effective method for reducing sim-to-real gaps.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13443" title="Abstract">arXiv:2402.13443</a> [<a href="/pdf/2402.13443" title="Download PDF">pdf</a>, <a href="/format/2402.13443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Mapless Navigation on Uneven Terrains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jardali%2C+H">Hassan Jardali</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M">Mahmoud Ali</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lantao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has 7 pages, 7 figures, 2 tables. It has been accepted for publication at the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We propose a new method for autonomous navigation in uneven terrains by
utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP
local perception model is trained on local ranging observation (pointcloud) to
learn the terrain elevation profile and extract the feasible navigation
subgoals around the robot. Subsequently, a cost function, which prioritizes the
safety of the robot in terms of keeping the robot's roll and pitch angles
bounded within a specified range, is used to select a safety-aware subgoal that
leads the robot to its final destination. The algorithm is designed to run in
real-time and is intensively evaluated in simulation and real world
experiments. The results compellingly demonstrate that our proposed algorithm
consistently navigates uneven terrains with high efficiency and surpasses the
performance of other planners. The code and video can be found here:
https://rb.gy/3ov2r8
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13444" title="Abstract">arXiv:2402.13444</a> [<a href="/pdf/2402.13444" title="Download PDF">pdf</a>, <a href="/format/2402.13444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effectiveness of Graph Contrastive Learning on Mathematical  Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei-Syuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hung-Hsuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">This paper details an empirical investigation into using Graph Contrastive
Learning (GCL) to generate mathematical equation representations, a critical
aspect of Mathematical Information Retrieval (MIR). Our findings reveal that
this simple approach consistently exceeds the performance of the current
leading formula retrieval model, TangentCFT. To support ongoing research and
development in this field, we have made our source code accessible to the
public at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13446" title="Abstract">arXiv:2402.13446</a> [<a href="/pdf/2402.13446" title="Download PDF">pdf</a>, <a href="/format/2402.13446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Data Annotation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Beigi%2C+A">Alimohammad Beigi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Amrita Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bohan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Karami%2C+M">Mansooreh Karami</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Data annotation is the labeling or tagging of raw data with relevant
information, essential for improving the efficacy of machine learning models.
The process, however, is labor-intensive and expensive. The emergence of
advanced Large Language Models (LLMs), exemplified by GPT-4, presents an
unprecedented opportunity to revolutionize and automate the intricate process
of data annotation. While existing surveys have extensively covered LLM
architecture, training, and general applications, this paper uniquely focuses
on their specific utility for data annotation. This survey contributes to three
core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations,
and Learning with LLM-generated annotations. Furthermore, the paper includes an
in-depth taxonomy of methodologies employing LLMs for data annotation, a
comprehensive review of learning strategies for models incorporating
LLM-generated annotations, and a detailed discussion on primary challenges and
limitations associated with using LLMs for data annotation. As a key guide,
this survey aims to direct researchers and practitioners in exploring the
potential of the latest LLMs for data annotation, fostering future advancements
in this critical domain. We provide a comprehensive papers list at
\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13448" title="Abstract">arXiv:2402.13448</a> [<a href="/pdf/2402.13448" title="Download PDF">pdf</a>, <a href="/format/2402.13448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liwen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Abhineet Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Kornblith%2C+A">Aaron Kornblith</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13449" title="Abstract">arXiv:2402.13449</a> [<a href="/pdf/2402.13449" title="Download PDF">pdf</a>, <a href="/format/2402.13449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Krotov%2C+D">Dmitry Krotov</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) struggle to handle long input sequences due to
high memory and runtime costs. Memory-augmented models have emerged as a
promising solution to this problem, but current methods are hindered by limited
memory capacity and require costly re-training to integrate with a new LLM. In
this work, we introduce an associative memory module which can be coupled to
any pre-trained (frozen) attention-based LLM without re-training, enabling it
to handle arbitrarily long input sequences. Unlike previous methods, our
associative memory module consolidates representations of individual tokens
into a non-parametric distribution model, dynamically managed by properly
balancing the novelty and recency of the incoming data. By retrieving
information from this consolidated associative memory, the base LLM can achieve
significant (up to 29.7% on Arxiv) perplexity reduction in long-context
modeling compared to other baselines evaluated on standard benchmarks. This
architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced
Long Transformer), demonstrates superior performance even with a tiny context
window of 128 tokens, and also enables improved in-context learning with a much
larger set of demonstrations.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13452" title="Abstract">arXiv:2402.13452</a> [<a href="/pdf/2402.13452" title="Download PDF">pdf</a>, <a href="/format/2402.13452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based  on Twitter Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+V">Vijeta Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minhwa Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gibbons%2C+J+B">Jason Brian Gibbons</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13454" title="Abstract">arXiv:2402.13454</a> [<a href="/pdf/2402.13454" title="Download PDF">pdf</a>, <a href="/format/2402.13454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Analysis of Submodular Information Measures for Targeted  Data Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+N">Nathan Beck</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Truong Pham</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">With increasing volume of data being used across machine learning tasks, the
capability to target specific subsets of data becomes more important. To aid in
this capability, the recently proposed Submodular Mutual Information (SMI) has
been effectively applied across numerous tasks in literature to perform
targeted subset selection with the aid of a exemplar query set. However, all
such works are deficient in providing theoretical guarantees for SMI in terms
of its sensitivity to a subset's relevance and coverage of the targeted data.
For the first time, we provide such guarantees by deriving similarity-based
bounds on quantities related to relevance and coverage of the targeted data.
With these bounds, we show that the SMI functions, which have empirically shown
success in multiple applications, are theoretically sound in achieving good
query relevance and query coverage.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13457" title="Abstract">arXiv:2402.13457</a> [<a href="/pdf/2402.13457" title="Download PDF">pdf</a>, <a href="/format/2402.13457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gelei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuekang Li</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMS) have increasingly become central to generating
content with potential societal impacts. Notably, these models have
demonstrated capabilities for generating content that could be deemed harmful.
To mitigate these risks, researchers have adopted safety training techniques to
align model outputs with societal values to curb the generation of malicious
content. However, the phenomenon of "jailbreaking", where carefully crafted
prompts elicit harmful responses from models, persists as a significant
challenge. This research conducts a comprehensive analysis of existing studies
on jailbreaking LLMs and their defense techniques. We meticulously investigate
nine attack techniques and seven defense techniques applied across three
distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate
the effectiveness of these attack and defense techniques. Our findings reveal
that existing white-box attacks underperform compared to universal techniques
and that including special tokens in the input significantly affects the
likelihood of successful attacks. This research highlights the need to
concentrate on the security facets of LLMs. Additionally, we contribute to the
field by releasing our datasets and testing framework, aiming to foster further
research into LLM security. We believe these contributions will facilitate the
exploration of security measures within this domain.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13459" title="Abstract">arXiv:2402.13459</a> [<a href="/pdf/2402.13459" title="Download PDF">pdf</a>, <a href="/format/2402.13459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Poison Large Language Models During Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiang%2C+Y">Yao Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiangyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zade%2C+S+Z">Saleh Zare Zade</a>, 
<a href="/search/cs?searchtype=author&query=Roshani%2C+M+A">Mohammad Amin Roshani</a>, 
<a href="/search/cs?searchtype=author&query=Zytko%2C+D">Douglas Zytko</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dongxiao Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has marked significant
achievements in language processing and reasoning capabilities. Despite their
advancements, LLMs face vulnerabilities to data poisoning attacks, where
adversaries insert backdoor triggers into training data to manipulate outputs
for malicious purposes. This work further identifies additional security risks
in LLMs by designing a new data poisoning attack tailored to exploit the
instruction tuning process. We propose a novel gradient-guided backdoor trigger
learning approach to identify adversarial triggers efficiently, ensuring an
evasion of detection by conventional defenses while maintaining content
integrity. Through experimental validation across various LLMs and tasks, our
strategy demonstrates a high success rate in compromising model outputs;
poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance
Drop Rate (PDR) of around 80\%. Our work highlights the need for stronger
defenses against data poisoning attack, offering insights into safeguarding
LLMs against these more sophisticated attacks. The source code can be found on
this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13462" title="Abstract">arXiv:2402.13462</a> [<a href="/pdf/2402.13462" title="Download PDF">pdf</a>, <a href="/format/2402.13462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Potential and Challenges of Model Editing for Social Debiasing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Futing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yafu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) trained on vast corpora suffer from inevitable
stereotype biases. Mitigating these biases with fine-tuning could be both
costly and data-hungry. Model editing methods, which focus on modifying LLMs in
a post-hoc manner, are of great potential to address debiasing. However, it
lacks a comprehensive study that facilitates both internal and external model
editing methods, supports various bias types, as well as understands the pros
and cons of applying editing methods to stereotypical debiasing. To mitigate
this gap, we carefully formulate social debiasing into an editing problem and
benchmark seven existing model editing algorithms on stereotypical debiasing,
i.e., debias editing. Our findings in three scenarios reveal both the potential
and challenges of debias editing: (1) Existing model editing methods can
effectively preserve knowledge and mitigate biases, while the generalization of
debias effect from edited sentences to semantically equivalent sentences is
limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et
al. 2022b), while internal editing methods degenerate with the number of edits.
(3) Model editing algorithms achieve generalization towards unseen biases both
within the same type and from different types. In light of these findings, we
further propose two simple but effective methods to improve debias editing, and
experimentally show the effectiveness of the proposed methods.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13463" title="Abstract">arXiv:2402.13463</a> [<a href="/pdf/2402.13463" title="Download PDF">pdf</a>, <a href="/format/2402.13463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The application scope of large language models (LLMs) is increasingly
expanding. In practical use, users might provide feedback based on the model's
output, hoping for a responsive model that can complete responses according to
their feedback. Whether the model can appropriately respond to users' refuting
feedback and consistently follow through with execution has not been thoroughly
analyzed. In light of this, this paper proposes a comprehensive benchmark,
RefuteBench, covering tasks such as question answering, machine translation,
and email writing. The evaluation aims to assess whether models can positively
accept feedback in form of refuting instructions and whether they can
consistently adhere to user demands throughout the conversation. We conduct
evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit
inclination to their internal knowledge, often failing to comply with user
feedback. Additionally, as the length of the conversation increases, models
gradually forget the user's stated feedback and roll back to their own
responses. We further propose a recall-and-repeat prompts as a simple and
effective way to enhance the model's responsiveness to feedback.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13464" title="Abstract">arXiv:2402.13464</a> [<a href="/pdf/2402.13464" title="Download PDF">pdf</a>, <a href="/format/2402.13464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Why Clinicians Deviate from Standards of Care: Liberating  Patients from Mechanical Ventilation in the ICU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+N">Nur Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=Zlotnikov%2C+S">Susanna Zlotnikov</a>, 
<a href="/search/cs?searchtype=author&query=Venkat%2C+A">Aradhana Venkat</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+G">Gursimran Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jennifer Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bukowski%2C+L+A">Leigh A. Bukowski</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+J+M">Jeremy M. Kahn</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+J">James McCann</a>, 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+J">John Zimmerman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Clinical practice guidelines, care pathways, and protocols are designed to
support evidence-based practices for clinicians; however, their adoption
remains a challenge. We set out to investigate why clinicians deviate from the
``Wake Up and Breathe'' protocol, an evidence-based guideline for liberating
patients from mechanical ventilation in the intensive care unit (ICU). We
conducted over 40 hours of direct observations of live clinical workflows, 17
interviews with frontline care providers, and 4 co-design workshops at three
different medical intensive care units. Our findings indicate that unlike prior
literature suggests, disagreement with the protocol is not a substantial
barrier to adoption. Instead, the uncertainty surrounding the application of
the protocol for individual patients leads clinicians to deprioritize adoption
in favor of tasks where they have high certainty. Reflecting on these insights,
we identify opportunities for technical systems to help clinicians in
effectively executing the protocol and discuss future directions for HCI
research to support the integration of protocols into clinical practice in
complex, team-based healthcare settings.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13465" title="Abstract">arXiv:2402.13465</a> [<a href="/pdf/2402.13465" title="Download PDF">pdf</a>, <a href="/format/2402.13465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised learning based object detection using Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+C">Chandan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Herrera-Gerena%2C+J">Jansel Herrera-Gerena</a>, 
<a href="/search/cs?searchtype=author&query=Just%2C+J">John Just</a>, 
<a href="/search/cs?searchtype=author&query=Darr%2C+M">Matthew Darr</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Training image-based object detectors presents formidable challenges, as it
entails not only the complexities of object detection but also the added
intricacies of precisely localizing objects within potentially diverse and
noisy environments. However, the collection of imagery itself can often be
straightforward; for instance, cameras mounted in vehicles can effortlessly
capture vast amounts of data in various real-world scenarios. In light of this,
we introduce a groundbreaking method for training single-stage object detectors
through unsupervised/self-supervised learning.
<br />Our state-of-the-art approach has the potential to revolutionize the labeling
process, substantially reducing the time and cost associated with manual
annotation. Furthermore, it paves the way for previously unattainable research
opportunities, particularly for large, diverse, and challenging datasets
lacking extensive labels.
<br />In contrast to prevalent unsupervised learning methods that primarily target
classification tasks, our approach takes on the unique challenge of object
detection. We pioneer the concept of intra-image contrastive learning alongside
inter-image counterparts, enabling the acquisition of crucial location
information essential for object detection. The method adeptly learns and
represents this location information, yielding informative heatmaps. Our
results showcase an outstanding accuracy of \textbf{89.2\%}, marking a
significant breakthrough of approximately \textbf{15x} over random
initialization in the realm of unsupervised object detection within the field
of computer vision.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13466" title="Abstract">arXiv:2402.13466</a> [<a href="/pdf/2402.13466" title="Download PDF">pdf</a>, <a href="/format/2402.13466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Demonstrator-perceived Precision for Safe Interactive  Imitation Learning of Clearance-limited Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hanbit Oh</a>, 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+T">Takamitsu Matsubara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, accepted by IEEE Robotics and Automation Letters (RA-L) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Interactive imitation learning is an efficient, model-free method through
which a robot can learn a task by repetitively iterating an execution of a
learning policy and a data collection by querying human demonstrations.
However, deploying unmatured policies for clearance-limited tasks, like
industrial insertion, poses significant collision risks. For such tasks, a
robot should detect the collision risks and request intervention by ceding
control to a human when collisions are imminent. The former requires an
accurate model of the environment, a need that significantly limits the scope
of IIL applications. In contrast, humans implicitly demonstrate environmental
precision by adjusting their behavior to avoid collisions when performing
tasks. Inspired by human behavior, this paper presents a novel interactive
learning method that uses demonstrator-perceived precision as a criterion for
human intervention called Demonstrator-perceived Precision-aware Interactive
Imitation Learning (DPIIL). DPIIL captures precision by observing the
speed-accuracy trade-off exhibited in human demonstrations and cedes control to
a human to avoid collisions in states where high precision is estimated. DPIIL
improves the safety of interactive policy learning and ensures efficiency
without explicitly providing precise information of the environment. We
assessed DPIIL's effectiveness through simulations and real-robot experiments
that trained a UR5e 6-DOF robotic arm to perform assembly tasks. Our results
significantly improved training safety, and our best performance compared
favorably with other learning methods.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13468" title="Abstract">arXiv:2402.13468</a> [<a href="/pdf/2402.13468" title="Download PDF">pdf</a>, <a href="/format/2402.13468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STENCIL: Submodular Mutual Information Based Weak Supervision for  Cold-Start Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+N">Nathan Beck</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+A">Adithya Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">As supervised fine-tuning of pre-trained models within NLP applications
increases in popularity, larger corpora of annotated data are required,
especially with increasing parameter counts in large language models. Active
learning, which attempts to mine and annotate unlabeled instances to improve
model performance maximally fast, is a common choice for reducing the
annotation cost; however, most methods typically ignore class imbalance and
either assume access to initial annotated data or require multiple rounds of
active learning selection before improving rare classes. We present STENCIL,
which utilizes a set of text exemplars and the recently proposed submodular
mutual information to select a set of weakly labeled rare-class instances that
are then strongly labeled by an annotator. We show that STENCIL improves
overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on
multiple text classification datasets over common active learning methods
within the class-imbalanced cold-start setting.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13470" title="Abstract">arXiv:2402.13470</a> [<a href="/pdf/2402.13470" title="Download PDF">pdf</a>, <a href="/format/2402.13470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brokman%2C+A">Aviv Brokman</a>, 
<a href="/search/cs?searchtype=author&query=Kavuluru%2C+R">Ramakanth Kavuluru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Cutting edge techniques developed in the general NLP domain are often
subsequently applied to the high-value, data-rich biomedical domain. The past
few years have seen generative language models (LMs), instruction finetuning,
and few-shot learning become foci of NLP research. As such, generative LMs
pretrained on biomedical corpora have proliferated and biomedical instruction
finetuning has been attempted as well, all with the hope that domain
specificity improves performance on downstream tasks. Given the nontrivial
effort in training such models, we investigate what, if any, benefits they have
in the key biomedical NLP task of relation extraction. Specifically, we address
two questions: (1) Do LMs trained on biomedical corpora outperform those
trained on general domain corpora? (2) Do models instruction finetuned on
biomedical datasets outperform those finetuned on assorted datasets or those
simply pretrained? We tackle these questions using existing LMs, testing across
four datasets. In a surprising result, general-domain models typically
outperformed biomedical-domain models. However, biomedical instruction
finetuning improved performance to a similar degree as general instruction
finetuning, despite having orders of magnitude fewer instructions. Our findings
suggest it may be more fruitful to focus research effort on larger-scale
biomedical instruction finetuning of general LMs over building domain-specific
biomedical LMs
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13473" title="Abstract">arXiv:2402.13473</a> [<a href="/pdf/2402.13473" title="Download PDF">pdf</a>, <a href="/format/2402.13473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Highly Dynamic Behaviors for Quadrupedal Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+J">Jiapeng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Learning highly dynamic behaviors for robots has been a longstanding
challenge. Traditional approaches have demonstrated robust locomotion, but the
exhibited behaviors lack diversity and agility. They employ approximate models,
which lead to compromises in performance. Data-driven approaches have been
shown to reproduce agile behaviors of animals, but typically have not been able
to learn highly dynamic behaviors. In this paper, we propose a learning-based
approach to enable robots to learn highly dynamic behaviors from animal motion
data. The learned controller is deployed on a quadrupedal robot and the results
show that the controller is able to reproduce highly dynamic behaviors
including sprinting, jumping and sharp turning. Various behaviors can be
activated through human interaction using a stick with markers attached to it.
Based on the motion pattern of the stick, the robot exhibits walking, running,
sitting and jumping, much like the way humans interact with a pet.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13475" title="Abstract">arXiv:2402.13475</a> [<a href="/pdf/2402.13475" title="Download PDF">pdf</a>, <a href="/format/2402.13475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal  Learning for Glaucoma Forecasting from Irregular Time Series Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xikai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuchen Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N+L">Ning Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P">Pheng-Ann Heng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Glaucoma is one of the major eye diseases that leads to progressive optic
nerve fiber damage and irreversible blindness, afflicting millions of
individuals. Glaucoma forecast is a good solution to early screening and
intervention of potential patients, which is helpful to prevent further
deterioration of the disease. It leverages a series of historical fundus images
of an eye and forecasts the likelihood of glaucoma occurrence in the future.
However, the irregular sampling nature and the imbalanced class distribution
are two challenges in the development of disease forecasting approaches. To
this end, we introduce the Multi-scale Spatio-temporal Transformer Network
(MST-former) based on the transformer architecture tailored for sequential
image inputs, which can effectively learn representative semantic information
from sequential images on both temporal and spatial dimensions. Specifically,
we employ a multi-scale structure to extract features at various resolutions,
which can largely exploit rich spatial information encoded in each image.
Besides, we design a time distance matrix to scale time attention in a
non-linear manner, which could effectively deal with the irregularly sampled
data. Furthermore, we introduce a temperature-controlled Balanced Softmax
Cross-entropy loss to address the class imbalance issue. Extensive experiments
on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset
demonstrate the superiority of the proposed MST-former method, achieving an AUC
of 98.6% for glaucoma forecasting. Besides, our method shows excellent
generalization capability on the Alzheimer's Disease Neuroimaging Initiative
(ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and
Alzheimer's disease prediction, outperforming the compared method by a large
margin.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13481" title="Abstract">arXiv:2402.13481</a> [<a href="/pdf/2402.13481" title="Download PDF">pdf</a>, <a href="/format/2402.13481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Model Diverse Driving Behaviors in Highly Interactive  Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weiwei%2C+L">Liu Weiwei</a>, 
<a href="/search/cs?searchtype=author&query=Wenxuan%2C+H">Hu Wenxuan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lanxin%2C+L">Lei Lanxin</a>, 
<a href="/search/cs?searchtype=author&query=Lingping%2C+G">Gao Lingping</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+L">Liu Yong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL)
have shown impressive results in many driving scenarios. However, the
performance of these trained policies can be impacted when faced with diverse
driving styles and personalities, particularly in highly interactive
situations. This is because conventional MARL algorithms usually operate under
the assumption of fully cooperative behavior among all agents and focus on
maximizing team rewards during training. To address this issue, we introduce
the Personality Modeling Network (PeMN), which includes a cooperation value
function and personality parameters to model the varied interactions in
high-interactive scenarios. The PeMN also enables the training of a background
traffic flow with diverse behaviors, thereby improving the performance and
generalization of the ego vehicle. Our extensive experimental studies, which
incorporate different personality parameters in high-interactive driving
scenarios, demonstrate that the personality parameters effectively model
diverse driving styles and that policies trained with PeMN demonstrate better
generalization compared to traditional MARL methods.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13482" title="Abstract">arXiv:2402.13482</a> [<a href="/pdf/2402.13482" title="Download PDF">pdf</a>, <a href="/format/2402.13482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minju Seo</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jinheon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite large successes of recent language models on diverse tasks, they
suffer from severe performance degeneration in low-resource settings with
limited training data available. Many existing works tackle this problem by
generating synthetic data from the training data and then training models on
them, recently using Large Language Models (LLMs). However, in low-resource
settings, the amount of seed data samples to use for data augmentation is very
small, which makes generated samples suboptimal and less diverse. To tackle
this challenge, we propose a novel method that augments training data by
incorporating a wealth of examples from other datasets, along with the given
training data. Specifically, we first retrieve the relevant instances from
other datasets, such as their input-output pairs or contexts, based on their
similarities with the given seed data, and then prompt LLMs to generate new
samples with the contextual information within and across the original and
retrieved samples. This approach can ensure that the generated data is not only
relevant but also more diverse than what could be achieved using the limited
seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation
(RADA) framework on multiple datasets under low-resource settings of training
and test-time data augmentation scenarios, on which it outperforms existing
LLM-powered data augmentation baselines.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13485" title="Abstract">arXiv:2402.13485</a> [<a href="/pdf/2402.13485" title="Download PDF">pdf</a>, <a href="/format/2402.13485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shuzhang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zebin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ru Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advancements in generative large language models (LLMs) have
significantly boosted the performance in natural language processing tasks.
However, their efficiency is hampered by the inherent limitations in
autoregressive token generation. While parallel decoding with token tree
verification, e.g., Medusa, has been proposed to improve decoding parallelism
and efficiency, it often struggles with maintaining contextual relationships
due to its independent token prediction approach and incurs significant
verification overhead, especially with large tree sizes and batch processing.
In this paper, we propose ProPD, an efficient LLM parallel decoding framework
based on dynamic token tree pruning and generation. ProPD features an advanced
early pruning mechanism to efficiently eliminate unpromising token sequences to
improve verification efficiency. Additionally, it introduces a dynamic token
tree generation algorithm to balance the computation and parallelism of the
verification phase in real-time and maximize the overall efficiency across
different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across
a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD
consistently outperforms existing decoding algorithms by 1.1-3.2x.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13487" title="Abstract">arXiv:2402.13487</a> [<a href="/pdf/2402.13487" title="Download PDF">pdf</a>, <a href="/format/2402.13487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms
have been extensively studied in the literature. In this work, we focus on
reward poisoning attacks and find most existing attacks can be easily detected
by our proposed detection method based on the test of homogeneity, due to their
aggressive nature in reward manipulations. This motivates us to study the
notion of stealthy attack against stochastic MABs and investigate the resulting
attackability. Our analysis shows that against two popularly employed MAB
algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack
depends on the environmental conditions and the realized reward of the arm
pulled in the first round. We also analyze the situation for general MAB
algorithms equipped with our attack detection method and find that it is
possible to have a stealthy attack that almost always succeeds. This brings new
insights into the security risks of MAB algorithms.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13488" title="Abstract">arXiv:2402.13488</a> [<a href="/pdf/2402.13488" title="Download PDF">pdf</a>, <a href="/format/2402.13488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Feature Matching Method Based on Multi-Level Refinement Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaxing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Feature matching is a fundamental and crucial process in visual SLAM, and
precision has always been a challenging issue in feature matching. In this
paper, based on a multi-level fine matching strategy, we propose a new feature
matching method called KTGP-ORB. This method utilizes the similarity of local
appearance in the Hamming space generated by feature descriptors to establish
initial correspondences. It combines the constraint of local image motion
smoothness, uses the GMS algorithm to enhance the accuracy of initial matches,
and finally employs the PROSAC algorithm to optimize matches, achieving precise
matching based on global grayscale information in Euclidean space. Experimental
results demonstrate that the KTGP-ORB method reduces the error by an average of
29.92% compared to the ORB algorithm in complex scenes with illumination
variations and blur.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13490" title="Abstract">arXiv:2402.13490</a> [<a href="/pdf/2402.13490" title="Download PDF">pdf</a>, <a href="/format/2402.13490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>, 
<a href="/search/cs?searchtype=author&query=De+la+Torre%2C+F">Fernando De la Torre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image diffusion models have achieved remarkable performance in image
synthesis, while the text interface does not always provide fine-grained
control over certain image factors. For instance, changing a single token in
the text can have unintended effects on the image. This paper shows a simple
modification of classifier-free guidance can help disentangle image factors in
text-to-image models. The key idea of our method, Contrastive Guidance, is to
characterize an intended factor with two prompts that differ in minimal tokens:
the positive prompt describes the image to be synthesized, and the baseline
prompt serves as a "baseline" that disentangles other factors. Contrastive
Guidance is a general method we illustrate whose benefits in three scenarios:
(1) to guide domain-specific diffusion models trained on an object class, (2)
to gain continuous, rig-like controls for text-to-image generation, and (3) to
improve the performance of zero-shot image editors.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13492" title="Abstract">arXiv:2402.13492</a> [<a href="/pdf/2402.13492" title="Download PDF">pdf</a>, <a href="/format/2402.13492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maekawa%2C+S">Seiji Maekawa</a>, 
<a href="/search/cs?searchtype=author&query=Iso%2C+H">Hayate Iso</a>, 
<a href="/search/cs?searchtype=author&query=Gurajada%2C+S">Sairam Gurajada</a>, 
<a href="/search/cs?searchtype=author&query=Bhutani%2C+N">Nikita Bhutani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LMs) demonstrate remarkable performance, they
encounter challenges in providing accurate responses when queried for
information beyond their pre-trained memorization. Although augmenting them
with relevant external information can mitigate these issues, failure to
consider the necessity of retrieval may adversely affect overall performance.
Previous research has primarily focused on examining how entities influence
retrieval models and knowledge recall in LMs, leaving other aspects relatively
unexplored. In this work, our goal is to offer a more detailed, fact-centric
analysis by exploring the effects of combinations of entities and relations. To
facilitate this, we construct a new question answering (QA) dataset called
WiTQA (Wikipedia Triple Question Answers). This dataset includes questions
about entities and relations of various popularity levels, each accompanied by
a supporting passage. Our extensive experiments with diverse LMs and retrievers
reveal when retrieval does not consistently enhance LMs from the viewpoints of
fact-centric popularity.Confirming earlier findings, we observe that larger LMs
excel in recalling popular facts. However, they notably encounter difficulty
with infrequent entity-relation pairs compared to retrievers. Interestingly,
they can effectively retain popular relations of less common entities. We
demonstrate the efficacy of our finer-grained metric and insights through an
adaptive retrieval system that selectively employs retrieval and recall based
on the frequencies of entities and relations in the question.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13494" title="Abstract">arXiv:2402.13494</a> [<a href="/pdf/2402.13494" title="Download PDF">pdf</a>, <a href="/format/2402.13494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yueqi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Minghong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+R">Renjie Pi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N">Neil Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) face threats from unsafe prompts. Existing
methods for detecting unsafe prompts are primarily online moderation APIs or
finetuned LLMs. These strategies, however, often require extensive and
resource-intensive data collection and training processes. In this study, we
propose GradSafe, which effectively detects unsafe prompts by scrutinizing the
gradients of safety-critical parameters in LLMs. Our methodology is grounded in
a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired
with compliance response exhibit similar patterns on certain safety-critical
parameters. In contrast, safe prompts lead to markedly different gradient
patterns. Building on this observation, GradSafe analyzes the gradients from
prompts (paired with compliance responses) to accurately detect unsafe prompts.
We show that GradSafe, applied to Llama-2 without further training, outperforms
Llama Guard, despite its extensive finetuning with a large dataset, in
detecting unsafe prompts. This superior performance is consistent across both
zero-shot and adaptation scenarios, as evidenced by our evaluations on the
ToxicChat and XSTest. The source code is available at
https://github.com/xyq7/GradSafe.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13495" title="Abstract">arXiv:2402.13495</a> [<a href="/pdf/2402.13495" title="Download PDF">pdf</a>, <a href="/format/2402.13495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards  Improving User Interest Diversity Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minghua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuzhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yiwei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R">Rashidul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender systems (RSs) have gained widespread applications across various
domains owing to the superior ability to capture users' interests. However, the
complexity and nuanced nature of users' interests, which span a wide range of
diversity, pose a significant challenge in delivering fair recommendations. In
practice, user preferences vary significantly; some users show a clear
preference toward certain item categories, while others have a broad interest
in diverse ones. Even though it is expected that all users should receive
high-quality recommendations, the effectiveness of RSs in catering to this
disparate interest diversity remains under-explored.
<br />In this work, we investigate whether users with varied levels of interest
diversity are treated fairly. Our empirical experiments reveal an inherent
disparity: users with broader interests often receive lower-quality
recommendations. To mitigate this, we propose a multi-interest framework that
uses multiple (virtual) interest embeddings rather than single ones to
represent users. Specifically, the framework consists of stacked multi-interest
representation layers, which include an interest embedding generator that
derives virtual interests from shared parameters, and a center embedding
aggregator that facilitates multi-hop aggregation. Experiments demonstrate the
effectiveness of the framework in achieving better trade-off between fairness
and utility across various datasets and backbones.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13496" title="Abstract">arXiv:2402.13496</a> [<a href="/pdf/2402.13496" title="Download PDF">pdf</a>, <a href="/format/2402.13496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HetTree: Heterogeneous Tree Graph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+M">Mingyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Stokes%2C+J+W">Jack W. Stokes</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qinlong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fuchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+P">Purvanshi Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesoo Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The recent past has seen an increasing interest in Heterogeneous Graph Neural
Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from
citation graphs to email graphs. However, existing methods ignore a tree
hierarchy among metapaths, which is naturally constituted by different node
types and relation types. In this paper, we present HetTree, a novel
heterogeneous tree graph neural network that models both the graph structure
and heterogeneous aspects in a scalable and effective manner. Specifically,
HetTree builds a semantic tree data structure to capture the hierarchy among
metapaths. Existing tree encoding techniques aggregate children nodes by
weighting the contribution of children nodes based on similarity to the parent
node. However, we find that this tree encoding fails to capture the entire
parent-children hierarchy by only considering the parent node. Hence, HetTree
uses a novel subtree attention mechanism to emphasize metapaths that are more
helpful in encoding parent-children relationships. Moreover, instead of
separating feature learning from label learning or treating features and labels
equally by projecting them to the same latent space, HetTree proposes to match
them carefully based on corresponding metapaths, which provides more accurate
and richer information between node features and labels. Our evaluation of
HetTree on a variety of real-world datasets demonstrates that it outperforms
all existing baselines on open benchmarks and efficiently scales to large
real-world graphs with millions of nodes and edges.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13497" title="Abstract">arXiv:2402.13497</a> [<a href="/pdf/2402.13497" title="Download PDF">pdf</a>, <a href="/format/2402.13497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Push Quantization-Aware Training Toward Full Precision Performances via  Consistency Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Junbiao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianyang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Baochang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiaqi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Ye Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing Quantization-Aware Training (QAT) methods intensively depend on the
complete labeled dataset or knowledge distillation to guarantee the
performances toward Full Precision (FP) accuracies. However, empirical results
show that QAT still has inferior results compared to its FP counterpart. One
question is how to push QAT toward or even surpass FP performances. In this
paper, we address this issue from a new perspective by injecting the vicinal
data distribution information to improve the generalization performances of QAT
effectively. We present a simple, novel, yet powerful method introducing an
Consistency Regularization (CR) for QAT. Concretely, CR assumes that augmented
samples should be consistent in the latent feature space. Our method
generalizes well to different network architectures and various QAT methods.
Extensive experiments demonstrate that our approach significantly outperforms
the current state-of-the-art QAT methods and even FP counterparts.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13498" title="Abstract">arXiv:2402.13498</a> [<a href="/pdf/2402.13498" title="Download PDF">pdf</a>, <a href="/format/2402.13498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Lay Person&#x27;s Guide to Biomedicine: Orchestrating Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zheheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated lay summarisation (LS) aims to simplify complex technical documents
into a more accessible format to non-experts. Existing approaches using
pre-trained language models, possibly augmented with external background
knowledge, tend to struggle with effective simplification and explanation.
Moreover, automated methods that can effectively assess the `layness' of
generated summaries are lacking. Recently, large language models (LLMs) have
demonstrated a remarkable capacity for text simplification, background
information generation, and text evaluation. This has motivated our systematic
exploration into using LLMs to generate and evaluate lay summaries of
biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS
framework, which leverages LLMs to generate high-quality background knowledge
to improve supervised LS. We also evaluate the performance of LLMs for
zero-shot LS and propose two novel LLM-based LS evaluation metrics, which
assess layness from multiple perspectives. Finally, we conduct a human
assessment of generated lay summaries. Our experiments reveal that
LLM-generated background information can support improved supervised LS.
Furthermore, our novel zero-shot LS evaluation metric demonstrates a high
degree of alignment with human preferences. We conclude that LLMs have an
important part to play in improving both the performance and evaluation of LS
methods.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13499" title="Abstract">arXiv:2402.13499</a> [<a href="/pdf/2402.13499" title="Download PDF">pdf</a>, <a href="/format/2402.13499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Dissecting the Nvidia Hopper GPU Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weile Luo</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Ruibo Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zeyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+D">Dayou Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaowen Chu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Graphics processing units (GPUs) are continually evolving to cater to the
computational demands of contemporary general-purpose workloads, particularly
those driven by artificial intelligence (AI) utilizing deep learning
techniques. A substantial body of studies have been dedicated to dissecting the
microarchitectural metrics characterizing diverse GPU generations, which helps
researchers understand the hardware details and leverage them to optimize the
GPU programs. However, the latest Hopper GPUs present a set of novel
attributes, including new tensor cores supporting FP8, DPX, and distributed
shared memory. Their details still remain mysterious in terms of performance
and operational characteristics. In this research, we propose an extensive
benchmarking study focused on the Hopper GPU. The objective is to unveil its
microarchitectural intricacies through an examination of the new
instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new
CUDA APIs. Our approach involves two main aspects. Firstly, we conduct
conventional latency and throughput comparison benchmarks across the three most
recent GPU architectures, namely Hopper, Ada, and Ampere. Secondly, we delve
into a comprehensive discussion and benchmarking of the latest Hopper features,
encompassing the Hopper DPX dynamic programming (DP) instruction set,
distributed shared memory, and the availability of FP8 tensor cores. The
microbenchmarking results we present offer a deeper understanding of the novel
GPU AI function units and programming features introduced by the Hopper
architecture. This newfound understanding is expected to greatly facilitate
software optimization and modeling efforts for GPU architectures. To the best
of our knowledge, this study makes the first attempt to demystify the tensor
core performance and programming instruction sets unique to Hopper GPUs.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13500" title="Abstract">arXiv:2402.13500</a> [<a href="/pdf/2402.13500" title="Download PDF">pdf</a>, <a href="/format/2402.13500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Translation For Optimal Recall: Tailoring LLM Personalization  With User Profiles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravichandran%2C+K">Karthik Ravichandran</a>, 
<a href="/search/cs?searchtype=author&query=Gomasta%2C+S+S">Sarmistha Sarna Gomasta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is just an initial idea and it's implementation. The results are computed for the first 100 data points. Detailed results will be published with the actual paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper explores a novel technique for improving recall in cross-language
information retrieval (CLIR) systems using iterative query refinement grounded
in the user's lexical-semantic space. The proposed methodology combines
multi-level translation, semantic embedding-based expansion, and user
profile-centered augmentation to address the challenge of matching variance
between user queries and relevant documents. Through an initial BM25 retrieval,
translation into intermediate languages, embedding lookup of similar terms, and
iterative re-ranking, the technique aims to expand the scope of potentially
relevant results personalized to the individual user. Comparative experiments
on news and Twitter datasets demonstrate superior performance over baseline
BM25 ranking for the proposed approach across ROUGE metrics. The translation
methodology also showed maintained semantic accuracy through the multi-step
process. This personalized CLIR framework paves the path for improved
context-aware retrieval attentive to the nuances of user language.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13503" title="Abstract">arXiv:2402.13503</a> [<a href="/pdf/2402.13503" title="Download PDF">pdf</a>, <a href="/ps/2402.13503" title="Download PostScript">ps</a>, <a href="/format/2402.13503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple-Error-Correcting Codes for Analog Computing on Resistive  Crossbars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hengjia Wei</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+R+M">Ron M. Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Error-correcting codes over the real field are studied which can locate
outlying computational errors when performing approximate computing of real
vector--matrix multiplication on resistive crossbars. Prior work has
concentrated on locating a single outlying error and, in this work, several
classes of codes are presented which can handle multiple errors. It is first
shown that one of the known constructions, which is based on spherical codes,
can in fact handle multiple outlying errors. A second family of codes is then
presented with $\zeroone$~parity-check matrices which are sparse and disjunct;
such matrices have been used in other applications as well, especially in
combinatorial group testing. In addition, a certain class of the codes that are
obtained through this construction is shown to be efficiently decodable. As
part of the study of sparse disjunct matrices, this work also contains improved
lower and upper bounds on the maximum Hamming weight of the rows in such
matrices.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13505" title="Abstract">arXiv:2402.13505</a> [<a href="/pdf/2402.13505" title="Download PDF">pdf</a>, <a href="/format/2402.13505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed  Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chaoqun Du</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yizeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent advancements in semi-supervised learning have focused on a more
realistic yet challenging task: addressing imbalances in labeled data while the
class distribution of unlabeled data remains both unknown and potentially
mismatched. Current approaches in this sphere often presuppose rigid
assumptions regarding the class distribution of unlabeled data, thereby
limiting the adaptability of models to only certain distribution ranges. In
this study, we propose a novel approach, introducing a highly adaptable
framework, designated as SimPro, which does not rely on any predefined
assumptions about the distribution of unlabeled data. Our framework, grounded
in a probabilistic model, innovatively refines the expectation-maximization
(EM) algorithm by explicitly decoupling the modeling of conditional and
marginal class distributions. This separation facilitates a closed-form
solution for class distribution estimation during the maximization phase,
leading to the formulation of a Bayes classifier. The Bayes classifier, in
turn, enhances the quality of pseudo-labels in the expectation phase.
Remarkably, the SimPro framework not only comes with theoretical guarantees but
also is straightforward to implement. Moreover, we introduce two novel class
distributions broadening the scope of the evaluation. Our method showcases
consistent state-of-the-art performance across diverse benchmarks and data
distribution scenarios. Our code is available at
https://github.com/LeapLabTHU/SimPro.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13506" title="Abstract">arXiv:2402.13506</a> [<a href="/pdf/2402.13506" title="Download PDF">pdf</a>, <a href="/format/2402.13506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient Verification of Constant-Time Cryptographic  Implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Luwei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+F">Fu Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Taolue Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM FSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Timing side-channel attacks exploit secret-dependent execution time to fully
or partially recover secrets of cryptographic implementations, posing a severe
threat to software security. Constant-time programming discipline is an
effective software-based countermeasure against timing side-channel attacks,
but developing constant-time implementations turns out to be challenging and
error-prone. Current verification approaches/tools suffer from scalability and
precision issues when applied to production software in practice. In this
paper, we put forward practical verification approaches based on a novel
synergy of taint analysis and safety verification of self-composed programs.
Specifically, we first use an IFDS-based lightweight taint analysis to prove
that a large number of potential (timing) side-channel sources do not actually
leak secrets. We then resort to a precise taint analysis and a safety
verification approach to determine whether the remaining potential side-channel
sources can actually leak secrets. These include novel constructions of
taint-directed semi-cross-product of the original program and its Boolean
abstraction, and a taint-directed self-composition of the program. Our approach
is implemented as a cross-platform and fully automated tool CT-Prover. The
experiments confirm its efficiency and effectiveness in verifying real-world
benchmarks from modern cryptographic and SSL/TLS libraries. In particular,
CT-Prover identify new, confirmed vulnerabilities of open-source SSL libraries
(e.g., Mbed SSL, BearSSL) and significantly outperforms the state-of-the-art
tools.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13510" title="Abstract">arXiv:2402.13510</a> [<a href="/pdf/2402.13510" title="Download PDF">pdf</a>, <a href="/format/2402.13510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural  Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhentao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yukun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bruce%2C+N">Neil Bruce</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Minglun Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The widespread adoption of implicit neural representations, especially Neural
Radiance Fields (NeRF), highlights a growing need for editing capabilities in
implicit 3D models, essential for tasks like scene post-processing and 3D
content creation. Despite previous efforts in NeRF editing, challenges remain
due to limitations in editing flexibility and quality. The key issue is
developing a neural representation that supports local edits for real-time
updates. Current NeRF editing methods, offering pixel-level adjustments or
detailed geometry and color modifications, are mostly limited to static scenes.
This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level
editing in dynamic settings, specifically targeting the D-NeRF network. It
allows for consistent edits across sequences by mapping editing actions to a
specific timeframe, freezing the deformation network responsible for dynamic
scene representation, and using a teacher-student approach to integrate
changes.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13512" title="Abstract">arXiv:2402.13512</a> [<a href="/pdf/2402.13512" title="Download PDF">pdf</a>, <a href="/format/2402.13512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ildiz%2C+M+E">M. Emrullah Ildiz</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yixiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingcong Li</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit Singh Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Oymak%2C+S">Samet Oymak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Modern language models rely on the transformer architecture and attention
mechanism to perform language understanding and text generation. In this work,
we study learning a 1-layer self-attention model from a set of prompts and
associated output data sampled from the model. We first establish a precise
mapping between the self-attention mechanism and Markov models: Inputting a
prompt to the model samples the output token according to a context-conditioned
Markov chain (CCMC) which weights the transition matrix of a base Markov chain.
Additionally, incorporating positional encoding results in position-dependent
scaling of the transition probabilities. Building on this formalism, we develop
identifiability/coverage conditions for the prompt distribution that guarantee
consistent estimation and establish sample complexity guarantees under IID
samples. Finally, we study the problem of learning from a single output
trajectory generated from an initial prompt. We characterize an intriguing
winner-takes-all phenomenon where the generative process implemented by
self-attention collapses into sampling a limited subset of tokens due to its
non-mixing nature. This provides a mathematical explanation to the tendency of
modern LLMs to generate repetitive text. In summary, the equivalence to CCMC
provides a simple but powerful framework to study self-attention and its
properties.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13513" title="Abstract">arXiv:2402.13513</a> [<a href="/pdf/2402.13513" title="Download PDF">pdf</a>, <a href="/format/2402.13513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged  Accelerators from LLVM-IR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brumar%2C+I">Iulian Brumar</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+R">Rodrigo Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Bernat%2C+A">Alex Bernat</a>, 
<a href="/search/cs?searchtype=author&query=Tripathy%2C+D">Devashree Tripathy</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D">David Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gu-Yeon Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Designing accelerators for resource- and power-constrained applications is a
daunting task. High-level Synthesis (HLS) addresses these constraints through
resource sharing, an optimization at the HLS binding stage that maps multiple
operations to the same functional unit.
<br />However, resource sharing is often limited to reusing instructions within a
basic block. Instead of searching globally for the best control and dataflow
graphs (CDFGs) to combine, it is constrained by existing instruction mappings
and schedules.
<br />Coarse-grained function merging (CGFM) at the intermediate representation
(IR) level can reuse control and dataflow patterns without dealing with the
post-scheduling complexity of mapping operations onto functional units, wires,
and registers. The merged functions produced by CGFM can be translated to RTL
by HLS, yielding Coarse Grained Merged Accelerators (CGMAs). CGMAs are
especially profitable across applications with similar data- and control-flow
patterns. Prior work has used CGFM to generate CGMAs without regard for which
CGFM algorithms best optimize area, power, and energy costs.
<br />We propose Guac, an energy-aware and SSA-based (static single assignment)
CGMA generation methodology. Guac implements a novel ensemble of cost models
for efficient CGMA generation. We also show that CGFM algorithms using SSA form
to merge control- and dataflow graphs outperform prior non-SSA CGFM designs. We
demonstrate significant area, power, and energy savings with respect to the
state of the art. In particular, Guac more than doubles energy savings with
respect to the closest related work while using a strong resource-sharing
baseline.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13514" title="Abstract">arXiv:2402.13514</a> [<a href="/pdf/2402.13514" title="Download PDF">pdf</a>, <a href="/format/2402.13514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Baohang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cunxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanhua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huimin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieve-then-read and generate-then-read are two typical solutions to handle
unknown and known questions in open-domain question-answering, while the former
retrieves necessary external knowledge and the later prompt the large language
models to generate internal known knowledge encoded in the parameters. However,
few of previous works consider the compositional unknown questions, which
consist of several known or unknown sub-questions. Thus, simple binary
classification (known or unknown) becomes sub-optimal and inefficient since it
will call external retrieval excessively for each compositional unknown
question. To this end, we propose the first Compositional unknown
Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer
(Self-DC) framework to empower LLMs to adaptively call different methods
on-demand, resulting in better performance and efficiency. Experimental results
on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve
comparable or even better performance with much more less retrieval times
compared with several strong baselines.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13516" title="Abstract">arXiv:2402.13516</a> [<a href="/pdf/2402.13516" title="Download PDF">pdf</a>, <a href="/format/2402.13516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chenyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guangli Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, it has been proven a
promising paradigm to boost model inference efficiency. Nevertheless, most
large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces an effective sparsification method named "ProSparse" to push
LLMs for higher activation sparsity without decreasing model performance.
Specifically, after substituting the activation function of LLMs with ReLU,
ProSparse adopts progressive sparsity regularization with a factor smoothly
increasing along sine curves in multiple stages. This can enhance activation
sparsity and alleviate performance degradation by avoiding radical shifts in
activation distribution. With ProSparse, we obtain high sparsity of 89.32% and
88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable
performance to their original Swish-activated versions. Our inference
acceleration experiments further demonstrate the practical acceleration brought
by higher activation sparsity.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13517" title="Abstract">arXiv:2402.13517</a> [<a href="/pdf/2402.13517" title="Download PDF">pdf</a>, <a href="/format/2402.13517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yung%2C+C">Canaan Yung</a>, 
<a href="/search/cs?searchtype=author&query=Dolatabadi%2C+H+M">Hadi Mohaghegh Dolatabadi</a>, 
<a href="/search/cs?searchtype=author&query=Erfani%2C+S">Sarah Erfani</a>, 
<a href="/search/cs?searchtype=author&query=Leckie%2C+C">Christopher Leckie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are susceptible to social-engineered attacks
that are human-interpretable but require a high level of comprehension for LLMs
to counteract. Existing defensive measures can only mitigate less than half of
these attacks at most. To address this issue, we propose the Round Trip
Translation (RTT) method, the first algorithm specifically designed to defend
against social-engineered attacks on LLMs. RTT paraphrases the adversarial
prompt and generalizes the idea conveyed, making it easier for LLMs to detect
induced harmful behavior. This method is versatile, lightweight, and
transferrable to different LLMs. Our defense successfully mitigated over 70% of
Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the
most effective defense to the best of our knowledge. We are also the first to
attempt mitigating the MathsAttack and reduced its attack success rate by
almost 40%. Our code is publicly available at
https://github.com/Cancanxxx/Round_Trip_Translation_Defence
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13518" title="Abstract">arXiv:2402.13518</a> [<a href="/pdf/2402.13518" title="Download PDF">pdf</a>, <a href="/format/2402.13518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RITFIS: Robust input testing framework for LLMs-based intelligent  software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mingxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shunhui Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengcheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The dependence of Natural Language Processing (NLP) intelligent software on
Large Language Models (LLMs) is increasingly prominent, underscoring the
necessity for robustness testing. Current testing methods focus solely on the
robustness of LLM-based software to prompts. Given the complexity and diversity
of real-world inputs, studying the robustness of LLMbased software in handling
comprehensive inputs (including prompts and examples) is crucial for a thorough
understanding of its performance.
<br />To this end, this paper introduces RITFIS, a Robust Input Testing Framework
for LLM-based Intelligent Software. To our knowledge, RITFIS is the first
framework designed to assess the robustness of LLM-based intelligent software
against natural language inputs. This framework, based on given threat models
and prompts, primarily defines the testing process as a combinatorial
optimization problem. Successful test cases are determined by a goal function,
creating a transformation space for the original examples through perturbation
means, and employing a series of search methods to filter cases that meet both
the testing objectives and language constraints. RITFIS, with its modular
design, offers a comprehensive method for evaluating the robustness of LLMbased
intelligent software.
<br />RITFIS adapts 17 automated testing methods, originally designed for Deep
Neural Network (DNN)-based intelligent software, to the LLM-based software
testing scenario. It demonstrates the effectiveness of RITFIS in evaluating
LLM-based intelligent software through empirical validation. However, existing
methods generally have limitations, especially when dealing with lengthy texts
and structurally complex threat models. Therefore, we conducted a comprehensive
analysis based on five metrics and provided insightful testing method
optimization strategies, benefiting both researchers and everyday users.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13521" title="Abstract">arXiv:2402.13521</a> [<a href="/pdf/2402.13521" title="Download PDF">pdf</a>, <a href="/format/2402.13521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Driven Development for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathews%2C+N+S">Noble Saji Mathews</a>, 
<a href="/search/cs?searchtype=author&query=Nagappan%2C+M">Meiyappan Nagappan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) like GPT4, have shown proficiency in generating
code snippets from problem statements. Traditionally software development by
humans followed a similar methodology of writing code from problem statements
or requirements. However, in the past, there have been several studies that
have shown the value of test-driven development (TDD) where humans write tests
based on problem statements before the code for the functionality is written.
In the context of LLM-based code generation, one obvious benefit of TDD is that
the developer then knows for sure if the generated code has passed all the
given tests or not. Therefore, in this paper, we want to empirically evaluate
the hypothesis: giving the problem statements and tests as input to GPT4 is
better than just giving the problem statement as input. To test our hypothesis,
we build a framework TGen. In our experiments on the MBPP, HumanEval and
CodeChef datasets, we consistently find that including tests solves more
programming problems than not including them. Thus we show that TDD is a better
development model than just using a problem statement when using GPT4 for code
generation tasks.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13522" title="Abstract">arXiv:2402.13522</a> [<a href="/pdf/2402.13522" title="Download PDF">pdf</a>, <a href="/format/2402.13522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecMind: Japanese Movie Recommendation Dialogue with Seeker&#x27;s Internal  State
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kodama%2C+T">Takashi Kodama</a>, 
<a href="/search/cs?searchtype=author&query=Kiyomaru%2C+H">Hirokazu Kiyomaru</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y+J">Yin Jou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans pay careful attention to the interlocutor's internal state in
dialogues. For example, in recommendation dialogues, we make recommendations
while estimating the seeker's internal state, such as his/her level of
knowledge and interest. Since there are no existing annotated resources for the
analysis, we constructed RecMind, a Japanese movie recommendation dialogue
dataset with annotations of the seeker's internal state at the entity level.
Each entity has a subjective label annotated by the seeker and an objective
label annotated by the recommender. RecMind also features engaging dialogues
with long seeker's utterances, enabling a detailed analysis of the seeker's
internal state. Our analysis based on RecMind reveals that entities that the
seeker has no knowledge about but has an interest in contribute to
recommendation success. We also propose a response generation framework that
explicitly considers the seeker's internal state, utilizing the
chain-of-thought prompting. The human evaluation results show that our proposed
method outperforms the baseline method in both consistency and the success of
recommendations.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13524" title="Abstract">arXiv:2402.13524</a> [<a href="/pdf/2402.13524" title="Download PDF">pdf</a>, <a href="/format/2402.13524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Meng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liner Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Cunliang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Erhong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Modern large language models (LLMs) should generally benefit individuals from
various cultural backgrounds around the world. However, most recent advanced
generative evaluation benchmarks tailed for LLMs mainly focus on English. To
this end, we introduce OMGEval, the first Open-source Multilingual Generative
test set that can assess the capability of LLMs in different languages. For
each language, OMGEval provides 804 open-ended questions, covering a wide range
of important capabilities of LLMs, such as general knowledge, logical
reasoning, and so on. Each question is rigorously verified by human annotators.
Notably, to sufficiently reflect the compatibility of LLMs in different
cultural backgrounds, we perform localization for each non-English language.
Specifically, the current version of OMGEval includes 5 languages (i.e., Zh,
Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to
automatically score different model outputs, which is shown closely related to
human evaluation. We evaluate several representative multilingual LLMs on the
proposed OMGEval, which we believe will provide a valuable reference for the
community to further understand and improve the multilingual capability of
LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13525" title="Abstract">arXiv:2402.13525</a> [<a href="/pdf/2402.13525" title="Download PDF">pdf</a>, <a href="/format/2402.13525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via  Automating Deep Neural Network Porting for Mobile Deployment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hongtao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaojun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Recent years have seen the explosion of edge intelligence with powerful Deep
Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud
servers and subsequently porting them to mobile devices after being
lightweight. Conventional approaches manually specialized DNNs for various edge
platforms and retrain them with real-world data. However, as the number of
platforms increases, these approaches become labour-intensive and
computationally prohibitive. Additionally, real-world data tends to be
sparse-label, further increasing the difficulty of lightweight models. In this
paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices.
Specifically, we simultaneously optimise a large network family using both
labelled and unlabelled data and then automatically search for tailored
networks for different hardware platforms. MatchNAS acts as an intermediary
that bridges the gap between cloud-based DNNs and edge-based DNNs.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13528" title="Abstract">arXiv:2402.13528</a> [<a href="/pdf/2402.13528" title="Download PDF">pdf</a>, <a href="/format/2402.13528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrastructure Ombudsman: Mining Future Failure Concerns from Structural  Disaster Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+M+T+A">Md Towhidul Absar Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Soumyajit Datta</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Naveen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R">Ashiqur R. KhudaBukhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Current research concentrates on studying discussions on social media related
to structural failures to improve disaster response strategies. However,
detecting social web posts discussing concerns about anticipatory failures is
under-explored. If such concerns are channeled to the appropriate authorities,
it can aid in the prevention and mitigation of potential infrastructural
failures. In this paper, we develop an infrastructure ombudsman -- that
automatically detects specific infrastructure concerns. Our work considers
several recent structural failures in the US. We present a first-of-its-kind
dataset of 2,662 social web instances for this novel task mined from Reddit and
YouTube.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13529" title="Abstract">arXiv:2402.13529</a> [<a href="/pdf/2402.13529" title="Download PDF">pdf</a>, <a href="/ps/2402.13529" title="Download PostScript">ps</a>, <a href="/format/2402.13529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitier Service Migration Framework Based on Mobility Prediction in  Mobile Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Run Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weizhe Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Wireless Communications and Mobile Computing, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Mobile edge computing (MEC) pushes computing resources to the edge of the
network and distributes them at the edge of the mobile network. Offloading
computing tasks to the edge instead of the cloud can reduce computing latency
and backhaul load simultaneously. However, new challenges incurred by user
mobility and limited coverage of MEC server service arise. Services should be
dynamically migrated between multiple MEC servers to maintain service
performance due to user movement. Tackling this problem is nontrivial because
it is arduous to predict user movement, and service migration will generate
service interruptions and redundant network traffic. Service interruption time
must be minimized, and redundant network traffic should be reduced to ensure
service quality. In this paper, the container lives migration technology based
on prediction is studied, and an online prediction method based on map data
that does not rely on prior knowledge such as user trajectories is proposed to
address this challenge in terms of mobility prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13531" title="Abstract">arXiv:2402.13531</a> [<a href="/pdf/2402.13531" title="Download PDF">pdf</a>, <a href="/format/2402.13531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Gradient Descent for Linear Regression: Tighter Error Bounds and  Instance-Specific Uncertainty Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+G">Gavin Brown</a>, 
<a href="/search/cs?searchtype=author&query=Dvijotham%2C+K">Krishnamurthy Dvijotham</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+G">Georgina Evans</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Daogao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+A">Adam Smith</a>, 
<a href="/search/cs?searchtype=author&query=Thakurta%2C+A">Abhradeep Thakurta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We provide an improved analysis of standard differentially private gradient
descent for linear regression under the squared error loss. Under modest
assumptions on the input, we characterize the distribution of the iterate at
each time step.
<br />Our analysis leads to new results on the algorithm's accuracy: for a proper
fixed choice of hyperparameters, the sample complexity depends only linearly on
the dimension of the data. This matches the dimension-dependence of the
(non-private) ordinary least squares estimator as well as that of recent
private algorithms that rely on sophisticated adaptive gradient-clipping
schemes (Varshney et al., 2022; Liu et al., 2023).
<br />Our analysis of the iterates' distribution also allows us to construct
confidence intervals for the empirical optimizer which adapt automatically to
the variance of the algorithm on a particular data set. We validate our
theorems through experiments on synthetic data.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13532" title="Abstract">arXiv:2402.13532</a> [<a href="/pdf/2402.13532" title="Download PDF">pdf</a>, <a href="/format/2402.13532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Q">Quanyu Long</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yue Deng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">LeiLei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S+J">Sinno Jialin Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dense retrievers and retrieval-augmented language models have been widely
used in various NLP applications. Despite being designed to deliver reliable
and secure outcomes, the vulnerability of retrievers to potential attacks
remains unclear, raising concerns about their security. In this paper, we
introduce a novel scenario where the attackers aim to covertly disseminate
targeted misinformation, such as hate speech or advertisement, through a
retrieval system. To achieve this, we propose a perilous backdoor attack
triggered by grammar errors in dense passage retrieval. Our approach ensures
that attacked models can function normally for standard queries but are
manipulated to return passages specified by the attacker when users
unintentionally make grammatical mistakes in their queries. Extensive
experiments demonstrate the effectiveness and stealthiness of our proposed
attack method. When a user query is error-free, our model consistently
retrieves accurate information while effectively filtering out misinformation
from the top-k results. However, when a query contains grammar errors, our
system shows a significantly higher success rate in fetching the targeted
content.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13533" title="Abstract">arXiv:2402.13533</a> [<a href="/pdf/2402.13533" title="Download PDF">pdf</a>, <a href="/format/2402.13533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Weiqing Tong</a>, 
<a href="/search/cs?searchtype=author&query=Walid%2C+A">Anwar Walid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Large language models (LLMs) are computationally intensive. The computation
workload and the memory footprint grow quadratically with the dimension (layer
width). Most of LLMs' parameters come from the linear layers of the transformer
structure and are highly redundant. These linear layers contribute more than
80% of the computation workload and 99% of the model size. To pretrain and
finetune LLMs efficiently, there are three major challenges to address: 1)
reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)
improving GPU utilization when using distributed training. Prior methods, such
as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the
number of trainable parameters and model size, respectively. However, the
resulting model still consumes a large amount of GPU memory. In this paper, we
present high-performance GPU-based methods that exploit low-rank structures to
pretrain and finetune LLMs for financial applications. We replace one
conventional linear layer of the transformer structure with two narrower linear
layers, which allows us to reduce the number of parameters by several orders of
magnitude. By quantizing the parameters into low precision (8-bit and 4-bit),
the memory consumption of the resulting model is further reduced. Compared with
existing LLMs, our methods achieve a speedup of 1.3X and a model compression
ratio of 2.64X for pretaining without accuracy drop. For finetuning, our
methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks
and financial tasks, respectively, and GPU memory consumption ratio of 6.3X.
The sizes of our models are smaller than 0.59 GB, allowing inference on a
smartphone.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13534" title="Abstract">arXiv:2402.13534</a> [<a href="/pdf/2402.13534" title="Download PDF">pdf</a>, <a href="/format/2402.13534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Effective Incorporating Heterogeneous Knowledge Curriculum Learning  for Sequence Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuemei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qi Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sequence labeling models often benefit from incorporating external knowledge.
However, this practice introduces data heterogeneity and complicates the model
with additional modules, leading to increased expenses for training a
high-performing model. To address this challenge, we propose a two-stage
curriculum learning (TCL) framework specifically designed for sequence labeling
tasks. The TCL framework enhances training by gradually introducing data
instances from easy to hard, aiming to improve both performance and training
speed. Furthermore, we explore different metrics for assessing the difficulty
levels of sequence labeling tasks. Through extensive experimentation on six
Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we
demonstrate the effectiveness of our model in enhancing the performance of
sequence labeling models. Additionally, our analysis indicates that TCL
accelerates training and alleviates the slow training problem associated with
complex models.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13536" title="Abstract">arXiv:2402.13536</a> [<a href="/pdf/2402.13536" title="Download PDF">pdf</a>, <a href="/format/2402.13536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Limits of Semantic Image Compression at Micro-bits per  Pixel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dotzel%2C+J">Jordan Dotzel</a>, 
<a href="/search/cs?searchtype=author&query=Kotb%2C+B">Bahaa Kotb</a>, 
<a href="/search/cs?searchtype=author&query=Dotzel%2C+J">James Dotzel</a>, 
<a href="/search/cs?searchtype=author&query=Abdelfattah%2C+M">Mohamed Abdelfattah</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiru Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR Tiny Papers 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional methods, such as JPEG, perform image compression by operating on
structural information, such as pixel values or frequency content. These
methods are effective to bitrates around one bit per pixel (bpp) and higher at
standard image sizes. In contrast, text-based semantic compression directly
stores concepts and their relationships using natural language, which has
evolved with humans to efficiently represent these salient concepts. These
methods can operate at extremely low bitrates by disregarding structural
information like location, size, and orientation. In this work, we use GPT-4V
and DALL-E3 from OpenAI to explore the quality-compression frontier for image
compression and identify the limitations of current technology. We push
semantic compression as low as 100 $\mu$bpp (up to $10,000\times$ smaller than
JPEG) by introducing an iterative reflection process to improve the decoded
image. We further hypothesize this 100 $\mu$bpp level represents a soft limit
on semantic compression at standard image resolutions.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13537" title="Abstract">arXiv:2402.13537</a> [<a href="/pdf/2402.13537" title="Download PDF">pdf</a>, <a href="/format/2402.13537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera  Relocalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhendong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, ICRA 2024 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Camera relocalization is pivotal in computer vision, with applications in AR,
drones, robotics, and autonomous driving. It estimates 3D camera position and
orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent
strides use deep learning for direct end-to-end pose estimation. We propose
EffLoc, a novel efficient Vision Transformer for single-image camera
relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and
feed-forward layers boost memory efficiency and inter-channel communication.
Our introduced sequential group attention (SGA) module enhances computational
efficiency by diversifying input features, reducing redundancy, and expanding
model capacity. EffLoc excels in efficiency and accuracy, outperforming prior
methods, such as AtLoc and MapNet. It thrives on large-scale outdoor
car-driving scenario, ensuring simplicity, end-to-end trainability, and
eliminating handcrafted loss functions.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13540" title="Abstract">arXiv:2402.13540</a> [<a href="/pdf/2402.13540" title="Download PDF">pdf</a>, <a href="/format/2402.13540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scientific Impact of novel Instrumentation: the Case of MUSE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+M+M">Martin M. Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure. To appear in AAS Research Notes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM)

</div>
<p class="mathjax">In the process of transforming science cases into a viable and affordable
design for a novel instrument, there is the problem of how to gauge their
scientific impact, especially when they end up in competing top level
requirements that can be incompatible with each other. This research note
presents a case study for scientific impact of the integral field spectrograph
MUSE in terms of number of refereed publications from 2014 to 2024 as a figure
of merit, broken down by different research areas. The analysis is based on the
Basic ESO Publication Statistics service (BEPS) and NASA's Astrophysics Data
System (ADS).
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13542" title="Abstract">arXiv:2402.13542</a> [<a href="/pdf/2402.13542" title="Download PDF">pdf</a>, <a href="/format/2402.13542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13545" title="Abstract">arXiv:2402.13545</a> [<a href="/pdf/2402.13545" title="Download PDF">pdf</a>, <a href="/ps/2402.13545" title="Download PostScript">ps</a>, <a href="/format/2402.13545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Two-Stage Dual-Path Framework for Text Tampering Detection and  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guandong Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenpin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Document tamper detection has always been an important aspect of tamper
detection. Before the advent of deep learning, document tamper detection was
difficult. We have made some explorations in the field of text tamper detection
based on deep learning. Our Ps tamper detection method includes three steps:
feature assistance, audit point positioning, and tamper recognition. It
involves hierarchical filtering and graded output (tampered/suspected
tampered/untampered). By combining artificial tamper data features, we simulate
and augment data samples in various scenarios (cropping with noise
addition/replacement, single character/space replacement, smearing/splicing,
brightness/contrast adjustment, etc.). The auxiliary features include
exif/binary stream keyword retrieval/noise, which are used for branch detection
based on the results. Audit point positioning uses detection frameworks and
controls thresholds for high and low density detection. Tamper recognition
employs a dual-path dual-stream recognition network, with RGB and ELA stream
feature extraction. After dimensionality reduction through self-correlation
percentile pooling, the fused output is processed through vlad, yielding an
accuracy of 0.804, recall of 0.659, and precision of 0.913.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13546" title="Abstract">arXiv:2402.13546</a> [<a href="/pdf/2402.13546" title="Download PDF">pdf</a>, <a href="/format/2402.13546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotain Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Long video understanding is a significant and ongoing challenge in the
intersection of multimedia and artificial intelligence. Employing large
language models (LLMs) for comprehending video becomes an emerging and
promising method. However, this approach incurs high computational costs due to
the extensive array of video tokens, experiences reduced visual clarity as a
consequence of token aggregation, and confronts challenges arising from
irrelevant visual tokens while answering video-related questions. To alleviate
these issues, we present an Interactive Visual Adapter (IVA) within LLMs,
designed to enhance interaction with fine-grained visual elements.
Specifically, we first transform long videos into temporal video tokens via
leveraging a visual encoder alongside a pretrained causal transformer, then
feed them into LLMs with the video instructions. Subsequently, we integrated
IVA, which contains a lightweight temporal frame selector and a spatial feature
interactor, within the internal blocks of LLMs to capture instruction-aware and
fine-grained visual signals. Consequently, the proposed video-LLM facilitates a
comprehensive understanding of long video content through appropriate long
video modeling and precise visual interactions. We conducted extensive
experiments on nine video understanding benchmarks and experimental results
show that our interactive visual adapter significantly improves the performance
of video LLMs on long video QA tasks. Ablation studies further verify the
effectiveness of IVA in long and short video understandings.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13547" title="Abstract">arXiv:2402.13547</a> [<a href="/pdf/2402.13547" title="Download PDF">pdf</a>, <a href="/format/2402.13547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ActiveRAG: Revealing the Treasures of Knowledge via Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhipeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yibin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large
Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks.
However, current RAG models position LLMs as passive knowledge receptors,
thereby restricting their capacity for learning and comprehending external
knowledge. In this paper, we present ActiveRAG, an innovative RAG framework
that shifts from passive knowledge acquisition to an active learning mechanism.
This approach utilizes the Knowledge Construction mechanism to develop a deeper
understanding of external knowledge by associating it with previously acquired
or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism
to incorporate the outcomes from both chains of thought and knowledge
construction, thereby calibrating the intrinsic cognition of LLMs. Our
experimental results demonstrate that ActiveRAG surpasses previous RAG models,
achieving a 5% improvement on question-answering datasets. All data and codes
are available at https://github.com/OpenMatch/ActiveRAG.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13548" title="Abstract">arXiv:2402.13548</a> [<a href="/pdf/2402.13548" title="Download PDF">pdf</a>, <a href="/format/2402.13548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yize Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 23rd Power Systems Computation Conference (PSCC). Code is released at <a href="https://github.com/LSY-Cython/DiffPLF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Due to the vast electric vehicle (EV) penetration to distribution grid,
charging load forecasting is essential to promote charging station operation
and demand-side management.However, the stochastic charging behaviors and
associated exogenous factors render future charging load patterns quite
volatile and hard to predict. Accordingly, we devise a novel Diffusion model
termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can
explicitly approximate the predictive load distribution conditioned on
historical data and related covariates. Specifically, we leverage a denoising
diffusion model, which can progressively convert the Gaussian prior to real
time-series data by learning a reversal of the diffusion process. Besides, we
couple such diffusion model with a cross-attention-based conditioning mechanism
to execute conditional generation for possible charging demand profiles. We
also propose a task-informed fine-tuning technique to better adapt DiffPLF to
the probabilistic time-series forecasting task and acquire more accurate and
reliable predicted intervals. Finally, we conduct multiple experiments to
validate the superiority of DiffPLF to predict complex temporal patterns of
erratic charging load and carry out controllable generation based on certain
covariate. Results demonstrate that we can attain a notable rise of 39.58% and
49.87% on MAE and CRPS respectively compared to the conventional method.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13549" title="Abstract">arXiv:2402.13549</a> [<a href="/pdf/2402.13549" title="Download PDF">pdf</a>, <a href="/ps/2402.13549" title="Download PostScript">ps</a>, <a href="/format/2402.13549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-learning-based Joint Design of Adaptive Modulation and Precoding for  Physical Layer Security in Visible Light Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+M+T">Duc M. T. Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T+V">Thanh V. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+A+T">Anh T. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+T">Chuyen T Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">There has been an increasing interest in physical layer security (PLS),
which, compared with conventional cryptography, offers a unique approach to
guaranteeing information confidentiality against eavesdroppers. In this paper,
we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM)
and precoding, which aims to optimize wiretap visible-light channels' secrecy
capacity and bit error rate (BER) performances. The proposed design is
motivated by higher-order modulation, which results in better secrecy capacity
at the expense of a higher BER. On the other hand, a proper precoding design,
which can manipulate the received signal quality at the legitimate user and the
eavesdropper, can also enhance secrecy performance and influence the BER. A
reward function that considers the secrecy capacity and the BERs of the
legitimate user's (Bob) and the eavesdropper's (Eve) channels is introduced and
maximized. Due to the non-linearity and complexity of the reward function, it
is challenging to solve the optical design using classical optimization
techniques. Therefore, reinforcement learning-based designs using Q-learning
and Deep Q-learning are proposed to maximize the reward function. Simulation
results verify that compared with the baseline designs, the proposed joint
designs achieve better reward values while maintaining the BER of Bob's channel
(Eve's channel) well below (above) the pre-FEC (forward error correction) BER
threshold.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13550" title="Abstract">arXiv:2402.13550</a> [<a href="/pdf/2402.13550" title="Download PDF">pdf</a>, <a href="/format/2402.13550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LLMs Effective Negotiators? Systematic Evaluation of the  Multifaceted Capabilities of LLMs in Negotiation Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+D">Deuksin Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+E">Emily Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Kulshrestha%2C+T">Tara Kulshrestha</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+K">Kushal Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+G+M">Gale M. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Gratch%2C+J">Jonathan Gratch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A successful negotiation demands a deep comprehension of the conversation
context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as
strategic reasoning and effective communication, making it challenging for
automated systems. Given the remarkable performance of LLMs across a variety of
NLP tasks, in this work, we aim to understand how LLMs can advance different
aspects of negotiation research, ranging from designing dialogue systems to
providing pedagogical feedback and scaling up data collection practices. To
this end, we devise a methodology to analyze the multifaceted capabilities of
LLMs across diverse dialogue scenarios covering all the time stages of a
typical negotiation interaction. Our analysis adds to the increasing evidence
for the superiority of GPT-4 across various tasks while also providing insights
into specific tasks that remain difficult for LLMs. For instance, the models
correlate poorly with human players when making subjective assessments about
the negotiation dialogues and often struggle to generate responses that are
contextually appropriate as well as strategically advantageous.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13551" title="Abstract">arXiv:2402.13551</a> [<a href="/pdf/2402.13551" title="Download PDF">pdf</a>, <a href="/format/2402.13551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Representation of Narrative Context: Coherence Dependency via  Retrospective Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangnan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work introduces a novel and practical paradigm for narrative
comprehension, stemming from the observation that individual passages within
narratives are often cohesively related than being isolated. We therefore
propose to formulate a graph upon narratives dubbed NARCO that depicts a
task-agnostic coherence dependency of the entire context. Especially, edges in
NARCO encompass retrospective free-form questions between two context snippets
reflecting high-level coherent relations, inspired by the cognitive perception
of humans who constantly reinstate relevant events from prior context.
Importantly, our graph is instantiated through our designed two-stage LLM
prompting, thereby without reliance on human annotations. We present three
unique studies on its practical utility, examining the edge efficacy via recap
identification, local context augmentation via plot retrieval, and broader
applications exemplified by long document QA. Experiments suggest that our
approaches leveraging NARCO yield performance boost across all three tasks.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13552" title="Abstract">arXiv:2402.13552</a> [<a href="/pdf/2402.13552" title="Download PDF">pdf</a>, <a href="/ps/2402.13552" title="Download PostScript">ps</a>, <a href="/format/2402.13552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confluence of Logically Constrained Rewrite Systems Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6pf%2C+J">Jonas Sch&#xf6;pf</a>, 
<a href="/search/cs?searchtype=author&query=Mitterwallner%2C+F">Fabian Mitterwallner</a>, 
<a href="/search/cs?searchtype=author&query=Middeldorp%2C+A">Aart Middeldorp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 12th International Joint Conference on Automated Reasoning 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We show that (local) confluence of terminating locally constrained rewrite
systems is undecidable, even when the underlying theory is decidable. Several
confluence criteria for logically constrained rewrite systems are known. These
were obtained by replaying existing proofs for plain term rewrite systems in a
constrained setting, involving a non-trivial effort. We present a simple
transformation from logically constrained rewrite systems to term rewrite
systems such that critical pairs of the latter correspond to constrained
critical pairs of the former. The usefulness of the transformation is
illustrated by lifting the advanced confluence results based on (almost)
development closed critical pairs as well as on parallel critical pairs to the
constrained setting.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13553" title="Abstract">arXiv:2402.13553</a> [<a href="/pdf/2402.13553" title="Download PDF">pdf</a>, <a href="/format/2402.13553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Secure Physical Layer Communications: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Changyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+I">Dong In Kim</a>, 
<a href="/search/cs?searchtype=author&query=Xuemin">Xuemin</a> (Sherman)
<a href="/search/cs?searchtype=author&query=Shen">Shen</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22pages, 8figs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Generative Artificial Intelligence (GAI) stands at the forefront of AI
innovation, demonstrating rapid advancement and unparalleled proficiency in
generating diverse content. Beyond content creation, GAI has significant
analytical abilities to learn complex data distribution, offering numerous
opportunities to resolve security issues. In the realm of security from
physical layer perspectives, traditional AI approaches frequently struggle,
primarily due to their limited capacity to dynamically adjust to the evolving
physical attributes of transmission channels and the complexity of contemporary
cyber threats. This adaptability and analytical depth are precisely where GAI
excels. Therefore, in this paper, we offer an extensive survey on the various
applications of GAI in enhancing security within the physical layer of
communication networks. We first emphasize the importance of advanced GAI
models in this area, including Generative Adversarial Networks (GANs),
Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models
(DMs). We delve into the roles of GAI in addressing challenges of physical
layer security, focusing on communication confidentiality, authentication,
availability, resilience, and integrity. Furthermore, we also present future
research directions focusing model improvements, multi-scenario deployment,
resource-efficient optimization, and secure semantic communication,
highlighting the multifaceted potential of GAI to address emerging challenges
in secure physical layer communications and sensing.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13554" title="Abstract">arXiv:2402.13554</a> [<a href="/pdf/2402.13554" title="Download PDF">pdf</a>, <a href="/ps/2402.13554" title="Download PostScript">ps</a>, <a href="/format/2402.13554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secrecy Performance Analysis of Space-to-Ground Optical Satellite  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+V">Thang V. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T+V">Thanh V. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+A+T">Anh T. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Ngoc%2C+D+T">Dang T. Ngoc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Free-space optics (FSO)-based satellite communication systems have recently
received considerable attention due to their enhanced capacity compared to
their radio frequency (RF) counterparts. This paper analyzes the performance of
physical layer security of space-to-ground intensity modulation/direct
detection FSO satellite links under the effect of atmospheric loss,
misalignment, cloud attenuation, and atmospheric turbulence-induced fading.
Specifically, a wiretap channel consisting of a legitimate transmitter Alice
(i.e., the satellite), a legitimate user Bob, and an eavesdropper Eve over
turbulence channels modeled by the Fisher-Snedecor $\mathcal{F}$ distribution
is considered. The secrecy performance in terms of the average secrecy
capacity, secrecy outage probability, and strictly positive secrecy capacity
are derived in closed-form. Simulation results reveal significant impacts of
satellite altitude, zenith angle, and turbulence strength on the secrecy
performance.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13556" title="Abstract">arXiv:2402.13556</a> [<a href="/pdf/2402.13556" title="Download PDF">pdf</a>, <a href="/format/2402.13556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inductive Graph Alignment Prompt: Bridging the Gap between Graph  Pre-training and Inductive Fine-tuning From Spectral Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peiyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Q">Qingqing Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The "Graph pre-training and fine-tuning" paradigm has significantly improved
Graph Neural Networks(GNNs) by capturing general knowledge without manual
annotations for downstream tasks. However, due to the immense gap of data and
tasks between the pre-training and fine-tuning stages, the model performance is
still limited. Inspired by prompt fine-tuning in Natural Language
Processing(NLP), many endeavors have been made to bridge the gap in graph
domain. But existing methods simply reformulate the form of fine-tuning tasks
to the pre-training ones. With the premise that the pre-training graphs are
compatible with the fine-tuning ones, these methods typically operate in
transductive setting. In order to generalize graph pre-training to inductive
scenario where the fine-tuning graphs might significantly differ from
pre-training ones, we propose a novel graph prompt based method called
Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph
pre-training frameworks and analyze the essence of graph pre-training from
graph spectral theory. Then we identify the two sources of the data gap in
inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on
the insight of graph pre-training, we propose to bridge the graph signal gap
and the graph structure gap with learnable prompts in the spectral space. A
theoretical analysis ensures the effectiveness of our method. At last, we
conduct extensive experiments among nodes classification and graph
classification tasks under the transductive, semi-inductive and inductive
settings. The results demonstrate that our proposed method can successfully
bridge the data gap under different settings.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13561" title="Abstract">arXiv:2402.13561</a> [<a href="/pdf/2402.13561" title="Download PDF">pdf</a>, <a href="/format/2402.13561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haoyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working in progress, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Evaluating and Rethinking the current landscape of Large Multimodal Models
(LMMs), we observe that widely-used visual-language projection approaches
(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet
ignore the visual knowledge-dimension alignment, i.e., connecting visuals to
their relevant knowledge. Visual knowledge plays a significant role in
analyzing, inferring, and interpreting information from visuals, helping
improve the accuracy of answers to knowledge-based visual questions. In this
paper, we mainly explore improving LMMs with visual-language knowledge
alignment, especially aimed at challenging knowledge-based visual question
answering (VQA). To this end, we present a Cognitive Visual-Language Mapper
(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a
Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning
stage. Specifically, we design the VKA based on the interaction between a small
language model and a visual encoder, training it on collected image-knowledge
pairs to achieve visual knowledge acquisition and projection. FKA is employed
to distill the fine-grained visual knowledge of an image and inject it into
Large Language Models (LLMs). We conduct extensive experiments on
knowledge-based VQA benchmarks and experimental results show that CVLM
significantly improves the performance of LMMs on knowledge-based VQA (average
gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,
respectively.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13562" title="Abstract">arXiv:2402.13562</a> [<a href="/pdf/2402.13562" title="Download PDF">pdf</a>, <a href="/format/2402.13562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Multi-Source Language Training in Cross-Lingual Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+S+H">Seong Hoon Lim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taejun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinhyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taeuk Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The successful adaptation of multilingual language models (LMs) to a specific
language-task pair critically depends on the availability of data tailored for
that condition. While cross-lingual transfer (XLT) methods have contributed to
addressing this data scarcity problem, there still exists ongoing debate about
the mechanisms behind their effectiveness. In this work, we focus on one of
promising assumptions about inner workings of XLT, that it encourages
multilingual LMs to place greater emphasis on language-agnostic or
task-specific features. We test this hypothesis by examining how the patterns
of XLT change with a varying number of source languages involved in the
process. Our experimental findings show that the use of multiple source
languages in XLT-a technique we term Multi-Source Language Training
(MSLT)-leads to increased mingling of embedding spaces for different languages,
supporting the claim that XLT benefits from making use of language-independent
information. On the other hand, we discover that using an arbitrary combination
of source languages does not always guarantee better performance. We suggest
simple heuristics for identifying effective language combinations for MSLT and
empirically prove its effectiveness.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13565" title="Abstract">arXiv:2402.13565</a> [<a href="/pdf/2402.13565" title="Download PDF">pdf</a>, <a href="/format/2402.13565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the application of subspace migration from scattering matrix with  constant-valued diagonal elements in microwave imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+W">Won-Kwang Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the application of a subspace migration (SM) algorithm to quickly
identify small objects in microwave imaging. In various problems, it is easy to
measure the diagonal elements of the scattering matrix if the location of the
transmitter and the receiver is the same. To address this issue, several
studies have been conducted by setting the diagonal elements to zero. In this
paper, we generalize the imaging problem by setting diagonal elements of the
scattering matrix as a constant with the application of SM. To show the
applicability of SM and its dependence on the constant, we show that the
imaging function of SM can be represented in terms of an infinite series of the
Bessel functions of integer order, antenna number and arrangement, and applied
constant. This result enables us to discover some further properties, including
the unique determination of objects. We also demonstrated simulation results
with synthetic data to support the theoretical result.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13566" title="Abstract">arXiv:2402.13566</a> [<a href="/pdf/2402.13566" title="Download PDF">pdf</a>, <a href="/format/2402.13566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-aware Video Corpus Moment Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+D">Danyang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task
focused on identifying a specific moment within a vast corpus of untrimmed
videos using the natural language query. Existing methods for VCMR typically
rely on frame-aware video retrieval, calculating similarities between the query
and video frames to rank videos based on maximum frame similarity.However, this
approach overlooks the semantic structure embedded within the information
between frames, namely, the event, a crucial element for human comprehension of
videos. Motivated by this, we propose EventFormer, a model that explicitly
utilizes events within videos as fundamental units for video retrieval. The
model extracts event representations through event reasoning and hierarchical
event encoding. The event reasoning module groups consecutive and visually
similar frame representations into events, while the hierarchical event
encoding encodes information at both the frame and event levels. We also
introduce anchor multi-head self-attenion to encourage Transformer to capture
the relevance of adjacent content in the video. The training of EventFormer is
conducted by two-branch contrastive learning and dual optimization for two
sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo
benchmarks show the effectiveness and efficiency of EventFormer in VCMR,
achieving new state-of-the-art results. Additionally, the effectiveness of
EventFormer is also validated on partially relevant video retrieval task.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13567" title="Abstract">arXiv:2402.13567</a> [<a href="/pdf/2402.13567" title="Download PDF">pdf</a>, <a href="/format/2402.13567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spot Check Equivalence: an Interpretable Metric for Information  Elicitation Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shengwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Resnick%2C+P">Paul Resnick</a>, 
<a href="/search/cs?searchtype=author&query=Schoenebeck%2C+G">Grant Schoenebeck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Because high-quality data is like oxygen for AI systems, effectively
eliciting information from crowdsourcing workers has become a first-order
problem for developing high-performance machine learning algorithms. Two
prevalent paradigms, spot-checking and peer prediction, enable the design of
mechanisms to evaluate and incentivize high-quality data from human labelers.
So far, at least three metrics have been proposed to compare the performances
of these techniques [33, 8, 3]. However, different metrics lead to divergent
and even contradictory results in various contexts. In this paper, we harmonize
these divergent stories, showing that two of these metrics are actually the
same within certain contexts and explain the divergence of the third. Moreover,
we unify these different contexts by introducing \textit{Spot Check
Equivalence}, which offers an interpretable metric for the effectiveness of a
peer prediction mechanism. Finally, we present two approaches to compute spot
check equivalence in various contexts, where simulation results verify the
effectiveness of our proposed metric.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13571" title="Abstract">arXiv:2402.13571</a> [<a href="/pdf/2402.13571" title="Download PDF">pdf</a>, <a href="/ps/2402.13571" title="Download PostScript">ps</a>, <a href="/format/2402.13571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Coreference Resolution in Low-resource South Asian  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Ritwik Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Desur%2C+P">Pooja Desur</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Coreference resolution involves the task of identifying text spans within a
discourse that pertain to the same real-world entity. While this task has been
extensively explored in the English language, there has been a notable scarcity
of publicly accessible resources and models for coreference resolution in South
Asian languages. We introduce a Translated dataset for Multilingual Coreference
Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools
for translation and word-alignment. Nearly all of the predicted translations
successfully pass a sanity check, and 75% of English references align with
their predicted translations. Using multilingual encoders, two off-the-shelf
coreference resolution models were trained on a concatenation of TransMuCoRes
and a Hindi coreference resolution dataset with manual annotations. The best
performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,
respectively, on our test-split of Hindi golden set. This study is the first to
evaluate an end-to-end coreference resolution model on a Hindi golden set.
Furthermore, this work underscores the limitations of current coreference
evaluation metrics when applied to datasets with split antecedents, advocating
for the development of more suitable evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13572" title="Abstract">arXiv:2402.13572</a> [<a href="/pdf/2402.13572" title="Download PDF">pdf</a>, <a href="/format/2402.13572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Expressive Power of a Variant of the Looped Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yihang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+M+K">Michael K. Ng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoqiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Besides natural language processing, transformers exhibit extraordinary
performance in solving broader applications, including scientific computing and
computer vision. Previous works try to explain this from the expressive power
and capability perspectives that standard transformers are capable of
performing some algorithms. To empower transformers with algorithmic
capabilities and motivated by the recently proposed looped transformer (Yang et
al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed
Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard
transformer and vanilla looped transformer, the proposed AlgoFormer can achieve
significantly higher expressiveness in algorithm representation when using the
same number of parameters. In particular, inspired by the structure of
human-designed learning algorithms, our transformer block consists of a
pre-transformer that is responsible for task pre-processing, a looped
transformer for iterative optimization algorithms, and a post-transformer for
producing the desired results after post-processing. We provide theoretical
evidence of the expressive power of the AlgoFormer in solving some challenging
problems, mirroring human-designed algorithms. Furthermore, some theoretical
and empirical results are presented to show that the designed transformer has
the potential to be smarter than human-designed algorithms. Experimental
results demonstrate the empirical superiority of the proposed transformer in
that it outperforms the standard transformer and vanilla looped transformer in
some challenging tasks.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13573" title="Abstract">arXiv:2402.13573</a> [<a href="/pdf/2402.13573" title="Download PDF">pdf</a>, <a href="/format/2402.13573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+E">Ethan Smith</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+N">Nayan Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aninda Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Attention mechanism has been crucial for image diffusion models, however,
their quadratic computational complexity limits the sizes of images we can
process within reasonable time and memory constraints. This paper investigates
the importance of dense attention in generative image models, which often
contain redundant features, making them suitable for sparser attention
mechanisms. We propose a novel training-free method ToDo that relies on token
downsampling of key and value tokens to accelerate Stable Diffusion inference
by up to 2x for common sizes and up to 4.5x or more for high resolutions like
2048x2048. We demonstrate that our approach outperforms previous methods in
balancing efficient throughput and fidelity.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13575" title="Abstract">arXiv:2402.13575</a> [<a href="/pdf/2402.13575" title="Download PDF">pdf</a>, <a href="/format/2402.13575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Physical Camouflage Generation Based on a Differential Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Wenyi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenxing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuangju Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinkai Liang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Quan Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study introduces a novel approach to neural rendering, specifically
tailored for adversarial camouflage, within an extensive 3D rendering
framework. Our method, named FPA, goes beyond traditional techniques by
faithfully simulating lighting conditions and material variations, ensuring a
nuanced and realistic representation of textures on a 3D target. To achieve
this, we employ a generative approach that learns adversarial patterns from a
diffusion model. This involves incorporating a specially designed adversarial
loss and covert constraint loss to guarantee the adversarial and covert nature
of the camouflage in the physical world. Furthermore, we showcase the
effectiveness of the proposed camouflage in sticker mode, demonstrating its
ability to cover the target without compromising adversarial information.
Through empirical and physical experiments, FPA exhibits strong performance in
terms of attack success rate and transferability. Additionally, the designed
sticker-mode camouflage, coupled with a concealment constraint, adapts to the
environment, yielding diverse styles of texture. Our findings highlight the
versatility and efficacy of the FPA approach in adversarial camouflage
applications.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13576" title="Abstract">arXiv:2402.13576</a> [<a href="/pdf/2402.13576" title="Download PDF">pdf</a>, <a href="/format/2402.13576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Video Corpus Moment Retrieval with Partial Relevance  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+D">Danyang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at
retrieving a relevant moment from a large corpus of untrimmed videos using a
natural language text as query. The relevance between the video and query is
partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains
information-rich frames, and not all are relevant to the query. Strong
correlation is typically observed only within the relevant moment, emphasizing
the importance of capturing key content. (2) Modality: The relevance of query
to different modalities varies; action descriptions align more with the visual
elements, while character conversations are more related to textual
information. Recognizing and addressing these modality-specific nuances is
crucial for effective retrieval in VCMR. However, existing methods often treat
all video contents equally, leading to sub-optimal moment retrieval. We argue
that effectively capturing the partial relevance between the query and video is
essential for the VCMR task. To this end, we propose a Partial Relevance
Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video
retrieval and moment localization. To align with their distinct objectives, we
implement specialized partial relevance enhancement strategies. For video
retrieval, we introduce a multi-modal collaborative video retriever, generating
distinct query representations tailored for different modalities by
modality-specific pooling, ensuring a more effective match. For moment
localization, we propose the focus-then-fuse moment localizer, utilizing
modality-specific gates to capture essential content, followed by fusing
multi-modal information for moment localization. Experimental results on TVR
and DiDeMo datasets show that the proposed model outperforms the baselines,
achieving a new state-of-the-art of VCMR.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13577" title="Abstract">arXiv:2402.13577</a> [<a href="/pdf/2402.13577" title="Download PDF">pdf</a>, <a href="/format/2402.13577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BBA: Bi-Modal Behavioral Alignment for Reasoning with Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xueliang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tingchen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qintong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal reasoning stands as a pivotal capability for large vision-language
models (LVLMs). The integration with Domain-Specific Languages (DSL), offering
precise visual representations, equips these models with the opportunity to
execute more accurate reasoning in complex and professional domains. However,
the vanilla Chain-of-Thought (CoT) prompting method faces challenges in
effectively leveraging the unique strengths of visual and DSL representations,
primarily due to their differing reasoning mechanisms. Additionally, it often
falls short in addressing critical steps in multi-step reasoning tasks. To
mitigate these challenges, we introduce the \underline{B}i-Modal
\underline{B}ehavioral \underline{A}lignment (BBA) prompting method, designed
to maximize the potential of DSL in augmenting complex multi-modal reasoning
tasks. This method initiates by guiding LVLMs to create separate reasoning
chains for visual and DSL representations. Subsequently, it aligns these chains
by addressing any inconsistencies, thus achieving a cohesive integration of
behaviors from different modalities. Our experiments demonstrate that BBA
substantially improves the performance of GPT-4V(ision) on geometry problem
solving ($28.34\% \to 34.22\%$), chess positional advantage prediction
($42.08\% \to 46.99\%$) and molecular property prediction ($77.47\% \to
83.52\%$).
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13578" title="Abstract">arXiv:2402.13578</a> [<a href="/pdf/2402.13578" title="Download PDF">pdf</a>, <a href="/format/2402.13578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransGOP: Transformer-Based Gaze Object Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Binglu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chenxi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haisheng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nian Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a conference paper containing 7 pages of text and 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Gaze object prediction aims to predict the location and category of the
object that is watched by a human. Previous gaze object prediction works use
CNN-based object detectors to predict the object's location. However, we find
that Transformer-based object detectors can predict more accurate object
location for dense objects in retail scenarios. Moreover, the long-distance
modeling capability of the Transformer can help to build relationships between
the human head and the gaze object, which is important for the GOP task. To
this end, this paper introduces Transformer into the fields of gaze object
prediction and proposes an end-to-end Transformer-based gaze object prediction
method named TransGOP. Specifically, TransGOP uses an off-the-shelf
Transformer-based object detector to detect the location of objects and designs
a Transformer-based gaze autoencoder in the gaze regressor to establish
long-distance gaze relationships. Moreover, to improve gaze heatmap regression,
we propose an object-to-gaze cross-attention mechanism to let the queries of
the gaze autoencoder learn the global-memory position knowledge from the object
detector. Finally, to make the whole framework end-to-end trained, we propose a
Gaze Box loss to jointly optimize the object detector and gaze regressor by
enhancing the gaze heatmap energy in the box of the gaze object. Extensive
experiments on the GOO-Synth and GOO-Real datasets demonstrate that our
TransGOP achieves state-of-the-art performance on all tracks, i.e., object
detection, gaze estimation, and gaze object prediction. Our code will be
available at https://github.com/chenxi-Guo/TransGOP.git.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13579" title="Abstract">arXiv:2402.13579</a> [<a href="/pdf/2402.13579" title="Download PDF">pdf</a>, <a href="/format/2402.13579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Pixel-wise Continuous Depth Representation via Clustering for  Depth Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shenglun%2C+C">Chen Shenglun</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhang Hong</a>, 
<a href="/search/cs?searchtype=author&query=XinZhu%2C+M">Ma XinZhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhihui%2C+W">Wang Zhihui</a>, 
<a href="/search/cs?searchtype=author&query=Haojie%2C+L">Li Haojie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE TCSVT,15 pages,12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Depth completion is a long-standing challenge in computer vision, where
classification-based methods have made tremendous progress in recent years.
However, most existing classification-based methods rely on pre-defined
pixel-shared and discrete depth values as depth categories. This representation
fails to capture the continuous depth values that conform to the real depth
distribution, leading to depth smearing in boundary regions. To address this
issue, we revisit depth completion from the clustering perspective and propose
a novel clustering-based framework called CluDe which focuses on learning the
pixel-wise and continuous depth representation. The key idea of CluDe is to
iteratively update the pixel-shared and discrete depth representation to its
corresponding pixel-wise and continuous counterpart, driven by the real depth
distribution. Specifically, CluDe first utilizes depth value clustering to
learn a set of depth centers as the depth representation. While these depth
centers are pixel-shared and discrete, they are more in line with the real
depth distribution compared to pre-defined depth categories. Then, CluDe
estimates offsets for these depth centers, enabling their dynamic adjustment
along the depth axis of the depth distribution to generate the pixel-wise and
continuous depth representation. Extensive experiments demonstrate that CluDe
successfully reduces depth smearing around object boundaries by utilizing
pixel-wise and continuous depth representation. Furthermore, CluDe achieves
state-of-the-art performance on the VOID datasets and outperforms
classification-based methods on the KITTI dataset.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13582" title="Abstract">arXiv:2402.13582</a> [<a href="/pdf/2402.13582" title="Download PDF">pdf</a>, <a href="/format/2402.13582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mastering the Game of Guandan with Deep Reinforcement Learning and  Behavior Regulating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yanggong%2C+Y">Yifan Yanggong</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Games are a simplified model of reality and often serve as a favored platform
for Artificial Intelligence (AI) research. Much of the research is concerned
with game-playing agents and their decision making processes. The game of
Guandan (literally, "throwing eggs") is a challenging game where even
professional human players struggle to make the right decision at times. In
this paper we propose a framework named GuanZero for AI agents to master this
game using Monte-Carlo methods and deep neural networks. The main contribution
of this paper is about regulating agents' behavior through a carefully designed
neural network encoding scheme. We then demonstrate the effectiveness of the
proposed framework by comparing it with state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13583" title="Abstract">arXiv:2402.13583</a> [<a href="/pdf/2402.13583" title="Download PDF">pdf</a>, <a href="/format/2402.13583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongWanjuan: Towards Systematic Measurement for Long Text Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kai Lv</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The quality of training data are crucial for enhancing the long-text
capabilities of foundation models. Despite existing efforts to refine data
quality through heuristic rules and evaluations based on data diversity and
difficulty, there's a lack of systematic approaches specifically tailored for
assessing long texts. Addressing this gap, our work systematically measures the
quality of long texts by evaluating three fundamental linguistic dimensions:
coherence, cohesion, and complexity. Drawing inspiration from the
aforementioned three dimensions, we introduce a suite of metrics designed to
evaluate the quality of long texts, encompassing both statistical and
pre-trained language model-based ones. Leveraging these metrics, we present
LongWanjuan, a bilingual dataset specifically tailored to enhance the training
of language models for long-text tasks with over 160B tokens. In LongWanjuan,
we categorize long texts into holistic, aggregated, and chaotic types, enabling
a detailed analysis of long-text quality. Furthermore, we devise a data mixture
recipe that strategically balances different types of long texts within
LongWanjuan, leading to significant improvements in model performance on
long-text tasks. The code and dataset are available at
https://github.com/OpenLMLab/LongWanjuan.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13584" title="Abstract">arXiv:2402.13584</a> [<a href="/pdf/2402.13584" title="Download PDF">pdf</a>, <a href="/format/2402.13584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WinoViz: Probing Visual Properties of Objects Under Different States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Woojeong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans perceive and comprehend different visual properties of an object based
on specific contexts. For instance, we know that a banana turns brown ``when it
becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous
studies on probing visual commonsense knowledge have primarily focused on
examining language models' understanding of typical properties (e.g., colors
and shapes) of objects. We present WinoViz, a text-only evaluation dataset,
consisting of 1,380 examples that probe the reasoning abilities of language
models regarding variant visual properties of objects under different contexts
or states. Our task is challenging since it requires pragmatic reasoning
(finding intended meanings) and visual knowledge reasoning. We also present
multi-hop data, a more challenging version of our data, which requires
multi-step reasoning chains to solve our task. In our experimental analysis,
our findings are: a) Large language models such as GPT-4 demonstrate effective
performance, but when it comes to multi-hop data, their performance is
significantly degraded. b) Large models perform well on pragmatic reasoning,
but visual knowledge reasoning is a bottleneck in our task. c) Vision-language
models outperform their language-model counterparts. d) A model with
machine-generated images performs poorly in our task. This is due to the poor
quality of the generated images.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13586" title="Abstract">arXiv:2402.13586</a> [<a href="/pdf/2402.13586" title="Download PDF">pdf</a>, <a href="/format/2402.13586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delay-Aware Semantic Sampling in Power Electronic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gupta%2C+K">Kirti Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Sahoo%2C+S">Subham Sahoo</a>, 
<a href="/search/eess?searchtype=author&query=Panigrahi%2C+B+K">Bijaya Ketan Panigrahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Transactions on Smart Grid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In power electronic systems (PES), attacks on data availability such as
latency attacks, data dropouts, and time-synchronization attacks (TSAs)
continue to pose significant threats to both the communication network and the
control system performance. As per the conventional norms of communication
engineering, PES still rely on time synchronized sampling, which translates
every received message with equal importance. In this paper, we go beyond
event-triggered sampling/estimation to integrate semantic principles into the
sampling process for each distributed energy resource (DER), which not only
compensates for delayed communicated signals by reconstruction of a new signal
from the inner control layer dynamics, but also evaluates the reconstruction
stage using key semantic requirements, namely Freshness, Relevance and Priority
for good dynamic performance. As a result, the sparsity provided by
event-driven sampling of internal control loop dynamics translates as semantics
in PES. The proposed scheme has been extensively tested and validated on a
modified IEEE 37-bus AC distribution system, under many operating conditions
and noisy environment in OPAL-RT environment to establish its robustness,
model-free design ability and adaptive behavior to dynamic cyber graph
topologies.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13587" title="Abstract">arXiv:2402.13587</a> [<a href="/pdf/2402.13587" title="Download PDF">pdf</a>, <a href="/format/2402.13587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wenhan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuxin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we propose a new setting for generating product descriptions
from images, augmented by marketing keywords. It leverages the combined power
of visual and textual information to create descriptions that are more tailored
to the unique features of products. For this setting, previous methods utilize
visual and textual encoders to encode the image and keywords and employ a
language model-based decoder to generate the product description. However, the
generated description is often inaccurate and generic since same-category
products have similar copy-writings, and optimizing the overall framework on
large-scale samples makes models concentrate on common words yet ignore the
product features. To alleviate the issue, we present a simple and effective
Multimodal In-Context Tuning approach, named ModICT, which introduces a similar
product sample as the reference and utilizes the in-context learning capability
of language models to produce the description. During training, we keep the
visual encoder and language model frozen, focusing on optimizing the modules
responsible for creating multimodal in-context references and dynamic prompts.
This approach preserves the language generation prowess of large language
models (LLMs), facilitating a substantial increase in description diversity. To
assess the effectiveness of ModICT across various language model scales and
types, we collect data from three distinct product categories within the
E-commerce domain. Extensive experiments demonstrate that ModICT significantly
improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4%
on D-5) of generated results compared to conventional methods. Our findings
underscore the potential of ModICT as a valuable tool for enhancing automatic
generation of product descriptions in a wide range of applications.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13588" title="Abstract">arXiv:2402.13588</a> [<a href="/pdf/2402.13588" title="Download PDF">pdf</a>, <a href="/format/2402.13588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PI-CoF: A Bilevel Optimization Framework for Solving Active Learning  Problems using Physics-Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+L">Liqiu Dong</a>, 
<a href="/search/eess?searchtype=author&query=Zagorowska%2C+M">Marta Zagorowska</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Durkin%2C+A">Alex Durkin</a>, 
<a href="/search/eess?searchtype=author&query=Mercang%C3%B6z%2C+M">Mehmet Mercang&#xf6;z</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to The 8th IEEE Conference on Control Technology and Applications (CCTA) 2024, 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Physics informed neural networks (PINNs) have recently been proposed as
surrogate models for solving process optimization problems. However, in an
active learning setting collecting enough data for reliably training PINNs
poses a challenge. This study proposes a broadly applicable method for
incorporating physics information into existing machine learning (ML) models of
any type. The proposed method - referred to as PI-CoF for Physics-Informed
Correction Factors - introduces additive or multiplicative correction factors
for pointwise inference, which are identified by solving a regularized
unconstrained optimization problem for reconciliation of physics information
and ML model predictions. When ML models are used in an optimization context,
using the proposed approach translates into a bilevel optimization problem,
where the reconciliation problem is solved as an inner problem each time before
evaluating the objective and constraint functions of the outer problem. The
utility of the proposed approach is demonstrated through a numerical example,
emphasizing constraint satisfaction in a safe Bayesian optimization (BO)
setting. Furthermore, a simulation study is carried out by using PI-CoF for the
real-time optimization of a fuel cell system. The results show reduced fuel
consumption and better reference tracking performance when using the proposed
PI-CoF approach in comparison to a constrained BO algorithm not using physics
information.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13589" title="Abstract">arXiv:2402.13589</a> [<a href="/pdf/2402.13589" title="Download PDF">pdf</a>, <a href="/format/2402.13589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affective Computing for Healthcare: Recent Trends, Applications,  Challenges, and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dapeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhe Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Affective computing, which aims to recognize, interpret, and understand human
emotions, provides benefits in healthcare, such as improving patient care and
enhancing doctor-patient communication. However, there is a noticeable absence
of a comprehensive summary of recent advancements in affective computing for
healthcare, which could pose difficulties for researchers entering this field.
To address this, our paper aims to provide an extensive literature review of
related studies published in the last five years. We begin by analyzing trends,
benefits, and limitations of recent datasets and affective computing methods
devised for healthcare. Subsequently, we highlight several healthcare
application hotspots of current technologies that could be promising for
real-world deployment. Through our analysis, we identify and discuss some
ongoing challenges in the field as evidenced by the literature. Concluding with
a thorough review, we further offer potential future research directions and
hope our findings and insights could guide related researchers to make better
contributions to the evolution of affective computing in healthcare.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13593" title="Abstract">arXiv:2402.13593</a> [<a href="/pdf/2402.13593" title="Download PDF">pdf</a>, <a href="/format/2402.13593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph Enhanced Large Language Model Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) are pivotal in advancing natural language
processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and
outdated knowledge. Model editing emerges as a promising solution to address
these challenges. However, existing editing methods struggle to track and
incorporate changes in knowledge associated with edits, which limits the
generalization ability of postedit LLMs in processing edited knowledge. To
tackle these problems, we propose a novel model editing method that leverages
knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we
first utilize a knowledge graph augmentation module to uncover associated
knowledge that has changed due to editing, obtaining its internal
representations within LLMs. This approach allows knowledge alterations within
LLMs to be reflected through an external graph structure. Subsequently, we
design a graph-based knowledge edit module to integrate structured knowledge
into the model editing. This ensures that the updated parameters reflect not
only the modifications of the edited knowledge but also the changes in other
associated knowledge resulting from the editing process. Comprehensive
experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME
significantly improves the generalization capabilities of post-edit LLMs in
employing edited knowledge.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13597" title="Abstract">arXiv:2402.13597</a> [<a href="/pdf/2402.13597" title="Download PDF">pdf</a>, <a href="/format/2402.13597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Cunhua Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are
capable of improving spectral efficiency by employing far more antennas than
conventional massive MIMO at the base station (BS). However, beam training in
multiuser XL-MIMO systems is challenging. To tackle these issues, we conceive a
three-phase graph neural network (GNN)-based beam training scheme for multiuser
XL-MIMO systems. In the first phase, only far-field wide beams have to be
tested for each user and the GNN is utilized to map the beamforming gain
information of the far-field wide beams to the optimal near-field beam for each
user. In addition, the proposed GNN-based scheme can exploit the
position-correlation between adjacent users for further improvement of the
accuracy of beam training. In the second phase, a beam allocation scheme based
on the probability vectors produced at the outputs of GNNs is proposed to
address the above beam-direction conflicts between users. In the third phase,
the hybrid TBF is designed for further reducing the inter-user interference.
Our simulation results show that the proposed scheme improves the beam training
performance of the benchmarks. Moreover, the performance of the proposed beam
training scheme approaches that of an exhaustive search, despite requiring only
about 7% of the pilot overhead.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13598" title="Abstract">arXiv:2402.13598</a> [<a href="/pdf/2402.13598" title="Download PDF">pdf</a>, <a href="/format/2402.13598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User-LLM: Efficient LLM Contextualization with User Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+L">Lin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Luyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiaxing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+N">Neo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Berlowitz%2C+D">Devora Berlowitz</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+S">Sushant Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+B">Bradley Green</a>, 
<a href="/search/cs?searchtype=author&query=O%27Banion%2C+S">Shawn O&#x27;Banion</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jun Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized natural language processing.
However, effectively incorporating complex and potentially noisy user
interaction data remains a challenge. To address this, we propose User-LLM, a
novel framework that leverages user embeddings to contextualize LLMs. These
embeddings, distilled from diverse user interactions using self-supervised
pretraining, capture latent user preferences and their evolution over time. We
integrate these user embeddings with LLMs through cross-attention and
soft-prompting, enabling LLMs to dynamically adapt to user context. Our
comprehensive experiments on MovieLens, Amazon Review, and Google Local Review
datasets demonstrate significant performance gains across various tasks.
Notably, our approach outperforms text-prompt-based contextualization on long
sequence tasks and tasks that require deep user understanding while being
computationally efficient. We further incorporate Perceiver layers to
streamline the integration between user encoders and LLMs, reducing
computational demands.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13602" title="Abstract">arXiv:2402.13602</a> [<a href="/pdf/2402.13602" title="Download PDF">pdf</a>, <a href="/format/2402.13602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Reasoning Based on Large Language Models for Autonomous Car  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azarafza%2C+M">Mehdi Azarafza</a>, 
<a href="/search/cs?searchtype=author&query=Nayyeri%2C+M">Mojtaba Nayyeri</a>, 
<a href="/search/cs?searchtype=author&query=Steinmetz%2C+C">Charles Steinmetz</a>, 
<a href="/search/cs?searchtype=author&query=Staab%2C+S">Steffen Staab</a>, 
<a href="/search/cs?searchtype=author&query=Rettberg%2C+A">Achim Rettberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, results on GitHub
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have garnered significant attention for their
ability to understand text and images, generate human-like text, and perform
complex reasoning tasks. However, their ability to generalize this advanced
reasoning with a combination of natural language text for decision-making in
dynamic situations requires further exploration. In this study, we investigate
how well LLMs can adapt and apply a combination of arithmetic and common-sense
reasoning, particularly in autonomous driving scenarios. We hypothesize that
LLMs hybrid reasoning abilities can improve autonomous driving by enabling them
to analyze detected object and sensor data, understand driving regulations and
physical laws, and offer additional context. This addresses complex scenarios,
like decisions in low visibility (due to weather conditions), where traditional
methods might fall short. We evaluated Large Language Models (LLMs) based on
accuracy by comparing their answers with human-generated ground truth inside
CARLA. The results showed that when a combination of images (detected objects)
and sensor data is fed into the LLM, it can offer precise information for brake
and throttle control in autonomous vehicles across various weather conditions.
This formulation and answers can assist in decision-making for auto-pilot
systems.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13603" title="Abstract">arXiv:2402.13603</a> [<a href="/pdf/2402.13603" title="Download PDF">pdf</a>, <a href="/format/2402.13603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coding Theorems for Repetition and Superposition Codes over Binary-Input  Output-Symmetric Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper is concerned with a class of low density generator matrix codes
(LDGM), called repetition and superposition (RaS) codes, which have been proved
to be capacity-achieving over binary-input output-symmetric (BIOS) channels in
terms of bit-error rate (BER). We prove with a recently proposed framework that
the RaS codes are also capacity-achieving over BIOS channels in terms of
frame-error rate (FER). With this new framework, the theorem for the RaS codes
can be generalized to source coding and joint source and channel coding (JSCC).
In particular, we prove with this framework that the corresponding low-density
parity-check (LDPC) codes, as an enlarged ensemble of quasi-cyclic LDPC
(QC-LDPC) codes, can also achieve the capacity. To further improve the
iterative decoding performance, we consider the convolutional RaS (Conv-RaS)
code ensemble and prove it to be capacity-achieving over BIOS channels in terms
of the first error event probability. The construction of Conv-RaS codes is
flexible with rate (defined as the ratio of the input length to the encoding
output length) ranging from less than one (typically for channel codes) to
greater than one (typically for source codes), which can be implemented as a
universal JSCC scheme, as confirmed by simulations.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13604" title="Abstract">arXiv:2402.13604</a> [<a href="/pdf/2402.13604" title="Download PDF">pdf</a>, <a href="/format/2402.13604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the HISCO Barrier: Automatic Occupational Standardization with  OccCANINE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahl%2C+C+M">Christian M&#xf8;ller Dahl</a>, 
<a href="/search/cs?searchtype=author&query=Vedel%2C+C">Christian Vedel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> All code and guides on how to use OccCANINE is available on GitHub <a href="https://github.com/christianvedels/OccCANINE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Econometrics (econ.EM)

</div>
<p class="mathjax">This paper introduces a new tool, OccCANINE, to automatically transform
occupational descriptions into the HISCO classification system. The manual work
involved in processing and classifying occupational descriptions is
error-prone, tedious, and time-consuming. We finetune a preexisting language
model (CANINE) to do this automatically thereby performing in seconds and
minutes what previously took days and weeks. The model is trained on 14 million
pairs of occupational descriptions and HISCO codes in 13 different languages
contributed by 22 different sources. Our approach is shown to have accuracy,
recall and precision above 90 percent. Our tool breaks the metaphorical HISCO
barrier and makes this data readily available for analysis of occupational
structures with broad applicability in economics, economic history and various
related disciplines.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13605" title="Abstract">arXiv:2402.13605</a> [<a href="/pdf/2402.13605" title="Download PDF">pdf</a>, <a href="/format/2402.13605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jiyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junghwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seunghyun Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">For Large Language Models (LLMs) to be effectively deployed in a specific
country, they must possess an understanding of the nation's culture and basic
knowledge. To this end, we introduce National Alignment, which measures an
alignment between an LLM and a targeted country from two aspects: social value
alignment and common knowledge alignment. Social value alignment evaluates how
well the model understands nation-specific social values, while common
knowledge alignment examines how well the model captures basic knowledge
related to the nation. We constructed KorNAT, the first benchmark that measures
national alignment with South Korea. For the social value dataset, we obtained
ground truth labels from a large-scale survey involving 6,174 unique Korean
participants. For the common knowledge dataset, we constructed samples based on
Korean textbooks and GED reference materials. KorNAT contains 4K and 6K
multiple-choice questions for social value and common knowledge, respectively.
Our dataset creation process is meticulously designed and based on statistical
sampling theory and was refined through multiple rounds of human review. The
experiment results of seven LLMs reveal that only a few models met our
reference score, indicating a potential for further enhancement. KorNAT has
received government approval after passing an assessment conducted by a
government-affiliated organization dedicated to evaluating dataset quality.
Samples and detailed evaluation protocols of our dataset can be found in
\url{https://selectstar.ai/ko/papers-national-alignment#}
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13606" title="Abstract">arXiv:2402.13606</a> [<a href="/pdf/2402.13606" title="Download PDF">pdf</a>, <a href="/format/2402.13606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zeming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The tendency of Large Language Models to generate hallucinations and exhibit
overconfidence in predictions raises concerns regarding their reliability.
Confidence or uncertainty estimations indicating the extent of trustworthiness
of a model's response are essential to developing reliable AI systems. Current
research primarily focuses on LLM confidence estimations in English, remaining
a void for other widely used languages and impeding the global development of
reliable AI applications. This paper introduces a comprehensive investigation
of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce
an elaborated and expert-checked multilingual QA dataset. Second, we delve into
the performance of confidence estimations and examine how these confidence
scores can enhance LLM performance through self-refinement across diverse
languages. Finally, we propose a cross-lingual confidence estimation method to
achieve more precise confidence scores. The experimental results showcase the
performance of various confidence estimation methods across different languages
as well as present that our proposed cross-lingual confidence estimation
technique significantly enhances confidence estimation and outperforms several
baseline methods.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13607" title="Abstract">arXiv:2402.13607</a> [<a href="/pdf/2402.13607" title="Download PDF">pdf</a>, <a href="/format/2402.13607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CODIS: Benchmarking Context-Dependent Visual Comprehension for  Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zihao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zhaolu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qidong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+X">Xiaoyue Mi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Ning Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal large language models (MLLMs) have demonstrated promising results
in a variety of tasks that combine vision and language. As these models become
more integral to research and applications, conducting comprehensive
evaluations of their capabilities has grown increasingly important. However,
most existing benchmarks fail to consider that, in certain situations, images
need to be interpreted within a broader context. In this work, we introduce a
new benchmark, named as CODIS, designed to assess the ability of models to use
context provided in free-form text to enhance visual comprehension. Our
findings indicate that MLLMs consistently fall short of human performance on
this benchmark. Further analysis confirms that these models struggle to
effectively extract and utilize contextual information to improve their
understanding of images. This underscores the pressing need to enhance the
ability of MLLMs to comprehend visuals in a context-dependent manner. View our
project website at https://thunlp-mt.github.io/CODIS.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13609" title="Abstract">arXiv:2402.13609</a> [<a href="/pdf/2402.13609" title="Download PDF">pdf</a>, <a href="/format/2402.13609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VOOM: Robust Visual Object Odometry and Mapping using Hierarchical  Landmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaoyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 4 tables, conference icra 2024 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In recent years, object-oriented simultaneous localization and mapping (SLAM)
has attracted increasing attention due to its ability to provide high-level
semantic information while maintaining computational efficiency. Some
researchers have attempted to enhance localization accuracy by integrating the
modeled object residuals into bundle adjustment. However, few have demonstrated
better results than feature-based visual SLAM systems, as the generic coarse
object models, such as cuboids or ellipsoids, are less accurate than feature
points. In this paper, we propose a Visual Object Odometry and Mapping
framework VOOM using high-level objects and low-level points as the
hierarchical landmarks in a coarse-to-fine manner instead of directly using
object residuals in bundle adjustment. Firstly, we introduce an improved
observation model and a novel data association method for dual quadrics,
employed to represent physical objects. It facilitates the creation of a 3D map
that closely reflects reality. Next, we use object information to enhance the
data association of feature points and consequently update the map. In the
visual object odometry backend, the updated map is employed to further optimize
the camera pose and the objects. Meanwhile, local bundle adjustment is
performed utilizing the objects and points-based covisibility graphs in our
visual object mapping process. Experiments show that VOOM outperforms both
object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms
of localization. The implementation of our method is available at
https://github.com/yutongwangBIT/VOOM.git.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13610" title="Abstract">arXiv:2402.13610</a> [<a href="/pdf/2402.13610" title="Download PDF">pdf</a>, <a href="/format/2402.13610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Discovery with Large Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Surana%2C+H">Harshit Surana</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+D">Dhruv Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Hazra%2C+S">Sanchaita Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the accumulation of data at an unprecedented rate, its potential to fuel
scientific discovery is growing exponentially. This position paper urges the
Machine Learning (ML) community to exploit the capabilities of large generative
models (LGMs) to develop automated systems for end-to-end data-driven discovery
-- a paradigm encompassing the search and verification of hypotheses purely
from a set of provided datasets, without the need for additional data
collection or physical experiments. We first outline several desiderata for an
ideal data-driven discovery system. Then, through DATAVOYAGER, a
proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of
these desiderata -- a feat previously unattainable -- while also highlighting
important limitations in the current system that open up opportunities for
novel ML research. We contend that achieving accurate, reliable, and robust
end-to-end discovery systems solely through the current capabilities of LGMs is
challenging. We instead advocate for fail-proof tool integration, along with
active user moderation through feedback mechanisms, to foster data-driven
scientific discoveries with efficiency and reproducibility.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13613" title="Abstract">arXiv:2402.13613</a> [<a href="/pdf/2402.13613" title="Download PDF">pdf</a>, <a href="/format/2402.13613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for  Comparative Opinion Mining from Vietnamese Product Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hoang-Quynh Le</a>, 
<a href="/search/cs?searchtype=author&query=Can%2C+D">Duy-Cat Can</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh-Vinh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Mai-Vu Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a comprehensive overview of the Comparative Opinion
Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the
10$^{th}$ International Workshop on Vietnamese Language and Speech Processing
(VLSP 2023). The primary objective of this shared task is to advance the field
of natural language processing by developing techniques that proficiently
extract comparative opinions from Vietnamese product reviews. Participants are
challenged to propose models that adeptly extract a comparative "quintuple"
from a comparative sentence, encompassing Subject, Object, Aspect, Predicate,
and Comparison Type Label. We construct a human-annotated dataset comprising
$120$ documents, encompassing $7427$ non-comparative sentences and $2468$
comparisons within $1798$ sentences. Participating models undergo evaluation
and ranking based on the Exact match macro-averaged quintuple F1 score.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13615" title="Abstract">arXiv:2402.13615</a> [<a href="/pdf/2402.13615" title="Download PDF">pdf</a>, <a href="/format/2402.13615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyizing the Conjunction Fallacy as a Fact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veloz%2C+T">Tomas Veloz</a>, 
<a href="/search/cs?searchtype=author&query=Sobetska%2C+O">Olha Sobetska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> book chapter
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Veloz, Khrennikov, Toni, Castillo (eds) Trends and Challenges
  in Cognitive Modeling. STEAM-H: Springer (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Probability (math.PR); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has
been the subject of multiple debates and become a fundamental challenge for
cognitive theories in decision-making. In this article, we take a rather
uncommon perspective on this phenomenon. Instead of trying to explain the
nature or causes of the conjunction fallacy (intensional definition), we
analyze its range of factual possibilities (extensional definition). We show
that the majority of research on the conjunction fallacy, according to our
sample of experiments reviewed which covers literature between 1983 and 2016,
has focused on a narrow part of the a priori factual possibilities, implying
that explanations of the conjunction fallacy are fundamentally biased by the
short scope of possibilities explored. The latter is a rather curious aspect of
the research evolution in the conjunction fallacy considering that the very
nature of it is motivated by extensional considerations.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13616" title="Abstract">arXiv:2402.13616</a> [<a href="/pdf/2402.13616" title="Download PDF">pdf</a>, <a href="/format/2402.13616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOv9: Learning What You Want to Learn Using Programmable Gradient  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chien-Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+I">I-Hau Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H+M">Hong-Yuan Mark Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Today's deep learning methods focus on how to design the most appropriate
objective functions so that the prediction results of the model can be closest
to the ground truth. Meanwhile, an appropriate architecture that can facilitate
acquisition of enough information for prediction has to be designed. Existing
methods ignore a fact that when input data undergoes layer-by-layer feature
extraction and spatial transformation, large amount of information will be
lost. This paper will delve into the important issues of data loss when data is
transmitted through deep networks, namely information bottleneck and reversible
functions. We proposed the concept of programmable gradient information (PGI)
to cope with the various changes required by deep networks to achieve multiple
objectives. PGI can provide complete input information for the target task to
calculate objective function, so that reliable gradient information can be
obtained to update network weights. In addition, a new lightweight network
architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based
on gradient path planning is designed. GELAN's architecture confirms that PGI
has gained superior results on lightweight models. We verified the proposed
GELAN and PGI on MS COCO dataset based object detection. The results show that
GELAN only uses conventional convolution operators to achieve better parameter
utilization than the state-of-the-art methods developed based on depth-wise
convolution. PGI can be used for variety of models from lightweight to large.
It can be used to obtain complete information, so that train-from-scratch
models can achieve better results than state-of-the-art models pre-trained
using large datasets, the comparison results are shown in Figure 1. The source
codes are at: https://github.com/WongKinYiu/yolov9.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13617" title="Abstract">arXiv:2402.13617</a> [<a href="/pdf/2402.13617" title="Download PDF">pdf</a>, <a href="/format/2402.13617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Monolithic Cybersecurity Architecture for Power Electronic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gupta%2C+K">Kirti Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Sahoo%2C+S">Subham Sahoo</a>, 
<a href="/search/eess?searchtype=author&query=Panigrahi%2C+B+K">Bijaya Ketan Panigrahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Transactions on Smart Grid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Power electronic systems (PES) face significant threats from various data
availability and integrity attacks, significantly affecting the performance of
communication networks and power system operation. As a result, several attack
detection and reconstruction techniques are deployed, which makes it a costly
\&amp; complex cybersecurity operational platform with significant room for
incremental extensions for mitigation against future threats. Unlike the said
traditional arrangements, our paper introduces a foundational approach by
establishing a monolithic cybersecurity architecture (MCA) via incorporating
semantic principles into the sampling process for distributed energy resources
(DERs). This unified approach concurrently compensates for the intrusion
challenges posed by cyber attacks by reconstructing signals using the dynamics
of the inner control layer. This reconstruction considers essential semantic
attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic
performance. Hence, the proposed scheme promises a generalized route to
concurrently tackle a global set of cyber attacks in elevating the resilience
of PES. Finally, rigorous validation on a modified IEEE 69-bus distribution
system and a real-world South California Edison (SCE) 47-bus network, using
OPAL-RT under diverse operating conditions, underscores its robustness,
model-free design capability, scalability, and adaptability to dynamic cyber
graphs and system reconfiguration.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13618" title="Abstract">arXiv:2402.13618</a> [<a href="/pdf/2402.13618" title="Download PDF">pdf</a>, <a href="/format/2402.13618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong Linearizability using Primitives with Consensus Number 2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attiya%2C+H">Hagit Attiya</a>, 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1eda%2C+A">Armando Casta&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Enea%2C+C">Constantin Enea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">A powerful tool for designing complex concurrent programs is through
composition with object implementations from lower-level primitives.
Strongly-linearizable implementations allow to preserve hyper-properties, e.g.,
probabilistic guarantees of randomized programs. However, the only known
wait-free strongly-linearizable implementations for many objects rely on
compare&amp;swap, a universal primitive that allows any number of processes to
solve consensus. This is despite the fact that these objects have wait-free
linearizable implementations from read / write primitives, which do not support
consensus. This paper investigates a middle-ground, asking whether there are
wait-free strongly-linearizable implementations from realistic primitives such
as test&amp;set or fetch&amp;add, whose consensus number is 2.
<br />We show that many objects with consensus number 1 have wait-free
strongly-linearizable implementations from fetch&amp;add. We also show that several
objects with consensus number 2 have wait-free or lock-free implementations
from other objects with consensus number 2. In contrast, we prove that even
when fetch&amp;add, swap and test&amp;set primitives are used, some objects with
consensus number 2 do not have lock-free strongly-linearizable implementations.
This includes queues and stacks, as well as relaxed variants thereof.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13623" title="Abstract">arXiv:2402.13623</a> [<a href="/pdf/2402.13623" title="Download PDF">pdf</a>, <a href="/format/2402.13623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Sahil Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Sudev%2C+U">Ujjwal Sudev</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Taxonomies represent an arborescence hierarchical structure that establishes
relationships among entities to convey knowledge within a specific domain. Each
edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find
utility in various real-world applications, such as e-commerce search engines
and recommendation systems. Consequently, there arises a necessity to enhance
these taxonomies over time. However, manually curating taxonomies with neoteric
data presents challenges due to limitations in available human resources and
the exponential growth of data. Therefore, it becomes imperative to develop
automatic taxonomy expansion methods. Traditional supervised taxonomy expansion
approaches encounter difficulties stemming from limited resources, primarily
due to the small size of existing taxonomies. This scarcity of training data
often leads to overfitting. In this paper, we propose FLAME, a novel approach
for taxonomy expansion in low-resource environments by harnessing the
capabilities of large language models that are trained on extensive real-world
knowledge. LLMs help compensate for the scarcity of domain-specific knowledge.
Specifically, FLAME leverages prompting in few-shot settings to extract the
inherent knowledge within the LLMs, ascertaining the hypernym entities within
the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the
large language models, resulting in more accurate predictions. Experiments on
three real-world benchmark datasets demonstrate the effectiveness of FLAME in
real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy
and 12.3% in Wu &amp; Palmer metric over eight baselines. Furthermore, we elucidate
the strengths and weaknesses of FLAME through an extensive case study, error
analysis and ablation studies on the benchmarks.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13624" title="Abstract">arXiv:2402.13624</a> [<a href="/pdf/2402.13624" title="Download PDF">pdf</a>, <a href="/format/2402.13624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Linear Spanners in All Temporal Cliques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Angrick%2C+S">Sebastian Angrick</a>, 
<a href="/search/cs?searchtype=author&query=Bals%2C+B">Ben Bals</a>, 
<a href="/search/cs?searchtype=author&query=Friedrich%2C+T">Tobias Friedrich</a>, 
<a href="/search/cs?searchtype=author&query=Gawendowicz%2C+H">Hans Gawendowicz</a>, 
<a href="/search/cs?searchtype=author&query=Hastrich%2C+N">Niko Hastrich</a>, 
<a href="/search/cs?searchtype=author&query=Klodt%2C+N">Nicolas Klodt</a>, 
<a href="/search/cs?searchtype=author&query=Lenzner%2C+P">Pascal Lenzner</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J">Jonas Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Skretas%2C+G">George Skretas</a>, 
<a href="/search/cs?searchtype=author&query=Wells%2C+A">Armin Wells</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Many real-world networks, like transportation networks and social networks,
are dynamic in the sense that the edge set may change over time, but these
changes are known in advance. This behavior is captured by the temporal graphs
model, which has recently become a trending topic in theoretical computer
science. A core open problem in the field is to prove the existence of
linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of
complete temporal graphs that ensure all-pairs reachability via temporal paths.
So far, the best known result is the existence of temporal spanners with
$\mathcal{O}(n\log n)$ many edges. We present significant progress towards
proving that linear-size temporal spanners exist in all temporal cliques.
<br />We adapt techniques used in previous works and heavily expand and generalize
them to provide a simpler and more intuitive proof of the $\mathcal{O}(n\log
n)$ bound. Moreover, we use our novel approach to show that a large class of
temporal cliques, called edge-pivot graphs, admit linear-size temporal
spanners. To contrast this, we investigate other classes of temporal cliques
that do not belong to the class of edge-pivot graphs. We introduce two such
graph classes and we develop novel techniques for establishing the existence of
linear temporal spanners in these graph classes as well.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13625" title="Abstract">arXiv:2402.13625</a> [<a href="/pdf/2402.13625" title="Download PDF">pdf</a>, <a href="/format/2402.13625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wanqing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+K">Keping Bi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Since commonsense information has been recorded significantly less frequently
than its existence, language models pre-trained by text generation have
difficulty to learn sufficient commonsense knowledge. Several studies have
leveraged text retrieval to augment the models' commonsense ability. Unlike
text, images capture commonsense information inherently but little effort has
been paid to effectively utilize them. In this work, we propose a novel
Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and
images to enhance the commonsense ability of language models. Extensive
experiments on the Common-Gen task have demonstrated the efficacy of MORE based
on the pre-trained models of both single and multiple modalities.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13627" title="Abstract">arXiv:2402.13627</a> [<a href="/pdf/2402.13627" title="Download PDF">pdf</a>, <a href="/format/2402.13627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for Claims Trading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoefer%2C+M">Martin Hoefer</a>, 
<a href="/search/cs?searchtype=author&query=Ventre%2C+C">Carmine Ventre</a>, 
<a href="/search/cs?searchtype=author&query=Wilhelmi%2C+L">Lisa Wilhelmi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Risk Management (q-fin.RM)

</div>
<p class="mathjax">The recent banking crisis has again emphasized the importance of
understanding and mitigating systemic risk in financial networks. In this
paper, we study a market-driven approach to rescue a bank in distress based on
the idea of claims trading, a notion defined in Chapter 11 of the U.S.
Bankruptcy Code. We formalize the idea in the context of financial networks by
Eisenberg and Noe. For two given banks v and w, we consider the operation that
w takes over some claims of v and in return gives liquidity to v to ultimately
rescue v. We study the structural properties and computational complexity of
decision and optimization problems for several variants of claims trading.
<br />When trading incoming edges of v, we show that there is no trade in which
both banks v and w strictly improve their assets. We therefore consider
creditor-positive trades, in which v profits strictly and w remains
indifferent. For a given set C of incoming edges of v, we provide an efficient
algorithm to compute payments by w that result in maximal assets of v. When the
set C must also be chosen, the problem becomes weakly NP-hard. Our main result
here is a bicriteria FPTAS to compute an approximate trade. The approximate
trade results in nearly the optimal amount of assets of v in any exact trade.
Our results extend to the case in which banks use general monotone payment
functions and the emerging clearing state can be computed efficiently.
<br />In contrast, for trading outgoing edges of v, the goal is to maximize the
increase in assets for the creditors of v. Notably, for these results the
characteristics of the payment functions of the banks are essential. For
payments ranking creditors one by one, we show NP-hardness of approximation
within a factor polynomial in the network size, when the set of claims C is
part of the input or not. Instead, for proportional payments, our results
indicate more favorable conditions.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13628" title="Abstract">arXiv:2402.13628</a> [<a href="/pdf/2402.13628" title="Download PDF">pdf</a>, <a href="/format/2402.13628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Building Temperature Forecasting: A Data-driven Approach with  System Scenario Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dafang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengmao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiaolei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Taniguchi%2C+I">Ittetsu Taniguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and will be published on IEEE PES GM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in
maintaining a comfortable thermal environment and cost approximately 40% of
primary energy usage in the building sector. For smart energy management in
buildings, usage patterns and their resulting profiles allow the improvement of
control systems with prediction capabilities. However, for large-scale HVAC
system management, it is difficult to construct a detailed model for each
subsystem. In this paper, a new data-driven room temperature prediction model
is proposed based on the k-means clustering method. The proposed data-driven
temperature prediction approach extracts the system operation feature through
historical data analysis and further simplifies the system-level model to
improve generalization and computational efficiency. We evaluate the proposed
approach in the real world. The results demonstrated that our approach can
significantly reduce modeling time without reducing prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13630" title="Abstract">arXiv:2402.13630</a> [<a href="/pdf/2402.13630" title="Download PDF">pdf</a>, <a href="/format/2402.13630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yufei He</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure. Preliminary work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Foundation models like ChatGPT and GPT-4 have revolutionized artificial
intelligence, exhibiting remarkable abilities to generalize across a wide array
of tasks and applications beyond their initial training objectives. However,
when this concept is applied to graph learning, a stark contrast emerges. Graph
learning has predominantly focused on single-graph models, tailored to specific
tasks or datasets, lacking the ability to transfer learned knowledge to
different domains. This limitation stems from the inherent complexity and
diversity of graph structures, along with the different feature and label
spaces specific to graph data. In this paper, we present our UniGraph
framework, designed to train a graph foundation model capable of generalizing
to unseen graphs and tasks across diverse domains. Unlike single-graph models
that use pre-computed node features of varying dimensions as input, our
approach leverages Text-Attributed Graphs (TAGs) for unifying node
representations. We propose a cascaded architecture of Language Models (LMs)
and Graph Neural Networks (GNNs) as backbone networks with a self-supervised
training objective based on Masked Graph Modeling (MGM). We introduce graph
instruction tuning using Large Language Models (LLMs) to enable zero-shot
prediction ability. Our comprehensive experiments across various graph learning
tasks and domains demonstrate the model's effectiveness in self-supervised
representation learning on unseen graphs, few-shot in-context transfer, and
zero-shot transfer, even surpassing or matching the performance of GNNs that
have undergone supervised training on target datasets.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13631" title="Abstract">arXiv:2402.13631</a> [<a href="/pdf/2402.13631" title="Download PDF">pdf</a>, <a href="/format/2402.13631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delving into Dark Regions for Robust Shadow Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Huankang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+R+W+H">Rynson W.H. Lau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Shadow detection is a challenging task as it requires a comprehensive
understanding of shadow characteristics and global/local illumination
conditions. We observe from our experiment that state-of-the-art deep methods
tend to have higher error rates in differentiating shadow pixels from
non-shadow pixels in dark regions (ie, regions with low-intensity values). Our
key insight to this problem is that existing methods typically learn
discriminative shadow features from the whole image globally, covering the full
range of intensity values, and may not learn the subtle differences between
shadow and non-shadow pixels in dark regions. Hence, if we can design a model
to focus on a narrower range of low-intensity regions, it may be able to learn
better discriminative features for shadow detection. Inspired by this insight,
we propose a novel shadow detection approach that first learns global
contextual cues over the entire image and then zooms into the dark regions to
learn local shadow representations. To this end, we formulate an effective
dark-region recommendation (DRR) module to recommend regions of low-intensity
values, and a novel dark-aware shadow analysis (DASA) module to learn
dark-aware shadow features from the recommended dark regions. Extensive
experiments show that the proposed method outperforms the state-of-the-art
methods on three popular shadow detection datasets. Code is available at
https://github.com/guanhuankang/ShadowDetection2021.git.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13632" title="Abstract">arXiv:2402.13632</a> [<a href="/pdf/2402.13632" title="Download PDF">pdf</a>, <a href="/format/2402.13632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ordering Topological Descriptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fasy%2C+B+T">Brittany Terese Fasy</a>, 
<a href="/search/cs?searchtype=author&query=Millman%2C+D+L">David L. Millman</a>, 
<a href="/search/cs?searchtype=author&query=Schenfisch%2C+A">Anna Schenfisch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Recent developments in shape reconstruction and comparison call for the use
of many different types of topological descriptors (persistence diagrams, Euler
characteristic functions, etc.). We establish a framework that allows for
quantitative comparisons of topological descriptor types and therefore may be
used as a tool in more rigorously justifying choices made in applications. We
then use this framework to partially order a set of six common topological
descriptor types. In particular, the resulting poset gives insight into the
advantages of using verbose rather than concise topological descriptors. We
then provide lower bounds on the size of sets of descriptors that are complete
discrete invariants of simplicial complexes, both tight and worst case. This
work sets up a rigorous theory that allows for future comparisons and analysis
of topological descriptor types.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13634" title="Abstract">arXiv:2402.13634</a> [<a href="/pdf/2402.13634" title="Download PDF">pdf</a>, <a href="/format/2402.13634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Dual-arm Object Rearrangement for Cartesian Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shishun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+Q">Qijin She</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruizhen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work focuses on the dual-arm object rearrangement problem abstracted
from a realistic industrial scenario of Cartesian robots. The goal of this
problem is to transfer all the objects from sources to targets with the minimum
total completion time. To achieve the goal, the core idea is to develop an
effective object-to-arm task assignment strategy for minimizing the cumulative
task execution time and maximizing the dual-arm cooperation efficiency. One of
the difficulties in the task assignment is the scalability problem. As the
number of objects increases, the computation time of traditional
offline-search-based methods grows strongly for computational complexity.
Encouraged by the adaptability of reinforcement learning (RL) in long-sequence
task decisions, we propose an online task assignment decision method based on
RL, and the computation time of our method only increases linearly with the
number of objects. Further, we design an attention-based network to model the
dependencies between the input states during the whole task execution process
to help find the most reasonable object-to-arm correspondence in each task
assignment round. In the experimental part, we adapt some search-based methods
to this specific setting and compare our method with them. Experimental result
shows that our approach achieves outperformance over search-based methods in
total execution time and computational efficiency, and also verifies the
generalization of our method to different numbers of objects. In addition, we
show the effectiveness of our method deployed on the real robot in the
supplementary video.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13635" title="Abstract">arXiv:2402.13635</a> [<a href="/pdf/2402.13635" title="Download PDF">pdf</a>, <a href="/format/2402.13635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The METRIC-framework for assessing data quality for trustworthy AI in  medicine: a systematic review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwabe%2C+D">Daniel Schwabe</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+K">Katinka Becker</a>, 
<a href="/search/cs?searchtype=author&query=Seyferth%2C+M">Martin Seyferth</a>, 
<a href="/search/cs?searchtype=author&query=Kla%C3%9F%2C+A">Andreas Kla&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4ffter%2C+T">Tobias Sch&#xe4;ffter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The adoption of machine learning (ML) and, more specifically, deep learning
(DL) applications into all major areas of our lives is underway. The
development of trustworthy AI is especially important in medicine due to the
large implications for patients' lives. While trustworthiness concerns various
aspects including ethical, technical and privacy requirements, we focus on the
importance of data quality (training/test) in DL. Since data quality dictates
the behaviour of ML products, evaluating data quality will play a key part in
the regulatory approval of medical AI products. We perform a systematic review
following PRISMA guidelines using the databases PubMed and ACM Digital Library.
We identify 2362 studies, out of which 62 records fulfil our eligibility
criteria. From this literature, we synthesise the existing knowledge on data
quality frameworks and combine it with the perspective of ML applications in
medicine. As a result, we propose the METRIC-framework, a specialised data
quality framework for medical training data comprising 15 awareness dimensions,
along which developers of medical ML applications should investigate a dataset.
This knowledge helps to reduce biases as a major source of unfairness, increase
robustness, facilitate interpretability and thus lays the foundation for
trustworthy AI in medicine. Incorporating such systematic assessment of medical
datasets into regulatory approval processes has the potential to accelerate the
approval of ML products and builds the basis for new standards.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13636" title="Abstract">arXiv:2402.13636</a> [<a href="/pdf/2402.13636" title="Download PDF">pdf</a>, <a href="/format/2402.13636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework and Dataset for Assessing Gender Bias in  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Ashutosh Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prachi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Large vision-language models (VLMs) are widely getting adopted in industry
and academia. In this work we build a unified framework to systematically
evaluate gender-profession bias in VLMs. Our evaluation encompasses all
supported inference modes of the recent VLMs, including image-to-text,
text-to-text, text-to-image, and image-to-image. We construct a synthetic,
high-quality dataset of text and images that blurs gender distinctions across
professional actions to benchmark gender bias. In our benchmarking of recent
vision-language models (VLMs), we observe that different input-output
modalities result in distinct bias magnitudes and directions. We hope our work
will help guide future progress in improving VLMs to learn socially unbiased
representations. We will release our data and code.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13637" title="Abstract">arXiv:2402.13637</a> [<a href="/pdf/2402.13637" title="Download PDF">pdf</a>, <a href="/ps/2402.13637" title="Download PostScript">ps</a>, <a href="/format/2402.13637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unravelling Organisational Rule Systems in Requirements Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindeberg%2C+J">J&#xf6;ran Lindeberg</a>, 
<a href="/search/cs?searchtype=author&query=Svee%2C+E">Eric-Oluf Svee</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+M">Martin Henkel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Context and motivation: Requirements engineering of complex IT systems needs
to manage the many, and often vague and conflicting, organisational rules that
exist in the context of a modern enterprise. At the same time, IT systems
affect the organisation, essentially setting new rules on how the organisation
should work. Question/problem: Gathering requirements for an IT system involves
understanding the complex rules that govern an organisation. The research
question is: How can the holistic properties of organisational rules be
conceptualised? Principal ideas/results: This paper introduces the concept of
organisational rule systems that may be used to describe complex organisational
rules. The concept and its components are presented as a conceptual framework,
which in turn is condensed into a conceptual framework diagram. The framework
is grounded in a critical literature review. Contribution: The conceptual
framework will, as a first step of a wider research agenda, help requirements
engineers understand the influence of organisational rules.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13640" title="Abstract">arXiv:2402.13640</a> [<a href="/pdf/2402.13640" title="Download PDF">pdf</a>, <a href="/format/2402.13640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Green AI: A Preliminary Empirical Study on Energy Consumption in DL  Models Across Different Runtime Infrastructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+N">Negar Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Castor%2C+F">Fernando Castor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime
infrastructures responsible for executing trained models on target hardware,
managing memory, data transfers, and multi-accelerator execution, if
applicable. Additionally, it is a common practice to deploy pre-trained models
on environments distinct from their native development settings. This led to
the introduction of interchange formats such as ONNX, which includes its
runtime infrastructure, and ONNX Runtime, which work as standard formats that
can be used across diverse DL frameworks and languages. Even though these
runtime infrastructures have a great impact on inference performance, no
previous paper has investigated their energy efficiency. In this study, we
monitor the energy consumption and inference time in the runtime
infrastructures of three well-known DL frameworks as well as ONNX, using three
various DL models. To have nuance in our investigation, we also examine the
impact of using different execution providers. We find out that the performance
and energy efficiency of DL are difficult to predict. One framework, MXNet,
outperforms both PyTorch and TensorFlow for the computer vision models using
batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch
size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow
is outperformed consistently. For BERT, PyTorch exhibits the best performance.
Converting the models to ONNX usually yields significant performance
improvements but the ONNX converted ResNet model with batch size 64 consumes
approximately 10% more energy and time than the original PyTorch model.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13641" title="Abstract">arXiv:2402.13641</a> [<a href="/pdf/2402.13641" title="Download PDF">pdf</a>, <a href="/format/2402.13641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlexHB: a More Efficient and Flexible Framework for Hyperparameter  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuekui Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm
to find optimal configurations efficiently? Bayesian Optimization(BO) and the
multi-fidelity BO methods employ surrogate models to sample configurations
based on history evaluations. More recent studies obtain better performance by
integrating BO with HyperBand(HB), which accelerates evaluation by early
stopping mechanism. However, these methods ignore the advantage of a suitable
evaluation scheme over the default HyperBand, and the capability of BO is still
constrained by skewed evaluation results. In this paper, we propose FlexHB, a
new method pushing multi-fidelity BO to the limit as well as re-designing a
framework for early stopping with Successive Halving(SH). Comprehensive study
on FlexHB shows that (1) our fine-grained fidelity method considerably enhances
the efficiency of searching optimal configurations, (2) our FlexBand framework
(self-adaptive allocation of SH brackets, and global ranking of configurations
in both current and past SH procedures) grants the algorithm with more
flexibility and improves the anytime performance. Our method achieves superior
efficiency and outperforms other methods on various HPO tasks. Empirical
results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over
the state-of-the-art MFES-HB and BOHB respectively.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13643" title="Abstract">arXiv:2402.13643</a> [<a href="/pdf/2402.13643" title="Download PDF">pdf</a>, <a href="/format/2402.13643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingkun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Biao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Minghui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yingying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene text recognition is a rapidly developing field that faces numerous
challenges due to the complexity and diversity of scene text, including complex
backgrounds, diverse fonts, flexible arrangements, and accidental occlusions.
In this paper, we propose a novel approach called Class-Aware Mask-guided
feature refinement (CAM) to address these challenges. Our approach introduces
canonical class-aware glyph masks generated from a standard font to effectively
suppress background and text style noise, thereby enhancing feature
discrimination. Additionally, we design a feature alignment and fusion module
to incorporate the canonical mask guidance for further feature refinement for
text recognition. By enhancing the alignment between the canonical mask feature
and the text feature, the module ensures more effective fusion, ultimately
leading to improved recognition performance. We first evaluate CAM on six
standard text recognition benchmarks to demonstrate its effectiveness.
Furthermore, CAM exhibits superiority over the state-of-the-art method by an
average performance gain of 4.1% across six more challenging datasets, despite
utilizing a smaller model size. Our study highlights the importance of
incorporating canonical mask guidance and aligned feature refinement techniques
for robust scene text recognition. The code is available at
https://github.com/MelosY/CAM.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13647" title="Abstract">arXiv:2402.13647</a> [<a href="/pdf/2402.13647" title="Download PDF">pdf</a>, <a href="/format/2402.13647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Lei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
the domain of Natural Language Processing (NLP), aiming to transfer one
stylistic aspect of a sentence into another style without changing its
semantics, syntax, or other attributes. This task is especially challenging
given the intrinsic lack of parallel text pairings. Among existing methods for
UTST tasks, attention masking approach and Large Language Models (LLMs) are
deemed as two pioneering methods. However, they have shortcomings in generating
unsmooth sentences and changing the original contents, respectively. In this
paper, we investigate if we can combine these two methods effectively. We
propose four ways of interactions, that are pipeline framework with tuned
orders; knowledge distillation from LLMs to attention masking model; in-context
learning with constructed parallel examples. We empirically show these
multi-way interactions can improve the baselines in certain perspective of
style strength, content preservation and text fluency. Experiments also
demonstrate that simply conducting prompting followed by attention
masking-based revision can consistently surpass the other systems, including
supervised text style transfer systems. On Yelp-clean and Amazon-clean
datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute
percentages respectively, and achieves new SOTA results.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13649" title="Abstract">arXiv:2402.13649</a> [<a href="/pdf/2402.13649" title="Download PDF">pdf</a>, <a href="/format/2402.13649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning control strategy in soft robotics through a set of  configuration spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%A9nager%2C+E">Etienne M&#xe9;nager</a> (DEFROST), 
<a href="/search/cs?searchtype=author&query=Duriez%2C+C">Christian Duriez</a> (DEFROST, CRIStAL)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Soft Robotics (RoboSoft), IEEE,
  Apr 2024, San Diego (CA), United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The ability of a soft robot to perform specific tasks is determined by its
contact configuration, and transitioning between configurations is often
necessary to reach a desired position or manipulate an object. Based on this
observation, we propose a method for controlling soft robots that involves
defining a graph of configuration spaces. Different agents, whether learned or
not (convex optimization, expert trajectory, and collision detection), use the
structure of the graph to solve the desired task. The graph and the agents are
part of the prior knowledge that is intuitively integrated into the learning
process. They are used to combine different optimization methods, improve
sample efficiency, and provide interpretability. We construct the graph based
on the contact configurations and demonstrate its effectiveness through two
scenarios, a deformable beam in contact with its environment and a soft
manipulator, where it outperforms the baseline in terms of stability, learning
speed, and interpretability.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13650" title="Abstract">arXiv:2402.13650</a> [<a href="/pdf/2402.13650" title="Download PDF">pdf</a>, <a href="/ps/2402.13650" title="Download PostScript">ps</a>, <a href="/format/2402.13650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Obstacle crossing strategies for high-speed 4WD small-scale vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaslin%2C+P">Philippe Vaslin</a> (LIMOS), 
<a href="/search/cs?searchtype=author&query=N%27Chot%2C+D">Denis N&#x27;Chot</a> (IP), 
<a href="/search/cs?searchtype=author&query=Lenain%2C+R">Roland Lenain</a> (UR TSCF, INRAE), 
<a href="/search/cs?searchtype=author&query=Fauroux%2C+J">Jean-Christophe Fauroux</a> (IP, IP), 
<a href="/search/cs?searchtype=author&query=Bassit%2C+L+A">Lama Al Bassit</a> (UR TSAN, INRAE)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SYROM &amp; ROBOTICS 2022, Romanian Association for the Science of
  Mechanisms and Machines; ARoTMM; Robotics Society of Romania; Mechanical
  Engineering Faculty - ''Gheorghe Asachi'' Technical University of Iasi, Nov
  2022, IASI, Romania. pp.327-336
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Unmanned ground vehicle obstacle crossing generally relies on two strategies:
(i) applying a wheel torque for climbing and (ii) modifying the vehicle shape
by using a wheel-leg or wheel-paddle to lift the wheel on top of the obstacle.
However, most of those strategies sacrifice speed in order to have a longer
contact duration between the wheels and the obstacle. This paper investigates
the behaviour of a 4WD high-speed vehicle while crossing a step obstacle using
a design of experiment (DoE). A 3D multibody vehicle model is equipped with a
novel 2-DoF suspension system, which horizontal damping coefficient is modify
to dampen wheel motion in longitudinal and vertical directions in relation to
the chassis, for a given speed and obstacle height. The DoE results allow to
propose a novel high-speed obstacle crossing strategy based on three metrics:
(i) the kinetic energy variation of the vehicle, (ii) the contact duration
between the wheel and the obstacle, and (iii) the pitch rate at the start of
the ballistic phase. Experimental function are proposed to be able modify these
metric in real time.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13651" title="Abstract">arXiv:2402.13651</a> [<a href="/pdf/2402.13651" title="Download PDF">pdf</a>, <a href="/format/2402.13651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czerkawski%2C+M">Mikolaj Czerkawski</a>, 
<a href="/search/cs?searchtype=author&query=Clemente%2C+C">Carmine Clemente</a>, 
<a href="/search/cs?searchtype=author&query=Michie%2C+C+M">Craig MichieCraig Michie</a>, 
<a href="/search/cs?searchtype=author&query=Tachtatzis%2C+C">Christos Tachtatzis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DOI: 10.23919/IRS54158.2022.9905017
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Conference: International Radar Symposium 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">With the great capabilities of deep classifiers for radar data processing
come the risks of learning dataset-specific features that do not generalize
well. In this work, the robustness of two deep convolutional architectures,
trained and tested on the same data, is evaluated. When standard training
practice is followed, both classifiers exhibit sensitivity to subtle temporal
shifts of the input representation, an augmentation that carries minimal
semantic content. Furthermore, the models are extremely susceptible to
adversarial examples. Both small temporal shifts and adversarial examples are a
result of a model overfitting on features that do not generalize well. As a
remedy, it is shown that training on adversarial examples and temporally
augmented samples can reduce this effect and lead to models that generalise
better. Finally, models operating on cadence-velocity diagram representation
rather than Doppler-time are demonstrated to be naturally more immune to
adversarial examples.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13653" title="Abstract">arXiv:2402.13653</a> [<a href="/pdf/2402.13653" title="Download PDF">pdf</a>, <a href="/format/2402.13653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carrami%2C+E+M">Eli M Carrami</a>, 
<a href="/search/cs?searchtype=author&query=Sharifzadeh%2C+S">Sahand Sharifzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce the novel task of zero-shot Protein Question Answering (PQA) for
free-form scientific enquiry. Given a previously unseen protein sequence and a
natural language question, the task is to deliver a scientifically accurate
answer. This task not only supports future biological research, but could also
provide a test bed for assessing the scientific precision of large language
models (LLMs). We contribute the first specialized dataset for PQA model
training, containing 257K protein sequences annotated with 1.97M scientific
question-answer pairs. Additionally, we propose and study several novel
biologically relevant benchmarks for scientific PQA. Employing two robust
multi-modal architectures, we establish an initial state-of-the-art performance
for PQA and reveal key performance factors through ablation studies. Our
comprehensive PQA framework, named Pika, including dataset, code, model
checkpoints, and a user-friendly demo, is openly accessible on
github.com/EMCarrami/Pika, promoting wider research and application in the
field.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13654" title="Abstract">arXiv:2402.13654</a> [<a href="/pdf/2402.13654" title="Download PDF">pdf</a>, <a href="/format/2402.13654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving a Proportional Integral Controller with Reinforcement Learning  on a Throttle Valve Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Daoudi%2C+P">Paul Daoudi</a>, 
<a href="/search/eess?searchtype=author&query=Mavkov%2C+B">Bojan Mavkov</a>, 
<a href="/search/eess?searchtype=author&query=Robu%2C+B">Bogdan Robu</a>, 
<a href="/search/eess?searchtype=author&query=Prieur%2C+C">Christophe Prieur</a>, 
<a href="/search/eess?searchtype=author&query=Witrant%2C+E">Emmanuel Witrant</a>, 
<a href="/search/eess?searchtype=author&query=Barlier%2C+M">Merwan Barlier</a>, 
<a href="/search/eess?searchtype=author&query=Santos%2C+L+D">Ludovic Dos Santos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a learning-based control strategy for non-linear throttle
valves with an asymmetric hysteresis, leading to a near-optimal controller
without requiring any prior knowledge about the environment. We start with a
carefully tuned Proportional Integrator (PI) controller and exploit the recent
advances in Reinforcement Learning (RL) with Guides to improve the closed-loop
behavior by learning from the additional interactions with the valve. We test
the proposed control method in various scenarios on three different valves, all
highlighting the benefits of combining both PI and RL frameworks to improve
control performance in non-linear stochastic systems. In all the experimental
test cases, the resulting agent has a better sample efficiency than traditional
RL agents and outperforms the PI controller.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13655" title="Abstract">arXiv:2402.13655</a> [<a href="/pdf/2402.13655" title="Download PDF">pdf</a>, <a href="/format/2402.13655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Update of Regression Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bl%C3%B8rstad%2C+M">Morten Bl&#xf8;rstad</a>, 
<a href="/search/cs?searchtype=author&query=Lunde%2C+B+%C3%85+S">Berent &#xc5;. S. Lunde</a>, 
<a href="/search/cs?searchtype=author&query=Blaser%2C+N">Nello Blaser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Updating machine learning models with new information usually improves their
predictive performance, yet, in many applications, it is also desirable to
avoid changing the model predictions too much. This property is called
stability. In most cases when stability matters, so does explainability. We
therefore focus on the stability of an inherently explainable machine learning
method, namely regression trees. We aim to use the notion of empirical
stability and design algorithms for updating regression trees that provide a
way to balance between predictability and empirical stability. To achieve this,
we propose a regularization method, where data points are weighted based on the
uncertainty in the initial model. The balance between predictability and
empirical stability can be adjusted through hyperparameters. This
regularization method is evaluated in terms of loss and stability and assessed
on a broad range of data characteristics. The results show that the proposed
update method improves stability while achieving similar or better predictive
performance. This shows that it is possible to achieve both predictive and
stable results when updating regression trees.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13656" title="Abstract">arXiv:2402.13656</a> [<a href="/pdf/2402.13656" title="Download PDF">pdf</a>, <a href="/format/2402.13656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical methods for closed-loop systems with non-autonomous data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baran%2C+B">B. Baran</a>, 
<a href="/search/math?searchtype=author&query=Benner%2C+P">P. Benner</a>, 
<a href="/search/math?searchtype=author&query=Saak%2C+J">J. Saak</a>, 
<a href="/search/math?searchtype=author&query=Stillfjord%2C+T">T. Stillfjord</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">By computing a feedback control via the linear quadratic regulator (LQR)
approach and simulating a non-linear non-autonomous closed-loop system using
this feedback, we combine two numerically challenging tasks. For the first
task, the computation of the feedback control, we use the non-autonomous
generalized differential Riccati equation (DRE), whose solution determines the
time-varying feedback gain matrix. Regarding the second task, we want to be
able to simulate non-linear closed-loop systems for which it is known that the
regulator is only valid for sufficiently small perturbations. Thus, one easily
runs into numerical issues in the integrators when the closed-loop control
varies greatly. For these systems, e.g., the A-stable implicit Euler methods
fails.\newline On the one hand, we implement non-autonomous versions of
splitting schemes and BDF methods for the solution of our non-autonomous DREs.
These are well-established DRE solvers in the autonomous case. On the other
hand, to tackle the numerical issues in the simulation of the non-linear
closed-loop system, we apply a fractional-step-theta scheme with
time-adaptivity tuned specifically to this kind of challenge. That is, we
additionally base the time-adaptivity on the activity of the control. We
compare this approach to the more classical error-based
time-adaptivity.\newline We describe techniques to make these two tasks
computable in a reasonable amount of time and are able to simulate closed-loop
systems with strongly varying controls, while avoiding numerical issues. Our
time-adaptivity approach requires fewer time steps than the error-based
alternative and is more reliable.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13659" title="Abstract">arXiv:2402.13659</a> [<a href="/pdf/2402.13659" title="Download PDF">pdf</a>, <a href="/format/2402.13659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Instructions for Aligning Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Da Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kairouz%2C+P">Peter Kairouz</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zheng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Service providers of large language model (LLM) applications collect user
instructions in the wild and use them in further aligning LLMs with users'
intentions. These instructions, which potentially contain sensitive
information, are annotated by human workers in the process. This poses a new
privacy risk not addressed by the typical private optimization. To this end, we
propose using synthetic instructions to replace real instructions in data
annotation and model fine-tuning. Formal differential privacy is guaranteed by
generating those synthetic instructions using privately fine-tuned generators.
Crucial in achieving the desired utility is our novel filtering algorithm that
matches the distribution of the synthetic instructions to that of the real
ones. In both supervised fine-tuning and reinforcement learning from human
feedback, our extensive experiments demonstrate the high utility of the final
set of synthetic instructions by showing comparable results to real
instructions. In supervised fine-tuning, models trained with private synthetic
instructions outperform leading open-source models such as Vicuna.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13660" title="Abstract">arXiv:2402.13660</a> [<a href="/pdf/2402.13660" title="Download PDF">pdf</a>, <a href="/format/2402.13660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Incompatibles Blocks for Reliable JPEG Steganalysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levecque%2C+E">Etienne Levecque</a> (CRIStAL), 
<a href="/search/cs?searchtype=author&query=Butora%2C+J">Jan Butora</a> (CRIStAL), 
<a href="/search/cs?searchtype=author&query=Bas%2C+P">Patrick Bas</a> (CRIStAL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This article presents a refined notion of incompatible JPEG images for a
quality factor of 100. It can be used to detect the presence of steganographic
schemes embedding in DCT coefficients. We show that, within the JPEG pipeline,
the combination of the DCT transform with the quantization function can map
several distinct blocks in the pixel domain to the same block in the DCT
domain. However, not every DCT block can be obtained: we call those blocks
incompatible. In particular, incompatibility can happen when DCT coefficients
are manually modified to embed a message. We show that the problem of
distinguishing compatible blocks from incompatible ones is an inverse problem
with or without solution and we propose two different methods to solve it. The
first one is heuristic-based, fast to find a solution if it exists. The second
is formulated as an Integer Linear Programming problem and can detect
incompatible blocks only for a specific DCT transform in a reasonable amount of
time. We show that the probability for a block to become incompatible only
relies on the number of modifications. Finally, using the heuristic algorithm
we can derive a Likelihood Ratio Test depending on the number of compatible
blocks per image to perform steganalysis. We simulate the result of this test
and show that it outperforms a deep learning detector e-SRNet for every payload
between 0.001 and 0.01 bpp by using only 10% of the blocks from 256x256 images.
A Selection-Channel-Aware version of the test is even more powerful and
outperforms e-SRNet while using only 1% of the blocks.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13667" title="Abstract">arXiv:2402.13667</a> [<a href="/pdf/2402.13667" title="Download PDF">pdf</a>, <a href="/format/2402.13667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianghui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Ya Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuemin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lirong Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models(LLM) such as ChatGPT have substantially simplified the
generation of marketing copy, yet producing content satisfying domain specific
requirements, such as effectively engaging customers, remains a significant
challenge. In this work, we introduce the Genetic Copy Optimization Framework
(GCOF) designed to enhance both efficiency and engagememnt of marketing copy
creation. We conduct explicit feature engineering within the prompts of LLM.
Additionally, we modify the crossover operator in Genetic Algorithm (GA),
integrating it into the GCOF to enable automatic feature engineering. This
integration facilitates a self-iterative refinement of the marketing copy.
Compared to human curated copy, Online results indicate that copy produced by
our framework achieves an average increase in click-through rate (CTR) of over
$50\%$.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13669" title="Abstract">arXiv:2402.13669</a> [<a href="/pdf/2402.13669" title="Download PDF">pdf</a>, <a href="/format/2402.13669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaorui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Haozhe Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minfeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The surge in Large Language Models (LLMs) has revolutionized natural language
processing, but fine-tuning them for specific tasks often encounters challenges
in balancing performance and preserving general instruction-following
abilities. In this paper, we posit that the distribution gap between task
datasets and the LLMs serves as the primary underlying cause. To address the
problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach
that bridges the distribution gap by guiding fine-tuning with a distilled
dataset generated by the model itself to match its original distribution.
Experimental results on the Llama-2-chat model across various benchmarks
demonstrate that SDFT effectively mitigates catastrophic forgetting while
achieving comparable or superior performance on downstream tasks compared to
the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain
the helpfulness and safety alignment of LLMs. Our code is available at
\url{https://github.com/sail-sg/sdft}.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13671" title="Abstract">arXiv:2402.13671</a> [<a href="/pdf/2402.13671" title="Download PDF">pdf</a>, <a href="/format/2402.13671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spiegel%2C+M">Michal Spiegel</a>, 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">SemEval-2024 Task 8 is focused on multigenerator, multidomain, and
multilingual black-box machine-generated text detection. Such a detection is
important for preventing a potential misuse of large language models (LLMs),
the newest of which are very capable in generating multilingual human-like
texts. We have coped with this task in multiple ways, utilizing language
identification and parameter-efficient fine-tuning of smaller LLMs for text
classification. We have further used the per-language classification-threshold
calibration to uniquely combine fine-tuned models predictions with statistical
detection metrics to improve generalization of the system detection
performance. Our submitted method achieved competitive results, ranking at the
fourth place, just under 1 percentage point behind the winner.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13681" title="Abstract">arXiv:2402.13681</a> [<a href="/pdf/2402.13681" title="Download PDF">pdf</a>, <a href="/format/2402.13681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extrapolated Shock Tracking: Bridging shock-fitting and embedded  boundary methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ciallella%2C+M">Mirco Ciallella</a>, 
<a href="/search/math?searchtype=author&query=Ricchiuto%2C+M">Mario Ricchiuto</a>, 
<a href="/search/math?searchtype=author&query=Paciorri%2C+R">Renato Paciorri</a>, 
<a href="/search/math?searchtype=author&query=Bonfiglioli%2C+A">Aldo Bonfiglioli</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational Physics, 2020, 412: 109440
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel approach to approximate numerically shock waves. The
method combines the unstructured shock-fitting approach developed in the last
decade by some of the authors, with ideas coming from embedded boundary
techniques. The numerical method obtained allows avoiding the re-meshing phase
required by the unstructured fitting method, while guaranteeing accuracy
properties very close to those of the fitting approach. This new method has
many similarities with front tracking approaches, and paves the way to
shock-tracking techniques truly independent on the data and mesh structure used
by the flow solver. The approach is tested on several problems showing accuracy
properties very close to those of more expensive fitting methods, with a
considerable gain in flexibility and generality.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13688" title="Abstract">arXiv:2402.13688</a> [<a href="/pdf/2402.13688" title="Download PDF">pdf</a>, <a href="/format/2402.13688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring users&#x27; sense of safety in public using an Augmented Reality  application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vergari%2C+M">Maurizio Vergari</a>, 
<a href="/search/cs?searchtype=author&query=Koji%C4%87%2C+T">Tanja Koji&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Bertges%2C+N+S">Nicole Stefanie Bertges</a>, 
<a href="/search/cs?searchtype=author&query=Vona%2C+F">Francesco Vona</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+S">Sebastian M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Voigt-Antons%2C+J">Jan-Niklas Voigt-Antons</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Nowadays, Augmented Reality (AR) is available on almost all smartphones
creating some exciting interaction opportunities but also challenges. For
example, already after the famous AR app Pokemon GO was released in July 2016,
numerous accidents related to the use of the app were reported by users. At the
same time, the spread of AR can be noticed in the tourism industry, enabling
tourists to explore their surroundings in new ways but also exposing them to
safety issues. This preliminary study explores users' sense of safety when
manipulating the amount and UI elements visualization parameters of Point of
Interest (POI) markers in a developed AR application. The results show that the
amount of POI markers that are displayed is significant for participants' sense
of safety. The influence of manipulating UI elements in terms of transparency,
color, and size cannot be proven. Nevertheless, most tested people stated that
manipulating transparency and size somehow influences their sense of safety, so
a closer look at them should be taken in future studies.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13693" title="Abstract">arXiv:2402.13693</a> [<a href="/pdf/2402.13693" title="Download PDF">pdf</a>, <a href="/format/2402.13693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMNER: A Chinese Multimodal NER Dataset based on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yuanze Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bobo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fei Li</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+C">Chong Teng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Donghong Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal Named Entity Recognition (MNER) is a pivotal task designed to
extract named entities from text with the support of pertinent images.
Nonetheless, a notable paucity of data for Chinese MNER has considerably
impeded the progress of this natural language processing task within the
Chinese domain. Consequently, in this study, we compile a Chinese Multimodal
NER dataset (CMNER) utilizing data sourced from Weibo, China's largest social
media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326
corresponding images. The entities are classified into four distinct
categories: person, location, organization, and miscellaneous. We perform
baseline experiments on CMNER, and the outcomes underscore the effectiveness of
incorporating images for NER. Furthermore, we conduct cross-lingual experiments
on the publicly available English MNER dataset (Twitter2015), and the results
substantiate our hypothesis that Chinese and English multimodal NER data can
mutually enhance the performance of the NER model.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13695" title="Abstract">arXiv:2402.13695</a> [<a href="/pdf/2402.13695" title="Download PDF">pdf</a>, <a href="/format/2402.13695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational unique continuation with finite dimensional Neumann trace
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burman%2C+E">Erik Burman</a>, 
<a href="/search/math?searchtype=author&query=Oksanen%2C+L">Lauri Oksanen</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Z">Ziyao Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider finite element approximations of unique continuation problems
subject to elliptic equations in the case where the normal derivative of the
exact solution is known to reside in some finite dimensional space. To give
quantitative error estimates we prove Lipschitz stability of the unique
continuation problem in the global H1-norm. This stability is then leveraged to
derive optimal a posteriori and a priori error estimates for a primal-dual
stabilised finite method.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13696" title="Abstract">arXiv:2402.13696</a> [<a href="/pdf/2402.13696" title="Download PDF">pdf</a>, <a href="/format/2402.13696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Do Microservice API Patterns Impact Understandability? A Controlled  Experiment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogner%2C+J">Justus Bogner</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B3jcik%2C+P">Pawel W&#xf3;jcik</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+O">Olaf Zimmermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 21st IEEE International Conference on Software Architecture (ICSA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Microservices expose their functionality via remote Application Programming
Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology. To
solve recurring problems in this design space, Microservice API Patterns (MAPs)
have emerged to capture the collective experience of the API design community.
At present, there is a lack of empirical evidence for the effectiveness of
these patterns, e.g., how they impact understandability and API usability. We
therefore conducted a controlled experiment with 6 microservice patterns to
evaluate their impact on understandability with 65 diverse participants.
Additionally, we wanted to study how demographics like years of professional
experience or experience with MAPs influence the effects of the patterns. Per
pattern, we constructed two API examples, each in a pattern version "P" and a
functionally equivalent non-pattern version "N" (24 in total). Based on a
crossover design, participants had to answer comprehension questions, while we
measured the time. For five of the six patterns, we identified a significant
positive impact on understandability, i.e., participants answered faster and /
or more correctly for "P". However, effect sizes were mostly small, with one
pattern showing a medium effect. The correlations between performance and
demographics seem to suggest that certain patterns may introduce additional
complexity; people experienced with MAPs will profit more from their effects.
This has important implications for training and education around MAPs and
other patterns.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13697" title="Abstract">arXiv:2402.13697</a> [<a href="/pdf/2402.13697" title="Download PDF">pdf</a>, <a href="/format/2402.13697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Semantic Vision Query Generation for Zero-shot Panoptic  and Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+D">Daisuke Deguchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenkai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Murase%2C+H">Hiroshi Murase</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances
and background stuff without images containing unseen categories in training.
Due to the visual data sparsity and the difficulty of generalizing from seen to
unseen categories, this task remains challenging. To better generalize to
unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion
(CONCAT), to produce generalizable semantic vision queries. First, a feature
extractor is trained by CON to link the vision and semantics for providing
target queries. Formally, CON is proposed to align the semantic queries with
the CLIP visual CLS token extracted from complete and masked images. To address
the lack of unseen categories, a generator is required. However, one of the
gaps in synthesizing pseudo vision queries, ie, vision queries for unseen
categories, is describing fine-grained visual details through semantic
embeddings. Therefore, we approach CAT to train the generator in
semantic-vision and vision-semantic manners. In semantic-vision, visual query
contrast is proposed to model the high granularity of vision by pulling the
pseudo vision queries with the corresponding targets containing segments while
pushing those without segments away. To ensure the generated queries retain
semantic information, in vision-semantic, the pseudo vision queries are mapped
back to semantic and supervised by real semantic embeddings. Experiments on ZPS
achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and
open-vocabulary semantic segmentation and obtain comparative results while
being 2 times faster in testing.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13699" title="Abstract">arXiv:2402.13699</a> [<a href="/pdf/2402.13699" title="Download PDF">pdf</a>, <a href="/format/2402.13699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Classification Techniques for Quantum Dot Device  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schug%2C+D">Daniel Schug</a>, 
<a href="/search/cs?searchtype=author&query=Kovach%2C+T+J">Tyler J. Kovach</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+M+A">M. A. Wolfe</a>, 
<a href="/search/cs?searchtype=author&query=Benson%2C+J">Jared Benson</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sanghyeok Park</a>, 
<a href="/search/cs?searchtype=author&query=Dodson%2C+J+P">J. P. Dodson</a>, 
<a href="/search/cs?searchtype=author&query=Corrigan%2C+J">J. Corrigan</a>, 
<a href="/search/cs?searchtype=author&query=Eriksson%2C+M+A">M. A. Eriksson</a>, 
<a href="/search/cs?searchtype=author&query=Zwolak%2C+J+P">Justyna P. Zwolak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the XAI4Sci: Explainable machine learning for
  sciences workshop at AAAI 2024, Vancouver, Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the physical sciences, there is an increased need for robust feature
representations of image data: image acquisition, in the generalized sense of
two-dimensional data, is now widespread across a large number of fields,
including quantum information science, which we consider here. While
traditional image features are widely utilized in such cases, their use is
rapidly being supplanted by Neural Network-based techniques that often
sacrifice explainability in exchange for high accuracy. To ameliorate this
trade-off, we propose a synthetic data-based technique that results in
explainable features. We show, using Explainable Boosting Machines (EBMs), that
this method offers superior explainability without sacrificing accuracy.
Specifically, we show that there is a meaningful benefit to this technique in
the context of quantum dot tuning, where human intervention is necessary at the
current stage of development.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13700" title="Abstract">arXiv:2402.13700</a> [<a href="/pdf/2402.13700" title="Download PDF">pdf</a>, <a href="/format/2402.13700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Conflict of Robustness and Learning in Collaborative Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raynal%2C+M">Mathilde Raynal</a>, 
<a href="/search/cs?searchtype=author&query=Troncoso%2C+C">Carmela Troncoso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Collaborative Machine Learning (CML) allows participants to jointly train a
machine learning model while keeping their training data private. In scenarios
where privacy is a strong requirement, such as health-related applications,
safety is also a primary concern. This means that privacy-preserving CML
processes must produce models that output correct and reliable decisions
\emph{even in the presence of potentially untrusted participants}. In response
to this issue, researchers propose to use \textit{robust aggregators} that rely
on metrics which help filter out malicious contributions that could compromise
the training process. In this work, we formalize the landscape of robust
aggregators in the literature. Our formalization allows us to show that
existing robust aggregators cannot fulfill their goal: either they use
distance-based metrics that cannot accurately identify targeted malicious
updates; or propose methods whose success is in direct conflict with the
ability of CML participants to learn from others and therefore cannot eliminate
the risk of manipulation without preventing learning.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13703" title="Abstract">arXiv:2402.13703</a> [<a href="/pdf/2402.13703" title="Download PDF">pdf</a>, <a href="/format/2402.13703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber%2C+A+A">Alexander Arno Weber</a>, 
<a href="/search/cs?searchtype=author&query=Thellmann%2C+K">Klaudia Thellmann</a>, 
<a href="/search/cs?searchtype=author&query=Ebert%2C+J">Jan Ebert</a>, 
<a href="/search/cs?searchtype=author&query=Flores-Herr%2C+N">Nicolas Flores-Herr</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+J">Jens Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Fromm%2C+M">Michael Fromm</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M">Mehdi Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The adaption of multilingual pre-trained Large Language Models (LLMs) into
eloquent and helpful assistants is essential to facilitate their use across
different language regions. In that spirit, we are the first to conduct an
extensive study of the performance of multilingual models on parallel,
multi-turn instruction-tuning benchmarks across a selection of the most-spoken
Indo-European languages. We systematically examine the effects of language and
instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning
it on parallel instruction-tuning datasets. Our results demonstrate that
instruction-tuning on parallel instead of monolingual corpora benefits
cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we
show that the Superficial Alignment Hypothesis does not hold in general, as the
investigated multilingual 7B parameter model presents a counter-example
requiring large-scale instruction-tuning datasets. Finally, we conduct a human
annotation study to understand the alignment between human-based and
GPT-4-based evaluation within multilingual chat scenarios.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13709" title="Abstract">arXiv:2402.13709</a> [<a href="/pdf/2402.13709" title="Download PDF">pdf</a>, <a href="/format/2402.13709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SaGE: Evaluating Moral Consistency in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonagiri%2C+V+K">Vamshi Krishna Bonagiri</a>, 
<a href="/search/cs?searchtype=author&query=Vennam%2C+S">Sreeram Vennam</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+P">Priyanshul Govil</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+M">Manas Gaur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite recent advancements showcasing the impressive capabilities of Large
Language Models (LLMs) in conversational systems, we show that even
state-of-the-art LLMs are morally inconsistent in their generations,
questioning their reliability (and trustworthiness in general). Prior works in
LLM evaluation focus on developing ground-truth data to measure accuracy on
specific tasks. However, for moral scenarios that often lack universally
agreed-upon answers, consistency in model responses becomes crucial for their
reliability. To address this issue, we propose an information-theoretic measure
called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of
Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract
principles learned by a model and can help explain their decision-making
strategies effectively. To this extent, we construct the Moral Consistency
Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and
the RoTs that these models followed. Furthermore, to illustrate the
generalizability of SaGE, we use it to investigate LLM consistency on two
popular datasets -- TruthfulQA and HellaSwag. Our results reveal that
task-accuracy and consistency are independent problems, and there is a dire
need to investigate these issues further.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13710" title="Abstract">arXiv:2402.13710</a> [<a href="/pdf/2402.13710" title="Download PDF">pdf</a>, <a href="/format/2402.13710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RESTRuler: Towards Automatically Identifying Violations of RESTful  Design Rules in Web APIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogner%2C+J">Justus Bogner</a>, 
<a href="/search/cs?searchtype=author&query=Kotstein%2C+S">Sebastian Kotstein</a>, 
<a href="/search/cs?searchtype=author&query=Abajirov%2C+D">Daniel Abajirov</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+T">Timothy Ernst</a>, 
<a href="/search/cs?searchtype=author&query=Merkel%2C+M">Manuel Merkel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 21st IEEE International Conference on Software Architecture (ICSA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">RESTful APIs based on HTTP are one of the most important ways to make data
and functionality available to applications and software services. However, the
quality of the API design strongly impacts API understandability and usability,
and many rules have been specified for this. While we have evidence for the
effectiveness of many design rules, it is still difficult for practitioners to
identify rule violations in their design. We therefore present RESTRuler, a
Java-based open-source tool that uses static analysis to detect design rule
violations in OpenAPI descriptions. The current prototype supports 14 rules
that go beyond simple syntactic checks and partly rely on natural language
processing. The modular architecture also makes it easy to implement new rules.
To evaluate RESTRuler, we conducted a benchmark with over 2,300 public OpenAPI
descriptions and asked 7 API experts to construct 111 complicated rule
violations. For robustness, RESTRuler successfully analyzed 99% of the used
real-world OpenAPI definitions, with some failing due to excessive size. For
performance efficiency, the tool performed well for the majority of files and
could analyze 84% in less than 23 seconds with low CPU and RAM usage. Lastly,
for effectiveness, RESTRuler achieved a precision of 91% (ranging from 60% to
100% per rule) and recall of 68% (ranging from 46% to 100%). Based on these
variations between rule implementations, we identified several opportunities
for improvements. While RESTRuler is still a research prototype, the evaluation
suggests that the tool is quite robust to errors, resource-efficient for most
APIs, and shows good precision and decent recall. Practitioners can use it to
improve the quality of their API design.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13711" title="Abstract">arXiv:2402.13711</a> [<a href="/pdf/2402.13711" title="Download PDF">pdf</a>, <a href="/format/2402.13711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based  Graph Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungyoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjoong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM TheWebConf 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We investigate the replay buffer in rehearsal-based approaches for graph
continual learning (GCL) methods. Existing rehearsal-based GCL methods select
the most representative nodes for each class and store them in a replay buffer
for later use in training subsequent tasks. However, we discovered that
considering only the class representativeness of each replayed node makes the
replayed nodes to be concentrated around the center of each class, incurring a
potential risk of overfitting to nodes residing in those regions, which
aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach
heavily relies on a few replayed nodes to retain knowledge obtained from
previous tasks, involving the replayed nodes that have irrelevant neighbors in
the model training may have a significant detrimental impact on model
performance. In this paper, we propose a GCL model named DSLR, specifically, we
devise a coverage-based diversity (CD) approach to consider both the class
representativeness and the diversity within each class of the replayed nodes.
Moreover, we adopt graph structure learning (GSL) to ensure that the replayed
nodes are connected to truly informative neighbors. Extensive experimental
results demonstrate the effectiveness and efficiency of DSLR.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13715" title="Abstract">arXiv:2402.13715</a> [<a href="/pdf/2402.13715" title="Download PDF">pdf</a>, <a href="/format/2402.13715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Constellation Shaping for Enhancing Spectral Efficiency in  NOMA VLC Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kafizov%2C+A">Amanat Kafizov</a>, 
<a href="/search/cs?searchtype=author&query=Elzanaty%2C+A">Ahmed Elzanaty</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">The limited modulation bandwidth of the light emitting diodes (LEDs) presents
a challenge in the development of practical high-data-rate visible light
communication (VLC) systems. In this paper, a novel adaptive coded
probabilistic shaping (PS)-based nonorthogonal multiple access (NOMA) scheme is
proposed to improve spectral efficiency (SE) of VLC systems in multiuser uplink
communication scenarios. The proposed scheme adapts its rate to the optical
signal-to-noise ratio (OSNR) by utilizing non-uniformly distributed discrete
constellation symbols and low complexity channel encoder. Furthermore, an
alternate optimization algorithm is proposed to determine the optimal channel
coding rate, constellation spacing, and probability mass function (PMF) of each
user. The extensive numerical results show that the proposed PS-based NOMA
scheme closely approaches the capacity of NOMA with fine granularity. Presented
results demonstrate the effectiveness of our scheme in improving the SE of VLC
systems in multiuser scenarios. For instance, our scheme exhibits substantial
SE gains over existing schemes, namely, the pairwise coded modulation (PCM),
geometric shaping (GS), and uniform-distribution schemes. These findings
highlight the potential of our approach to significantly enhance VLC systems.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13716" title="Abstract">arXiv:2402.13716</a> [<a href="/pdf/2402.13716" title="Download PDF">pdf</a>, <a href="/format/2402.13716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-Disjoint Paths in Eulerian Digraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+D">Dario Cavallaro</a>, 
<a href="/search/cs?searchtype=author&query=Kawarabayashi%2C+K">Ken-ichi Kawarabayashi</a>, 
<a href="/search/cs?searchtype=author&query=Kreutzer%2C+S">Stephan Kreutzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at STOC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Disjoint paths problems are among the most prominent problems in
combinatorial optimization. The edge- as well as vertex-disjoint paths problem,
are NP-complete on directed and undirected graphs. But on undirected graphs,
Robertson and Seymour (Graph Minors XIII) developed an algorithm for the
vertex- and the edge-disjoint paths problem that runs in cubic time for every
fixed number $p$ of terminal pairs, i.e. they proved that the problem is
fixed-parameter tractable on undirected graphs. On directed graphs, Fortune,
Hopcroft, and Wyllie proved that both problems are NP-complete already for
$p=2$ terminal pairs. In this paper, we study the edge-disjoint paths problem
(EDPP) on Eulerian digraphs, a problem that has received significant attention
in the literature. Marx (Marx 2004) proved that the Eulerian EDPP is
NP-complete even on structurally very simple Eulerian digraphs. On the positive
side, polynomial time algorithms are known only for very restricted cases, such
as $p\leq 3$ or where the demand graph is a union of two stars (see e.g.
Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995).
<br />The question of which values of $p$ the edge-disjoint paths problem can be
solved in polynomial time on Eulerian digraphs has already been raised by
Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago. But despite
considerable effort, the complexity of the problem is still wide open and is
considered to be the main open problem in this area (see Chapter 4 of
Bang-Jensen, Gutin 2018 for a recent survey). In this paper, we solve this
long-open problem by showing that the Edge-Disjoint Paths Problem is
fixed-parameter tractable on Eulerian digraphs in general (parameterized by the
number of terminal pairs). The algorithm itself is reasonably simple but the
proof of its correctness requires a deep structural analysis of Eulerian
digraphs.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13717" title="Abstract">arXiv:2402.13717</a> [<a href="/pdf/2402.13717" title="Download PDF">pdf</a>, <a href="/format/2402.13717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaoyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tongxu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+P">Peng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized open-domain dialogue agents
but encounter challenges in multi-character role-playing (MCRP) scenarios. To
address the issue, we present Neeko, an innovative framework designed for
efficient multiple characters imitation. Unlike existing methods, Neeko employs
a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to
diverse characters. Our framework breaks down the role-playing process into
agent pre-training, multiple characters playing, and character incremental
learning, effectively handling both seen and unseen roles. This dynamic
approach, coupled with distinct LoRA blocks for each character, enhances
Neeko's adaptability to unique attributes, personalities, and speaking
patterns. As a result, Neeko demonstrates superior performance in MCRP over
most existing methods, offering more engaging and versatile user interaction
experiences. Code and data are available at
https://github.com/weiyifan1023/Neeko.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13718" title="Abstract">arXiv:2402.13718</a> [<a href="/pdf/2402.13718" title="Download PDF">pdf</a>, <a href="/format/2402.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingfa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+M+K">Moo Khai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+Z+L">Zhen Leng Thai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023.12.15ARR
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023.12.15ARR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Processing and reasoning over long contexts is crucial for many practical
applications of Large Language Models (LLMs), such as document comprehension
and agent construction. Despite recent strides in making LLMs process contexts
with more than 100K tokens, there is currently a lack of a standardized
benchmark to evaluate this long-context capability. Existing public benchmarks
typically focus on contexts around 10K tokens, limiting the assessment and
comparison of LLMs in processing longer contexts. In this paper, we propose
$\infty$Bench, the first LLM benchmark featuring an average data length
surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks
spanning diverse domains, presented in both English and Chinese. The tasks in
$\infty$Bench are designed to require well understanding of long dependencies
in contexts, and make simply retrieving a limited number of passages from
contexts not sufficient for these tasks. In our experiments, based on
$\infty$Bench, we evaluate the state-of-the-art proprietary and open-source
LLMs tailored for processing long contexts. The results indicate that existing
long context LLMs still require significant advancements to effectively process
100K+ context. We further present three intriguing analyses regarding the
behavior of LLMs processing long context.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13720" title="Abstract">arXiv:2402.13720</a> [<a href="/pdf/2402.13720" title="Download PDF">pdf</a>, <a href="/format/2402.13720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ouroboros: Speculative Decoding with Large Model Enhanced Drafting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuxiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Drafting-then-verifying decoding methods such as speculative decoding are
widely adopted training-free methods to accelerate the inference of large
language models (LLMs). Instead of employing an autoregressive process to
decode tokens sequentially, speculative decoding initially creates drafts with
an efficient small model. Then LLMs are required to conduct verification and
correction in a non-autoregressive fashion to minimize time overhead.
Generating longer drafts can lead to even more significant speedups once
verified, but also incurs substantial trial and error costs if it fails.
Suffering from the high verification failure probability, existing decoding
methods cannot draft too much content for verification at one time, achieving
sub-optimal inference acceleration. In this paper, we introduce Ouroboros,
which constructs a phrase candidate pool from the verification process of LLMs
to provide candidates for draft generation of the small model. Thereby,
Ouroboros can further improve the efficiency and effectiveness of the initial
drafts. The experimental results on typical text generation tasks show that
Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead
decoding and speculative decoding, respectively. The source code of Ouroboros
is available at https://github.com/thunlp/Ouroboros.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13722" title="Abstract">arXiv:2402.13722</a> [<a href="/pdf/2402.13722" title="Download PDF">pdf</a>, <a href="/format/2402.13722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafiuddin%2C+S+M">S M Rafiuddin</a>, 
<a href="/search/cs?searchtype=author&query=Rakib%2C+M">Mohammed Rakib</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+S">Sadia Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Bagavathi%2C+A">Arunkumar Bagavathi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, Accepted in PAKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem
that entails the extraction of multifaceted aspects, opinions, and sentiments
from the given text. Both standalone and compound ABSA tasks have been
extensively used in the literature to examine the nuanced information present
in online reviews and social media posts. Current ABSA methods often rely on
static hyperparameters for attention-masking mechanisms, which can struggle
with context adaptation and may overlook the unique relevance of words in
varied situations. This leads to challenges in accurately analyzing complex
sentences containing multiple aspects with differing sentiments. In this work,
we present adaptive masking methods that remove irrelevant tokens based on
context to assist in Aspect Term Extraction and Aspect Sentiment Classification
subtasks of ABSA. We show with our experiments that the proposed methods
outperform the baseline methods in terms of accuracy and F1 scores on four
benchmark online review datasets. Further, we show that the proposed methods
can be extended with multiple adaptations and demonstrate a qualitative
analysis of the proposed approach using sample text for aspect term extraction.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13723" title="Abstract">arXiv:2402.13723</a> [<a href="/pdf/2402.13723" title="Download PDF">pdf</a>, <a href="/format/2402.13723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effect of Batch Size on Contrastive Self-Supervised Speech  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaessen%2C+N">Nik Vaessen</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+D+A">David A. van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Foundation models in speech are often trained using many GPUs, which
implicitly leads to large effective batch sizes. In this paper we study the
effect of batch size on pre-training, both in terms of statistics that can be
monitored during training, and in the effect on the performance of a downstream
fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes
of speech we show that, for a fixed amount of iterations, larger batch sizes
result in better pre-trained models. However, there is lower limit for
stability, and an upper limit for effectiveness. We then show that the quality
of the pre-trained model depends mainly on the amount of speech data seen
during training, i.e., on the product of batch size and number of iterations.
All results are produced with an independent implementation of the wav2vec 2.0
architecture, which to a large extent reproduces the results of the original
work (<a href="/abs/2006.11477">arXiv:2006.11477</a>). Our extensions can help researchers choose effective
operating conditions when studying self-supervised learning in speech, and
hints towards benchmarking self-supervision with a fixed amount of seen data.
Code and model checkpoints are available at
https://github.com/nikvaessen/w2v2-batch-size.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13724" title="Abstract">arXiv:2402.13724</a> [<a href="/pdf/2402.13724" title="Download PDF">pdf</a>, <a href="/format/2402.13724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bring Your Own Character: A Holistic Solution for Automatic Facial  Animation Generation of Customized Characters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zechen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiaolan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+F">Feng Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages. To appear in IEEE-VR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Animating virtual characters has always been a fundamental research problem
in virtual reality (VR). Facial animations play a crucial role as they
effectively convey emotions and attitudes of virtual humans. However, creating
such facial animations can be challenging, as current methods often involve
utilization of expensive motion capture devices or significant investments of
time and effort from human animators in tuning animation parameters. In this
paper, we propose a holistic solution to automatically animate virtual human
faces. In our solution, a deep learning model was first trained to retarget the
facial expression from input face images to virtual human faces by estimating
the blendshape coefficients. This method offers the flexibility of generating
animations with characters of different appearances and blendshape topologies.
Second, a practical toolkit was developed using Unity 3D, making it compatible
with the most popular VR applications. The toolkit accepts both image and video
as input to animate the target virtual human faces and enables users to
manipulate the animation results. Furthermore, inspired by the spirit of
Human-in-the-loop (HITL), we leveraged user feedback to further improve the
performance of the model and toolkit, thereby increasing the customization
properties to suit user preferences. The whole solution, for which we will make
the code public, has the potential to accelerate the generation of facial
animations for use in VR applications.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13725" title="Abstract">arXiv:2402.13725</a> [<a href="/pdf/2402.13725" title="Download PDF">pdf</a>, <a href="/format/2402.13725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse and Structured Hopfield Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+S">Saul Santos</a>, 
<a href="/search/cs?searchtype=author&query=Niculae%2C+V">Vlad Niculae</a>, 
<a href="/search/cs?searchtype=author&query=McNamee%2C+D">Daniel McNamee</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andre F. T. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modern Hopfield networks have enjoyed recent interest due to their connection
to attention in transformers. Our paper provides a unified framework for sparse
Hopfield networks by establishing a link with Fenchel-Young losses. The result
is a new family of Hopfield-Fenchel-Young energies whose update rules are
end-to-end differentiable sparse transformations. We reveal a connection
between loss margins, sparsity, and exact memory retrieval. We further extend
this framework to structured Hopfield networks via the SparseMAP
transformation, which can retrieve pattern associations instead of a single
pattern. Experiments on multiple instance learning and text rationalization
demonstrate the usefulness of our approach.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13726" title="Abstract">arXiv:2402.13726</a> [<a href="/pdf/2402.13726" title="Download PDF">pdf</a>, <a href="/format/2402.13726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExaLogLog: Space-Efficient and Practical Approximate Distinct Counting  up to the Exa-Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ertl%2C+O">Otmar Ertl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. arXiv admin note: text overlap with <a href="/abs/2308.16862">arXiv:2308.16862</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Databases (cs.DB)

</div>
<p class="mathjax">This work introduces ExaLogLog, a new data structure for approximate distinct
counting, which has the same practical properties as the popular HyperLogLog
algorithm. It is commutative, idempotent, mergeable, reducible, has a
constant-time insert operation, and supports distinct counts up to the
exa-scale. At the same time, as theoretically derived and experimentally
verified, it requires 43% less space to achieve the same estimation error.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13728" title="Abstract">arXiv:2402.13728</a> [<a href="/pdf/2402.13728" title="Download PDF">pdf</a>, <a href="/format/2402.13728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Average gradient outer product as a mechanism for deep neural collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beaglehole%2C+D">Daniel Beaglehole</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%BAken%C3%ADk%2C+P">Peter S&#xfa;ken&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>, 
<a href="/search/cs?searchtype=author&query=Belkin%2C+M">Mikhail Belkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the
data representations in the final layers of Deep Neural Networks (DNNs). Though
the phenomenon has been measured in a wide variety of settings, its emergence
is only partially understood. In this work, we provide substantial evidence
that DNC formation occurs primarily through deep feature learning with the
average gradient outer product (AGOP). This takes a step further compared to
efforts that explain neural collapse via feature-agnostic approaches, such as
the unconstrained features model. We proceed by providing evidence that the
right singular vectors and values of the weights are responsible for the
majority of within-class variability collapse in DNNs. As shown in recent work,
this singular structure is highly correlated with that of the AGOP. We then
establish experimentally and theoretically that AGOP induces neural collapse in
a randomly initialized neural network. In particular, we demonstrate that Deep
Recursive Feature Machines, a method originally introduced as an abstraction
for AGOP feature learning in convolutional neural networks, exhibits DNC.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13729" title="Abstract">arXiv:2402.13729</a> [<a href="/pdf/2402.13729" title="Download PDF">pdf</a>, <a href="/format/2402.13729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kihong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Haneol Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jihye Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kwanghee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jaejun Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generating high-quality videos that synthesize desired realistic content is a
challenging task due to their intricate high-dimensionality and complexity of
videos. Several recent diffusion-based methods have shown comparable
performance by compressing videos to a lower-dimensional latent space, using
traditional video autoencoder architecture. However, such method that employ
standard frame-wise 2D and 3D convolution fail to fully exploit the
spatio-temporal nature of videos. To address this issue, we propose a novel
hybrid video diffusion model, called HVDM, which can capture spatio-temporal
dependencies more effectively. The HVDM is trained by a hybrid video
autoencoder which extracts a disentangled representation of the video
including: (i) a global context information captured by a 2D projected latent
(ii) a local volume information captured by 3D convolutions with wavelet
decomposition (iii) a frequency information for improving the video
reconstruction. Based on this disentangled representation, our hybrid
autoencoder provide a more comprehensive video latent enriching the generated
videos with fine structures and details. Experiments on video generation
benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed
approach achieves state-of-the-art video generation quality, showing a wide
range of video applications (e.g., long video generation, image-to-video, and
video dynamics control).
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13731" title="Abstract">arXiv:2402.13731</a> [<a href="/pdf/2402.13731" title="Download PDF">pdf</a>, <a href="/format/2402.13731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Pengfei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the mechanism of factual knowledge storage in pre-trained
language models (PLMs). Previous research suggests that factual knowledge is
stored within multi-layer perceptron weights, and some storage units exhibit
degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper
provides a comprehensive definition of DKNs that covers both structural and
functional aspects, pioneering the study of structures in PLMs' factual
knowledge storage units. Based on this, we introduce the Neurological Topology
Clustering method, which allows the formation of DKNs in any numbers and
structures, leading to a more accurate DKN acquisition. Furthermore, we
introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely
integrates model robustness, evolvability, and complexity for a holistic
assessment of PLMs. Within this framework, our execution of 34 experiments
across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.
The code will be available soon.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13737" title="Abstract">arXiv:2402.13737</a> [<a href="/pdf/2402.13737" title="Download PDF">pdf</a>, <a href="/format/2402.13737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">Xudong Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaorong Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+F">Fengqing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Peng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuanyuan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models are widely used in image generation because they can
generate high-quality and realistic samples. This is in contrast to generative
adversarial networks (GANs) and variational autoencoders (VAEs), which have
some limitations in terms of image quality.We introduce the diffusion model to
the precipitation forecasting task and propose a short-term precipitation
nowcasting with condition diffusion model based on historical observational
data, which is referred to as SRNDiff. By incorporating an additional
conditional decoder module in the denoising process, SRNDiff achieves
end-to-end conditional rainfall prediction. SRNDiff is composed of two
networks: a denoising network and a conditional Encoder network. The
conditional network is composed of multiple independent UNet networks. These
networks extract conditional feature maps at different resolutions, providing
accurate conditional information that guides the diffusion model for
conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy,
although it requires more computational resources.The SRNDiff model exhibits
higher stability and efficiency during training than GANs-based approaches, and
generates high-quality precipitation distribution samples that better reflect
future actual precipitation conditions. This fully validates the advantages and
potential of diffusion models in precipitation forecasting, providing new
insights for enhancing rainfall prediction.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13738" title="Abstract">arXiv:2402.13738</a> [<a href="/pdf/2402.13738" title="Download PDF">pdf</a>, <a href="/format/2402.13738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mixed finite-element, finite-volume, semi-implicit discretisation for  atmospheric dynamics: Spherical geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Melvin%2C+T">Thomas Melvin</a>, 
<a href="/search/math?searchtype=author&query=Shipway%2C+B">Ben Shipway</a>, 
<a href="/search/math?searchtype=author&query=Wood%2C+N">Nigel Wood</a>, 
<a href="/search/math?searchtype=author&query=Benacchio%2C+T">Tommaso Benacchio</a>, 
<a href="/search/math?searchtype=author&query=Bendall%2C+T">Thomas Bendall</a>, 
<a href="/search/math?searchtype=author&query=Boutle%2C+I">Ian Boutle</a>, 
<a href="/search/math?searchtype=author&query=Brown%2C+A">Alex Brown</a>, 
<a href="/search/math?searchtype=author&query=Johnson%2C+C">Christine Johnson</a>, 
<a href="/search/math?searchtype=author&query=Kent%2C+J">James Kent</a>, 
<a href="/search/math?searchtype=author&query=Pring%2C+S">Stephen Pring</a>, 
<a href="/search/math?searchtype=author&query=Smith%2C+C">Chris Smith</a>, 
<a href="/search/math?searchtype=author&query=Zerroukat%2C+M">Mohamed Zerroukat</a>, 
<a href="/search/math?searchtype=author&query=Cotter%2C+C">Colin Cotter</a>, 
<a href="/search/math?searchtype=author&query=Thuburn%2C+J">John Thuburn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 Pages, 8 Figures, 1 Table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">The reformulation of the Met Office's dynamical core for weather and climate
prediction previously described by the authors is extended to spherical domains
using a cubed-sphere mesh. This paper updates the semi-implicit mixed
finite-element formulation to be suitable for spherical domains. In particular
the finite-volume transport scheme is extended to take account of non-uniform,
non-orthogonal meshes and uses an advective-then-flux formulation so that
increment from the transport scheme is linear in the divergence. The resulting
model is then applied to a standard set of dry dynamical core tests and
compared to the existing semi-implicit semi-Lagrangian dynamical core currently
used in the Met Office's operational model.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13739" title="Abstract">arXiv:2402.13739</a> [<a href="/pdf/2402.13739" title="Download PDF">pdf</a>, <a href="/ps/2402.13739" title="Download PostScript">ps</a>, <a href="/format/2402.13739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Gems in the Rough: Computational Notebooks as an Uncharted Oasis  for IDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Titov%2C+S">Sergey Titov</a>, 
<a href="/search/cs?searchtype=author&query=Grotov%2C+K">Konstantin Grotov</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+A+P+S">Ashwin Prasad S. Venkatesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In this paper, we outline potential ways for the further development of
computational notebooks in Integrated Development Environments (IDEs). We
discuss notebooks integration with IDEs, focusing on three main areas:
facilitating experimentation, adding collaborative features, and improving code
comprehension. We propose that better support of notebooks will not only
benefit the notebooks, but also enhance IDEs by supporting new development
processes native to notebooks. In conclusion, we suggest that adapting IDEs for
more experimentation-oriented notebook processes will prepare them for the
future of AI-powered programming.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13740" title="Abstract">arXiv:2402.13740</a> [<a href="/pdf/2402.13740" title="Download PDF">pdf</a>, <a href="/format/2402.13740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to CQL: Bridging Natural Language and Corpus Search Engine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Luming Lu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jiyuan An</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujie Wang</a>, 
<a href="/search/cs?searchtype=author&query=yang%2C+L">Liner yang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Cunliang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haozhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Mingwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yaping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Erhong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Natural Language Processing (NLP) technologies have revolutionized the way we
interact with information systems, with a significant focus on converting
natural language queries into formal query languages such as SQL. However, less
emphasis has been placed on the Corpus Query Language (CQL), a critical tool
for linguistic research and detailed analysis within text corpora. The manual
construction of CQL queries is a complex and time-intensive task that requires
a great deal of expertise, which presents a notable challenge for both
researchers and practitioners. This paper presents the first text-to-CQL task
that aims to automate the translation of natural language into CQL. We present
a comprehensive framework for this task, including a specifically curated
large-scale dataset and methodologies leveraging large language models (LLMs)
for effective text-to-CQL task. In addition, we established advanced evaluation
metrics to assess the syntactic and semantic accuracy of the generated queries.
We created innovative LLM-based conversion approaches and detailed experiments.
The results demonstrate the efficacy of our methods and provide insights into
the complexities of text-to-CQL task.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13741" title="Abstract">arXiv:2402.13741</a> [<a href="/pdf/2402.13741" title="Download PDF">pdf</a>, <a href="/format/2402.13741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guozheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+W">Wenjun Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Ke Ji</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiajun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Z">Ziyu Shang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qiqing Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The in-context learning (ICL) for relational triple extraction (RTE) has
achieved promising performance, but still encounters two key challenges: (1)
how to design effective prompts and (2) how to select proper demonstrations.
Existing methods, however, fail to address these challenges appropriately. On
the one hand, they usually recast RTE task to text-to-text prompting formats,
which is unnatural and results in a mismatch between the output format at the
pre-training time and the inference time for large language models (LLMs). On
the other hand, they only utilize surface natural language features and lack
consideration of triple semantics in sample selection. These issues are
blocking improved performance in ICL for RTE, thus we aim to tackle prompt
designing and sample selection challenges simultaneously. To this end, we
devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task
into a table generation task to incorporate explicit structured information
into ICL, facilitating conversion of outputs to RTE structures. Then we propose
instructive in-context learning (I$^2$CL) which only selects and annotates a
few samples considering internal triple semantics in massive unlabeled samples.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13744" title="Abstract">arXiv:2402.13744</a> [<a href="/pdf/2402.13744" title="Download PDF">pdf</a>, <a href="/format/2402.13744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning Algorithmically in Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Numeroso%2C+D">Danilo Numeroso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The development of artificial intelligence systems with advanced reasoning
capabilities represents a persistent and long-standing research question.
Traditionally, the primary strategy to address this challenge involved the
adoption of symbolic approaches, where knowledge was explicitly represented by
means of symbols and explicitly programmed rules. However, with the advent of
machine learning, there has been a paradigm shift towards systems that can
autonomously learn from data, requiring minimal human guidance. In light of
this shift, in latest years, there has been increasing interest and efforts at
endowing neural networks with the ability to reason, bridging the gap between
data-driven learning and logical reasoning. Within this context, Neural
Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to
integrate the structured and rule-based reasoning of algorithms with the
adaptive learning capabilities of neural networks, typically by tasking neural
models to mimic classical algorithms. In this dissertation, we provide
theoretical and practical contributions to this area of research. We explore
the connections between neural networks and tropical algebra, deriving powerful
architectures that are aligned with algorithm execution. Furthermore, we
discuss and show the ability of such neural reasoners to learn and manipulate
complex algorithmic and combinatorial optimization concepts, such as the
principle of strong duality. Finally, in our empirical efforts, we validate the
real-world utility of NAR networks across different practical scenarios. This
includes tasks as diverse as planning problems, large-scale edge classification
tasks and the learning of polynomial-time approximate algorithms for NP-hard
combinatorial problems. Through this exploration, we aim to showcase the
potential integrating algorithmic reasoning in machine learning models.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13746" title="Abstract">arXiv:2402.13746</a> [<a href="/pdf/2402.13746" title="Download PDF">pdf</a>, <a href="/ps/2402.13746" title="Download PostScript">ps</a>, <a href="/format/2402.13746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Knowledge Graph to Permit Interoperability of Heterogeneous  Digital Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alshumrani%2C+A">Ali Alshumrani</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+N">Nathan Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Ghita%2C+B">Bogdan Ghita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures. To appear in the conference proceedings of the Third International Conference on Ubiquitous Security (UbiSec 2023), Exeter, UK, November, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The modern digital world is highly heterogeneous, encompassing a wide variety
of communications, devices, and services. This interconnectedness generates,
synchronises, stores, and presents digital information in multidimensional,
complex formats, often fragmented across multiple sources. When linked to
misuse, this digital information becomes vital digital evidence. Integrating
and harmonising these diverse formats into a unified system is crucial for
comprehensively understanding evidence and its relationships. However, existing
approaches to date have faced challenges limiting investigators' ability to
query heterogeneous evidence across large datasets. This paper presents a novel
approach in the form of a modern unified data graph. The proposed approach aims
to seamlessly integrate, harmonise, and unify evidence data, enabling
cross-platform interoperability, efficient data queries, and improved digital
investigation performance. To demonstrate its efficacy, a case study is
conducted, highlighting the benefits of the proposed approach and showcasing
its effectiveness in enabling the interoperability required for advanced
analytics in digital investigations.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13750" title="Abstract">arXiv:2402.13750</a> [<a href="/pdf/2402.13750" title="Download PDF">pdf</a>, <a href="/format/2402.13750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Barrier: Utilizing Large Language Models for Industrial  Recommendation Systems through an Inferential Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gong-Duo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Lihong Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Recommendation systems are widely used in e-commerce websites and online
platforms to address information overload. However, existing systems primarily
rely on historical data and user feedback, making it difficult to capture user
intent transitions. Recently, Knowledge Base (KB)-based models are proposed to
incorporate expert knowledge, but it struggle to adapt to new items and the
evolving e-commerce environment. To address these challenges, we propose a
novel Large Language Model based Complementary Knowledge Enhanced
Recommendation System (LLM-KERec). It introduces an entity extractor that
extracts unified concept terms from item and user information. To provide
cost-effective and reliable prior knowledge, entity pairs are generated based
on entity popularity and specific strategies. The large language model
determines complementary relationships in each entity pair, constructing a
complementary knowledge graph. Furthermore, a new complementary recall module
and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of
the ranking model using real complementary exposure-click samples. Extensive
experiments conducted on three industry datasets demonstrate the significant
performance improvement of our model compared to existing approaches.
Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm
for consumption by recommending complementary items. In summary, LLM-KERec
addresses the limitations of traditional recommendation systems by
incorporating complementary knowledge and utilizing a large language model to
capture user intent transitions, adapt to new items, and enhance recommendation
efficiency in the evolving e-commerce landscape.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13752" title="Abstract">arXiv:2402.13752</a> [<a href="/pdf/2402.13752" title="Download PDF">pdf</a>, <a href="/ps/2402.13752" title="Download PostScript">ps</a>, <a href="/format/2402.13752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Powered Predictions for Electricity Load in Prosumer Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kychkin%2C+A">Aleksei Kychkin</a>, 
<a href="/search/cs?searchtype=author&query=Chasparis%2C+G+C">Georgios C. Chasparis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It has been presented in the 18. Symposium Energieinnovation (14.-16.02.2024). Further information can be found at: <a href="https://www.tugraz.at/events/eninnov2024/home">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The flexibility in electricity consumption and production in communities of
residential buildings, including those with renewable energy sources and energy
storage (a.k.a., prosumers), can effectively be utilized through the
advancement of short-term demand response mechanisms. It is known that
flexibility can further be increased if demand response is performed at the
level of communities of prosumers, since aggregated groups can better
coordinate electricity consumption. However, the effectiveness of such
short-term optimization is highly dependent on the accuracy of electricity load
forecasts both for each building as well as for the whole community. Structural
variations in the electricity load profile can be associated with different
exogenous factors, such as weather conditions, calendar information and day of
the week, as well as user behavior. In this paper, we review a wide range of
electricity load forecasting techniques, that can provide significant
assistance in optimizing load consumption in prosumer communities. We present
and test artificial intelligence (AI) powered short-term load forecasting
methodologies that operate with black-box time series models, such as
Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based
SARIMA and smoothing Holt-Winters models; and empirical regression-based models
that utilize domain knowledge. The integration of weather forecasts into
data-driven time series forecasts is also tested. Results show that the
combination of persistent and regression terms (adapted to the load forecasting
task) achieves the best forecast accuracy.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13753" title="Abstract">arXiv:2402.13753</a> [<a href="/pdf/2402.13753" title="Download PDF">pdf</a>, <a href="/format/2402.13753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiran Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+L">Li Lyna Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+N">Ning Shang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large context window is a desirable feature in large language models (LLMs).
However, due to high fine-tuning costs, scarcity of long texts, and
catastrophic values introduced by new token positions, current extended context
windows are limited to around 128k tokens. This paper introduces LongRoPE that,
for the first time, extends the context window of pre-trained LLMs to an
impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k
training lengths, while maintaining performance at the original short context
window. This is achieved by three key innovations: (i) we identify and exploit
two forms of non-uniformities in positional interpolation through an efficient
search, providing a better initialization for fine-tuning and enabling an 8x
extension in non-fine-tuning scenarios; (ii) we introduce a progressive
extension strategy that first fine-tunes a 256k length LLM and then conducts a
second positional interpolation on the fine-tuned extended LLM to achieve a
2048k context window; (iii) we readjust LongRoPE on 8k length to recover the
short context window performance. Extensive experiments on LLaMA2 and Mistral
across various tasks demonstrate the effectiveness of our method. Models
extended via LongRoPE retain the original architecture with minor modifications
to the positional embedding, and can reuse most pre-existing optimizations.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13755" title="Abstract">arXiv:2402.13755</a> [<a href="/pdf/2402.13755" title="Download PDF">pdf</a>, <a href="/format/2402.13755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Massively Parallel Coloring in Sparse Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Latypov%2C+R">Rustam Latypov</a>, 
<a href="/search/cs?searchtype=author&query=Maus%2C+Y">Yannic Maus</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+S">Shreyas Pai</a>, 
<a href="/search/cs?searchtype=author&query=Uitto%2C+J">Jara Uitto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Classic symmetry-breaking problems on graphs have gained a lot of attention
in models of modern parallel computation. The Adaptive Massively Parallel
Computation (AMPC) is a model that captures central challenges in data center
computations. Chang et al. [PODC'2019] gave an extremely fast, constant time,
algorithm for the $(\Delta + 1)$-coloring problem, where $\Delta$ is the
maximum degree of an input graph of $n$ nodes. The algorithm works in the most
restrictive low-space setting, where each machine has $n^{\delta}$ local space
for a constant $0 &lt; \delta &lt; 1$.
<br />In this work, we study the vertex-coloring problem in sparse graphs
parameterized by their arboricity $\alpha$, a standard measure for sparsity. We
give deterministic algorithms that in constant, or almost constant, time give
$\text{poly}(\alpha)$ and $O(\alpha)$-colorings, where $\alpha$ can be
arbitrarily smaller than $\Delta$. A strong and standard approach to compute
arboricity-dependent colorings is through the Nash-Williams forest
decomposition, which gives rise to an (acyclic) orientation of the edges such
that each node has a small outdegree.
<br />Our main technical contribution is giving efficient deterministic algorithms
to compute these orientations and showing how to leverage them to find
colorings in low-space AMPC. A key technical challenge is that the color of a
node may depend on almost all of the other nodes in the graph and these
dependencies cannot be stored on a single machine. Nevertheless, our novel and
careful exploration technique yields the orientation, and the
arboricity-dependent coloring, with a sublinear number of adaptive queries per
node.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13756" title="Abstract">arXiv:2402.13756</a> [<a href="/pdf/2402.13756" title="Download PDF">pdf</a>, <a href="/format/2402.13756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-throughput Visual Nano-drone to Nano-drone Relative Localization  using Onboard Fully Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crupi%2C+L">Luca Crupi</a>, 
<a href="/search/cs?searchtype=author&query=Giusti%2C+A">Alessandro Giusti</a>, 
<a href="/search/cs?searchtype=author&query=Palossi%2C+D">Daniele Palossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024, IEEE Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Relative drone-to-drone localization is a fundamental building block for any
swarm operations. We address this task in the context of miniaturized
nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to
novel use cases enabled by their reduced form factor. The price for their
versatility comes with limited onboard resources, i.e., sensors, processing
units, and memory, which limits the complexity of the onboard algorithms. A
traditional solution to overcome these limitations is represented by
lightweight deep learning models directly deployed aboard nano-drones. This
work tackles the challenging relative pose estimation between nano-drones using
only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip
(SoC) hosted onboard. We present a vertically integrated system based on a
novel vision-based fully convolutional neural network (FCNN), which runs at
39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8
SoC. We compare our FCNN against three State-of-the-Art (SoA) systems.
Considering the best-performing SoA approach, our model results in an R-squared
improvement from 32 to 47% on the horizontal image coordinate and from 18 to
55% on the vertical image coordinate, on a real-world dataset of 30k images.
Finally, our in-field tests show a reduction of the average tracking error of
37% compared to a previous SoA work and an endurance performance up to the
entire battery lifetime of 4 minutes.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13758" title="Abstract">arXiv:2402.13758</a> [<a href="/pdf/2402.13758" title="Download PDF">pdf</a>, <a href="/format/2402.13758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zheheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Factual inconsistency with source documents in automatically generated
summaries can lead to misinformation or pose risks. Existing factual
consistency(FC) metrics are constrained by their performance, efficiency, and
explainability. Recent advances in Large language models (LLMs) have
demonstrated remarkable potential in text evaluation but their effectiveness in
assessing FC in summarisation remains underexplored. Prior research has mostly
focused on proprietary LLMs, leaving essential factors that affect their
assessment capabilities unexplored. Additionally, current FC evaluation
benchmarks are restricted to news articles, casting doubt on the generality of
the FC methods tested on them. In this paper, we first address the gap by
introducing TreatFact a dataset of LLM-generated summaries of clinical texts,
annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC
evaluation across news and clinical domains and analyse the impact of model
size, prompts, pre-training and fine-tuning data. Our findings reveal that
despite proprietary models prevailing on the task, open-source LLMs lag behind.
Nevertheless, there is potential for enhancing the performance of open-source
LLMs through increasing model size, expanding pre-training data, and developing
well-curated fine-tuning data. Experiments on TreatFact suggest that both
previous methods and LLM-based evaluators are unable to capture factual
inconsistencies in clinical summaries, posing a new challenge for FC
evaluation.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13763" title="Abstract">arXiv:2402.13763</a> [<a href="/pdf/2402.13763" title="Download PDF">pdf</a>, <a href="/format/2402.13763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Style Transfer with Time-Varying Inversion of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+F">Fan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chongyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=dong%2C+W">Weiming dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changsheng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">With the development of diffusion models, text-guided image style transfer
has demonstrated high-quality controllable synthesis results. However, the
utilization of text for diverse music style transfer poses significant
challenges, primarily due to the limited availability of matched audio-text
datasets. Music, being an abstract and complex art form, exhibits variations
and intricacies even within the same genre, thereby making accurate textual
descriptions challenging. This paper presents a music style transfer approach
that effectively captures musical attributes using minimal data. We introduce a
novel time-varying textual inversion module to precisely capture
mel-spectrogram features at different levels. During inference, we propose a
bias-reduced stylization technique to obtain stable results. Experimental
results demonstrate that our method can transfer the style of specific
instruments, as well as incorporate natural sounds to compose melodies. Samples
and source code are available at https://lsfhuihuiff.github.io/MusicTI/.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13764" title="Abstract">arXiv:2402.13764</a> [<a href="/pdf/2402.13764" title="Download PDF">pdf</a>, <a href="/format/2402.13764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CriticBench: Evaluating Large Language Models as Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xian-ling Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Critique ability are crucial in the scalable oversight and self-improvement
of Large Language Models (LLMs). While many recent studies explore the critique
ability of LLMs to judge and refine flaws in generations, how to
comprehensively and reliably measure the critique abilities of LLMs is
under-explored. This paper introduces \shortname, a novel benchmark designed to
comprehensively and reliably evaluate four key critique ability dimensions of
LLMs: feedback, comparison, refinement and meta-feedback.
\shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to
critique responses at varying levels of quality granularity. Our extensive
evaluations of open-source and closed-source LLMs reveal intriguing
relationships between the critique ability and tasks, response qualities, and
model scales. Datasets, resources and evaluation toolkit for \shortname~will be
publicly released at \url{https://github.com/gmftbyGMFTBY/CriticBench}.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13765" title="Abstract">arXiv:2402.13765</a> [<a href="/pdf/2402.13765" title="Download PDF">pdf</a>, <a href="/format/2402.13765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accuracy-Preserving Calibration via Statistical Modeling on Probability  Simplex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esaki%2C+Y">Yasushi Esaki</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+A">Akihiro Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Kawano%2C+K">Keisuke Kawano</a>, 
<a href="/search/cs?searchtype=author&query=Tokuhisa%2C+R">Ryoko Tokuhisa</a>, 
<a href="/search/cs?searchtype=author&query=Kutsuna%2C+T">Takuro Kutsuna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Classification models based on deep neural networks (DNNs) must be calibrated
to measure the reliability of predictions. Some recent calibration methods have
employed a probabilistic model on the probability simplex. However, these
calibration methods cannot preserve the accuracy of pre-trained models, even
those with a high classification accuracy. We propose an accuracy-preserving
calibration method using the Concrete distribution as the probabilistic model
on the probability simplex. We theoretically prove that a DNN model trained on
cross-entropy loss has optimality as the parameter of the Concrete
distribution. We also propose an efficient method that synthetically generates
samples for training probabilistic models on the probability simplex. We
demonstrate that the proposed method can outperform previous methods in
accuracy-preserving calibration tasks using benchmarks.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13767" title="Abstract">arXiv:2402.13767</a> [<a href="/pdf/2402.13767" title="Download PDF">pdf</a>, <a href="/format/2402.13767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Red-Blue Circular Annulus Cover Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maji%2C+S">Sukanya Maji</a>, 
<a href="/search/cs?searchtype=author&query=Pandit%2C+S">Supantha Pandit</a>, 
<a href="/search/cs?searchtype=author&query=Sadhu%2C+S">Sanjib Sadhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We study the Generalized Red-Blue Annulus Cover problem for two sets of
points, red ($R$) and blue ($B$), where each point $p \in R\cup B$ is
associated with a positive penalty ${\cal P}(p)$. The red points have
non-covering penalties, and the blue points have covering penalties. The
objective is to compute a circular annulus ${\cal A}$ such that the value of
the function ${\cal P}({R}^{out})$ + ${\cal P}({ B}^{in})$ is minimum, where
${R}^{out} \subseteq {R}$ is the set of red points not covered by ${\cal A}$
and ${B}^{in} \subseteq {B}$ is the set of blue points covered by $\cal A$. We
also study another version of this problem, where all the red points in $R$ and
the minimum number of points in $B$ are covered by the circular annulus in two
dimensions. We design polynomial-time algorithms for all such circular annulus
problems.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13768" title="Abstract">arXiv:2402.13768</a> [<a href="/pdf/2402.13768" title="Download PDF">pdf</a>, <a href="/format/2402.13768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Democratizing Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seelinger%2C+L">Linus Seelinger</a>, 
<a href="/search/cs?searchtype=author&query=Reinarz%2C+A">Anne Reinarz</a>, 
<a href="/search/cs?searchtype=author&query=Lykkegaard%2C+M+B">Mikkel B. Lykkegaard</a>, 
<a href="/search/cs?searchtype=author&query=Alghamdi%2C+A+M+A">Amal Mohammed A. Alghamdi</a>, 
<a href="/search/cs?searchtype=author&query=Aristoff%2C+D">David Aristoff</a>, 
<a href="/search/cs?searchtype=author&query=Bangerth%2C+W">Wolfgang Bangerth</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A9n%C3%A9zech%2C+J">Jean B&#xe9;n&#xe9;zech</a>, 
<a href="/search/cs?searchtype=author&query=Diez%2C+M">Matteo Diez</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+K">Kurt Frey</a>, 
<a href="/search/cs?searchtype=author&query=Jakeman%2C+J+D">John D. Jakeman</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B8rgensen%2C+J+S">Jakob Sauer J&#xf8;rgensen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Ki-Tae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Martinelli%2C+M">Massimiliano Martinelli</a>, 
<a href="/search/cs?searchtype=author&query=Parno%2C+M">Matthew Parno</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrini%2C+R">Riccardo Pellegrini</a>, 
<a href="/search/cs?searchtype=author&query=Petra%2C+N">Noemi Petra</a>, 
<a href="/search/cs?searchtype=author&query=Riis%2C+N+A+B">Nicolai A. B. Riis</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+K">Katherine Rosenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Serani%2C+A">Andrea Serani</a>, 
<a href="/search/cs?searchtype=author&query=Tamellini%2C+L">Lorenzo Tamellini</a>, 
<a href="/search/cs?searchtype=author&query=Villa%2C+U">Umberto Villa</a>, 
<a href="/search/cs?searchtype=author&query=Dodwell%2C+T+J">Tim J. Dodwell</a>, 
<a href="/search/cs?searchtype=author&query=Scheichl%2C+R">Robert Scheichl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Uncertainty Quantification (UQ) is vital to safety-critical model-based
analyses, but the widespread adoption of sophisticated UQ methods is limited by
technical complexity. In this paper, we introduce UM-Bridge (the UQ and
Modeling Bridge), a high-level abstraction and software protocol that
facilitates universal interoperability of UQ software with simulation codes. It
breaks down the technical complexity of advanced UQ applications and enables
separation of concerns between experts. UM-Bridge democratizes UQ by allowing
effective interdisciplinary collaboration, accelerating the development of
advanced UQ methods, and making it easy to perform UQ analyses from prototype
to High Performance Computing (HPC) scale.
<br />In addition, we present a library of ready-to-run UQ benchmark problems, all
easily accessible through UM-Bridge. These benchmarks support UQ methodology
research, enabling reproducible performance comparisons. We demonstrate
UM-Bridge with several scientific applications, harnessing HPC resources even
using UQ codes not designed with HPC support.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13769" title="Abstract">arXiv:2402.13769</a> [<a href="/pdf/2402.13769" title="Download PDF">pdf</a>, <a href="/format/2402.13769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Debiasing for Graph-based Collaborative Filtering via  Adversarial Graph Dropout
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenchang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+P">Pengbo Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+L">Leheng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Graph neural networks (GNNs) have shown impressive performance in recommender
systems, particularly in collaborative filtering (CF). The key lies in
aggregating neighborhood information on a user-item interaction graph to
enhance user/item representations. However, we have discovered that this
aggregation mechanism comes with a drawback, which amplifies biases present in
the interaction graph. For instance, a user's interactions with items can be
driven by both unbiased true interest and various biased factors like item
popularity or exposure. However, the current aggregation approach combines all
information, both biased and unbiased, leading to biased representation
learning. Consequently, graph-based recommenders can learn distorted views of
users/items, hindering the modeling of their true preferences and
generalizations. To address this issue, we introduce a novel framework called
Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and
biased interactions, enabling unbiased representation learning. For each
user/item, AdvDrop employs adversarial learning to split the neighborhood into
two views: one with bias-mitigated interactions and the other with bias-aware
interactions. After view-specific aggregation, AdvDrop ensures that the
bias-mitigated and bias-aware representations remain invariant, shielding them
from the influence of bias. We validate AdvDrop's effectiveness on five public
datasets that cover both general and specific biases, demonstrating significant
improvements. Furthermore, our method exhibits meaningful separation of
subgraphs and achieves unbiased representations for graph-based CF models, as
revealed by in-depth analysis. Our code is publicly available at
https://github.com/Arthurma71/AdvDrop.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13771" title="Abstract">arXiv:2402.13771</a> [<a href="/pdf/2402.13771" title="Download PDF">pdf</a>, <a href="/format/2402.13771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mask-up: Investigating Biases in Face Re-identification for Masked Faces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+S+D">Siddharth D Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A+K">Ankit Kr. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">AI based Face Recognition Systems (FRSs) are now widely distributed and
deployed as MLaaS solutions all over the world, moreso since the COVID-19
pandemic for tasks ranging from validating individuals' faces while buying SIM
cards to surveillance of citizens. Extensive biases have been reported against
marginalized groups in these systems and have led to highly discriminatory
outcomes. The post-pandemic world has normalized wearing face masks but FRSs
have not kept up with the changing times. As a result, these systems are
susceptible to mask based face occlusion. In this study, we audit four
commercial and nine open-source FRSs for the task of face re-identification
between different varieties of masked and unmasked images across five benchmark
datasets (total 14,722 images). These simulate a realistic
validation/surveillance task as deployed in all major countries around the
world. Three of the commercial and five of the open-source FRSs are highly
inaccurate; they further perpetuate biases against non-White individuals, with
the lowest accuracy being 0%. A survey for the same task with 85 human
participants also results in a low accuracy of 40%. Thus a human-in-the-loop
moderation in the pipeline does not alleviate the concerns, as has been
frequently hypothesized in literature. Our large-scale study shows that
developers, lawmakers and users of such services need to rethink the design
principles behind FRSs, especially for the task of face re-identification,
taking cognizance of observed biases.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13772" title="Abstract">arXiv:2402.13772</a> [<a href="/pdf/2402.13772" title="Download PDF">pdf</a>, <a href="/ps/2402.13772" title="Download PostScript">ps</a>, <a href="/format/2402.13772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter identification algorithm for a LTV system with partially  unknown state matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kozachek%2C+O">Olga Kozachek</a>, 
<a href="/search/eess?searchtype=author&query=Nikolaev%2C+N">Nikolay Nikolaev</a>, 
<a href="/search/eess?searchtype=author&query=Slita%2C+O">Olga Slita</a>, 
<a href="/search/eess?searchtype=author&query=Bobtsov%2C+A">Alexey Bobtsov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, submitted to IEEE Control Systems Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper an adaptive state observer and parameter identification
algorithm for a linear time-varying system are developed under condition that
the state matrix of the system contains unknown time-varying parameters of a
known form. The state vector is observed using only output and input
measurements without identification of the unknown parameters. When the state
vector estimate is obtained, the identification algorithm is applied to find
unknown parameters of the system.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13773" title="Abstract">arXiv:2402.13773</a> [<a href="/pdf/2402.13773" title="Download PDF">pdf</a>, <a href="/format/2402.13773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mackensen%2C+P">Philipp Mackensen</a>, 
<a href="/search/cs?searchtype=author&query=Staat%2C+P">Paul Staat</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+S">Stefan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Sezgin%2C+A">Aydin Sezgin</a>, 
<a href="/search/cs?searchtype=author&query=Paar%2C+C">Christof Paar</a>, 
<a href="/search/cs?searchtype=author&query=Moonsamy%2C+V">Veelasha Moonsamy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Today, we rely heavily on the constant availability of wireless communication
systems. As a result, wireless jamming continues to prevail as an imminent
threat: Attackers can create deliberate radio interference to overshadow
desired signals, leading to denial of service. Although the broadcast nature of
radio signal propagation makes such an attack possible in the first place, it
likewise poses a challenge for the attacker, preventing precise targeting of
single devices. In particular, the jamming signal will likely not only reach
the victim receiver but also other neighboring devices. In this work, we
introduce spatial control of wireless jamming signals, granting a new degree of
freedom to leverage for jamming attacks. Our novel strategy employs an
environment-adaptive reconfigurable intelligent surface (RIS), exploiting
multipath signal propagation to spatially focus jamming signals on particular
victim devices. We investigate this effect through extensive experimentation
and show that our approach can disable the wireless communication of a victim
device while leaving neighbouring devices unaffected. In particular, we
demonstrate complete denial-of-service of a Wi-Fi device while a second device
located at a distance as close as 5 mm remains unaffected, sustaining wireless
communication at a data rate of 60 Mbit/s. We also show that the attacker can
change the attack target on-the-fly, dynamically selecting the device to be
jammed.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13777" title="Abstract">arXiv:2402.13777</a> [<a href="/pdf/2402.13777" title="Download PDF">pdf</a>, <a href="/format/2402.13777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+B">Bhargav Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yongsheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep generative models (DGMs) have demonstrated great success across various
domains, particularly in generating texts, images, and videos using models
trained from offline data. Similarly, data-driven decision-making and robotic
control also necessitate learning a generator function from the offline data to
serve as the strategy or policy. In this case, applying deep generative models
in offline policy learning exhibits great potential, and numerous studies have
explored in this direction. However, this field still lacks a comprehensive
review and so developments of different branches are relatively independent.
Thus, we provide the first systematic review on the applications of deep
generative models for offline policy learning. In particular, we cover five
mainstream deep generative models, including Variational Auto-Encoders,
Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion
Models, and their applications in both offline reinforcement learning (offline
RL) and imitation learning (IL). Offline RL and IL are two main branches of
offline policy learning and are widely-adopted techniques for sequential
decision-making. Specifically, for each type of DGM-based offline policy
learning, we distill its fundamental scheme, categorize related works based on
the usage of the DGM, and sort out the development process of algorithms in
that field. Subsequent to the main content, we provide in-depth discussions on
deep generative models and offline policy learning as a summary, based on which
we present our perspectives on future research directions. This work offers a
hands-on reference for the research progress in deep generative models for
offline policy learning, and aims to inspire improved DGM-based offline RL or
IL algorithms.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13778" title="Abstract">arXiv:2402.13778</a> [<a href="/pdf/2402.13778" title="Download PDF">pdf</a>, <a href="/format/2402.13778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly supervised localisation of prostate cancer using reinforcement  learning for bi-parametric MR images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pocius%2C+M">Martynas Pocius</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Barratt%2C+D+C">Dean C. Barratt</a>, 
<a href="/search/cs?searchtype=author&query=Emberton%2C+M">Mark Emberton</a>, 
<a href="/search/cs?searchtype=author&query=Clarkson%2C+M+J">Matthew J. Clarkson</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yipeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Saeed%2C+S+U">Shaheer U. Saeed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ISBI 2024 (21st IEEE International Symposium on Biomedical Imaging)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper we propose a reinforcement learning based weakly supervised
system for localisation. We train a controller function to localise regions of
interest within an image by introducing a novel reward definition that utilises
non-binarised classification probability, generated by a pre-trained binary
classifier which classifies object presence in images or image crops. The
object-presence classifier may then inform the controller of its localisation
quality by quantifying the likelihood of the image containing an object. Such
an approach allows us to minimize any potential labelling or human bias
propagated via human labelling for fully supervised localisation. We evaluate
our proposed approach for a task of cancerous lesion localisation on a large
dataset of real clinical bi-parametric MR images of the prostate. Comparisons
to the commonly used multiple-instance learning weakly supervised localisation
and to a fully supervised baseline show that our proposed method outperforms
the multi-instance learning and performs comparably to fully-supervised
learning, using only image-level classification labels for training.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13779" title="Abstract">arXiv:2402.13779</a> [<a href="/pdf/2402.13779" title="Download PDF">pdf</a>, <a href="/format/2402.13779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Molecule Representation Learning from Chemical Reaction  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Han Tang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shikun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bicheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yuyan Ni</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">JIngjing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei-Ying Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yanyan Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">In recent years, self-supervised learning has emerged as a powerful tool to
harness abundant unlabelled data for representation learning and has been
broadly adopted in diverse areas. However, when applied to molecular
representation learning (MRL), prevailing techniques such as masked sub-unit
reconstruction often fall short, due to the high degree of freedom in the
possible combinations of atoms within molecules, which brings insurmountable
complexity to the masking-reconstruction paradigm. To tackle this challenge, we
introduce REMO, a self-supervised learning framework that takes advantage of
well-defined atom-combination rules in common chemistry. Specifically, REMO
pre-trains graph/Transformer encoders on 1.7 million known chemical reactions
in the literature. We propose two pre-training objectives: Masked Reaction
Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO
offers a novel solution to MRL by exploiting the underlying shared patterns in
chemical reactions as \textit{context} for pre-training, which effectively
infers meaningful representations of common chemistry knowledge. Such
contextual representations can then be utilized to support diverse downstream
molecular tasks with minimum finetuning, such as affinity prediction and
drug-drug interaction prediction. Extensive experimental results on
MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type
classification show that across all tested downstream tasks, REMO outperforms
the standard baseline of single-molecule masked modeling used in current MRL.
Remarkably, REMO is the pioneering deep learning model surpassing
fingerprint-based methods in activity cliff benchmarks.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13781" title="Abstract">arXiv:2402.13781</a> [<a href="/pdf/2402.13781" title="Download PDF">pdf</a>, <a href="/format/2402.13781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Near-Optimal Gradient Sparsification Cost for Scalable  Distributed Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+D">Daegun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sangyoon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24th IEEE/ACM International Symposium on Cluster, Cloud, and Internet Computing (CCGrid 2024). Code: <a href="https://github.com/kljp/exdyna">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Communication overhead is a major obstacle to scaling distributed training
systems. Gradient sparsification is a potential optimization approach to reduce
the communication volume without significant loss of model fidelity. However,
existing gradient sparsification methods have low scalability owing to
inefficient design of their algorithms, which raises the communication overhead
significantly. In particular, gradient build-up and inadequate sparsity control
methods degrade the sparsification performance considerably. Moreover,
communication traffic increases drastically owing to workload imbalance of
gradient selection between workers.
<br />To address these challenges, we propose a novel gradient sparsification
scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises
fined-grained blocks, and contiguous blocks are grouped into non-overlapping
partitions. Each worker selects gradients in its exclusively allocated
partition so that gradient build-up never occurs. To balance the workload of
gradient selection between workers, ExDyna adjusts the topology of partitions
by comparing the workloads of adjacent partitions. In addition, ExDyna supports
online threshold scaling, which estimates the accurate threshold of gradient
selection on-the-fly. Accordingly, ExDyna can satisfy the user-required
sparsity level during a training period regardless of models and datasets.
Therefore, ExDyna can enhance the scalability of distributed training systems
by preserving near-optimal gradient sparsification cost. In experiments, ExDyna
outperformed state-of-the-art sparsifiers in terms of training speed and
sparsification performance while achieving high accuracy.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13782" title="Abstract">arXiv:2402.13782</a> [<a href="/pdf/2402.13782" title="Download PDF">pdf</a>, <a href="/format/2402.13782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semirings for Probabilistic and Neuro-Symbolic Logic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Derkinderen%2C+V">Vincent Derkinderen</a>, 
<a href="/search/cs?searchtype=author&query=Manhaeve%2C+R">Robin Manhaeve</a>, 
<a href="/search/cs?searchtype=author&query=Martires%2C+P+Z+D">Pedro Zuidberg Dos Martires</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+L">Luc De Raedt</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Approximate Reasoning (2024): 109130
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The field of probabilistic logic programming (PLP) focuses on integrating
probabilistic models into programming languages based on logic. Over the past
30 years, numerous languages and frameworks have been developed for modeling,
inference and learning in probabilistic logic programs. While originally PLP
focused on discrete probability, more recent approaches have incorporated
continuous distributions as well as neural networks, effectively yielding
neural-symbolic methods. We provide a unified algebraic perspective on PLP,
showing that many if not most of the extensions of PLP can be cast within a
common algebraic logic programming framework, in which facts are labeled with
elements of a semiring and disjunction and conjunction are replaced by addition
and multiplication. This does not only hold for the PLP variations itself but
also for the underlying execution mechanism that is based on (algebraic) model
counting.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13785" title="Abstract">arXiv:2402.13785</a> [<a href="/pdf/2402.13785" title="Download PDF">pdf</a>, <a href="/format/2402.13785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesis of Hierarchical Controllers Based on Deep Reinforcement  Learning Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgrange%2C+F">Florent Delgrange</a>, 
<a href="/search/cs?searchtype=author&query=Avni%2C+G">Guy Avni</a>, 
<a href="/search/cs?searchtype=author&query=Lukina%2C+A">Anna Lukina</a>, 
<a href="/search/cs?searchtype=author&query=Schilling%2C+C">Christian Schilling</a>, 
<a href="/search/cs?searchtype=author&query=Now%C3%A9%2C+A">Ann Now&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+G+A">Guillermo A. P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages main text, 17 pages Appendix (excluding references)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We propose a novel approach to the problem of controller design for
environments modeled as Markov decision processes (MDPs). Specifically, we
consider a hierarchical MDP a graph with each vertex populated by an MDP called
a "room". We first apply deep reinforcement learning (DRL) to obtain low-level
policies for each room, scaling to large rooms of unknown structure. We then
apply reactive synthesis to obtain a high-level planner that chooses which
low-level policy to execute in each room. The central challenge in synthesizing
the planner is the need for modeling rooms. We address this challenge by
developing a DRL procedure to train concise "latent" policies together with PAC
guarantees on their performance. Unlike previous approaches, ours circumvents a
model distillation step. Our approach combats sparse rewards in DRL and enables
reusability of low-level policies. We demonstrate feasibility in a case study
involving agent navigation amid moving obstacles.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13787" title="Abstract">arXiv:2402.13787</a> [<a href="/pdf/2402.13787" title="Download PDF">pdf</a>, <a href="/format/2402.13787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoica%2C+A">Ana-Andreea Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Litvak%2C+N">Nelly Litvak</a>, 
<a href="/search/cs?searchtype=author&query=Chaintreau%2C+A">Augustin Chaintreau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Proceedings of The Web Conference, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">In this paper, we investigate the conditions under which link analysis
algorithms prevent minority groups from reaching high ranking slots. We find
that the most common link-based algorithms using centrality metrics, such as
PageRank and HITS, can reproduce and even amplify bias against minority groups
in networks. Yet, their behavior differs: one one hand, we empirically show
that PageRank mirrors the degree distribution for most of the ranking positions
and it can equalize representation of minorities among the top ranked nodes; on
the other hand, we find that HITS amplifies pre-existing bias in homophilic
networks through a novel theoretical analysis, supported by empirical results.
We find the root cause of bias amplification in HITS to be the level of
homophily present in the network, modeled through an evolving network model
with two communities. We illustrate our theoretical analysis on both synthetic
and real datasets and we present directions for future work.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13788" title="Abstract">arXiv:2402.13788</a> [<a href="/pdf/2402.13788" title="Download PDF">pdf</a>, <a href="/format/2402.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unifying Theory for Runge--Kutta-like Time Integrators: Convergence  and Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Izgin%2C+T">Thomas Izgin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Doctoral thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The work deals with two major topics concerning the numerical analysis of
Runge-Kutta-like (RK-like) methods, namely their stability and order of
convergence. RK-like methods differ from additive RK methods in that their
coefficients are allowed to depend on the solution and the step size. As a
result of this, we also refer to them as non-standard additive RK (NSARK)
methods. The first major part of this thesis is dedicated to providing a tool
for deriving order conditions for NSARK methods. The proposed approach may
yield implicit order conditions, which can be rewritten in explicit form using
the NB-series of the stages. The obtained explicit order conditions can be
further reduced using Gr\"obner bases computations. With the presented
approach, it was possible for the first time to obtain conditions for the
construction of 3rd and 4th order GeCo as well as 4th order MPRK schemes.
Moreover, a new fourth order MPRK method is constructed using our theory and
the order of convergence is validated numerically. The second major part is
concerned with the stability of nonlinear time integrators preserving at least
one linear invariant. We discuss how the given approach generalizes the notion
of A-stability. We can prove that investigating the Jacobian of the generating
map is sufficient to understand the stability of the nonlinear method in a
neighborhood of the steady state. This approach allows for the first time the
investigation of several modified Patankar. In the case of MPRK schemes, we
compute a general stability function in a way that can be easily adapted to the
case of PDRS. Finally, the approach from the theory of dynamical systems is
used to derive a necessary condition for avoiding unrealistic oscillations of
the numerical approximation.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13791" title="Abstract">arXiv:2402.13791</a> [<a href="/pdf/2402.13791" title="Download PDF">pdf</a>, <a href="/format/2402.13791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opening the Black-Box: A Systematic Review on Explainable AI in Remote  Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B6hl%2C+A">Adrian H&#xf6;hl</a>, 
<a href="/search/cs?searchtype=author&query=Obadic%2C+I">Ivica Obadic</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+M+%C3%81+F">Miguel &#xc1;ngel Fern&#xe1;ndez Torres</a>, 
<a href="/search/cs?searchtype=author&query=Najjar%2C+H">Hiba Najjar</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+D">Dario Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, black-box machine learning approaches have become a dominant
modeling paradigm for knowledge extraction in Remote Sensing. Despite the
potential benefits of uncovering the inner workings of these models with
explainable AI, a comprehensive overview summarizing the used explainable AI
methods and their objectives, findings, and challenges in Remote Sensing
applications is still missing. In this paper, we address this issue by
performing a systematic review to identify the key trends of how explainable AI
is used in Remote Sensing and shed light on novel explainable AI approaches and
emerging directions that tackle specific Remote Sensing challenges. We also
reveal the common patterns of explanation interpretation, discuss the extracted
scientific insights in Remote Sensing, and reflect on the approaches used for
explainable AI methods evaluation. Our review provides a complete summary of
the state-of-the-art in the field. Further, we give a detailed outlook on the
challenges and promising research directions, representing a basis for novel
methodological development and a useful starting point for new researchers in
the field of explainable AI in Remote Sensing.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13792" title="Abstract">arXiv:2402.13792</a> [<a href="/pdf/2402.13792" title="Download PDF">pdf</a>, <a href="/format/2402.13792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Distributed Certification of Program Execution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolf%2C+A">Alex Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Palma%2C+M+E">Marco Eduardo Palma</a>, 
<a href="/search/cs?searchtype=author&query=Salza%2C+P">Pasquale Salza</a>, 
<a href="/search/cs?searchtype=author&query=Gall%2C+H+C">Harald C. Gall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Verifying the execution of a program is complicated and often limited by the
inability to validate the code's correctness. It is a crucial aspect of
scientific research, where it is needed to ensure the reproducibility and
validity of experimental results. Similarly, in customer software testing, it
is difficult for customers to verify that their specific program version was
tested or executed at all. Existing state-of-the-art solutions, such as
hardware-based approaches, constraint solvers, and verifiable computation
systems, do not provide definitive proof of execution, which hinders reliable
testing and analysis of program results. In this paper, we propose an
innovative approach that combines a prototype programming language called Mona
with a certification protocol OCCP to enable the distributed and decentralized
re-execution of program segments. Our protocol allows for certification of
program segments in a distributed, immutable, and trustworthy system without
the need for naive re-execution, resulting in significant improvements in terms
of time and computational resources used. We also explore the use of blockchain
technology to manage the protocol workflow following other approaches in this
space. Our approach offers a promising solution to the challenges of program
execution verification and opens up opportunities for further research and
development in this area. Our findings demonstrate the efficiency of our
approach in reducing the number of program executions compared to existing
state-of-the-art methods, thus improving the efficiency of certifying program
executions.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13796" title="Abstract">arXiv:2402.13796</a> [<a href="/pdf/2402.13796" title="Download PDF">pdf</a>, <a href="/format/2402.13796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Methods for Brick Kiln Detection and Compliance Monitoring from  Satellite Imagery: A Deployment Case Study in India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+R">Rishabh Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+Z+B">Zeel B Patel</a>, 
<a href="/search/cs?searchtype=author&query=Jani%2C+V">Vannsh Jani</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+N">Nipun Batra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Air pollution kills 7 million people annually. Brick manufacturing industry
is the second largest consumer of coal contributing to 8%-14% of air pollution
in Indo-Gangetic plain (highly populated tract of land in the Indian
subcontinent). As brick kilns are an unorganized sector and present in large
numbers, detecting policy violations such as distance from habitat is
non-trivial. Air quality and other domain experts rely on manual human
annotation to maintain brick kiln inventory. Previous work used computer vision
based machine learning methods to detect brick kilns from satellite imagery but
they are limited to certain geographies and labeling the data is laborious. In
this paper, we propose a framework to deploy a scalable brick kiln detection
system for large countries such as India and identify 7477 new brick kilns from
28 districts in 5 states in the Indo-Gangetic plain. We then showcase efficient
ways to check policy violations such as high spatial density of kilns and
abnormal increase over time in a region. We show that 90% of brick kilns in
Delhi-NCR violate a density-based policy. Our framework can be directly adopted
by the governments across the world to automate the policy regulations around
brick kilns.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13798" title="Abstract">arXiv:2402.13798</a> [<a href="/pdf/2402.13798" title="Download PDF">pdf</a>, <a href="/format/2402.13798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AFPR-CIM: An Analog-Domain Floating-Point RRAM-based Compute-In-Memory  Architecture with Dynamic Range Adaptive FP-ADC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Haobo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Qian%2C+Z">Zhengyang Qian</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+W">Wei Wu</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+H">Hongwei Ren</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+L">Leibin Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by DATE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Power consumption has become the major concern in neural network accelerators
for edge devices. The novel non-volatile-memory (NVM) based computing-in-memory
(CIM) architecture has shown great potential for better energy efficiency.
However, most of the recent NVM-CIM solutions mainly focus on fixed-point
calculation and are not applicable to floating-point (FP) processing. In this
paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM)
based on resistive random-access memory (RRAM). A novel adaptive dynamic-range
FP-ADC is designed to convert the analog computation results into FP codes.
Output current with high dynamic range is converted to a normalized voltage
range for readout, to prevent precision loss at low power consumption.
Moreover, a novel FP-DAC is also implemented which reconstructs FP digital
codes into analog values to perform analog computation. The proposed AFPR-CIM
architecture enables neural network acceleration with FP8 (E2M5) activation for
better accuracy and energy efficiency. Evaluation results show that AFPR-CIM
can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput.
Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM,
this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement,
respectively.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13800" title="Abstract">arXiv:2402.13800</a> [<a href="/pdf/2402.13800" title="Download PDF">pdf</a>, <a href="/ps/2402.13800" title="Download PostScript">ps</a>, <a href="/format/2402.13800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Geography of Information Diffusion in Online Discourse on Europe and  Migration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leonardelli%2C+E">Elisa Leonardelli</a>, 
<a href="/search/cs?searchtype=author&query=Tonelli%2C+S">Sara Tonelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The online diffusion of information related to Europe and migration has been
little investigated from an external point of view. However, this is a very
relevant topic, especially if users have had no direct contact with Europe and
its perception depends solely on information retrieved online. In this work we
analyse the information circulating online about Europe and migration after
retrieving a large amount of data from social media (Twitter), to gain new
insights into topics, magnitude, and dynamics of their diffusion. We combine
retweets and hashtags network analysis with geolocation of users, linking thus
data to geography and allowing analysis from an "outside Europe" perspective,
with a special focus on Africa. We also introduce a novel approach based on
cross-lingual quotes, i.e. when content in a language is commented and
retweeted in another language, assuming these interactions are a proxy for
connections between very distant communities. Results show how the majority of
online discussions occurs at a national level, especially when discussing
migration. Language (English) is pivotal for information to become
transnational and reach far. Transnational information flow is strongly
unbalanced, with content mainly produced in Europe and amplified outside.
Conversely Europe-based accounts tend to be self-referential when they discuss
migration-related topics. Football is the most exported topic from Europe
worldwide. Moreover, important nodes in the communities discussing
migration-related topics include accounts of official institutions and
international agencies, together with journalists, news, commentators and
activists.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13802" title="Abstract">arXiv:2402.13802</a> [<a href="/pdf/2402.13802" title="Download PDF">pdf</a>, <a href="/format/2402.13802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Checking Logical Actions in Magic Tricks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Weijun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures, 1 table and 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Some Magic Tricks (MT), such as many kinds of Card Magic (CM), consisting of
human computational or logical actions. How to ensure the logical correctness
of these MTs? In this paper, the Model Checking (MC) technique is employed to
study a typical CM via a case study. First, computational operations of a CM
called shousuigongcishi can be described by a Magic Algorithm (MAR). Second,
the logical correctness is portrayed by a temporal logic formula. On the basis
of it, this MT logical correctness problem is reduced to the model checking
problem. As a result, the Magic Trick Model Checking (MTMC) technique aims to
verify whether a designed MT meets its architect's anticipation and
requirements, or not, in terms of logic and computations.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13804" title="Abstract">arXiv:2402.13804</a> [<a href="/pdf/2402.13804" title="Download PDF">pdf</a>, <a href="/format/2402.13804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and  Switching Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matos%2C+S">S&#xe9;rgio Matos</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yihan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Deuermeier%2C+J">Jonas Deuermeier</a>, 
<a href="/search/cs?searchtype=author&query=Lucci%2C+L">Luca Lucci</a>, 
<a href="/search/cs?searchtype=author&query=Gavriilidis%2C+P">Panagiotis Gavriilidis</a>, 
<a href="/search/cs?searchtype=author&query=Kiazadeh%2C+A">Asal Kiazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Lain-Rubio%2C+V">Ver&#xf3;nica Lain-Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+T+D">Tung D. Phan</a>, 
<a href="/search/cs?searchtype=author&query=Soh%2C+P+J">Ping Jack Soh</a>, 
<a href="/search/cs?searchtype=author&query=Clemente%2C+A">Antonio Clemente</a>, 
<a href="/search/cs?searchtype=author&query=Pessoa%2C+L+M">Lu&#xed;s M. Pessoa</a>, 
<a href="/search/cs?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, submitted for a conference presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The demand for unprecedented performance in the upcoming 6G wireless networks
is fomenting the research on THz communications empowered by Reconfigurable
Inteligent Surfaces (RISs). A wide range of use cases have been proposed, most
of them, assuming high-level RIS models that overlook some of the hardware
impairments that this technology faces. The expectation is that the emergent
reconfigurable THz technologies will eventually overcome its current
limitations. This disassociation from the hardware may mask nonphysical
assumptions, perceived as hardware limitations. In this paper, a top-down
approach bounded by physical constraints is presented, distilling from
system-level specifications, hardware requirements, and upper bounds for the
RIS-aided system performance. We consider D-band indoor and outdoor scenarios
where a more realistic assessment of the state-of-the-art solution can be made.
The goal is to highlight the intricacies of the design procedure based on sound
assumptions for the RIS performance. For a given signal range and angular
coverage, we quantify the required RIS size, number of switching elements, and
maximum achievable bandwidth and capacity.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13809" title="Abstract">arXiv:2402.13809</a> [<a href="/pdf/2402.13809" title="Download PDF">pdf</a>, <a href="/format/2402.13809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual  Feature Guided Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Badong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reconstructing visual stimuli from functional Magnetic Resonance Imaging
(fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval
of the brain. A challenge persists in reconstructing a cohesive alignment of
details (such as structure, background, texture, color, etc.). Moreover, LDMs
would generate different image results even under the same conditions. For
these, we first uncover the neuroscientific perspective of LDM-based methods
that is top-down creation based on pre-trained knowledge from massive images
but lack of detail-driven bottom-up perception resulting in unfaithful details.
We propose NeuralDiffuser which introduces primary visual feature guidance to
provide detail cues in the form of gradients, extending the bottom-up process
for LDM-based methods to achieve faithful semantics and details. We also
developed a novel guidance strategy to ensure the consistency of repeated
reconstructions rather than a variety of results. We obtain the
state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset
(NSD), which offers more faithful details and consistent results.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13810" title="Abstract">arXiv:2402.13810</a> [<a href="/pdf/2402.13810" title="Download PDF">pdf</a>, <a href="/format/2402.13810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expected Loss of Preconditioned Langevin Dynamics Reveals the  Hessian Rank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bar%2C+A">Amitay Bar</a>, 
<a href="/search/cs?searchtype=author&query=Mulayoff%2C+R">Rotem Mulayoff</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>, 
<a href="/search/cs?searchtype=author&query=Talmon%2C+R">Ronen Talmon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI-24 main track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Langevin dynamics (LD) is widely used for sampling from distributions and for
optimization. In this work, we derive a closed-form expression for the expected
loss of preconditioned LD near stationary points of the objective function. We
use the fact that at the vicinity of such points, LD reduces to an
Ornstein-Uhlenbeck process, which is amenable to convenient mathematical
treatment. Our analysis reveals that when the preconditioning matrix satisfies
a particular relation with respect to the noise covariance, LD's expected loss
becomes proportional to the rank of the objective's Hessian. We illustrate the
applicability of this result in the context of neural networks, where the
Hessian rank has been shown to capture the complexity of the predictor function
but is usually computationally hard to probe. Finally, we use our analysis to
compare SGD-like and Adam-like preconditioners and identify the regimes under
which each of them leads to a lower expected loss.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13812" title="Abstract">arXiv:2402.13812</a> [<a href="/pdf/2402.13812" title="Download PDF">pdf</a>, <a href="/format/2402.13812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voice-Driven Mortality Prediction in Hospitalized Heart Failure  Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadli%2C+N">Nihat Ahmadli</a>, 
<a href="/search/cs?searchtype=author&query=Sarsil%2C+M+A">Mehmet Ali Sarsil</a>, 
<a href="/search/cs?searchtype=author&query=Mizrak%2C+B">Berk Mizrak</a>, 
<a href="/search/cs?searchtype=author&query=Karauzum%2C+K">Kurtulus Karauzum</a>, 
<a href="/search/cs?searchtype=author&query=Shaker%2C+A">Ata Shaker</a>, 
<a href="/search/cs?searchtype=author&query=Tulumen%2C+E">Erol Tulumen</a>, 
<a href="/search/cs?searchtype=author&query=Mirzamidinov%2C+D">Didar Mirzamidinov</a>, 
<a href="/search/cs?searchtype=author&query=Ural%2C+D">Dilek Ural</a>, 
<a href="/search/cs?searchtype=author&query=Ergen%2C+O">Onur Ergen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, 5 tables. THe first 2 authors have contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Addressing heart failure (HF) as a prevalent global health concern poses
difficulties in implementing innovative approaches for enhanced patient care.
Predicting mortality rates in HF patients, in particular, is difficult yet
critical, necessitating individualized care, proactive management, and enabling
educated decision-making to enhance outcomes. Recently, the significance of
voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating
remarkable efficacy, particularly in predicting heart failure. The synergy of
voice analysis and ML algorithms provides a non-invasive and easily accessible
means to evaluate patients' health. However, there is a lack of voice
biomarkers for predicting mortality rates among heart failure patients with
standardized speech protocols. Here, we demonstrate a powerful and effective ML
model for predicting mortality rates in hospitalized HF patients through the
utilization of voice biomarkers. By seamlessly integrating voice biomarkers
into routine patient monitoring, this strategy has the potential to improve
patient outcomes, optimize resource allocation, and advance patient-centered HF
management. In this study, a Machine Learning system, specifically a logistic
regression model, is trained to predict patients' 5-year mortality rates using
their speech as input. The model performs admirably and consistently, as
demonstrated by cross-validation and statistical approaches (p-value &lt; 0.001).
Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the
model's predictive accuracy substantially.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13815" title="Abstract">arXiv:2402.13815</a> [<a href="/pdf/2402.13815" title="Download PDF">pdf</a>, <a href="/format/2402.13815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Oculus Virtual Reality Applications: Security and  Privacy Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hanyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hong-Ning Dai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiapu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gengyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+F">Fengliang He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Although Virtual Reality (VR) has accelerated its prevalent adoption in
emerging metaverse applications, it is not a fundamentally new technology. On
one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.
As a result, VR apps also inherit privacy and security deficiencies from
conventional mobile apps. On the other hand, in contrast to conventional mobile
apps, VR apps can achieve immersive experience via diverse VR devices, such as
head-mounted displays, body sensors, and controllers though achieving this
requires the extensive collection of privacy-sensitive human biometrics.
Moreover, VR apps have been typically implemented by 3D gaming engines (e.g.,
Unity), which also contain intrinsic security vulnerabilities. Inappropriate
use of these technologies may incur privacy leaks and security vulnerabilities
although these issues have not received significant attention compared to the
proliferation of diverse VR apps. In this paper, we develop a security and
privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP
detector has integrated program static analysis tools and privacy-policy
analysis methods. Using the VR-SP detector, we conduct a comprehensive
empirical study on 500 popular VR apps. We obtain the original apps from the
popular Oculus and SideQuest app stores and extract APK files via the Meta
Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data
leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy
analysis. We find that a number of security vulnerabilities and privacy leaks
widely exist in VR apps. Moreover, our results also reveal conflicting
representations in the privacy policies of these apps and inconsistencies of
the actual data collection with the privacy-policy statements of the apps.
Based on these findings, we make suggestions for the future development of VR
apps.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13816" title="Abstract">arXiv:2402.13816</a> [<a href="/pdf/2402.13816" title="Download PDF">pdf</a>, <a href="/format/2402.13816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified framework of non-local parametric methods for image denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbreteau%2C+S">S&#xe9;bastien Herbreteau</a>, 
<a href="/search/cs?searchtype=author&query=Kervrann%2C+C">Charles Kervrann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2203.00570">arXiv:2203.00570</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a unified view of non-local methods for single-image denoising,
for which BM3D is the most popular representative, that operate by gathering
noisy patches together according to their similarities in order to process them
collaboratively. Our general estimation framework is based on the minimization
of the quadratic risk, which is approximated in two steps, and adapts to photon
and electronic noises. Relying on unbiased risk estimation (URE) for the first
step and on ``internal adaptation'', a concept borrowed from deep learning
theory, for the second, we show that our approach enables to reinterpret and
reconcile previous state-of-the-art non-local methods. Within this framework,
we propose a novel denoiser called NL-Ridge that exploits linear combinations
of patches. While conceptually simpler, we show that NL-Ridge can outperform
well-established state-of-the-art single-image denoisers.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13817" title="Abstract">arXiv:2402.13817</a> [<a href="/pdf/2402.13817" title="Download PDF">pdf</a>, <a href="/format/2402.13817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in  Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+L">Lukas Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Abate%2C+M">Marcus Abate</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Carlone%2C+L">Luca Carlone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code to be released at: <a href="https://github.com/MIT-SPARK/Khronos">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Perceiving and understanding highly dynamic and changing environments is a
crucial capability for robot autonomy. While large strides have been made
towards developing dynamic SLAM approaches that estimate the robot pose
accurately, a lesser emphasis has been put on the construction of dense
spatio-temporal representations of the robot environment. A detailed
understanding of the scene and its evolution through time is crucial for
long-term robot autonomy and essential to tasks that require long-term
reasoning, such as operating effectively in environments shared with humans and
other agents and thus are subject to short and long-term dynamics. To address
this challenge, this work defines the Spatio-temporal Metric-semantic SLAM
(SMS) problem, and presents a framework to factorize and solve it efficiently.
We show that the proposed factorization suggests a natural organization of a
spatio-temporal perception system, where a fast process tracks short-term
dynamics in an active temporal window, while a slower process reasons over
long-term changes in the environment using a factor graph formulation. We
provide an efficient implementation of the proposed spatio-temporal perception
approach, that we call Khronos, and show that it unifies exiting
interpretations of short-term and long-term dynamics and is able to construct a
dense spatio-temporal map in real-time. We provide simulated and real results,
showing that the spatio-temporal maps built by Khronos are an accurate
reflection of a 3D scene over time and that Khronos outperforms baselines
across multiple metrics. We further validate our approach on two heterogeneous
robots in challenging, large-scale real-world environments.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13818" title="Abstract">arXiv:2402.13818</a> [<a href="/pdf/2402.13818" title="Download PDF">pdf</a>, <a href="/format/2402.13818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Hate Speech: NLP&#x27;s Challenges and Opportunities in Uncovering  Dehumanizing Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hezhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+L">Lasana Harris</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dehumanization, characterized as a subtle yet harmful manifestation of hate
speech, involves denying individuals of their human qualities and often results
in violence against marginalized groups. Despite significant progress in
Natural Language Processing across various domains, its application in
detecting dehumanizing language is limited, largely due to the scarcity of
publicly available annotated data for this domain. This paper evaluates the
performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2,
in identifying dehumanizing language. Our findings reveal that while these
models demonstrate potential, achieving a 70\% accuracy rate in distinguishing
dehumanizing language from broader hate speech, they also display biases. They
are over-sensitive in classifying other forms of hate speech as dehumanization
for a specific subset of target groups, while more frequently failing to
identify clear cases of dehumanization for other target groups. Moreover,
leveraging one of the best-performing models, we automatically annotated a
larger dataset for training more accessible models. However, our findings
indicate that these models currently do not meet the high-quality data
generation threshold necessary for this task.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13820" title="Abstract">arXiv:2402.13820</a> [<a href="/pdf/2402.13820" title="Download PDF">pdf</a>, <a href="/format/2402.13820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLD: Fourier Latent Dynamics for Structured Motion Representation and  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Stanger-Jones%2C+E">Elijah Stanger-Jones</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+S">Steve Heim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangbae Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">Motion trajectories offer reliable references for physics-based motion
learning but suffer from sparsity, particularly in regions that lack sufficient
data coverage. To address this challenge, we introduce a self-supervised,
structured representation and generation method that extracts spatial-temporal
relationships in periodic or quasi-periodic motions. The motion dynamics in a
continuously parameterized latent space enable our method to enhance the
interpolation and generalization capabilities of motion learning algorithms.
The motion learning controller, informed by the motion parameterization,
operates online tracking of a wide range of motions, including targets unseen
during training. With a fallback mechanism, the controller dynamically adapts
its tracking strategy and automatically resorts to safe action execution when a
potentially risky target is proposed. By leveraging the identified
spatial-temporal structure, our work opens new possibilities for future
advancements in general motion representation and learning algorithms.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13821" title="Abstract">arXiv:2402.13821</a> [<a href="/pdf/2402.13821" title="Download PDF">pdf</a>, <a href="/ps/2402.13821" title="Download PostScript">ps</a>, <a href="/format/2402.13821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Improvement Bounds for Lipschitz Configurable Markov  Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Configurable Markov Decision Processes (Conf-MDPs) have recently been
introduced as an extension of the traditional Markov Decision Processes (MDPs)
to model the real-world scenarios in which there is the possibility to
intervene in the environment in order to configure some of its parameters. In
this paper, we focus on a particular subclass of Conf-MDP that satisfies
regularity conditions, namely Lipschitz continuity. We start by providing a
bound on the Wasserstein distance between $\gamma$-discounted stationary
distributions induced by changing policy and configuration. This result
generalizes the already existing bounds both for Conf-MDPs and traditional
MDPs. Then, we derive a novel performance improvement lower bound.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13822" title="Abstract">arXiv:2402.13822</a> [<a href="/pdf/2402.13822" title="Download PDF">pdf</a>, <a href="/format/2402.13822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSTAR: Multi-Scale Backbone Architecture Search for Timeseries  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+T+M">Tue M. Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+H">Nhat H. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H+H">Hieu H. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+T">Hung T. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+P">Le P. Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most of the previous approaches to Time Series Classification (TSC) highlight
the significance of receptive fields and frequencies while overlooking the time
resolution. Hence, unavoidably suffered from scalability issues as they
integrated an extensive range of receptive fields into classification models.
Other methods, while having a better adaptation for large datasets, require
manual design and yet not being able to reach the optimal architecture due to
the uniqueness of each dataset. We overcome these challenges by proposing a
novel multi-scale search space and a framework for Neural architecture search
(NAS), which addresses both the problem of frequency and time resolution,
discovering the suitable scale for a specific dataset. We further show that our
model can serve as a backbone to employ a powerful Transformer module with both
untrained and pre-trained weights. Our search space reaches the
state-of-the-art performance on four datasets on four different domains while
introducing more than ten highly fine-tuned models for each data.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13823" title="Abstract">arXiv:2402.13823</a> [<a href="/pdf/2402.13823" title="Download PDF">pdf</a>, <a href="/format/2402.13823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models for Natural Language Processing Tasks in  Requirements Engineering: A Systematic Guideline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vogelsang%2C+A">Andreas Vogelsang</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">To use Large Language Models (LLMs) in a targeted way for NLP problems in RE,
we require both (1) basic knowledge about the inner workings of LLMs and (2) a
guideline on how to select and systematically utilize or repurpose LLMs for
NLP4RE tasks. This chapter establishes the required knowledge and introduces
the fundamentals of LLMs in the first part. In the second part, we present a
detailed guideline for students, researchers, and practitioners on using LLMs
for their purposes.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13824" title="Abstract">arXiv:2402.13824</a> [<a href="/pdf/2402.13824" title="Download PDF">pdf</a>, <a href="/ps/2402.13824" title="Download PostScript">ps</a>, <a href="/format/2402.13824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Contract Design beyond Binary Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cacciamani%2C+F">Federico Cacciamani</a>, 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+M">Martino Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study hidden-action principal-agent problems with multiple agents. Unlike
previous work, we consider a general setting in which each agent has an
arbitrary number of actions, and the joint action induces outcomes according to
an arbitrary distribution. We study two classes of mechanisms: a class of
deterministic mechanisms that is the natural extension of single-agent
contracts, in which the agents play a Nash equilibrium of the game induced by
the contract, and a class of randomized mechanisms that is inspired by
single-agent randomized contracts and correlated equilibria.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13827" title="Abstract">arXiv:2402.13827</a> [<a href="/pdf/2402.13827" title="Download PDF">pdf</a>, <a href="/ps/2402.13827" title="Download PostScript">ps</a>, <a href="/format/2402.13827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+J">Joongho Jo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeongwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jongsun Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms
the neural radiance field (NeRF) in terms of both speed and image quality.
3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects
these Gaussians onto the 2D image plane for rendering. However, during the
rendering process, a substantial number of unnecessary 3D Gaussians exist for
the current view direction, resulting in significant computation costs
associated with their identification. In this paper, we propose a computational
reduction technique that quickly identifies unnecessary 3D Gaussians in
real-time for rendering the current view without compromising image quality.
This is accomplished through the offline clustering of 3D Gaussians that are
close in distance, followed by the projection of these clusters onto a 2D image
plane during runtime. Additionally, we analyze the bottleneck associated with
the proposed technique when executed on GPUs and propose an efficient hardware
architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360
dataset, the proposed technique excludes 63% of 3D Gaussians on average before
the 2D image projection, which reduces the overall rendering computation by
almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The
proposed accelerator also achieves a speedup of 10.7x compared to a GPU.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13828" title="Abstract">arXiv:2402.13828</a> [<a href="/pdf/2402.13828" title="Download PDF">pdf</a>, <a href="/format/2402.13828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Origami: (un)folding the abstraction of recursion schemes for program  synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+M+C">Matheus Campos Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=de+Franca%2C+F+O">Fabricio Olivetti de Franca</a>, 
<a href="/search/cs?searchtype=author&query=Francesquini%2C+E">Emilio Francesquini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Program synthesis with Genetic Programming searches for a correct program
that satisfies the input specification, which is usually provided as
input-output examples. One particular challenge is how to effectively handle
loops and recursion avoiding programs that never terminate. A helpful
abstraction that can alleviate this problem is the employment of Recursion
Schemes that generalize the combination of data production and consumption.
Recursion Schemes are very powerful as they allow the construction of programs
that can summarize data, create sequences, and perform advanced calculations.
The main advantage of writing a program using Recursion Schemes is that the
programs are composed of well defined templates with only a few parts that need
to be synthesized. In this paper we make an initial study of the benefits of
using program synthesis with fold and unfold templates, and outline some
preliminary experimental results. To highlight the advantages and disadvantages
of this approach, we manually solved the entire GPSB benchmark using recursion
schemes, highlighting the parts that should be evolved compared to alternative
implementations. We noticed that, once the choice of which recursion scheme is
made, the synthesis process can be simplified as each of the missing parts of
the template are reduced to simpler functions, which are further constrained by
their own input and output types.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13831" title="Abstract">arXiv:2402.13831</a> [<a href="/pdf/2402.13831" title="Download PDF">pdf</a>, <a href="/format/2402.13831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLXP: A framework for conducting replicable Machine Learning eXperiments  in Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arbel%2C+M">Michael Arbel</a>, 
<a href="/search/cs?searchtype=author&query=Zouaoui%2C+A">Alexandre Zouaoui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Replicability in machine learning (ML) research is increasingly concerning
due to the utilization of complex non-deterministic algorithms and the
dependence on numerous hyper-parameter choices, such as model architecture and
training datasets. Ensuring reproducible and replicable results is crucial for
advancing the field, yet often requires significant technical effort to conduct
systematic and well-organized experiments that yield robust conclusions.
Several tools have been developed to facilitate experiment management and
enhance reproducibility; however, they often introduce complexity that hinders
adoption within the research community, despite being well-handled in
industrial settings. To address the challenge of low adoption, we propose MLXP,
an open-source, simple, and lightweight experiment management tool based on
Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the
experimental process with minimal practitioner overhead while ensuring a high
level of reproducibility.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13837" title="Abstract">arXiv:2402.13837</a> [<a href="/pdf/2402.13837" title="Download PDF">pdf</a>, <a href="/format/2402.13837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of a Miniature Underwater Vehicle and Data Collection System for  Indoor Experimentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbert%2C+J">Jacob Herbert</a>, 
<a href="/search/cs?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, To be presented at IEEE SoutheastCon 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper describes the design of a miniature uncrewed underwater vehicle
(MiniUUV) and related instrumentation for indoor experimentation. The MiniUUV
was developed using 3D printed components and low-cost, off-the-shelf
electronics. The vehicle uses a propeller differential propulsion drive and a
peristaltic pump with a syringe for buoyancy control. A water tank with an
overhead camera system was constructed to allow for convenient indoor data
collection in a controlled environment. Several tests were conducted to
demonstrate the capabilities of the MiniUUV and data collection system,
including buoyancy pump actuation tests and straight line, circular, and
zig-zag motion tests on the surface. During each planar motion test an AprilTag
was attached to the MiniUUV and an overhead camera system obtained video
recordings that were processed offline to estimate vehicle position, surge
velocity, sway velocity, yaw angle, and yaw rate.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13840" title="Abstract">arXiv:2402.13840</a> [<a href="/pdf/2402.13840" title="Download PDF">pdf</a>, <a href="/format/2402.13840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4SBR: A Lightweight and Effective Framework for Integrating Large  Language Models in Session-based Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shutong Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Junhao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional session-based recommendation (SBR) utilizes session behavior
sequences from anonymous users for recommendation. Although this strategy is
highly efficient, it sacrifices the inherent semantic information of the items,
making it difficult for the model to understand the true intent of the session
and resulting in a lack of interpretability in the recommended results.
Recently, large language models (LLMs) have flourished across various domains,
offering a glimpse of hope in addressing the aforementioned challenges.
Inspired by the impact of LLMs, research exploring the integration of LLMs with
the Recommender system (RS) has surged like mushrooms after rain. However,
constrained by high time and space costs, as well as the brief and anonymous
nature of session data, the first LLM recommendation framework suitable for
industrial deployment has yet to emerge in the field of SBR. To address the
aforementioned challenges, we have proposed the LLM Integration Framework for
SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR
adopts a two-step strategy. Firstly, we transform session data into a bimodal
form of text and behavior. In the first step, leveraging the inferential
capabilities of LLMs, we conduct inference on session text data from different
perspectives and design the component for auxiliary enhancement. In the second
step, the SBR model is trained on behavior data, aligning and averaging two
modal session representations from different perspectives. Finally, we fuse
session representations from different perspectives and modalities as the
ultimate session representation for recommendation. We conducted experiments on
two real-world datasets, and the results demonstrate that LLM4SBR significantly
improves the performance of traditional SBR models and is highly lightweight
and efficient, making it suitable for industrial deployment.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13841" title="Abstract">arXiv:2402.13841</a> [<a href="/pdf/2402.13841" title="Download PDF">pdf</a>, <a href="/format/2402.13841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equilibria, Efficiency, and Inequality in Network Formation for Hiring  and Opportunity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwork%2C+C">Cynthia Dwork</a>, 
<a href="/search/cs?searchtype=author&query=Hays%2C+C">Chris Hays</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+M">Manish Raghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Professional networks -- the social networks among people in a given line of
work -- can serve as a conduit for job prospects and other opportunities. Here
we propose a model for the formation of such networks and the transfer of
opportunities within them. In our theoretical model, individuals strategically
connect with others to maximize the probability that they receive opportunities
from them. We explore how professional networks balance connectivity, where
connections facilitate opportunity transfers to those who did not get them from
outside sources, and congestion, where some individuals receive too many
opportunities from their connections and waste some of them.
<br />We show that strategic individuals are over-connected at equilibrium relative
to a social optimum, leading to a price of anarchy for which we derive nearly
tight asymptotic bounds. We also show that, at equilibrium, individuals form
connections to those who provide similar benefit to them as they provide to
others. Thus, our model provides a microfoundation in professional networking
contexts for the fundamental sociological principle of homophily, that
"similarity breeds connection," which in our setting is realized as a form of
status homophily based on alignment in individual benefit. We further explore
how, even if individuals are a priori equally likely to receive opportunities
from outside sources, equilibria can be unequal, and we provide nearly tight
bounds on how unequal they can be. Finally, we explore the ability for online
platforms to intervene to improve social welfare and show that natural
heuristics may result in adverse effects at equilibrium. Our simple model
allows for a surprisingly rich analysis of coordination problems in
professional networks and suggests many directions for further exploration.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13845" title="Abstract">arXiv:2402.13845</a> [<a href="/pdf/2402.13845" title="Download PDF">pdf</a>, <a href="/ps/2402.13845" title="Download PostScript">ps</a>, <a href="/format/2402.13845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+den+Akker%2C+E">Erik van den Akker</a>, 
<a href="/search/cs?searchtype=author&query=Buchin%2C+K">Kevin Buchin</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+K">Klaus-Tycho Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We study the problem of multi-agent online graph exploration, in which a team
of k agents has to explore a given graph, starting and ending on the same node.
The graph is initially unknown. Whenever a node is visited by an agent, its
neighborhood and adjacent edges are revealed. The agents share a global view of
the explored parts of the graph. The cost of the exploration has to be
minimized, where cost either describes the time needed for the entire
exploration (time model), or the length of the longest path traversed by any
agent (energy model). We investigate graph exploration on cycles and tadpole
graphs for 2-4 agents, providing optimal results on the competitive ratio in
the energy model (1-competitive with two agents on cycles and three agents on
tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with
four agents). We also show competitive upper bounds of 2 for the exploration of
tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs
with two agents in the time model.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13846" title="Abstract">arXiv:2402.13846</a> [<a href="/pdf/2402.13846" title="Download PDF">pdf</a>, <a href="/format/2402.13846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Advanced Anonymizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Staab%2C+R">Robin Staab</a>, 
<a href="/search/cs?searchtype=author&query=Vero%2C+M">Mark Vero</a>, 
<a href="/search/cs?searchtype=author&query=Balunovi%C4%87%2C+M">Mislav Balunovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work in privacy research on large language models has shown that they
achieve near human-level performance at inferring personal data from real-world
online texts. With consistently increasing model capabilities, existing text
anonymization methods are currently lacking behind regulatory requirements and
adversarial threats. This raises the question of how individuals can
effectively protect their personal data in sharing online texts. In this work,
we take two steps to answer this question: We first present a new setting for
evaluating anonymizations in the face of adversarial LLMs inferences, allowing
for a natural measurement of anonymization performance while remedying some of
the shortcomings of previous metrics. We then present our LLM-based adversarial
anonymization framework leveraging the strong inferential capabilities of LLMs
to inform our anonymization procedure. In our experimental evaluation, we show
on real-world and synthetic online texts how adversarial anonymization
outperforms current industry-grade anonymizers both in terms of the resulting
utility and privacy.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13848" title="Abstract">arXiv:2402.13848</a> [<a href="/pdf/2402.13848" title="Download PDF">pdf</a>, <a href="/format/2402.13848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monaci%2C+G">Gianluca Monaci</a>, 
<a href="/search/cs?searchtype=author&query=Antsfeld%2C+L">Leonid Antsfeld</a>, 
<a href="/search/cs?searchtype=author&query=Chidlovskii%2C+B">Boris Chidlovskii</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+C">Christian Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Bird's-eye view (BEV) maps are an important geometrically structured
representation widely used in robotics, in particular self-driving vehicles and
terrestrial robots. Existing algorithms either require depth information for
the geometric projection, which is not always reliably available, or are
trained end-to-end in a fully supervised way to map visual first-person
observations to BEV representation, and are therefore restricted to the output
modality they have been trained for. In contrast, we propose a new model
capable of performing zero-shot projections of any modality available in a
first person view to the corresponding BEV map. This is achieved by
disentangling the geometric inverse perspective projection from the modality
transformation, eg. RGB to occupancy. The method is general and we showcase
experiments projecting to BEV three different modalities: semantic
segmentation, motion vectors and object bounding boxes detected in first
person. We experimentally show that the model outperforms competing methods, in
particular the widely used baseline resorting to monocular depth estimation.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13851" title="Abstract">arXiv:2402.13851</a> [<a href="/pdf/2402.13851" title="Download PDF">pdf</a>, <a href="/format/2402.13851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiawei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Man Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongchen Han</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ee-Chien Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Autoregressive Visual Language Models (VLMs) showcase impressive few-shot
learning capabilities in a multimodal context. Recently, multimodal instruction
tuning has been proposed to further enhance instruction-following abilities.
However, we uncover the potential threat posed by backdoor attacks on
autoregressive VLMs during instruction tuning. Adversaries can implant a
backdoor by injecting poisoned samples with triggers embedded in instructions
or images, enabling malicious manipulation of the victim model's predictions
with predefined triggers. Nevertheless, the frozen visual encoder in
autoregressive VLMs imposes constraints on the learning of conventional image
triggers. Additionally, adversaries may encounter restrictions in accessing the
parameters and architectures of the victim model. To address these challenges,
we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our
approach facilitates image trigger learning through an isolating and clustering
strategy and enhance black-box-attack efficacy via an iterative character-level
text trigger generation method. Our attack successfully induces target outputs
during inference, significantly surpassing baselines (+62.52\%) in ASR.
Moreover, it demonstrates robustness across various model scales and few-shot
in-context reasoning scenarios.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13852" title="Abstract">arXiv:2402.13852</a> [<a href="/pdf/2402.13852" title="Download PDF">pdf</a>, <a href="/format/2402.13852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Control System for Continuous Glucose Monitoring and Maintenance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wasi%2C+A+T">Azmine Toushik Wasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 4 figures, ICLR 2024 Tiny Papers Track <a href="https://openreview.net/forum?id=Te4P3Cn54g">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Second Tiny Papers Track at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Precise glucose level management is pivotal for individuals with diabetes,
averting severe complications. In this work, we introduce a novel neural
control system for continuous glucose monitoring and maintenance, utilizing
differential predictive control. Our system, guided by a sophisticated neural
policy and differentiable modeling, dynamically adjusts insulin delivery in
real-time, enhancing glucose optimization. This end-to-end approach maximizes
efficiency, ensuring personalized care and improved health outcomes, as
affirmed by empirical findings.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13853" title="Abstract">arXiv:2402.13853</a> [<a href="/pdf/2402.13853" title="Download PDF">pdf</a>, <a href="/format/2402.13853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RealDex: Towards Human-like Grasping for Robotic Dexterous Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yumeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaxun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Youzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaofei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiamin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yichen Yao</a>, 
<a href="/search/cs?searchtype=author&query=Schwertfeger%2C+S">S&#xf6;ren Schwertfeger</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sibei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuming He</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we introduce RealDex, a pioneering dataset capturing authentic
dexterous hand grasping motions infused with human behavioral patterns,
enriched by multi-view and multimodal visual data. Utilizing a teleoperation
system, we seamlessly synchronize human-robot hand poses in real time. This
collection of human-like motions is crucial for training dexterous hands to
mimic human movements more naturally and precisely. RealDex holds immense
promise in advancing humanoid robot for automated perception, cognition, and
manipulation in real-world scenarios. Moreover, we introduce a cutting-edge
dexterous grasping motion generation framework, which aligns with human
experience and enhances real-world applicability through effectively utilizing
Multimodal Large Language Models. Extensive experiments have demonstrated the
superior performance of our method on RealDex and other open datasets. The
complete dataset and code will be made available upon the publication of this
work.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13855" title="Abstract">arXiv:2402.13855</a> [<a href="/pdf/2402.13855" title="Download PDF">pdf</a>, <a href="/format/2402.13855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What we can learn from TikTok through its Research API
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+F">Francesco Corso</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=De+Francisci+Morales%2C+G">Gianmarco De Francisci Morales</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 Figures, submitted to WWW'24 - Short paper track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">TikTok is a social media platform that has gained immense popularity over the
last few years, particularly among younger demographics, due to the viral
trends and challenges shared worldwide. The recent release of a free Research
API opens doors to collect data on posted videos, associated comments, and user
activities. Our study focuses on evaluating the reliability of results returned
by the Research API, by collecting and analyzing a random sample of TikTok
videos posted in a span of 6 years. Our preliminary results are instrumental
for future research that aims to study the platform, highlighting caveats on
the geographical distribution of videos and on the global prevalence of viral
hashtags.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13857" title="Abstract">arXiv:2402.13857</a> [<a href="/pdf/2402.13857" title="Download PDF">pdf</a>, <a href="/ps/2402.13857" title="Download PostScript">ps</a>, <a href="/format/2402.13857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicable Learning of Large-Margin Halfspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalavasis%2C+A">Alkis Kalavasis</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+K+G">Kasper Green Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Velegkas%2C+G">Grigoris Velegkas</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Felix Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We provide efficient replicable algorithms for the problem of learning
large-margin halfspaces. Our results improve upon the algorithms provided by
Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first
dimension-independent replicable algorithms for this task which runs in
polynomial time, is proper, and has strictly improved sample complexity
compared to the one achieved by Impagliazzo et al. [2022] with respect to all
the relevant parameters. Moreover, our first algorithm has sample complexity
that is optimal with respect to the accuracy parameter $\epsilon$. We also
design an SGD-based replicable algorithm that, in some parameters' regimes,
achieves better sample and time complexity than our first algorithm.
<br />Departing from the requirement of polynomial time algorithms, using the
DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei,
Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a
replicable algorithm for large-margin halfspaces with improved sample
complexity with respect to the margin parameter $\tau$, but running time doubly
exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$
than one of our previous algorithms. We then design an improved algorithm with
better sample complexity than all three of our previous algorithms and running
time exponential in $1/\tau^{2}$.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13858" title="Abstract">arXiv:2402.13858</a> [<a href="/pdf/2402.13858" title="Download PDF">pdf</a>, <a href="/format/2402.13858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity-Aware $k$-Maximum Inner Product Search Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiqun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+A+K+H">Anthony K. H. Tung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures, and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational
component in recommender systems and various data mining tasks. However, while
most existing $k$MIPS approaches prioritize the efficient retrieval of highly
relevant items for users, they often neglect an equally pivotal facet of search
results: \emph{diversity}. To bridge this gap, we revisit and refine the
diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known
diversity objectives -- minimizing the average and maximum pairwise item
similarities within the results -- into the original relevance objective. This
enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a
controllable trade-off between relevance and diversity. We introduce
\textsc{Greedy} and \textsc{DualGreedy}, two linear scan-based algorithms
tailored for D$k$MIPS. They both achieve data-dependent approximations and,
when aiming to minimize the average pairwise similarity, \textsc{DualGreedy}
attains an approximation ratio of $1/4$ with an additive term for
regularization. To further improve query efficiency, we integrate a lightweight
Ball-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive
experiments on ten real-world data sets demonstrate the efficacy of our
proposed methods, showcasing their capability to efficiently deliver diverse
and relevant search results to users.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13861" title="Abstract">arXiv:2402.13861</a> [<a href="/pdf/2402.13861" title="Download PDF">pdf</a>, <a href="/format/2402.13861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Efficiency of Iso-Surface Extraction on Implicit Neural  Representations Using Uncertainty Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Han-Wei Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Visualization and Computer Graphics, presented in VIS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Implicit Neural representations (INRs) are widely used for scientific data
reduction and visualization by modeling the function that maps a spatial
location to a data value. Without any prior knowledge about the spatial
distribution of values, we are forced to sample densely from INRs to perform
visualization tasks like iso-surface extraction which can be very
computationally expensive. Recently, range analysis has shown promising results
in improving the efficiency of geometric queries, such as ray casting and
hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic
rules to bound the output range of the network within a spatial region.
However, the analysis bounds are often too conservative for complex scientific
data. In this paper, we present an improved technique for range analysis by
revisiting the arithmetic rules and analyzing the probability distribution of
the network output within a spatial region. We model this distribution
efficiently as a Gaussian distribution by applying the central limit theorem.
Excluding low probability values, we are able to tighten the output bounds,
resulting in a more accurate estimation of the value range, and hence more
accurate identification of iso-surface cells and more efficient iso-surface
extraction on INRs. Our approach demonstrates superior performance in terms of
the iso-surface extraction time on four datasets compared to the original range
analysis method and can also be generalized to other geometric query tasks.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13866" title="Abstract">arXiv:2402.13866</a> [<a href="/pdf/2402.13866" title="Download PDF">pdf</a>, <a href="/format/2402.13866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kuaiji: the First Chinese Accounting Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiayuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songhua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xiaoling Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Panyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nai%2C+Y">Yufei Nai</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenxuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinke Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> version 1.0
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated
impressive proficiency in comprehending and generating natural language.
However, they encounter difficulties when tasked with adapting to specialized
domains such as accounting. To address this challenge, we introduce Kuaiji, a
tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned
using the Baichuan framework, which encompasses continuous pre-training and
supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing
large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy
and response speed. Our contributions encompass the creation of the first
Chinese accounting dataset, the establishment of Kuaiji as a leading
open-source Chinese accounting LLM, and the validation of its efficacy through
real-world accounting scenarios.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13868" title="Abstract">arXiv:2402.13868</a> [<a href="/pdf/2402.13868" title="Download PDF">pdf</a>, <a href="/format/2402.13868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Uniformly Random Solution to Algorithmic Redistricting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jin-Yi Cai</a>, 
<a href="/search/cs?searchtype=author&query=Kruse%2C+J">Jacob Kruse</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+K">Kenneth Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Szabo%2C+D+P">Daniel P. Szabo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">The process of drawing electoral district boundaries is known as political
redistricting. Within this context, gerrymandering is the practice of drawing
these boundaries such that they unfairly favor a particular political party,
often leading to unequal representation and skewed electoral outcomes. One of
the few ways to detect gerrymandering is by algorithmically sampling
redistricting plans. Previous methods mainly focus on sampling from some
neighborhood of ``realistic' districting plans, rather than a uniform sample of
the entire space. We present a deterministic subexponential time algorithm to
uniformly sample from the space of all possible $ k $-partitions of a bounded
degree planar graph, and with this construct a sample of the entire space of
redistricting plans. We also give a way to restrict this sample space to plans
that match certain compactness and population constraints at the cost of added
complexity. The algorithm runs in $ 2^{O(\sqrt{n}\log n)} $ time, although we
only give a heuristic implementation. Our method generalizes an algorithm to
count self-avoiding walks on a square to count paths that split general planar
graphs into $ k $ regions, and uses this to sample from the space of all $ k
$-partitions of a planar graph.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13870" title="Abstract">arXiv:2402.13870</a> [<a href="/pdf/2402.13870" title="Download PDF">pdf</a>, <a href="/ps/2402.13870" title="Download PostScript">ps</a>, <a href="/format/2402.13870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+L">Lang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CISS 2024. arXiv admin note: text overlap with <a href="/abs/2306.03782">arXiv:2306.03782</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Applications (stat.AP)

</div>
<p class="mathjax">Generative probabilistic forecasting produces future time series samples
according to the conditional probability distribution given past time series
observations. Such techniques are essential in risk-based decision-making and
planning under uncertainty with broad applications in grid operations,
including electricity price forecasting, risk-based economic dispatch, and
stochastic optimizations. Inspired by Wiener and Kallianpur's innovation
representation, we propose a weak innovation autoencoder architecture and a
learning algorithm to extract independent and identically distributed
innovation sequences from nonparametric stationary time series. We show that
the weak innovation sequence is Bayesian sufficient, which makes the proposed
weak innovation autoencoder a canonical architecture for generative
probabilistic forecasting. The proposed technique is applied to forecasting
highly volatile real-time electricity prices, demonstrating superior
performance across multiple forecasting measures over leading probabilistic and
point forecasting techniques.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13871" title="Abstract">arXiv:2402.13871</a> [<a href="/pdf/2402.13871" title="Download PDF">pdf</a>, <a href="/format/2402.13871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+A">Mohammad Amaz Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Sarker%2C+I+H">Iqbal H. Sarker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Phishing email is a serious cyber threat that tries to deceive users by
sending false emails with the intention of stealing confidential information or
causing financial harm. Attackers, often posing as trustworthy entities,
exploit technological advancements and sophistication to make detection and
prevention of phishing more challenging. Despite extensive academic research,
phishing detection remains an ongoing and formidable challenge in the
cybersecurity landscape. Large Language Models (LLMs) and Masked Language
Models (MLMs) possess immense potential to offer innovative solutions to
address long-standing challenges. In this research paper, we present an
optimized, fine-tuned transformer-based DistilBERT model designed for the
detection of phishing emails. In the detection process, we work with a phishing
email dataset and utilize the preprocessing techniques to clean and solve the
imbalance class issues. Through our experiments, we found that our model
effectively achieves high accuracy, demonstrating its capability to perform
well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)
techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and
Transformer Interpret to explain how our model makes predictions in the context
of text classification for phishing emails.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13874" title="Abstract">arXiv:2402.13874</a> [<a href="/pdf/2402.13874" title="Download PDF">pdf</a>, <a href="/format/2402.13874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yuefeng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The remarkable capability of large language models (LLMs) for in-context
learning (ICL) needs to be activated by demonstration examples. Prior work has
extensively explored the selection of examples for ICL, predominantly following
the "select then organize" paradigm, such approaches often neglect the internal
relationships between examples and exist an inconsistency between the training
and inference. In this paper, we formulate the problem as a
$\textit{se}$quential $\textit{se}$lection problem and introduce
$\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on
varying context, aiding in capturing inter-relationships and sequential
information among examples, significantly enriching the contextuality and
relevance of ICL prompts. Meanwhile, we utilize beam search to seek and
construct example sequences, enhancing both quality and diversity. Extensive
experiments across 23 NLP tasks from 8 distinct categories illustrate that
$\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42%
relative improvement over random selection. Further in-depth analysis show the
effectiveness of proposed strategies, highlighting $\texttt{Se}^2$'s
exceptional stability and adaptability across various scenarios. Our code will
be released to facilitate future research.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13876" title="Abstract">arXiv:2402.13876</a> [<a href="/pdf/2402.13876" title="Download PDF">pdf</a>, <a href="/format/2402.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Prior Filtering for Depth Map Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengxue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiqiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jinshan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Ying Tai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guangwei Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modal fusion is vital to the success of super-resolution of depth
images. However, commonly used fusion strategies, such as addition and
concatenation, fall short of effectively bridging the modal gap. As a result,
guided image filtering methods have been introduced to mitigate this issue.
Nevertheless, it is observed that their filter kernels usually encounter
significant texture interference and edge inaccuracy. To tackle these two
challenges, we introduce a Scene Prior Filtering network, SPFNet, which
utilizes the priors surface normal and semantic map from large-scale models.
Specifically, we design an All-in-one Prior Propagation that computes the
similarity between multi-modal scene priors, \textit{i.e.}, RGB, normal,
semantic, and depth, to reduce the texture interference. In addition, we
present a One-to-one Prior Embedding that continuously embeds each single-modal
prior into depth using Mutual Guided Filtering, further alleviating the texture
interference while enhancing edges. Our SPFNet has been extensively evaluated
on both real and synthetic datasets, achieving state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13887" title="Abstract">arXiv:2402.13887</a> [<a href="/pdf/2402.13887" title="Download PDF">pdf</a>, <a href="/format/2402.13887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Chenyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Minghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable capabilities across
various applications, fundamentally reshaping the landscape of natural language
processing (NLP) research. However, recent evaluation frameworks often rely on
the output probabilities of LLMs for predictions, primarily due to
computational constraints, diverging from real-world LLM usage scenarios. While
widely employed, the efficacy of these probability-based evaluation strategies
remains an open research question. This study aims to scrutinize the validity
of such probability-based evaluation methods within the context of using LLMs
for Multiple Choice Questions (MCQs), highlighting their inherent limitations.
Our empirical investigation reveals that the prevalent probability-based
evaluation method inadequately aligns with generation-based prediction.
Furthermore, current evaluation frameworks typically assess LLMs through
predictive tasks based on output probabilities rather than directly generating
responses, owing to computational limitations. We illustrate that these
probability-based approaches do not effectively correspond with generative
predictions. The outcomes of our study can enhance the understanding of LLM
evaluation methodologies and provide insights for future research in this
domain.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13891" title="Abstract">arXiv:2402.13891</a> [<a href="/pdf/2402.13891" title="Download PDF">pdf</a>, <a href="/format/2402.13891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Saturation in Density Ratio Estimation by Iterated  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+L">Lukas Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Holzleitner%2C+M">Markus Holzleitner</a>, 
<a href="/search/cs?searchtype=author&query=Lehner%2C+J">Johannes Lehner</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>, 
<a href="/search/cs?searchtype=author&query=Zellinger%2C+W">Werner Zellinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Estimating the ratio of two probability densities from finitely many samples,
is a central task in machine learning and statistics. In this work, we show
that a large class of kernel methods for density ratio estimation suffers from
error saturation, which prevents algorithms from achieving fast error
convergence rates on highly regular learning problems. To resolve saturation,
we introduce iterated regularization in density ratio estimation to achieve
fast error rates. Our methods outperform its non-iteratively regularized
versions on benchmarks for density ratio estimation as well as on large-scale
evaluations for importance-weighted ensembling of deep unsupervised domain
adaptation models.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13897" title="Abstract">arXiv:2402.13897</a> [<a href="/pdf/2402.13897" title="Download PDF">pdf</a>, <a href="/format/2402.13897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Science Checker Reloaded: A Bidirectional Paradigm for Transparency and  Logical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakotoson%2C+L">Lo&#xef;c Rakotoson</a>, 
<a href="/search/cs?searchtype=author&query=Massip%2C+S">Sylvain Massip</a>, 
<a href="/search/cs?searchtype=author&query=Laleye%2C+F+A+A">Fr&#xe9;jus A. A. Laleye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Information retrieval is a rapidly evolving field. However it still faces
significant limitations in the scientific and industrial vast amounts of
information, such as semantic divergence and vocabulary gaps in sparse
retrieval, low precision and lack of interpretability in semantic search, or
hallucination and outdated information in generative models. In this paper, we
introduce a two-block approach to tackle these hurdles for long documents. The
first block enhances language understanding in sparse retrieval by query
expansion to retrieve relevant documents. The second block deepens the result
by providing comprehensive and informative answers to the complex question
using only the information spread in the long document, enabling bidirectional
engagement. At various stages of the pipeline, intermediate results are
presented to users to facilitate understanding of the system's reasoning. We
believe this bidirectional approach brings significant advancements in terms of
transparency, logical thinking, and comprehensive understanding in the field of
scientific information retrieval.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13901" title="Abstract">arXiv:2402.13901</a> [<a href="/pdf/2402.13901" title="Download PDF">pdf</a>, <a href="/format/2402.13901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuchen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+P">Peizhong Ju</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shroff%2C+N">Ness Shroff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">The denoising diffusion model emerges recently as a powerful generative
technique that converts noise into data. Theoretical convergence guarantee has
been mainly studied for continuous-time diffusion models, and has been obtained
for discrete-time diffusion models only for distributions with bounded support
in the literature. In this paper, we establish the convergence guarantee for
substantially larger classes of distributions under discrete-time diffusion
models and further improve the convergence rate for distributions with bounded
support. In particular, we first establish the convergence rates for both
smooth and general (possibly non-smooth) distributions having finite second
moment. We then specialize our results to a number of interesting classes of
distributions with explicit parameter dependencies, including distributions
with Lipschitz scores, Gaussian mixture distributions, and distributions with
bounded support. We further propose a novel accelerated sampler and show that
it improves the convergence rates of the corresponding regular sampler by
orders of magnitude with respect to all system parameters. For distributions
with bounded support, our result improves the dimensional dependence of the
previous convergence rate by orders of magnitude. Our study features a novel
analysis technique that constructs tilting factor representation of the
convergence error and exploits Tweedie's formula for handling Taylor expansion
power terms.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13903" title="Abstract">arXiv:2402.13903</a> [<a href="/pdf/2402.13903" title="Download PDF">pdf</a>, <a href="/ps/2402.13903" title="Download PostScript">ps</a>, <a href="/format/2402.13903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dealing with unbounded gradients in stochastic saddle-point optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neu%2C+G">Gergely Neu</a>, 
<a href="/search/cs?searchtype=author&query=Okolo%2C+N">Nneka Okolo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the performance of stochastic first-order methods for finding saddle
points of convex-concave functions. A notorious challenge faced by such methods
is that the gradients can grow arbitrarily large during optimization, which may
result in instability and divergence. In this paper, we propose a simple and
effective regularization technique that stabilizes the iterates and yields
meaningful performance guarantees even if the domain and the gradient noise
scales linearly with the size of the iterates (and is thus potentially
unbounded). Besides providing a set of general results, we also apply our
algorithm to a specific problem in reinforcement learning, where it leads to
performance guarantees for finding near-optimal policies in an average-reward
MDP without prior knowledge of the bias span.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13904" title="Abstract">arXiv:2402.13904</a> [<a href="/pdf/2402.13904" title="Download PDF">pdf</a>, <a href="/format/2402.13904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrating Large Language Models with Sample Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Q">Qing Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+K">Kumar Shridhar</a>, 
<a href="/search/cs?searchtype=author&query=Malaviya%2C+C">Chaitanya Malaviya</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Apidianaki%2C+M">Marianna Apidianaki</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Accurately gauging the confidence level of Large Language Models' (LLMs)
predictions is pivotal for their reliable application. However, LLMs are often
uncalibrated inherently and elude conventional calibration techniques due to
their proprietary nature and massive scale. In this work, we explore the
potential of deriving confidence from the distribution of multiple randomly
sampled model generations, via three measures of consistency. We perform an
extensive evaluation across various open and closed-source models on nine
reasoning datasets. Results show that consistency-based calibration methods
outperform existing post-hoc approaches. Meanwhile, we find that factors such
as intermediate explanations, model scaling, and larger sample sizes enhance
calibration, while instruction-tuning makes calibration more difficult.
Moreover, confidence scores obtained from consistency have the potential to
enhance model performance. Finally, we offer practical guidance on choosing
suitable consistency metrics for calibration, tailored to the characteristics
of various LMs.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13906" title="Abstract">arXiv:2402.13906</a> [<a href="/pdf/2402.13906" title="Download PDF">pdf</a>, <a href="/format/2402.13906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Collection-Wide Similarities for Unsupervised Document  Structure Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lior%2C+G">Gili Lior</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Stanovsky%2C+G">Gabriel Stanovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document collections of various domains, e.g., legal, medical, or financial,
often share some underlying collection-wide structure, which captures
information that can aid both human users and structure-aware models. We
propose to identify the typical structure of document within a collection,
which requires to capture recurring topics across the collection, while
abstracting over arbitrary header paraphrases, and ground each topic to
respective document locations. These requirements pose several challenges:
headers that mark recurring topics frequently differ in phrasing, certain
section headers are unique to individual documents and do not reflect the
typical structure, and the order of topics can vary between documents.
Subsequently, we develop an unsupervised graph-based method which leverages
both inter- and intra-document similarities, to extract the underlying
collection-wide structure. Our evaluations on three diverse domains in both
English and Hebrew indicate that our method extracts meaningful collection-wide
structure, and we hope that future work will leverage our method for
multi-document applications and structure-aware models.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13911" title="Abstract">arXiv:2402.13911</a> [<a href="/pdf/2402.13911" title="Download PDF">pdf</a>, <a href="/ps/2402.13911" title="Download PostScript">ps</a>, <a href="/format/2402.13911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replication Study: Enhancing Hydrological Modeling with Physics-Guided  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmaeilzadeh%2C+M">Mostafa Esmaeilzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Amirzadeh%2C+M">Melika Amirzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current hydrological modeling methods combine data-driven Machine Learning
(ML) algorithms and traditional physics-based models to address their
respective limitations incorrect parameter estimates from rigid physics-based
models and the neglect of physical process constraints by ML algorithms.
Despite the accuracy of ML in outcome prediction, the integration of scientific
knowledge is crucial for reliable predictions. This study introduces a Physics
Informed Machine Learning (PIML) model, which merges the process understanding
of conceptual hydrological models with the predictive efficiency of ML
algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates
superior performance in forecasting monthly streamflow and actual
evapotranspiration over both standalone conceptual models and ML algorithms,
ensuring physical consistency of the outputs. This study replicates the
methodologies of Bhasme, P., Vagadiya, J., &amp; Bhatia, U. (2022) from their
pivotal work on Physics Informed Machine Learning for hydrological processes,
utilizing their shared code and datasets to further explore the predictive
capabilities in hydrological modeling.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13914" title="Abstract">arXiv:2402.13914</a> [<a href="/pdf/2402.13914" title="Download PDF">pdf</a>, <a href="/format/2402.13914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explain to Question not to Justify
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biecek%2C+P">Przemyslaw Biecek</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Explainable Artificial Intelligence (XAI) is a young but very promising field
of research. Unfortunately, the progress in this field is currently slowed down
by divergent and incompatible goals. In this paper, we separate various threads
tangled within the area of XAI into two complementary cultures of
human/value-oriented explanations (BLUE XAI) and model/validation-oriented
explanations (RED XAI). We also argue that the area of RED XAI is currently
under-explored and hides great opportunities and potential for important
research necessary to ensure the safety of AI systems. We conclude this paper
by presenting promising challenges in this area.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13915" title="Abstract">arXiv:2402.13915</a> [<a href="/pdf/2402.13915" title="Download PDF">pdf</a>, <a href="/format/2402.13915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Combined Learning and Optimization Framework to Transfer Human  Whole-body Loco-manipulation Skills to Mobile Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianzhuang Zhao</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Tassi%2C+F">Francesco Tassi</a> (1), 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanlong Huang</a> (3), 
<a href="/search/cs?searchtype=author&query=De+Momi%2C+E">Elena De Momi</a> (2), 
<a href="/search/cs?searchtype=author&query=Ajoudani%2C+A">Arash Ajoudani</a> (1) ((1) Human-Robot Interfaces and Interaction Lab, Istituto Italiano di Tecnologia, Genoa, Italy, (2) Dept. of Electronics, Information, and Bioengineering, Politecnico di Milano, Italy, (3) School of Computing, University of Leeds, Leeds, UK)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Humans' ability to smoothly switch between locomotion and manipulation is a
remarkable feature of sensorimotor coordination. Leaning and replication of
such human-like strategies can lead to the development of more sophisticated
robots capable of performing complex whole-body tasks in real-world
environments. To this end, this paper proposes a combined learning and
optimization framework for transferring human's loco-manipulation
soft-switching skills to mobile manipulators. The methodology departs from data
collection of human demonstrations for a locomotion-integrated manipulation
task through a vision system. Next, the wrist and pelvis motions are mapped to
mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement
primitive algorithm learns the wrist and pelvis trajectories and generalizes to
new desired points according to task requirements. Next, the reference
trajectories are sent to a hierarchical quadratic programming controller, where
the EE and the mobile base reference trajectories are provided as the first and
second priority tasks, generating the feasible and optimal joint level
commands. A locomotion-integrated pick-and-place task is executed to validate
the proposed approach. After a human demonstrates the task, a mobile
manipulator executes the task with the same and new settings, grasping a bottle
at non-zero velocity. The results showed that the proposed approach
successfully transfers the human loco-manipulation skills to mobile
manipulators, even with different geometry.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13916" title="Abstract">arXiv:2402.13916</a> [<a href="/pdf/2402.13916" title="Download PDF">pdf</a>, <a href="/format/2402.13916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias correction of wind power forecasts with SCADA data and continuous  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jonas%2C+S">Stefan Jonas</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+K">Kevin Winter</a>, 
<a href="/search/cs?searchtype=author&query=Brodbeck%2C+B">Bernhard Brodbeck</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+A">Angela Meyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Wind energy plays a critical role in the transition towards renewable energy
sources. However, the uncertainty and variability of wind can impede its full
potential and the necessary growth of wind power capacity. To mitigate these
challenges, wind power forecasting methods are employed for applications in
power management, energy trading, or maintenance scheduling. In this work, we
present, evaluate, and compare four machine learning-based wind power
forecasting models. Our models correct and improve 48-hour forecasts extracted
from a numerical weather prediction (NWP) model. The models are evaluated on
datasets from a wind park comprising 65 wind turbines. The best improvement in
forecasting error and mean bias was achieved by a convolutional neural network,
reducing the average NRMSE down to 22%, coupled with a significant reduction in
mean bias, compared to a NRMSE of 35% from the strongly biased baseline model
using uncorrected NWP forecasts. Our findings further indicate that changes to
neural network architectures play a minor role in affecting the forecasting
performance, and that future research should rather investigate changes in the
model pipeline. Moreover, we introduce a continuous learning strategy, which is
shown to achieve the highest forecasting performance improvements when new data
is made available.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13917" title="Abstract">arXiv:2402.13917</a> [<a href="/pdf/2402.13917" title="Download PDF">pdf</a>, <a href="/format/2402.13917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Linguistic Features and Languages are Important in LLM Translation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diandaru%2C+R">Ryandito Diandaru</a>, 
<a href="/search/cs?searchtype=author&query=Susanto%2C+L">Lucky Susanto</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zilu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Purwarianti%2C+A">Ayu Purwarianti</a>, 
<a href="/search/cs?searchtype=author&query=Wijaya%2C+D">Derry Wijaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate strong capability across multiple
tasks, including machine translation. Our study focuses on evaluating Llama2's
machine translation capabilities and exploring how translation depends on
languages in its training data. Our experiments show that the 7B Llama2 model
yields above 10 BLEU score for all languages it has seen, but not always for
languages it has not seen. Most gains for those unseen languages are observed
the most with the model scale compared to using chat versions or adding shot
count. Furthermore, our linguistic distance analysis reveals that syntactic
similarity is not always the primary linguistic factor in determining
translation quality. Interestingly, we discovered that under specific
circumstances, some languages, despite having significantly less training data
than English, exhibit strong correlations comparable to English. Our
discoveries here give new perspectives for the current landscape of LLMs,
raising the possibility that LLMs centered around languages other than English
may offer a more effective foundation for a multilingual model.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13918" title="Abstract">arXiv:2402.13918</a> [<a href="/pdf/2402.13918" title="Download PDF">pdf</a>, <a href="/format/2402.13918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for  Cloud Detection and Segmentation in Remote Sensing Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabio%2C+L">Loddo Fabio</a>, 
<a href="/search/cs?searchtype=author&query=Piga%2C+D">Dario Piga</a>, 
<a href="/search/cs?searchtype=author&query=Umberto%2C+M">Michelucci Umberto</a>, 
<a href="/search/cs?searchtype=author&query=Safouane%2C+E+G">El Ghazouali Safouane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Expert Systems and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Satellites equipped with optical sensors capture high-resolution imagery,
providing valuable insights into various environmental phenomena. In recent
years, there has been a surge of research focused on addressing some challenges
in remote sensing, ranging from water detection in diverse landscapes to the
segmentation of mountainous and terrains. Ongoing investigations goals to
enhance the precision and efficiency of satellite imagery analysis. Especially,
there is a growing emphasis on developing methodologies for accurate water body
detection, snow and clouds, important for environmental monitoring, resource
management, and disaster response. Within this context, this paper focus on the
cloud segmentation from remote sensing imagery. Accurate remote sensing data
analysis can be challenging due to the presence of clouds in optical
sensor-based applications. The quality of resulting products such as
applications and research is directly impacted by cloud detection, which plays
a key role in the remote sensing data processing pipeline. This paper examines
seven cutting-edge semantic segmentation and detection algorithms applied to
clouds identification, conducting a benchmark analysis to evaluate their
architectural approaches and identify the most performing ones. To increase the
model's adaptability, critical elements including the type of imagery and the
amount of spectral bands used during training are analyzed. Additionally, this
research tries to produce machine learning algorithms that can perform cloud
segmentation using only a few spectral bands, including RGB and RGBN-IR
combinations. The model's flexibility for a variety of applications and user
scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as
datasets. This benchmark can be reproduced using the material from this github
link: \url{https://github.com/toelt-llc/cloud\_segmentation\_comparative}.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13919" title="Abstract">arXiv:2402.13919</a> [<a href="/pdf/2402.13919" title="Download PDF">pdf</a>, <a href="/format/2402.13919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Prakamya Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Vashisht%2C+P">Parth Vashisht</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+F">Feiyun Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mody%2C+V+D">Vidhi Dhaval Mody</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Equal contribution for the first two authors. arXiv admin note: text overlap with <a href="/abs/2310.20033">arXiv:2310.20033</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) such as GPT and Llama have demonstrated
significant achievements in summarization tasks but struggle with factual
inaccuracies, a critical issue in clinical NLP applications where errors could
lead to serious consequences. To counter the high costs and limited
availability of expert-annotated data for factual alignment, this study
introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate
high-quality feedback aimed at enhancing factual consistency in clinical note
summarization. Our research primarily focuses on edit feedback, mirroring the
practical scenario in which medical professionals refine AI system outputs
without the need for additional annotations. Despite GPT's proven expertise in
various clinical NLP tasks, such as the Medical Licensing Examination, there is
scant research on its capacity to deliver expert-level edit feedback for
improving weaker LMs or LLMs generation quality. This work leverages GPT's
advanced capabilities in clinical NLP to offer expert-level edit feedback.
Through the use of two distinct alignment algorithms (DPO and SALT) based on
GPT edit feedback, our goal is to reduce hallucinations and align closely with
medical facts, endeavoring to narrow the divide between AI-generated content
and factual accuracy. This highlights the substantial potential of GPT edits in
enhancing the alignment of clinical factuality.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13920" title="Abstract">arXiv:2402.13920</a> [<a href="/pdf/2402.13920" title="Download PDF">pdf</a>, <a href="/format/2402.13920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical algorithms for Hierarchical overlap graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Talera%2C+S">Saumya Talera</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+P">Parth Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Shabnam Khan</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Shahbaz Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Genome assembly is a prominent problem studied in bioinformatics, which
computes the source string using a set of its overlapping substrings.
Classically, genome assembly uses assembly graphs built using this set of
substrings to compute the source string efficiently, having a tradeoff between
scalability and avoiding information loss. The scalable de Bruijn graphs come
at the price of losing crucial overlap information. The complete overlap
information is stored in overlap graphs using quadratic space. Hierarchical
overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information
loss despite using linear space. After a series of suboptimal improvements,
Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021],
where only the former was seemingly practical.
<br />We empirically analyze all the practical algorithms for computing HOG, where
the optimal algorithm [CPM2021] outperforms the previous algorithms as
expected, though at the expense of extra memory. However, it uses non-intuitive
approach and non-trivial data structures. We present arguably the most
intuitive algorithm, using only elementary arrays, which is also optimal. Our
algorithm empirically proves even better for both time and memory over all the
algorithms, highlighting its significance in both theory and practice.
<br />We further explore the applications of hierarchical overlap graphs to solve
various forms of suffix-prefix queries on a set of strings. Loukides et al.
[CPM2023] recently presented state-of-the-art algorithms for these queries.
However, these algorithms require complex black-box data structures and are
seemingly impractical. Our algorithms, despite failing to match the
state-of-the-art algorithms theoretically, answer different queries ranging
from 0.01-100 milliseconds for a data set having around a billion characters.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13921" title="Abstract">arXiv:2402.13921</a> [<a href="/pdf/2402.13921" title="Download PDF">pdf</a>, <a href="/ps/2402.13921" title="Download PostScript">ps</a>, <a href="/format/2402.13921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust recovery for stochastic block models, simplified and generalized
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohanty%2C+S">Sidhanth Mohanty</a>, 
<a href="/search/cs?searchtype=author&query=Raghavendra%2C+P">Prasad Raghavendra</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D+X">David X. Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Probability (math.PR)

</div>
<p class="mathjax">We study the problem of $\textit{robust community recovery}$: efficiently
recovering communities in sparse stochastic block models in the presence of
adversarial corruptions. In the absence of adversarial corruptions, there are
efficient algorithms when the $\textit{signal-to-noise ratio}$ exceeds the
$\textit{Kesten--Stigum (KS) threshold}$, widely believed to be the
computational threshold for this problem. The question we study is: does the
computational threshold for robust community recovery also lie at the KS
threshold? We answer this question affirmatively, providing an algorithm for
robust community recovery for arbitrary stochastic block models on any constant
number of communities, generalizing the work of Ding, d'Orsi, Nasser &amp; Steurer
on an efficient algorithm above the KS threshold in the case of $2$-community
block models.
<br />There are three main ingredients to our work:
<br />(i) The Bethe Hessian of the graph is defined as $H_G(t) \triangleq
(D_G-I)t^2 - A_Gt + I$ where $D_G$ is the diagonal matrix of degrees and $A_G$
is the adjacency matrix. Empirical work suggested that the Bethe Hessian for
the stochastic block model has outlier eigenvectors corresponding to the
communities right above the Kesten-Stigum threshold. We formally confirm the
existence of outlier eigenvalues for the Bethe Hessian, by explicitly
constructing outlier eigenvectors from the community vectors.
<br />(ii) We develop an algorithm for a variant of robust PCA on sparse matrices.
Specifically, an algorithm to partially recover top eigenspaces from
adversarially corrupted sparse matrices under mild delocalization constraints.
<br />(iii) A rounding algorithm to turn vector assignments of vertices into a
community assignment, inspired by the algorithm of Charikar \&amp; Wirth
\cite{CW04} for $2$XOR.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13922" title="Abstract">arXiv:2402.13922</a> [<a href="/pdf/2402.13922" title="Download PDF">pdf</a>, <a href="/ps/2402.13922" title="Download PostScript">ps</a>, <a href="/format/2402.13922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development of multi-physics finite element model to investigate  electromagnetic forming and simultaneous multi-point perforations of  aluminium tube
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chetry%2C+A">Avinash Chetry</a>, 
<a href="/search/math?searchtype=author&query=Nandy%2C+A">Arup Nandy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26, 14, ICAMEMS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Electromagnetic forming and perforations (EMFP) are complex and innovative
high strain rate processes that involve electromagnetic-mechanical interactions
for simultaneous metal forming and perforations. Instead of spending costly
resources on repetitive experimental work, a properly designed numerical model
can be effectively used for detailed analysis and characterization of the
complex process. A coupled finite element (FE) model is considered for
analyzing the multi-physics of the EMFP because of its robustness and improved
accuracy. In this work, a detailed understanding of the process has been
achieved by numerically simulating forming and perforations of Al6061-T6 tube
for 12 holes and 36 holes with two different punches, i.e., pointed and concave
punches using Ls-Dyna software. In order to shed light on EMFP physics, a
comparison between experimental data and the formulated numerical simulation
has been carried out to compare the average hole diameter and the number of
perforated holes, for different types of punches and a range of discharge
energies. The simulated results show acceptable agreement with experimental
studies, with maximum deviations being less than or equal to 6%, which clearly
illustrates the efficacy and capability of the developed coupled Multi-physics
FE model.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13923" title="Abstract">arXiv:2402.13923</a> [<a href="/pdf/2402.13923" title="Download PDF">pdf</a>, <a href="/format/2402.13923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Lower Bound on the Number of Pseudoline Arrangements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dallant%2C+J">Justin Dallant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript was accepted at SoCG'24 and will be merged with Fernando Cort\'es K\"uhnast, Stefan Felsner and Manfred Scheucher's manuscript "An Improved Lower Bound on the Number of Pseudoline Arrangements'' for the proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">We show that for large enough $n$, the number of non-isomorphic pseudoline
arrangements of order $n$ is greater than $2^{c\cdot n^2}$ for some constant $c
&gt; 0.2604$, improving the previous best bound of $c&gt;0.2083$ by Dumitrescu and
Mandal (2020). Arrangements of pseudolines (and in particular arrangements of
lines) are important objects appearing in many forms in discrete and
computational geometry. They have strong ties for example with oriented
matroids, sorting networks and point configurations. Let $B_n$ be the number of
non-isomorphic pseudoline arrangements of order $n$ and let $b_n :=
\log_2(B_n)$. The problem of estimating $b_n$ dates back to Knuth, who
conjectured that $b_n \leq 0.5n^2 + o(n^2)$ and derived the first bounds
$n^2/6-O(n) \leq b_n \leq 0.7924(n^2+n)$. Both the upper and the lower bound
have been improved a couple of times since. For the upper bound, it was first
improved to $b_n &lt; 0.6988n^2$ (Felsner, 1997), then $b_n &lt; 0.6571 n^2$ by
Felsner and Valtr (2011), for large enough $n$. In the same paper, Felsner and
Valtr improved the constant in the lower bound to $c&gt; 0.1887$, which was
subsequently improved by Dumitrescu and Mandal to $c&gt;0.2083$. Our new bound is
based on a construction which starts with one of the constructions of
Dumitrescu and Mandal and breaks it into constant sized pieces. We then use
software to compute the contribution of each piece to the overall number of
pseudoline arrangements. This method adds a lot of flexibility to the
construction and thus offers many avenues for future tweaks and improvements
which could lead to further tightening of the lower bound.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13925" title="Abstract">arXiv:2402.13925</a> [<a href="/pdf/2402.13925" title="Download PDF">pdf</a>, <a href="/ps/2402.13925" title="Download PostScript">ps</a>, <a href="/format/2402.13925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMAT4COMSOL: An Abaqus user material (UMAT) subroutine wrapper for  COMSOL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lucarini%2C+S">S. Lucarini</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Pa%C3%B1eda%2C+E">E. Mart&#xed;nez-Pa&#xf1;eda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Mathematical Software (cs.MS); Software Engineering (cs.SE)

</div>
<p class="mathjax">We present a wrapper that allows Abaqus user material subroutines (UMATs) to
be used as an External Material library in the software COMSOL Multiphysics.
The wrapper, written in C language, transforms COMSOL's external material
subroutine inputs and outputs into Fortran-coded Abaqus UMAT inputs and
outputs, by means of a consistent variable transformation. This significantly
facilitates conducting coupled, multi-physics studies employing the advanced
material models that the solid mechanics community has developed over the past
decades. We exemplify the potential of our new framework, UMAT4COMSOL, by
conducting numerical experiments in the areas of elastoplasticity,
hyperelasticity and crystal plasticity. The source code, detailed documentation
and example tutorials are made freely available to download at
www.empaneda.com/codes.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13926" title="Abstract">arXiv:2402.13926</a> [<a href="/pdf/2402.13926" title="Download PDF">pdf</a>, <a href="/format/2402.13926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bianchi%2C+F">Federico Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The risks derived from large language models (LLMs) generating deceptive and
damaging content have been the subject of considerable research, but even safe
generations can lead to problematic downstream impacts. In our study, we shift
the focus to how even safe text coming from LLMs can be easily turned into
potentially dangerous content through Bait-and-Switch attacks. In such attacks,
the user first prompts LLMs with safe questions and then employs a simple
find-and-replace post-hoc technique to manipulate the outputs into harmful
narratives. The alarming efficacy of this approach in generating toxic content
highlights a significant challenge in developing reliable safety guardrails for
LLMs. In particular, we stress that focusing on the safety of the verbatim LLM
outputs is insufficient and that we also need to consider post-hoc
transformations.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13927" title="Abstract">arXiv:2402.13927</a> [<a href="/pdf/2402.13927" title="Download PDF">pdf</a>, <a href="/format/2402.13927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Delusional Hedge Algorithm as a Model of Human Learning from Diverse  Opinions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jerry Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Whereas cognitive models of learning often assume direct experience with both
the features of an event and with a true label or outcome, much of everyday
learning arises from hearing the opinions of others, without direct access to
either the experience or the ground truth outcome. We consider how people can
learn which opinions to trust in such scenarios by extending the hedge
algorithm: a classic solution for learning from diverse information sources. We
first introduce a semi-supervised variant we call the delusional hedge capable
of learning from both supervised and unsupervised experiences. In two
experiments, we examine the alignment between human judgments and predictions
from the standard hedge, the delusional hedge, and a heuristic baseline model.
Results indicate that humans effectively incorporate both labeled and unlabeled
information in a manner consistent with the delusional hedge algorithm --
suggesting that human learners not only gauge the accuracy of information
sources but also their consistency with other reliable sources. The findings
advance our understanding of human learning from diverse opinions, with
implications for the development of algorithms that better capture how people
learn to weigh conflicting information sources.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13929" title="Abstract">arXiv:2402.13929</a> [<a href="/pdf/2402.13929" title="Download PDF">pdf</a>, <a href="/format/2402.13929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDXL-Lightning: Progressive Adversarial Diffusion Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shanchuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Anran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a diffusion distillation method that achieves new state-of-the-art
in one-step/few-step 1024px text-to-image generation based on SDXL. Our method
combines progressive and adversarial distillation to achieve a balance between
quality and mode coverage. In this paper, we discuss the theoretical analysis,
discriminator design, model formulation, and training techniques. We
open-source our distilled SDXL-Lightning models both as LoRA and full UNet
weights.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13930" title="Abstract">arXiv:2402.13930</a> [<a href="/pdf/2402.13930" title="Download PDF">pdf</a>, <a href="/format/2402.13930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Reinforcement Learning Agents with Local Guides
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daoudi%2C+P">Paul Daoudi</a>, 
<a href="/search/cs?searchtype=author&query=Robu%2C+B">Bogdan Robu</a>, 
<a href="/search/cs?searchtype=author&query=Prieur%2C+C">Christophe Prieur</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+L+D">Ludovic Dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Barlier%2C+M">Merwan Barlier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper addresses the problem of integrating local guide policies into a
Reinforcement Learning agent. For this, we show how to adapt existing
algorithms to this setting before introducing a novel algorithm based on a
noisy policy-switching procedure. This approach builds on a proper Approximate
Policy Evaluation (APE) scheme to provide a perturbation that carefully leads
the local guides towards better actions. We evaluated our method on a set of
classical Reinforcement Learning problems, including safety-critical systems
where the agent cannot enter some areas at the risk of triggering catastrophic
consequences. In all the proposed environments, our agent proved to be
efficient at leveraging those policies to improve the performance of any
APE-based Reinforcement Learning algorithm, especially in its first learning
stages.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13932" title="Abstract">arXiv:2402.13932</a> [<a href="/pdf/2402.13932" title="Download PDF">pdf</a>, <a href="/format/2402.13932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tumor segmentation on whole slide images: training or prompting?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huaqian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A9mond-Martin%2C+C">Clara Br&#xe9;mond-Martin</a>, 
<a href="/search/cs?searchtype=author&query=Bouaou%2C+K">K&#xe9;vin Bouaou</a>, 
<a href="/search/cs?searchtype=author&query=Clouchoux%2C+C">C&#xe9;dric Clouchoux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the
immense dimensions of whole slide images (WSI) in histology, deep learning
approaches for WSI classification mainly operate at patch-wise or
superpixel-wise level. However, these solutions often struggle to capture
global WSI information and cannot directly generate the binary mask.
Downsampling the WSI and performing semantic segmentation is another possible
approach. While this method offers computational efficiency, it necessitates a
large amount of annotated data since resolution reduction may lead to
information loss. Visual prompting is a novel paradigm that allows the model to
perform new tasks by making subtle modifications to the input space, rather
than adapting the model itself. Such approach has demonstrated promising
results on many computer vision tasks. In this paper, we show the efficacy of
visual prompting in the context of tumor segmentation for three distinct
organs. In comparison to classical methods trained for this specific task, our
findings reveal that, with appropriate prompt examples, visual prompting can
achieve comparable or better performance without extensive fine-tuning.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13934" title="Abstract">arXiv:2402.13934</a> [<a href="/pdf/2402.13934" title="Download PDF">pdf</a>, <a href="/format/2402.13934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Efficient Transformers Really Save Computation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ackermann%2C+J">Jan Ackermann</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhenyu He</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+G">Guhao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bohang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunzhen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qiwei Ye</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">As transformer-based language models are trained on increasingly large
datasets and with vast numbers of parameters, finding more efficient
alternatives to the standard Transformer has become very valuable. While many
efficient Transformers and Transformer alternatives have been proposed, none
provide theoretical guarantees that they are a suitable replacement for the
standard Transformer. This makes it challenging to identify when to use a
specific model and what directions to prioritize for further investigation. In
this paper, we aim to understand the capabilities and limitations of efficient
Transformers, specifically the Sparse Transformer and the Linear Transformer.
We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)
prompts and follow previous works to model them as Dynamic Programming (DP)
problems. Our results show that while these models are expressive enough to
solve general DP tasks, contrary to expectations, they require a model size
that scales with the problem size. Nonetheless, we identify a class of DP
problems for which these models can be more efficient than the standard
Transformer. We confirm our theoretical results through experiments on
representative DP tasks, adding to the understanding of efficient Transformers'
practical strengths and weaknesses.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13936" title="Abstract">arXiv:2402.13936</a> [<a href="/pdf/2402.13936" title="Download PDF">pdf</a>, <a href="/format/2402.13936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP  Guided Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaffin%2C+A">Antoine Chaffin</a>, 
<a href="/search/cs?searchtype=author&query=Kijak%2C+E">Ewa Kijak</a>, 
<a href="/search/cs?searchtype=author&query=Claveau%2C+V">Vincent Claveau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Training image captioning models using teacher forcing results in very
generic samples, whereas more distinctive captions can be very useful in
retrieval applications or to produce alternative texts describing images for
accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval
similarity score between the generated caption and the input image as reward to
guide the training, leading to more distinctive captions. Recent studies show
that pre-trained cross-modal retrieval models can be used to provide this
reward, completely eliminating the need for reference captions. However, we
argue in this paper that Ground Truth (GT) captions can still be useful in this
RL framework. We propose a new image captioning model training strategy that
makes use of GT captions in different ways. Firstly, they can be used to train
a simple MLP discriminator that serves as a regularization to prevent reward
hacking and ensures the fluency of generated captions, resulting in a textual
GAN setup extended for multimodal inputs. Secondly, they can serve as
additional trajectories in the RL strategy, resulting in a teacher forcing loss
weighted by the similarity of the GT to the image. This objective acts as an
additional learning signal grounded to the distribution of the GT captions.
Thirdly, they can serve as strong baselines when added to the pool of captions
used to compute the proposed contrastive reward to reduce the variance of
gradient estimate. Experiments on MS-COCO demonstrate the interest of the
proposed training strategy to produce highly distinctive captions while
maintaining high writing quality.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13938" title="Abstract">arXiv:2402.13938</a> [<a href="/pdf/2402.13938" title="Download PDF">pdf</a>, <a href="/format/2402.13938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A $(5/3+&#x3b5;)$-Approximation for Tricolored Non-crossing Euclidean  TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balig%C3%A1cs%2C+J">J&#xfa;lia Balig&#xe1;cs</a>, 
<a href="/search/cs?searchtype=author&query=Disser%2C+Y">Yann Disser</a>, 
<a href="/search/cs?searchtype=author&query=Feldmann%2C+A+E">Andreas Emil Feldmann</a>, 
<a href="/search/cs?searchtype=author&query=Zych-Pawlewicz%2C+A">Anna Zych-Pawlewicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the Tricolored Euclidean Traveling Salesperson problem, we are given~$k=3$
sets of points in the plane and are looking for disjoint tours, each covering
one of the sets. Arora (1998) famously gave a PTAS based on ``patching'' for
the case $k=1$ and, recently, Dross et al.~(2023) generalized this result
to~$k=2$. Our contribution is a $(5/3+\epsilon)$-approximation algorithm
for~$k=3$ that further generalizes Arora's approach. It is believed that
patching is generally no longer possible for more than two tours. We circumvent
this issue by either applying a conditional patching scheme for three tours or
using an alternative approach based on a weighted solution for $k=2$.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13939" title="Abstract">arXiv:2402.13939</a> [<a href="/pdf/2402.13939" title="Download PDF">pdf</a>, <a href="/format/2402.13939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is the focus of XAI in UI design? Prioritizing UI design principles  for enhancing XAI user experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+D">Dian Lei</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yao He</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jianyou Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">With the widespread application of artificial intelligence(AI), the
explainable AI (XAI) field has undergone a notable resurgence. In this
background, the importance of user experience in XAI has become increasingly
prominent. Simultaneously, the user interface (UI) serves as a crucial link
between XAI and users. However, despite the existence of UI design principles
for XAI, there is a lack of prioritization based on their significance. This
will lead practitioners to have a vague understanding of different design
principles, making it difficult to allocate design space reasonably and
emphasize design focal points. This paper aims to prioritize four design
principles, providing clear guidance for UI design in XAI. Initially, we
conducted a lightweight summary to derive five user experience standards for
non-expert users in XAI. Subsequently, we developed four corresponding webpage
prototypes for the four design principles. Nineteen participants then
interacted with these prototypes, providing ratings based on five user
experience standards, and We calculated the weights of the design principles.
Our findings indicate that, for non-expert users, "sensitivity" is the optimal
UI design principle (weight = 0.3296), followed by "flexibility" (weight =
0.3014). Finally, we engage in further discussion and summarization of our
research results, and present future works and limitations.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13946" title="Abstract">arXiv:2402.13946</a> [<a href="/pdf/2402.13946" title="Download PDF">pdf</a>, <a href="/format/2402.13946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gohil%2C+V">Vasudev Gohil</a>, 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+S">Satwik Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Kalathil%2C+D">Dileep Kalathil</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">Jeyavijayan Rajendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Machine learning has shown great promise in addressing several critical
hardware security problems. In particular, researchers have developed novel
graph neural network (GNN)-based techniques for detecting intellectual property
(IP) piracy, detecting hardware Trojans (HTs), and reverse engineering
circuits, to name a few. These techniques have demonstrated outstanding
accuracy and have received much attention in the community. However, since
these techniques are used for security applications, it is imperative to
evaluate them thoroughly and ensure they are robust and do not compromise the
security of integrated circuits.
<br />In this work, we propose AttackGNN, the first red-team attack on GNN-based
techniques in hardware security. To this end, we devise a novel reinforcement
learning (RL) agent that generates adversarial examples, i.e., circuits,
against the GNN-based techniques. We overcome three challenges related to
effectiveness, scalability, and generality to devise a potent RL agent. We
target five GNN-based techniques for four crucial classes of problems in
hardware security: IP piracy, detecting/localizing HTs, reverse engineering,
and hardware obfuscation. Through our approach, we craft circuits that fool all
GNNs considered in this work. For instance, to evade IP piracy detection, we
generate adversarial pirated circuits that fool the GNN-based defense into
classifying our crafted circuits as not pirated. For attacking HT localization
GNN, our attack generates HT-infested circuits that fool the defense on all
tested circuits. We obtain a similar 100% success rate against GNNs for all
classes of problems.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13948" title="Abstract">arXiv:2402.13948</a> [<a href="/pdf/2402.13948" title="Download PDF">pdf</a>, <a href="/ps/2402.13948" title="Download PostScript">ps</a>, <a href="/format/2402.13948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Syndrome-based Neural Decoder for Linear Block Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Boni+Rovella%2C+G">Gast&#xf3;n De Boni Rovella</a>, 
<a href="/search/cs?searchtype=author&query=Benammar%2C+M">Meryem Benammar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures. To be published in Proc. IEEE Global Communications Conference (GLOBECOM 2023), Kuala Lumpur, Malaysia, December 4-8, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this work, we investigate the problem of neural-based error correction
decoding, and more specifically, the new so-called syndrome-based decoding
technique introduced to tackle scalability in the training phase for larger
code sizes. We improve on previous works in terms of allowing full decoding of
the message rather than codewords, allowing thus the application to
non-systematic codes, and proving that the single-message training property is
still viable. The suggested system is implemented and tested on polar codes of
sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant
improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains
between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise
Ratio (SNR) regime.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13949" title="Abstract">arXiv:2402.13949</a> [<a href="/pdf/2402.13949" title="Download PDF">pdf</a>, <a href="/format/2402.13949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Realistic Arm Movements in Reinforcement Learning: A  Quantitative Comparison of Reward Terms and Task Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charaja%2C+J">Jhon Charaja</a>, 
<a href="/search/cs?searchtype=author&query=Wochner%2C+I">Isabell Wochner</a>, 
<a href="/search/cs?searchtype=author&query=Schumacher%2C+P">Pierre Schumacher</a>, 
<a href="/search/cs?searchtype=author&query=Ilg%2C+W">Winfried Ilg</a>, 
<a href="/search/cs?searchtype=author&query=Giese%2C+M">Martin Giese</a>, 
<a href="/search/cs?searchtype=author&query=Maufroy%2C+C">Christophe Maufroy</a>, 
<a href="/search/cs?searchtype=author&query=Bulling%2C+A">Andreas Bulling</a>, 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+S">Syn Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Haeufle%2C+D+F+B">Daniel F.B. Haeufle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The mimicking of human-like arm movement characteristics involves the
consideration of three factors during control policy synthesis: (a) chosen task
requirements, (b) inclusion of noise during movement execution and (c) chosen
optimality principles. Previous studies showed that when considering these
factors (a-c) individually, it is possible to synthesize arm movements that
either kinematically match the experimental data or reproduce the stereotypical
triphasic muscle activation pattern. However, to date no quantitative
comparison has been made on how realistic the arm movement generated by each
factor is; as well as whether a partial or total combination of all factors
results in arm movements with human-like kinematic characteristics and a
triphasic muscle pattern. To investigate this, we used reinforcement learning
to learn a control policy for a musculoskeletal arm model, aiming to discern
which combination of factors (a-c) results in realistic arm movements according
to four frequently reported stereotypical characteristics. Our findings
indicate that incorporating velocity and acceleration requirements into the
reaching task, employing reward terms that encourage minimization of mechanical
work, hand jerk, and control effort, along with the inclusion of noise during
movement, leads to the emergence of realistic human arm movements in
reinforcement learning. We expect that the gained insights will help in the
future to better predict desired arm movements and corrective forces in
wearable assistive devices.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13950" title="Abstract">arXiv:2402.13950</a> [<a href="/pdf/2402.13950" title="Download PDF">pdf</a>, <a href="/format/2402.13950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=Faltings%2C+B">Boi Faltings</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been shown to perform better when asked to
reason step-by-step before answering a question. However, it is unclear to what
degree the model's final answer is faithful to the stated reasoning steps. In
this paper, we perform a causal mediation analysis on twelve LLMs to examine
how intermediate reasoning steps generated by the LLM influence the final
outcome and find that LLMs do not reliably use their intermediate reasoning
steps when generating an answer. To address this issue, we introduce FRODO, a
framework to tailor small-sized LMs to generate correct reasoning steps and
robustly reason over these steps. FRODO consists of an inference module that
learns to generate correct reasoning steps using an implicit causal reward
function and a reasoning module that learns to faithfully reason over these
intermediate inferences using a counterfactual and causal preference objective.
Our experiments show that FRODO significantly outperforms four competitive
baselines. Furthermore, FRODO improves the robustness and generalization
ability of the reasoning LM, yielding higher performance on out-of-distribution
test sets. Finally, we find that FRODO's rationales are more faithful to its
final answer predictions than standard supervised fine-tuning.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13952" title="Abstract">arXiv:2402.13952</a> [<a href="/pdf/2402.13952" title="Download PDF">pdf</a>, <a href="/ps/2402.13952" title="Download PostScript">ps</a>, <a href="/format/2402.13952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aaronson-Ambainis Conjecture Is True For Random Restrictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S+K">Sreejata Kishor Bhattacharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">In an attempt to show that the acceptance probability of a quantum query
algorithm making $q$ queries can be well-approximated almost everywhere by a
classical decision tree of depth $\leq \text{poly}(q)$, Aaronson and Ambainis
proposed the following conjecture: let $f: \{ \pm 1\}^n \rightarrow [0,1]$ be a
degree $d$ polynomial with variance $\geq \epsilon$. Then, there exists a
coordinate of $f$ with influence $\geq \text{poly} (\epsilon, 1/d)$.
<br />We show that for any polynomial $f: \{ \pm 1\}^n \rightarrow [0,1]$ of degree
$d$ $(d \geq 2)$ and variance $\text{Var}[f] \geq 1/d$, if $\rho$ denotes a
random restriction with survival probability $\dfrac{\log(d)}{C_1 d}$, $$
\text{Pr} \left[f_{\rho} \text{ has a coordinate with influence} \geq
\dfrac{\text{Var}[f]^2 }{d^{C_2}} \right] \geq \dfrac{\text{Var}[f]
\log(d)}{50C_1 d}$$ where $C_1, C_2&gt;0$ are universal constants. Thus,
Aaronson-Ambainis conjecture is true for a non-negligible fraction of random
restrictions of the given polynomial assuming its variance is not too low.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13954" title="Abstract">arXiv:2402.13954</a> [<a href="/pdf/2402.13954" title="Download PDF">pdf</a>, <a href="/format/2402.13954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zalkikar%2C+R">Rahul Zalkikar</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+K">Kanchan Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Social and political scientists often aim to discover and measure distinct
biases from text data representations (embeddings). Innovative
transformer-based language models produce contextually-aware token embeddings
and have achieved state-of-the-art performance for a variety of natural
language tasks, but have been shown to encode unwanted biases for downstream
applications. In this paper, we evaluate the social biases encoded by
transformers trained with the masked language modeling objective using proposed
proxy functions within an iterative masking experiment to measure the quality
of transformer models' predictions, and assess the preference of MLMs towards
disadvantaged and advantaged groups. We compare bias estimations with those
produced by other evaluation methods using two benchmark datasets, finding
relatively high religious and disability biases across considered MLMs and low
gender bias in one dataset relative to the other. Our measures outperform
others in their agreement with human annotators. We extend on previous work by
evaluating social biases introduced after re-training an MLM under the masked
language modeling objective (w.r.t. the model's pre-trained base), and find
that proposed measures produce more accurate estimations of relative preference
for biased sentences between transformers than others based on our methods.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13955" title="Abstract">arXiv:2402.13955</a> [<a href="/pdf/2402.13955" title="Download PDF">pdf</a>, <a href="/format/2402.13955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEE-NET: A deep neural network to identify in-the-wild Bodily Expression  of Emotions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dehshibi%2C+M+M">Mohammad Mahdi Dehshibi</a>, 
<a href="/search/cs?searchtype=author&query=Masip%2C+D">David Masip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this study, we investigate how environmental factors, specifically the
scenes and objects involved, can affect the expression of emotions through body
language. To this end, we introduce a novel multi-stream deep convolutional
neural network named BEE-NET. We also propose a new late fusion strategy that
incorporates meta-information on places and objects as prior knowledge in the
learning process. Our proposed probabilistic pooling model leverages this
information to generate a joint probability distribution of both available and
anticipated non-available contextual information in latent space. Importantly,
our fusion strategy is differentiable, allowing for end-to-end training and
capturing of hidden associations among data points without requiring further
post-processing or regularisation. To evaluate our deep model, we use the Body
Language Database (BoLD), which is currently the largest available database for
the Automatic Identification of the in-the-wild Bodily Expression of Emotions
(AIBEE). Our experimental results demonstrate that our proposed approach
surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving
an Emotional Recognition Score of 66.33%.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13956" title="Abstract">arXiv:2402.13956</a> [<a href="/pdf/2402.13956" title="Download PDF">pdf</a>, <a href="/format/2402.13956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can You Learn Semantics Through Next-Word Prediction? The Case of  Entailment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhaofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Naka%2C+N">Norihito Naka</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Do LMs infer the semantics of text from co-occurrence patterns in their
training data? Merrill et al. (2022) argue that, in theory, probabilities
predicted by an optimal LM encode semantic information about entailment
relations, but it is unclear whether neural LMs trained on corpora learn
entailment in this way because of strong idealizing assumptions made by Merrill
et al. In this work, we investigate whether their theory can be used to decode
entailment judgments from neural LMs. We find that a test similar to theirs can
decode entailment relations between natural sentences, well above random
chance, though not perfectly, across many datasets and LMs. This suggests LMs
implicitly model aspects of semantics to predict semantic effects on sentence
co-occurrence patterns. However, we find the test that predicts entailment in
practice works in the opposite direction to the theoretical test. We thus
revisit the assumptions underlying the original test, finding its derivation
did not adequately account for redundancy in human-written text. We argue that
correctly accounting for redundancy related to explanations might derive the
observed flipped test and, more generally, improve linguistic theories of human
speakers.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13957" title="Abstract">arXiv:2402.13957</a> [<a href="/pdf/2402.13957" title="Download PDF">pdf</a>, <a href="/ps/2402.13957" title="Download PostScript">ps</a>, <a href="/format/2402.13957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Audio Fingerprinting Accuracy Addressing Background Noise and  Distortion Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamuni%2C+N">Navin Kamuni</a>, 
<a href="/search/cs?searchtype=author&query=Chintala%2C+S">Sathishkumar Chintala</a>, 
<a href="/search/cs?searchtype=author&query=Kunchakuri%2C+N">Naveen Kunchakuri</a>, 
<a href="/search/cs?searchtype=author&query=Narasimharaju%2C+J+S+A">Jyothi Swaroop Arlagadda Narasimharaju</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Venkat Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio fingerprinting, exemplified by pioneers like Shazam, has transformed
digital audio recognition. However, existing systems struggle with accuracy in
challenging conditions, limiting broad applicability. This research proposes an
AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built
on the Dejavu Project's foundations, the study emphasizes real-world scenario
simulations with diverse background noises and distortions. Signal processing,
central to Dejavu's model, includes the Fast Fourier Transform, spectrograms,
and peak extraction. The "constellation" concept and fingerprint hashing enable
unique song identification. Performance evaluation attests to 100% accuracy
within a 5-second audio input, with a system showcasing predictable matching
speed for efficiency. Storage analysis highlights the critical space-speed
trade-off for practical implementation. This research advances audio
fingerprinting's adaptability, addressing challenges in varied environments and
applications.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13959" title="Abstract">arXiv:2402.13959</a> [<a href="/pdf/2402.13959" title="Download PDF">pdf</a>, <a href="/format/2402.13959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retention Induced Biases in a Recommendation System with Heterogeneous  Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shichao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">I examine a conceptual model of a recommendation system (RS) with user inflow
and churn dynamics. When inflow and churn balance out, the user distribution
reaches a steady state. Changing the recommendation algorithm alters the steady
state and creates a transition period. During this period, the RS behaves
differently from its new steady state. In particular, A/B experiment metrics
obtained in transition periods are biased indicators of the RS's long term
performance. Scholars and practitioners, however, often conduct A/B tests
shortly after introducing new algorithms to validate their effectiveness. This
A/B experiment paradigm, widely regarded as the gold standard for assessing RS
improvements, may consequently yield false conclusions. I also briefly discuss
the data bias caused by the user retention dynamics.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13963" title="Abstract">arXiv:2402.13963</a> [<a href="/pdf/2402.13963" title="Download PDF">pdf</a>, <a href="/format/2402.13963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Building Multilingual Language Model for Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+P">Pengcheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chaoyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoman Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weixiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we aim to develop an open-source, multilingual language model
for medicine, that the benefits a wider, linguistically diverse audience from
different regions. In general, we present the contribution from the following
aspects: first, for multilingual medical-specific adaptation, we construct a
new multilingual medical corpus, that contains approximately 25.5B tokens
encompassing 6 main languages, termed as MMedC, that enables auto-regressive
training for existing general LLMs. second, to monitor the development of
multilingual LLMs in medicine, we propose a new multilingual medical
multi-choice question-answering benchmark with rationale, termed as MMedBench;
third, we have assessed a number of popular, opensource large language models
(LLMs) on our benchmark, along with those further auto-regressive trained on
MMedC, as a result, our final model, termed as MMedLM 2, with only 7B
parameters, achieves superior performance compared to all other open-source
models, even rivaling GPT-4 on MMedBench. We will make the resources publicly
available, including code, model weights, and datasets.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13965" title="Abstract">arXiv:2402.13965</a> [<a href="/pdf/2402.13965" title="Download PDF">pdf</a>, <a href="/format/2402.13965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cybersecurity as a Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+J">John Morris</a>, 
<a href="/search/cs?searchtype=author&query=Tatschner%2C+S">Stefan Tatschner</a>, 
<a href="/search/cs?searchtype=author&query=Heinl%2C+M+P">Michael P. Heinl</a>, 
<a href="/search/cs?searchtype=author&query=Heinl%2C+P">Patrizia Heinl</a>, 
<a href="/search/cs?searchtype=author&query=Newe%2C+T">Thomas Newe</a>, 
<a href="/search/cs?searchtype=author&query=Plaga%2C+S">Sven Plaga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally to this work. Springer Link: <a href="https://link.springer.com/chapter/10.1007/978-3-031-45162-1_9">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the increasing sophistication and sheer number of cyberattacks, more and
more companies come to the conclusion that they have to strengthen their
cybersecurity posture. At the same time, well-educated Information technology
(IT) security personnel are scarce. Cybersecurity as a service (CSaaS) is one
possible solution to tackle this problem by outsourcing security functions to
managed security service providers (MSSP). This chapter gives an overview of
common CSaaS functions and their providers. Moreover, it provides guidance
especially for small- and medium-sized businesses, for asking the appropriate
questions when it comes to the selection of a specific MSSP.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13971" title="Abstract">arXiv:2402.13971</a> [<a href="/pdf/2402.13971" title="Download PDF">pdf</a>, <a href="/format/2402.13971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-indice B-series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bruned%2C+Y">Yvain Bruned</a>, 
<a href="/search/math?searchtype=author&query=Ebrahimi-Fard%2C+K">Kurusch Ebrahimi-Fard</a>, 
<a href="/search/math?searchtype=author&query=Hou%2C+Y">Yingtong Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel way to describe numerical methods for ordinary
differential equations via the notion of multi-indice. The main idea is to
replace rooted trees in Butcher's B-series by multi-indices. The latter were
introduced recently in the context of describing solutions of singular
stochastic partial differential equations. The combinatorial shift away from
rooted trees allows for a compressed description of numerical schemes.
Moreover, these multi-indices B-series characterise uniquely the Taylor
development of local and affine equivariant maps.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13973" title="Abstract">arXiv:2402.13973</a> [<a href="/pdf/2402.13973" title="Download PDF">pdf</a>, <a href="/format/2402.13973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear-Time Graph Neural Networks for Scalable Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+R">Rui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jian Pei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaorui Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, accepted by The Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In an era of information explosion, recommender systems are vital tools to
deliver personalized recommendations for users. The key of recommender systems
is to forecast users' future behaviors based on previous user-item
interactions. Due to their strong expressive power of capturing high-order
connectivities in user-item interaction data, recent years have witnessed a
rising interest in leveraging Graph Neural Networks (GNNs) to boost the
prediction performance of recommender systems. Nonetheless, classic Matrix
Factorization (MF) and Deep Neural Network (DNN) approaches still play an
important role in real-world large-scale recommender systems due to their
scalability advantages. Despite the existence of GNN-acceleration solutions, it
remains an open question whether GNN-based recommender systems can scale as
efficiently as classic MF and DNN methods. In this paper, we propose a
Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender
systems to achieve comparable scalability as classic MF approaches while
maintaining GNNs' powerful expressiveness for superior prediction accuracy.
Extensive experiments and ablation studies are presented to validate the
effectiveness and scalability of the proposed algorithm. Our implementation
based on PyTorch is available.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13975" title="Abstract">arXiv:2402.13975</a> [<a href="/pdf/2402.13975" title="Download PDF">pdf</a>, <a href="/ps/2402.13975" title="Download PostScript">ps</a>, <a href="/format/2402.13975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A sublinear-time randomized algorithm for column and row subset  selection based on strong rank-revealing QR factorizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cortinovis%2C+A">Alice Cortinovis</a>, 
<a href="/search/math?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, we analyze a sublinear-time algorithm for selecting a few rows
and columns of a matrix for low-rank approximation purposes. The algorithm is
based on an initial uniformly random selection of rows and columns, followed by
a refinement of this choice using a strong rank-revealing QR factorization. We
prove bounds on the error of the corresponding low-rank approximation (more
precisely, the CUR approximation error) when the matrix is a perturbation of a
low-rank matrix that can be factorized into the product of matrices with
suitable incoherence and/or sparsity assumptions.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13979" title="Abstract">arXiv:2402.13979</a> [<a href="/pdf/2402.13979" title="Download PDF">pdf</a>, <a href="/format/2402.13979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Importance of Architecture Choice in Deep Learning for Climate  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dr%C3%A4ger%2C+S">Simon Dr&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Sonnewald%2C+M">Maike Sonnewald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine Learning has become a pervasive tool in climate science applications.
However, current models fail to address nonstationarity induced by
anthropogenic alterations in greenhouse emissions and do not routinely quantify
the uncertainty of proposed projections. In this paper, we model the Atlantic
Meridional Overturning Circulation (AMOC) which is of major importance to
climate in Europe and the US East Coast by transporting warm water to these
regions, and has the potential for abrupt collapse. We can generate arbitrarily
extreme climate scenarios through arbitrary time scales which we then predict
using neural networks. Our analysis shows that the AMOC is predictable using
neural networks under a diverse set of climate scenarios. Further experiments
reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead
of imitating its progression through autocorrelation. With quantified
uncertainty, an intriguing pattern of "spikes" before critical points of
collapse in the AMOC casts doubt on previous analyses that predicted an AMOC
collapse within this century. Our results show that Bayesian Neural Networks
perform poorly compared to more dense architectures and care should be taken
when applying neural networks to nonstationary scenarios such as climate
projections. Further, our results highlight that big NN models might have
difficulty in modeling global Earth System dynamics accurately and be
successfully applied in nonstationary climate scenarios due to the physics
being challenging for neural networks to capture.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13984" title="Abstract">arXiv:2402.13984</a> [<a href="/pdf/2402.13984" title="Download PDF">pdf</a>, <a href="/format/2402.13984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability-Aware Training of Neural Network Interatomic Potentials with  Differentiable Boltzmann Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raja%2C+S">Sanjeev Raja</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+I">Ishan Amin</a>, 
<a href="/search/cs?searchtype=author&query=Pedregosa%2C+F">Fabian Pedregosa</a>, 
<a href="/search/cs?searchtype=author&query=Krishnapriyan%2C+A+S">Aditi S. Krishnapriyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Materials Science (cond-mat.mtrl-sci); Chemical Physics (physics.chem-ph); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Neural network interatomic potentials (NNIPs) are an attractive alternative
to ab-initio methods for molecular dynamics (MD) simulations. However, they can
produce unstable simulations which sample unphysical states, limiting their
usefulness for modeling phenomena occurring over longer timescales. To address
these challenges, we present Stability-Aware Boltzmann Estimator (StABlE)
Training, a multi-modal training procedure which combines conventional
supervised training from quantum-mechanical energies and forces with reference
system observables, to produce stable and accurate NNIPs. StABlE Training
iteratively runs MD simulations to seek out unstable regions, and corrects the
instabilities via supervision with a reference observable. The training
procedure is enabled by the Boltzmann Estimator, which allows efficient
computation of gradients required to train neural networks to system
observables, and can detect both global and local instabilities. We demonstrate
our methodology across organic molecules, tetrapeptides, and condensed phase
systems, along with using three modern NNIP architectures. In all three cases,
StABlE-trained models achieve significant improvements in simulation stability
and recovery of structural and dynamic observables. In some cases,
StABlE-trained models outperform conventional models trained on datasets 50
times larger. As a general framework applicable across NNIP architectures and
systems, StABlE Training is a powerful tool for training stable and accurate
NNIPs, particularly in the absence of large reference datasets.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13985" title="Abstract">arXiv:2402.13985</a> [<a href="/pdf/2402.13985" title="Download PDF">pdf</a>, <a href="/format/2402.13985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Distributed Computation of the Minimum Triangle Edge Transversal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Censor-Hillel%2C+K">Keren Censor-Hillel</a>, 
<a href="/search/cs?searchtype=author&query=Khoury%2C+M">Majd Khoury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The distance of a graph from being triangle-free is a fundamental graph
parameter, counting the number of edges that need to be removed from a graph in
order for it to become triangle-free. Its corresponding computational problem
is the classic minimum triangle edge transversal problem, and its normalized
value is the baseline for triangle-freeness testing algorithms. While
triangle-freeness testing has been successfully studied in the distributed
setting, computing the distance itself in a distributed setting is unknown, to
the best of our knowledge, despite being well-studied in the centralized
setting.
<br />This work addresses the computation of the minimum triangle edge transversal
in distributed networks. We show with a simple warm-up construction that this
is a global task, requiring $\Omega(D)$ rounds even in the $\mathsf{LOCAL}$
model with unbounded messages, where $D$ is the diameter of the network.
However, we show that approximating this value can be done much faster. A
$(1+\epsilon)$-approximation can be obtained in $\text{poly}\log{n}$ rounds,
where $n$ is the size of the network graph. Moreover, faster approximations can
be obtained, at the cost of increasing the approximation factor to roughly 3,
by a reduction to the minimum hypergraph vertex cover problem. With a time
overhead of the maximum degree $\Delta$, this can also be applied to the
$\mathsf{CONGEST}$ model, in which messages are bounded.
<br />Our key technical contribution is proving that computing an exact solution is
``as hard as it gets'' in $\mathsf{CONGEST}$, requiring a near-quadratic number
of rounds. Because this problem is an edge selection problem, as opposed to
previous lower bounds that were for node selection problems, major challenges
arise in constructing the lower bound, requiring us to develop novel
ingredients.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13987" title="Abstract">arXiv:2402.13987</a> [<a href="/pdf/2402.13987" title="Download PDF">pdf</a>, <a href="/format/2402.13987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple and Yet Fairly Effective Defense for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ennadir%2C+S">Sofiane Ennadir</a>, 
<a href="/search/cs?searchtype=author&query=Abbahaddou%2C+Y">Yassine Abbahaddou</a>, 
<a href="/search/cs?searchtype=author&query=Lutzeyer%2C+J+F">Johannes F. Lutzeyer</a>, 
<a href="/search/cs?searchtype=author&query=Vazirgiannis%2C+M">Michalis Vazirgiannis</a>, 
<a href="/search/cs?searchtype=author&query=Bostr%C3%B6m%2C+H">Henrik Bostr&#xf6;m</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have emerged as the dominant approach for
machine learning on graph-structured data. However, concerns have arisen
regarding the vulnerability of GNNs to small adversarial perturbations.
Existing defense methods against such perturbations suffer from high time
complexity and can negatively impact the model's performance on clean graphs.
To address these challenges, this paper introduces NoisyGNNs, a novel defense
method that incorporates noise into the underlying model's architecture. We
establish a theoretical connection between noise injection and the enhancement
of GNN robustness, highlighting the effectiveness of our approach. We further
conduct extensive empirical evaluations on the node classification task to
validate our theoretical findings, focusing on two popular GNNs: the GCN and
GIN. The results demonstrate that NoisyGNN achieves superior or comparable
defense performance to existing methods while minimizing added time complexity.
The NoisyGNN approach is model-agnostic, allowing it to be integrated with
different GNN architectures. Successful combinations of our NoisyGNN approach
with existing defense techniques demonstrate even further improved adversarial
defense results. Our code is publicly available at:
https://github.com/Sennadir/NoisyGNN.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13989" title="Abstract">arXiv:2402.13989</a> [<a href="/pdf/2402.13989" title="Download PDF">pdf</a>, <a href="/format/2402.13989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yongcun Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zuazua%2C+E">Enrique Zuazua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
<p class="mathjax">Federated learning (FL) is a promising framework for learning from
distributed data while maintaining privacy. The development of efficient FL
algorithms encounters various challenges, including heterogeneous data and
systems, limited communication capacities, and constrained local computational
resources. Recently developed FedADMM methods show great resilience to both
data and system heterogeneity. However, they still suffer from performance
deterioration if the hyperparameters are not carefully tuned. To address this
issue, we propose an inexact and self-adaptive FedADMM algorithm, termed
FedADMM-InSa. First, we design an inexactness criterion for the clients' local
updates to eliminate the need for empirically setting the local training
accuracy. This inexactness criterion can be assessed by each client
independently based on its unique condition, thereby reducing the local
computational cost and mitigating the undesirable straggle effect. The
convergence of the resulting inexact ADMM is proved under the assumption of
strongly convex loss functions. Additionally, we present a self-adaptive scheme
that dynamically adjusts each client's penalty parameter, enhancing algorithm
robustness by mitigating the need for empirical penalty parameter choices for
each client. Extensive numerical experiments on both synthetic and real-world
datasets are conducted. As validated by some numerical tests, our proposed
algorithm can reduce the clients' local computational load significantly and
also accelerate the learning process compared to the vanilla FedADMM.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13991" title="Abstract">arXiv:2402.13991</a> [<a href="/pdf/2402.13991" title="Download PDF">pdf</a>, <a href="/format/2402.13991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysing The Impact of Sequence Composition on Language Model  Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuanbin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Staniszewski%2C+K">Konrad Staniszewski</a>, 
<a href="/search/cs?searchtype=author&query=Tworkowski%2C+S">Szymon Tworkowski</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%82o%C5%9B%2C+P">Piotr Mi&#x142;o&#x15b;</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Minervini%2C+P">Pasquale Minervini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Most language model pre-training frameworks concatenate multiple documents
into fixed-length sequences and use causal masking to compute the likelihood of
each token given its context; this strategy is widely adopted due to its
simplicity and efficiency. However, to this day, the influence of the
pre-training sequence composition strategy on the generalisation properties of
the model remains under-explored. In this work, we find that applying causal
masking can lead to the inclusion of distracting information from previous
documents during pre-training, which negatively impacts the performance of the
models on language modelling and downstream tasks. In intra-document causal
masking, the likelihood of each token is only conditioned on the previous
tokens in the same document, eliminating potential distracting information from
previous documents and significantly improving performance. Furthermore, we
find that concatenating related documents can reduce some potential
distractions during pre-training, and our proposed efficient retrieval-based
sequence construction method, BM25Chunk, can improve in-context learning
(+11.6\%), knowledge memorisation (+9.8\%), and context utilisation (+7.2\%)
abilities of language models without sacrificing efficiency.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13992" title="Abstract">arXiv:2402.13992</a> [<a href="/pdf/2402.13992" title="Download PDF">pdf</a>, <a href="/format/2402.13992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meditating in Live Stream: An Autoethnographic and Interview Study to  Investigate Motivations, Interactions and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingjin Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiajing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Leshed%2C+G">Gilly Leshed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint for CSCW24
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW1, Article 140.
  Publication date: April 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Mindfulness practice has many mental and physical well-being benefits. With
the increased popularity of live stream technologies and the impact of
COVID-19, many people have turned to live stream tools to participate in online
meditation sessions. To better understand the practices, challenges, and
opportunities in live-stream meditation, we conducted a three-month
autoethnographic study, during which two researchers participated in
live-stream meditation sessions as the audience. Then we conducted a follow-up
semi-structured interview study with 10 experienced live meditation teachers
who use different live-stream tools. We found that live meditation, although
having a weaker social presence than in-person meditation, facilitates
attendees in establishing a practice routine and connecting with other
meditators. Teachers use live streams to deliver the meditation practice to the
world which also enhances their practice and brand building. We identified the
challenges of using live-stream tools for meditation from the perspectives of
both audiences and teachers, and provided design recommendations to better
utilize live meditation as a resource for mental wellbeing.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14000" title="Abstract">arXiv:2402.14000</a> [<a href="/pdf/2402.14000" title="Download PDF">pdf</a>, <a href="/format/2402.14000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time 3D-aware Portrait Editing from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qingyan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zifan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+H">Hao Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiuyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Ceyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wetzstein%2C+G">Gordon Wetzstein</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work presents 3DPE, a practical tool that can efficiently edit a face
image following given prompts, like reference images or text descriptions, in
the 3D-aware manner. To this end, a lightweight module is distilled from a 3D
portrait generator and a text-to-image model, which provide prior knowledge of
face geometry and open-vocabulary editing capability, respectively. Such a
design brings two compelling advantages over existing approaches. First, our
system achieves real-time editing with a feedforward network (i.e., ~0.04s per
image), over 100x faster than the second competitor. Second, thanks to the
powerful priors, our module could focus on the learning of editing-related
variations, such that it manages to handle various types of editing
simultaneously in the training phase and further supports fast adaptation to
user-specified novel types of editing during inference (e.g., with ~5min
fine-tuning per case). The code, the model, and the interface will be made
publicly available to facilitate future research.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14002" title="Abstract">arXiv:2402.14002</a> [<a href="/pdf/2402.14002" title="Download PDF">pdf</a>, <a href="/format/2402.14002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ioste%2C+A">Aline Ioste</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models with transformer architecture have revolutionized the
domain of text generation, setting unprecedented benchmarks. Despite their
impressive capabilities, LLMs have been criticized for generating outcomes that
deviate from factual accuracy or display logical inconsistencies, phenomena
commonly referred to as hallucinations. This term, however, has often been
misapplied to any results deviating from the instructor's expectations, which
this paper defines as attention misdirection rather than true hallucinations.
Understanding the distinction between hallucinations and attention misdirection
becomes increasingly relevant in business contexts, where the ramifications of
such errors can significantly impact the value extraction from these inherently
pre-trained models. This paper highlights the best practices of the PGI,
Persona, Grouping, and Intelligence, method, a strategic framework that
achieved a remarkable error rate of only 3,15 percent across 4,000 responses
generated by GPT in response to a real business challenge. It emphasizes that
by equipping experimentation with knowledge, businesses can unlock
opportunities for innovation through the use of these natively pre-trained
models. This reinforces the notion that strategic application grounded in a
skilled team can maximize the benefits of emergent technologies such as the
LLMs.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14005" title="Abstract">arXiv:2402.14005</a> [<a href="/pdf/2402.14005" title="Download PDF">pdf</a>, <a href="/format/2402.14005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Elicitation in Agency Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Serena Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Ligett%2C+K">Katrina Ligett</a>, 
<a href="/search/cs?searchtype=author&query=McAfee%2C+R+P">R. Preston McAfee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">Rapid progress in scalable, commoditized tools for data collection and data
processing has made it possible for firms and policymakers to employ ever more
complex metrics as guides for decision-making. These developments have
highlighted a prevailing challenge -- deciding *which* metrics to compute. In
particular, a firm's ability to compute a wider range of existing metrics does
not address the problem of *unknown unknowns*, which reflects informational
limitations on the part of the firm. To guide the choice of metrics in the face
of this informational problem, we turn to the evaluated agents themselves, who
may have more information than a principal about how to measure outcomes
effectively. We model this interaction as a simple agency game, where we ask:
*When does an agent have an incentive to reveal the observability of a
cost-correlated variable to the principal?* There are two effects: better
information reduces the agent's information rents but also makes some projects
go forward that otherwise would fail. We show that the agent prefers to reveal
information that exposes a strong enough differentiation between high and low
costs. Expanding the agent's action space to include the ability to *garble*
their information, we show that the agent often prefers to garble over full
revelation. Still, giving the agent the ability to garble can lead to higher
total welfare. Our model has analogies with price discrimination, and we
leverage some of these synergies to analyze total welfare.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14007" title="Abstract">arXiv:2402.14007</a> [<a href="/pdf/2402.14007" title="Download PDF">pdf</a>, <a href="/format/2402.14007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Binglin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongkun Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text watermarking technology aims to tag and identify content produced by
large language models (LLMs) to prevent misuse. In this study, we introduce the
concept of ''cross-lingual consistency'' in text watermarking, which assesses
the ability of text watermarks to maintain their effectiveness after being
translated into other languages. Preliminary empirical results from two LLMs
and three watermarking methods reveal that current text watermarking
technologies lack consistency when texts are translated into various languages.
Based on this observation, we propose a Cross-lingual Watermark Removal Attack
(CWRA) to bypass watermarking by first obtaining a response from an LLM in a
pivot language, which is then translated into the target language. CWRA can
effectively remove watermarks by reducing the Area Under the Curve (AUC) from
0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors
that contribute to the cross-lingual consistency in text watermarking and
propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14008" title="Abstract">arXiv:2402.14008</a> [<a href="/pdf/2402.14008" title="Download PDF">pdf</a>, <a href="/format/2402.14008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoqun He</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Renjie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuzhuo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+Z+L">Zhen Leng Thai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14009" title="Abstract">arXiv:2402.14009</a> [<a href="/pdf/2402.14009" title="Download PDF">pdf</a>, <a href="/format/2402.14009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berzins%2C+A">Arturs Berzins</a>, 
<a href="/search/cs?searchtype=author&query=Radler%2C+A">Andreas Radler</a>, 
<a href="/search/cs?searchtype=author&query=Sanokowski%2C+S">Sebastian Sanokowski</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We introduce the concept of geometry-informed neural networks (GINNs), which
encompass (i) learning under geometric constraints, (ii) neural fields as a
suitable representation, and (iii) generating diverse solutions to
under-determined systems often encountered in geometric tasks. Notably, the
GINN formulation does not require training data, and as such can be considered
generative modeling driven purely by constraints. We add an explicit diversity
loss to mitigate mode collapse. We consider several constraints, in particular,
the connectedness of components which we convert to a differentiable loss
through Morse theory. Experimentally, we demonstrate the efficacy of the GINN
learning paradigm across a range of two and three-dimensional scenarios with
increasing levels of complexity.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14012" title="Abstract">arXiv:2402.14012</a> [<a href="/pdf/2402.14012" title="Download PDF">pdf</a>, <a href="/format/2402.14012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chasing Convex Functions with Long-term Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lechowicz%2C+A">Adam Lechowicz</a>, 
<a href="/search/cs?searchtype=author&query=Christianson%2C+N">Nicolas Christianson</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce and study a family of online metric problems with long-term
constraints. In these problems, an online player makes decisions $\mathbf{x}_t$
in a metric space $(X,d)$ to simultaneously minimize their hitting cost
$f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the
time horizon $T$, the player must satisfy a long-term demand constraint
$\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction
of demand satisfied at time $t$. Such problems can find a wide array of
applications to online resource allocation in sustainable energy and computing
systems. We devise optimal competitive and learning-augmented algorithms for
specific instantiations of these problems, and further show that our proposed
algorithms perform well in numerical experiments.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14013" title="Abstract">arXiv:2402.14013</a> [<a href="/pdf/2402.14013" title="Download PDF">pdf</a>, <a href="/ps/2402.14013" title="Download PostScript">ps</a>, <a href="/format/2402.14013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Misalignment, Learning, and Ranking: Harnessing Users Limited Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Arpit Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Niazadeh%2C+R">Rad Niazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+P">Prathamesh Patil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In digital health and EdTech, recommendation systems face a significant
challenge: users often choose impulsively, in ways that conflict with the
platform's long-term payoffs. This misalignment makes it difficult to
effectively learn to rank items, as it may hinder exploration of items with
greater long-term payoffs. Our paper tackles this issue by utilizing users'
limited attention spans. We propose a model where a platform presents items
with unknown payoffs to the platform in a ranked list to $T$ users over time.
Each user selects an item by first considering a prefix window of these ranked
items and then picking the highest preferred item in that window (and the
platform observes its payoff for this item). We study the design of online
bandit algorithms that obtain vanishing regret against hindsight optimal
benchmarks.
<br />We first consider adversarial window sizes and stochastic iid payoffs. We
design an active-elimination-based algorithm that achieves an optimal
instance-dependent regret bound of $O(\log(T))$, by showing matching regret
upper and lower bounds. The key idea is using the combinatorial structure of
the problem to either obtain a large payoff from each item or to explore by
getting a sample from that item. This method systematically narrows down the
item choices to enhance learning efficiency and payoff.
<br />Second, we consider adversarial payoffs and stochastic iid window sizes. We
start from the full-information problem of finding the permutation that
maximizes the expected payoff. By a novel combinatorial argument, we
characterize the polytope of admissible item selection probabilities by a
permutation and show it has a polynomial-size representation. Using this
representation, we show how standard algorithms for adversarial online linear
optimization in the space of admissible probabilities can be used to obtain a
polynomial-time algorithm with $O(\sqrt{T})$ regret.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14015" title="Abstract">arXiv:2402.14015</a> [<a href="/pdf/2402.14015" title="Download PDF">pdf</a>, <a href="/format/2402.14015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrective Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Shashwat Goel</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+A">Ameya Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+A">Amartya Sanyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Machine Learning models increasingly face data integrity challenges due to
the use of large-scale training datasets drawn from the internet. We study what
model developers can do if they detect that some data was manipulated or
incorrect. Such manipulated data can cause adverse effects like vulnerability
to backdoored samples, systematic biases, and in general, reduced accuracy on
certain input domains. Often, all manipulated training samples are not known,
and only a small, representative subset of the affected data is flagged.
<br />We formalize "Corrective Machine Unlearning" as the problem of mitigating the
impact of data affected by unknown manipulations on a trained model, possibly
knowing only a subset of impacted samples. We demonstrate that the problem of
corrective unlearning has significantly different requirements from traditional
privacy-oriented unlearning. We find most existing unlearning methods,
including the gold-standard retraining-from-scratch, require most of the
manipulated data to be identified for effective corrective unlearning. However,
one approach, SSD, achieves limited success in unlearning adverse effects with
just a small portion of the manipulated samples, showing the tractability of
this setting. We hope our work spurs research towards developing better methods
for corrective unlearning and offers practitioners a new strategy to handle
data integrity challenges arising from web-scale training.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14016" title="Abstract">arXiv:2402.14016</a> [<a href="/pdf/2402.14016" title="Download PDF">pdf</a>, <a href="/format/2402.14016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vyas Raina</a>, 
<a href="/search/cs?searchtype=author&query=Liusie%2C+A">Adian Liusie</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M">Mark Gales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are powerful zero-shot assessors and are
increasingly used in real-world situations such as for written exams or
benchmarking systems. Despite this, no existing work has analyzed the
vulnerability of judge-LLMs against adversaries attempting to manipulate
outputs. This work presents the first study on the adversarial robustness of
assessment LLMs, where we search for short universal phrases that when appended
to texts can deceive LLMs to provide high assessment scores. Experiments on
SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise
LLM-comparative assessment are vulnerable to simple concatenation attacks,
where in particular LLM-scoring is very susceptible and can yield maximum
assessment scores irrespective of the input text quality. Interestingly, such
attacks are transferable and phrases learned on smaller open-source LLMs can be
applied to larger closed-source models, such as GPT3.5. This highlights the
pervasive nature of the adversarial vulnerabilities across different judge-LLM
sizes, families and methods. Our findings raise significant concerns on the
reliability of LLMs-as-a-judge methods, and underscore the importance of
addressing vulnerabilities in LLM assessment methods before deployment in
high-stakes real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14017" title="Abstract">arXiv:2402.14017</a> [<a href="/pdf/2402.14017" title="Download PDF">pdf</a>, <a href="/format/2402.14017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D-Flow: Differentiating through Flows for Controlled Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-Hamu%2C+H">Heli Ben-Hamu</a>, 
<a href="/search/cs?searchtype=author&query=Puny%2C+O">Omri Puny</a>, 
<a href="/search/cs?searchtype=author&query=Gat%2C+I">Itai Gat</a>, 
<a href="/search/cs?searchtype=author&query=Karrer%2C+B">Brian Karrer</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+U">Uriel Singer</a>, 
<a href="/search/cs?searchtype=author&query=Lipman%2C+Y">Yaron Lipman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Taming the generation outcome of state of the art Diffusion and Flow-Matching
(FM) models without having to re-train a task-specific model unlocks a powerful
tool for solving inverse problems, conditional generation, and controlled
generation in general. In this work we introduce D-Flow, a simple framework for
controlling the generation process by differentiating through the flow,
optimizing for the source (noise) point. We motivate this framework by our key
observation stating that for Diffusion/FM models trained with Gaussian
probability paths, differentiating through the generation process projects
gradient on the data manifold, implicitly injecting the prior into the
optimization process. We validate our framework on linear and non-linear
controlled generation problems including: image and audio inverse problems and
conditional molecule generation reaching state of the art performance across
all.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14020" title="Abstract">arXiv:2402.14020</a> [<a href="/pdf/2402.14020" title="Download PDF">pdf</a>, <a href="/format/2402.14020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coercing LLMs to do and reveal (almost) anything
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+A">Alex Stein</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+M">Manli Shu</a>, 
<a href="/search/cs?searchtype=author&query=Saifullah%2C+K">Khalid Saifullah</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yuxin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages. Implementation available at <a href="https://github.com/JonasGeiping/carving">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">It has recently been shown that adversarial attacks on large language models
(LLMs) can "jailbreak" the model into making harmful statements. In this work,
we argue that the spectrum of adversarial attacks on LLMs is much larger than
merely jailbreaking. We provide a broad overview of possible attack surfaces
and attack goals. Based on a series of concrete examples, we discuss,
categorize and systematize attacks that coerce varied unintended behaviors,
such as misdirection, model control, denial-of-service, or data extraction.
<br />We analyze these attacks in controlled experiments, and find that many of
them stem from the practice of pre-training LLMs with coding capabilities, as
well as the continued existence of strange "glitch" tokens in common LLM
vocabularies that should be removed for security reasons.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu, 22 Feb 24</h3>
<dl>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07240" title="Abstract">arXiv:2402.07240</a> (cross-list from math.ST) [<a href="/pdf/2402.07240" title="Download PDF">pdf</a>, <a href="/format/2402.07240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thresholded Oja does Sparse PCA?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kumar%2C+S">Syamantak Kumar</a>, 
<a href="/search/math?searchtype=author&query=Sarkar%2C+P">Purnamrita Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of Sparse Principal Component Analysis (PCA) when the
ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on
sparse PCA in the offline setting, where all the data is available for multiple
passes. In contrast, when the population eigenvector is $s$-sparse, streaming
algorithms that have $O(d)$ storage and $O(nd)$ time complexity either
typically require strong initialization conditions or have a suboptimal error.
We show that a simple algorithm that thresholds and renormalizes the output of
Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is
very surprising because, without thresholding, the Oja vector has a large
error. Our analysis centers around bounding the entries of the unnormalized Oja
vector, which involves the projection of a product of independent random
matrices on a random initial vector. This is nontrivial and novel since
previous analyses of Oja's algorithm and matrix products have been done when
the trace of the population covariance matrix is bounded while in our setting,
this quantity can be as large as $n$.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13259" title="Abstract">arXiv:2402.13259</a> (cross-list from stat.ME) [<a href="/pdf/2402.13259" title="Download PDF">pdf</a>, <a href="/format/2402.13259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Discrete-Event Simulation of Markovian Queueing Networks through  Euler Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hong%2C+L+J">L.Jeff Hong</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Y">Yingda Song</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+T">Tan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA); Probability (math.PR)

</div>
<p class="mathjax">The efficient management of large-scale queueing networks is critical for a
variety of sectors, including healthcare, logistics, and customer service,
where system performance has profound implications for operational
effectiveness and cost management. To address this key challenge, our paper
introduces simulation techniques tailored for complex, large-scale Markovian
queueing networks. We develop two simulation schemes based on Euler
approximation, namely the backward and forward schemes. These schemes can
accommodate time-varying dynamics and are optimized for efficient
implementation using vectorization. Assuming a feedforward queueing network
structure, we establish that the two schemes provide stochastic upper and lower
bounds for the system state, while the approximation error remains bounded over
the simulation horizon. With the recommended choice of time step, we show that
our approximation schemes exhibit diminishing asymptotic relative error as the
system scales up, while maintaining much lower computational complexity
compared to traditional discrete-event simulation and achieving speedups up to
tens of thousands times. This study highlights the substantial potential of
Euler approximation in simulating large-scale discrete systems.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13270" title="Abstract">arXiv:2402.13270</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.13270" title="Download PDF">pdf</a>, <a href="/format/2402.13270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Tropical Cyclone Intensity Forecasting with Multi-modal  Multi-scale Causal Autoregressive Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+K">Kang Chen</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/physics?searchtype=author&query=Han%2C+T">Tao Han</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/physics?searchtype=author&query=Bai%2C+L">Lei Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Accurate forecasting of Tropical cyclone (TC) intensity is crucial for
formulating disaster risk reduction strategies. Current methods predominantly
rely on limited spatiotemporal information from ERA5 data and neglect the
causal relationships between these physical variables, failing to fully capture
the spatial and temporal patterns required for intensity forecasting. To
address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive
model (MSCAR), which is the first model that combines causal relationships with
large-scale multi-modal data for global TC intensity autoregressive
forecasting. Furthermore, given the current absence of a TC dataset that offers
a wide range of spatial variables, we present the Satellite and ERA5-based
Tropical Cyclone Dataset (SETCD), which stands as the longest and most
comprehensive global dataset related to TCs. Experiments on the dataset show
that MSCAR outperforms the state-of-the-art methods, achieving maximum
reductions in global and regional forecast errors of 9.52% and 6.74%,
respectively. The code and dataset are publicly available at
https://anonymous.4open.science/r/MSCAR.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13275" title="Abstract">arXiv:2402.13275</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.13275" title="Download PDF">pdf</a>, <a href="/format/2402.13275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementation of a Model of the Cortex Basal Ganglia Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Arakawa%2C+N">Naoya Arakawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This article presents a simple model of the cortex-basal ganglia-thalamus
loop, which is thought to serve for action selection and executions, and
reports the results of its implementation. The model is based on the hypothesis
that the cerebral cortex predicts actions, while the basal ganglia use
reinforcement learning to decide whether to perform the actions predicted by
the cortex. The implementation is intended to be used as a component of models
of the brain consisting of cortical regions or brain-inspired cognitive
architectures.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13276" title="Abstract">arXiv:2402.13276</a> (cross-list from eess.AS) [<a href="/pdf/2402.13276" title="Download PDF">pdf</a>, <a href="/format/2402.13276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate  Speech into Large Language Models for Depression Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hexin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+K">Kaishuai Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qiquan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Daijiao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+B">Beena Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Epps%2C+J">Julien Epps</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD)

</div>
<p class="mathjax">Depression is a critical concern in global mental health, prompting extensive
research into AI-based detection methods. Among various AI technologies, Large
Language Models (LLMs) stand out for their versatility in mental healthcare
applications. However, their primary limitation arises from their exclusive
dependence on textual input, which constrains their overall capabilities.
Furthermore, the utilization of LLMs in identifying and analyzing depressive
states is still relatively untapped. In this paper, we present an innovative
approach to integrating acoustic speech information into the LLMs framework for
multimodal depression detection. We investigate an efficient method for
depression detection by integrating speech signals into LLMs utilizing Acoustic
Landmarks. By incorporating acoustic landmarks, which are specific to the
pronunciation of spoken words, our method adds critical dimensions to text
transcripts. This integration also provides insights into the unique speech
patterns of individuals, revealing the potential mental states of individuals.
Evaluations of the proposed approach on the DAIC-WOZ dataset reveal
state-of-the-art results when compared with existing Audio-Text baselines. In
addition, this approach is not only valuable for the detection of depression
but also represents a new perspective in enhancing the ability of LLMs to
comprehend and process speech signals.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13285" title="Abstract">arXiv:2402.13285</a> (cross-list from stat.ML) [<a href="/pdf/2402.13285" title="Download PDF">pdf</a>, <a href="/format/2402.13285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization  Bounds with Complexity Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Viallard%2C+P">Paul Viallard</a>, 
<a href="/search/stat?searchtype=author&query=Emonet%2C+R">R&#xe9;mi Emonet</a>, 
<a href="/search/stat?searchtype=author&query=Habrard%2C+A">Amaury Habrard</a>, 
<a href="/search/stat?searchtype=author&query=Morvant%2C+E">Emilie Morvant</a>, 
<a href="/search/stat?searchtype=author&query=Zantedeschi%2C+V">Valentina Zantedeschi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In statistical learning theory, a generalization bound usually involves a
complexity measure imposed by the considered theoretical framework. This limits
the scope of such bounds, as other forms of capacity measures or
regularizations are used in algorithms. In this paper, we leverage the
framework of disintegrated PAC-Bayes bounds to derive a general generalization
bound instantiable with arbitrary complexity measures. One trick to prove such
a result involves considering a commonly used family of distributions: the
Gibbs distributions. Our bound stands in probability jointly over the
hypothesis and the learning sample, which allows the complexity to be adapted
to the generalization gap as it can be customized to fit both the hypothesis
class and the task.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13297" title="Abstract">arXiv:2402.13297</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.13297" title="Download PDF">pdf</a>, <a href="/format/2402.13297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Deep Learning and Synthetic Biology: A Co-Design Approach  for Enhancing Gene Expression via N-terminal Coding Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yan%2C+Z">Zhanglu Yan</a>, 
<a href="/search/q-bio?searchtype=author&query=Chu%2C+W">Weiran Chu</a>, 
<a href="/search/q-bio?searchtype=author&query=Sheng%2C+Y">Yuhua Sheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+K">Kaiwen Tang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+S">Shida Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Y">Yanfeng Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wong%2C+W">Weng-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">N-terminal coding sequence (NCS) influences gene expression by impacting the
translation initiation rate. The NCS optimization problem is to find an NCS
that maximizes gene expression. The problem is important in genetic
engineering. However, current methods for NCS optimization such as rational
design and statistics-guided approaches are labor-intensive yield only
relatively small improvements. This paper introduces a deep learning/synthetic
biology co-designed few-shot training workflow for NCS optimization. Our method
utilizes k-nearest encoding followed by word2vec to encode the NCS, then
performs feature extraction using attention mechanisms, before constructing a
time-series network for predicting gene expression intensity, and finally a
direct search algorithm identifies the optimal NCS with limited training data.
We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a
reporting protein of NCSs, and employed the fluorescence enhancement factor as
the metric of NCS optimization. Within just six iterative experiments, our
model generated an NCS (MLD62) that increased average GFP expression by
5.41-fold, outperforming the state-of-the-art NCS designs. Extending our
findings beyond GFP, we showed that our engineered NCS (MLD62) can effectively
boost the production of N-acetylneuraminic acid by enhancing the expression of
the crucial rate-limiting GNA1 gene, demonstrating its practical utility. We
have open-sourced our NCS expression database and experimental procedures for
public use.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13321" title="Abstract">arXiv:2402.13321</a> (cross-list from hep-th) [<a href="/pdf/2402.13321" title="Download PDF">pdf</a>, <a href="/format/2402.13321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rigor with Machine Learning from Field Theory to the Poincar&#xe9;  Conjecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Gukov%2C+S">Sergei Gukov</a>, 
<a href="/search/hep-th?searchtype=author&query=Halverson%2C+J">James Halverson</a>, 
<a href="/search/hep-th?searchtype=author&query=Ruehle%2C+F">Fabian Ruehle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages. Preprint of edited version in Nature Reviews Physics. Please cite journal version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning techniques are increasingly powerful, leading to many
breakthroughs in the natural sciences, but they are often stochastic,
error-prone, and blackbox. How, then, should they be utilized in fields such as
theoretical physics and pure mathematics that place a premium on rigor and
understanding? In this Perspective we discuss techniques for obtaining rigor in
the natural sciences with machine learning. Non-rigorous methods may lead to
rigorous results via conjecture generation or verification by reinforcement
learning. We survey applications of these techniques-for-rigor ranging from
string theory to the smooth $4$d Poincar\'e conjecture in low-dimensional
topology. One can also imagine building direct bridges between machine learning
theory and either mathematics or theoretical physics. As examples, we describe
a new approach to field theory motivated by neural network theory, and a theory
of Riemannian metric flows induced by neural network gradient descent, which
encompasses Perelman's formulation of the Ricci flow that was utilized to
resolve the $3$d Poincar\'e conjecture.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13324" title="Abstract">arXiv:2402.13324</a> (cross-list from quant-ph) [<a href="/pdf/2402.13324" title="Download PDF">pdf</a>, <a href="/ps/2402.13324" title="Download PostScript">ps</a>, <a href="/format/2402.13324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bouaziz--Ermann%2C+S">Samuel Bouaziz--Ermann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Muguruza%2C+G">Garazi Muguruza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Pseudorandom Quantum States (PRS) were introduced by Ji, Liu and Song as
quantum analogous to Pseudorandom Generators. They are an ensemble of states
efficiently computable but computationally indistinguishable from Haar random
states. Subsequent works have shown that some cryptographic primitives can be
constructed from PRSs. Moreover, recent classical and quantum oracle
separations of PRS from One-Way Functions strengthen the interest in a purely
quantum alternative building block for quantum cryptography, potentially weaker
than OWFs.
<br />However, our lack of knowledge of extending or shrinking the number of qubits
of the PRS output still makes it difficult to reproduce some of the classical
proof techniques and results. Short-PRSs, that is PRSs with logarithmic size
output, have been introduced in the literature along with cryptographic
applications, but we still do not know how they relate to PRSs. Here we answer
half of the question, by showing that it is not possible to shrink the output
of a PRS from polynomial to logarithmic qubit length while still preserving the
pseudorandomness property, in a relativized way. More precisely, we show that
relative to Kretschmer's quantum oracle (TQC 2021) short-PRSs cannot exist
(while PRSs exist, as shown by Kretschmer's work).
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13326" title="Abstract">arXiv:2402.13326</a> (cross-list from q-fin.CP) [<a href="/pdf/2402.13326" title="Download PDF">pdf</a>, <a href="/format/2402.13326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Hedging with Market Impact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Neagu%2C+A">Andrei Neagu</a>, 
<a href="/search/q-fin?searchtype=author&query=Godin%2C+F">Fr&#xe9;d&#xe9;ric Godin</a>, 
<a href="/search/q-fin?searchtype=author&query=Simard%2C+C">Clarence Simard</a>, 
<a href="/search/q-fin?searchtype=author&query=Kosseim%2C+L">Leila Kosseim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Dynamic hedging is the practice of periodically transacting financial
instruments to offset the risk caused by an investment or a liability. Dynamic
hedging optimization can be framed as a sequential decision problem; thus,
Reinforcement Learning (RL) models were recently proposed to tackle this task.
However, existing RL works for hedging do not consider market impact caused by
the finite liquidity of traded instruments. Integrating such feature can be
crucial to achieve optimal performance when hedging options on stocks with
limited liquidity. In this paper, we propose a novel general market impact
dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers
several realistic features such as convex market impacts, and impact
persistence through time. The optimal policy obtained from the DRL model is
analysed using several option hedging simulations and compared to commonly used
procedures such as delta hedging. Results show our DRL model behaves better in
contexts of low liquidity by, among others: 1) learning the extent to which
portfolio rebalancing actions should be dampened or delayed to avoid high
costs, 2) factoring in the impact of features not considered by conventional
approaches, such as previous hedging errors through the portfolio value, and
the underlying asset's drift (i.e. the magnitude of its expected return).
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13352" title="Abstract">arXiv:2402.13352</a> (cross-list from quant-ph) [<a href="/pdf/2402.13352" title="Download PDF">pdf</a>, <a href="/format/2402.13352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Apak%2C+B">Boran Apak</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bandic%2C+M">Medina Bandic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sarkar%2C+A">Aritra Sarkar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Feld%2C+S">Sebastian Feld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum algorithms, represented as quantum circuits, can be used as
benchmarks for assessing the performance of quantum systems. Existing datasets,
widely utilized in the field, suffer from limitations in size and versatility,
leading researchers to employ randomly generated circuits. Random circuits are,
however, not representative benchmarks as they lack the inherent properties of
real quantum algorithms for which the quantum systems are manufactured. This
shortage of `useful' quantum benchmarks poses a challenge to advancing the
development and comparison of quantum compilers and hardware.
<br />This research aims to enhance the existing quantum circuit datasets by
generating what we refer to as `realistic-looking' circuits by employing the
Transformer machine learning architecture. For this purpose, we introduce
KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose
structure is based on quantum circuits derived from existing quantum algorithms
and follows the typical patterns of human-written algorithm-based code (e.g.,
order of gates and qubits). Our three-fold verification process, involving
manual inspection and Qiskit framework execution, transformer-based
classification, and structural analysis, demonstrates the efficacy of KetGPT in
producing large amounts of additional circuits that closely align with
algorithm-based structures. Beyond benchmarking, we envision KetGPT
contributing substantially to AI-driven quantum compilers and systems.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13378" title="Abstract">arXiv:2402.13378</a> (cross-list from econ.TH) [<a href="/pdf/2402.13378" title="Download PDF">pdf</a>, <a href="/format/2402.13378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable matching as transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Echenique%2C+F">Federico Echenique</a>, 
<a href="/search/econ?searchtype=author&query=Root%2C+J">Joseph Root</a>, 
<a href="/search/econ?searchtype=author&query=Sandomirskiy%2C+F">Fedor Sandomirskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study matching markets with aligned preferences and establish a connection
between common design objectives -- stability, efficiency, and fairness -- and
the theory of optimal transport. Optimal transport gives new insights into the
structural properties of matchings obtained from pursuing these objectives, and
into the trade-offs between different objectives. Matching markets with aligned
preferences provide a tractable stylized model capturing supply-demand
imbalances in a range of settings such as partnership formation, school choice,
organ donor exchange, and markets with transferable utility where bargaining
over transfers happens after a match is formed.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13400" title="Abstract">arXiv:2402.13400</a> (cross-list from stat.ML) [<a href="/pdf/2402.13400" title="Download PDF">pdf</a>, <a href="/format/2402.13400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Dimension of Self-Directed Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Devulapalli%2C+P">Pramith Devulapalli</a>, 
<a href="/search/stat?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ALT 2024 Camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the self-directed learning complexity has been an important
problem that has captured the attention of the online learning theory community
since the early 1990s. Within this framework, the learner is allowed to
adaptively choose its next data point in making predictions unlike the setting
in adversarial online learning.
<br />In this paper, we study the self-directed learning complexity in both the
binary and multi-class settings, and we develop a dimension, namely $SDdim$,
that exactly characterizes the self-directed learning mistake-bound for any
concept class. The intuition behind $SDdim$ can be understood as a two-player
game called the "labelling game". Armed with this two-player game, we calculate
$SDdim$ on a whole host of examples with notable results on axis-aligned
rectangles, VC dimension $1$ classes, and linear separators. We demonstrate
several learnability gaps with a central focus on self-directed learning and
offline sequence learning models that include either the best or worst
ordering. Finally, we extend our analysis to the self-directed binary agnostic
setting where we derive upper and lower bounds.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13436" title="Abstract">arXiv:2402.13436</a> (cross-list from physics.app-ph) [<a href="/pdf/2402.13436" title="Download PDF">pdf</a>, <a href="/format/2402.13436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimisation of design parameters to improve performance of a planar  electromagnetic actuator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Vikrant%2C+K+S">K. S. Vikrant</a>, 
<a href="/search/physics?searchtype=author&query=Dadkhah%2C+D">D. Dadkhah</a>, 
<a href="/search/physics?searchtype=author&query=Moheimani%2C+S+O+R">S. O. Reza Moheimani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Planar electromagnetic actuators based on the principle of linear motors are
widely employed for micro and nano positioning applications. These actuators
usually employ a planar magnetic platform driven by a co-planar electromagnetic
coil. While these actuators offer a large motion range and high positioning
resolution, their actuation bandwidth is limited due to relatively small
electromagnetic stiffness. We report optimization of the design parameters of
the electromagnetic coil and the magnetic assembly to maximize the
electromagnetic force and stiffness. Firstly, we derive closed-form expressions
for the electromagnetic forces and stiffness, which enable us to express these
quantities in terms of the design parameters of the actuator. Secondly, based
on these derived expressions, we estimate the optimum values of the design
parameters to maximize force and stiffness. Notably, for the optimum design
parameters, the force and stiffness per unit volume can be increased by two and
three orders of magnitude, respectively by reducing the pitch of the
electromagnetic coil by a factor of 10. Lastly, we develop an electromagnetic
actuator and evaluate its performance using a Microelectromechanical system
(MEMS) based force sensor. By operating the force sensor in a feedback loop, we
precisely measure the generated electromagnetic forces for different design
parameters of the actuator. The experimental results obtained align closely
with the analytical values, with an error of less than 15%.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13453" title="Abstract">arXiv:2402.13453</a> (cross-list from math.DS) [<a href="/pdf/2402.13453" title="Download PDF">pdf</a>, <a href="/ps/2402.13453" title="Download PostScript">ps</a>, <a href="/format/2402.13453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A rational logit dynamic for decision-making under uncertainty:  well-posedness, vanishing-noise limit, and numerical approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yoshioka%2C+H">Hidekazu Yoshioka</a>, 
<a href="/search/math?searchtype=author&query=Tsujimura%2C+M">Motoh Tsujimura</a>, 
<a href="/search/math?searchtype=author&query=Yoshioka%2C+Y">Yumi Yoshioka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first draft under review at some international conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The classical logit dynamic on a continuous action space for decision-making
un-der uncertainty is generalized to the dynamic where the exponential function
for the softmax part has been replaced by a rational one that includes the
former as a special case. We call the new dynamic as the rational logit
dynamic. The use of the rational logit function implies that the uncertainties
have a longer tail than that assumed in the classical one. We show that the
rational logit dynamic admits a unique measure-valued solution and the solution
can be approximated using a fi-nite difference discretization. We also show
that the vanishing-noise limit of the rational logit dynamic exists and is
different from the best-response one, demon-strating that influences of the
uncertainty tail persist in the rational logit dynamic. We finally apply the
rational logit dynamic to a unique fishing competition data that has been
recently acquired by the authors.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13469" title="Abstract">arXiv:2402.13469</a> (cross-list from quant-ph) [<a href="/pdf/2402.13469" title="Download PDF">pdf</a>, <a href="/format/2402.13469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Quantum Abstract Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+L">Liyi Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chang%2C+L">Le Chang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cleaveland%2C+R">Rance Cleaveland</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+M">Mingwei Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+X">Xiaodi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">This paper develops a model of quantum behavior that is intended to support
the abstract yet accurate design and functional verification of quantum
communication protocols. The work is motivated by the need for conceptual tools
for the development of quantum-communication systems that are usable by
non-specialists in quantum physics while also correctly capturing at a useful
abstraction the underlying quantum phenomena. Our approach involves defining a
quantum abstract machine (QAM) whose operations correspond to well-known
quantum circuits; these operations, however, are given direct abstract
semantics in a style similar to that of Berry's and Boudol's Chemical Abstract
Machine. This paper defines the QAM's semantics and shows via examples how it
may be used to model and reason about existing quantum communication protocols.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13523" title="Abstract">arXiv:2402.13523</a> (cross-list from eess.SP) [<a href="/pdf/2402.13523" title="Download PDF">pdf</a>, <a href="/format/2402.13523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Spectral, Temporal and Spatial Information for EEG-based  Alzheimer&#x27;s Disease Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Goerttler%2C+S">Stephan Goerttler</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+F">Fei He</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The prospect of future treatment warrants the development of cost-effective
screening for Alzheimer's disease (AD). A promising candidate in this regard is
electroencephalography (EEG), as it is one of the most economic imaging
modalities. Recent efforts in EEG analysis have shifted towards leveraging
spatial information, employing novel frameworks such as graph signal processing
or graph neural networks. Here, we systematically investigate the importance of
spatial information relative to spectral or temporal information by varying the
proportion of each dimension for AD classification. To do so, we test various
dimension resolution configurations on two routine EEG datasets. We find that
spatial information is consistently more relevant than temporal information and
equally relevant as spectral information. These results emphasise the necessity
to consider spatial information for EEG-based AD classification. On our second
dataset, we further find that well-balanced feature resolutions boost
classification accuracy by up to 1.6%. Our resolution-based feature extraction
has the potential to improve AD classification specifically, and multivariate
signal classification generally.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13530" title="Abstract">arXiv:2402.13530</a> (cross-list from math.OC) [<a href="/pdf/2402.13530" title="Download PDF">pdf</a>, <a href="/format/2402.13530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best of Many in Both Worlds: Online Resource Allocation with Predictions  under Unknown Arrival Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=An%2C+L">Lin An</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+A+A">Andrew A. Li</a>, 
<a href="/search/math?searchtype=author&query=Moseley%2C+B">Benjamin Moseley</a>, 
<a href="/search/math?searchtype=author&query=Visotsky%2C+G">Gabriel Visotsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Online decision-makers today can often obtain predictions on future
variables, such as arrivals, demands, inventories, and so on. These predictions
can be generated from simple forecasting algorithms for univariate time-series,
all the way to state-of-the-art machine learning models that leverage multiple
time-series and additional feature information. However, the prediction quality
is often unknown to decisions-makers a priori, hence blindly following the
predictions can be harmful. In this paper, we address this problem by giving
algorithms that take predictions as inputs and perform robustly against the
unknown prediction quality.
<br />We consider the online resource allocation problem, one of the most generic
models in revenue management and online decision-making. In this problem, a
decision maker has a limited amount of resources, and requests arrive
sequentially. For each request, the decision-maker needs to decide on an
action, which generates a certain amount of rewards and consumes a certain
amount of resources, without knowing the future requests. The decision-maker's
objective is to maximize the total rewards subject to resource constraints. We
take the shadow price of each resource as prediction, which can be obtained by
predictions on future requests. Prediction quality is naturally defined to be
the $\ell_1$ distance between the prediction and the actual shadow price. Our
main contribution is an algorithm which takes the prediction of unknown quality
as an input, and achieves asymptotically optimal performance under both
requests arrival models (stochastic and adversarial) without knowing the
prediction quality and the requests arrival model beforehand. We show our
algorithm's performance matches the best achievable performance of any
algorithm had the arrival models and the accuracy of the predictions been
known. We empirically validate our algorithm with experiments.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13595" title="Abstract">arXiv:2402.13595</a> (cross-list from math.OC) [<a href="/pdf/2402.13595" title="Download PDF">pdf</a>, <a href="/ps/2402.13595" title="Download PostScript">ps</a>, <a href="/format/2402.13595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cutting plane algorithm for globally solving low dimensional k-means  clustering problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ryner%2C+M">Martin Ryner</a>, 
<a href="/search/math?searchtype=author&query=Kronqvist%2C+J">Jan Kronqvist</a>, 
<a href="/search/math?searchtype=author&query=Karlsson%2C+J">Johan Karlsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Clustering is one of the most fundamental tools in data science and machine
learning, and k-means clustering is one of the most common such methods. There
is a variety of approximate algorithms for the k-means problem, but computing
the globally optimal solution is in general NP-hard. In this paper we consider
the k-means problem for instances with low dimensional data and formulate it as
a structured concave assignment problem. This allows us to exploit the low
dimensional structure and solve the problem to global optimality within
reasonable time for large data sets with several clusters. The method builds on
iteratively solving a small concave problem and a large linear programming
problem. This gives a sequence of feasible solutions along with bounds which we
show converges to zero optimality gap. The paper combines methods from global
optimization theory to accelerate the procedure, and we provide numerical
results on their performance.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13608" title="Abstract">arXiv:2402.13608</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2402.13608" title="Download PDF">pdf</a>, <a href="/ps/2402.13608" title="Download PostScript">ps</a>, <a href="/format/2402.13608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Acceleration of Markov Chain Monte Carlo-based Gradient  Descent by Deep Unfolding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Hagiwara%2C+R">Ryo Hagiwara</a>, 
<a href="/search/cond-mat?searchtype=author&query=Takabe%2C+S">Satoshi Takabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This study proposes a trainable sampling-based solver for combinatorial
optimization problems (COPs) using a deep-learning technique called deep
unfolding. The proposed solver is based on the Ohzeki method that combines
Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are
trained by minimizing a loss function. In the training process, we propose a
sampling-based gradient estimation that substitutes auto-differentiation with a
variance estimation, thereby circumventing the failure of back propagation due
to the non-differentiability of MCMC. The numerical results for a few COPs
demonstrated that the proposed solver significantly accelerated the convergence
speed compared with the original Ohzeki method.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13622" title="Abstract">arXiv:2402.13622</a> (cross-list from stat.ML) [<a href="/pdf/2402.13622" title="Download PDF">pdf</a>, <a href="/ps/2402.13622" title="Download PostScript">ps</a>, <a href="/format/2402.13622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Bootstrap and Subsampling in High-dimensional Regularized  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Clart%C3%A9%2C+L">Lucas Clart&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Vandenbroucque%2C+A">Adrien Vandenbroucque</a>, 
<a href="/search/stat?searchtype=author&query=Dalle%2C+G">Guillaume Dalle</a>, 
<a href="/search/stat?searchtype=author&query=Loureiro%2C+B">Bruno Loureiro</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>, 
<a href="/search/stat?searchtype=author&query=Zdeborov%C3%A1%2C+L">Lenka Zdeborov&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate popular resampling methods for estimating the uncertainty of
statistical models, such as subsampling, bootstrap and the jackknife, and their
performance in high-dimensional supervised regression tasks. We provide a tight
asymptotic description of the biases and variances estimated by these methods
in the context of generalized linear models, such as ridge and logistic
regression, taking the limit where the number of samples $n$ and dimension $d$
of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our
findings are three-fold: i) resampling methods are fraught with problems in
high dimensions and exhibit the double-descent-like behavior typical of these
situations; ii) only when $\alpha$ is large enough do they provide consistent
and reliable error estimations (we give convergence rates); iii) in the
over-parametrized regime $\alpha\!&lt;\!1$ relevant to modern machine learning
practice, their predictions are not consistent, even with optimal
regularization.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13629" title="Abstract">arXiv:2402.13629</a> (cross-list from eess.IV) [<a href="/pdf/2402.13629" title="Download PDF">pdf</a>, <a href="/format/2402.13629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Purification and Fine-tuning for Robust UDC Image  Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+Z">Zhenbo Song</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhenyuan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+K">Kaihao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+W">Wenhan Luo</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+Z">Zhaoxin Fan</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This study delves into the enhancement of Under-Display Camera (UDC) image
restoration models, focusing on their robustness against adversarial attacks.
Despite its innovative approach to seamless display integration, UDC technology
faces unique image degradation challenges exacerbated by the susceptibility to
adversarial perturbations. Our research initially conducts an in-depth
robustness evaluation of deep-learning-based UDC image restoration models by
employing several white-box and black-box attacking methods. This evaluation is
pivotal in understanding the vulnerabilities of current UDC image restoration
techniques. Following the assessment, we introduce a defense framework
integrating adversarial purification with subsequent fine-tuning processes.
First, our approach employs diffusion-based adversarial purification,
effectively neutralizing adversarial perturbations. Then, we apply the
fine-tuning methodologies to refine the image restoration models further,
ensuring that the quality and fidelity of the restored images are maintained.
The effectiveness of our proposed approach is validated through extensive
experiments, showing marked improvements in resilience against typical
adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13646" title="Abstract">arXiv:2402.13646</a> (cross-list from stat.ML) [<a href="/pdf/2402.13646" title="Download PDF">pdf</a>, <a href="/format/2402.13646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large Dimensional Analysis of Multi-task Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Leger%2C+V">Victor Leger</a>, 
<a href="/search/stat?searchtype=author&query=Couillet%2C+R">Romain Couillet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This article conducts a large dimensional study of a simple yet quite
versatile classification model, encompassing at once multi-task and
semi-supervised learning, and taking into account uncertain labeling. Using
tools from random matrix theory, we characterize the asymptotics of some key
functionals, which allows us on the one hand to predict the performances of the
algorithm, and on the other hand to reveal some counter-intuitive guidance on
how to use it efficiently. The model, powerful enough to provide good
performance guarantees, is also straightforward enough to provide strong
insights into its behavior.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13662" title="Abstract">arXiv:2402.13662</a> (cross-list from math.PR) [<a href="/pdf/2402.13662" title="Download PDF">pdf</a>, <a href="/ps/2402.13662" title="Download PostScript">ps</a>, <a href="/format/2402.13662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Method For Bounding Tail Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zlatanov%2C+N">Nikola Zlatanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a method for upper and lower bounding the right and the left tail
probabilities of continuous random variables (RVs). For the right tail
probability of RV $X$ with probability density function $f_X(x)$, this method
requires first setting a continuous, positive, and strictly decreasing function
$g_X(x)$ such that $-f_X(x)/g'_X(x)$ is a decreasing and increasing function,
$\forall x&gt;x_0$, which results in upper and lower bounds, respectively, given
in the form $-f_X(x) g_X(x)/g'_X(x)$, $\forall x&gt;x_0$, where $x_0$ is some
point. Similarly, for the upper and lower bounds on the left tail probability
of $X$, this method requires first setting a continuous, positive, and strictly
increasing function $g_X(x)$ such that $f_X(x)/g'_X(x)$ is an increasing and
decreasing function, $\forall x&lt;x_0$, which results in upper and lower bounds,
respectively, given in the form $f_X(x) g_X(x)/g'_X(x)$, $\forall x&lt;x_0$. We
provide some examples of good candidates for the function $g_X(x)$. We also
establish connections between the new bounds and Markov's inequality and
Chernoff's bound. In addition, we provide an iterative method for obtaining
ever tighter lower and upper bounds, under certain conditions. Finally, we
provide numerical examples, where we show the tightness of these bounds, for
some chosen $g_X(x)$.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13666" title="Abstract">arXiv:2402.13666</a> (cross-list from stat.AP) [<a href="/pdf/2402.13666" title="Download PDF">pdf</a>, <a href="/format/2402.13666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measurement Uncertainty: Relating the uncertainties of physical and  virtual measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cramer%2C+S">Simon Cramer</a>, 
<a href="/search/stat?searchtype=author&query=M%C3%BCller%2C+T">Tobias M&#xfc;ller</a>, 
<a href="/search/stat?searchtype=author&query=Schmitt%2C+R+H">Robert H. Schmitt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">In the context of industrially mass-manufactured products, quality management
is based on physically inspecting a small sample from a large batch and
reasoning about the batch's quality conformance. When complementing physical
inspections with predictions from machine learning models, it is crucial that
the uncertainty of the prediction is known. Otherwise, the application of
established quality management concepts is not legitimate. Deterministic
(machine learning) models lack quantification of their predictive uncertainty
and are therefore unsuitable. Probabilistic (machine learning) models provide a
predictive uncertainty along with the prediction. However, a concise
relationship is missing between the measurement uncertainty of physical
inspections and the predictive uncertainty of probabilistic models in their
application in quality management. Here, we show how the predictive uncertainty
of probabilistic (machine learning) models is related to the measurement
uncertainty of physical inspections. This enables the use of probabilistic
models for virtual inspections and integrates them into existing quality
management concepts. Thus, we can provide a virtual measurement for any quality
characteristic based on the process data and achieve a 100 percent inspection
rate. In the field of Predictive Quality, the virtual measurement is of great
interest. Based on our results, physical inspections with a low sampling rate
can be accompanied by virtual measurements that allow an inspection rate of 100
percent. We add substantial value, especially to complex process chains, as
faulty products/parts are identified promptly and upcoming process steps can be
aborted.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13670" title="Abstract">arXiv:2402.13670</a> (cross-list from math.OC) [<a href="/pdf/2402.13670" title="Download PDF">pdf</a>, <a href="/format/2402.13670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Riemannian Convex Bundle Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bergmann%2C+R">Ronny Bergmann</a>, 
<a href="/search/math?searchtype=author&query=Herzog%2C+R">Roland Herzog</a>, 
<a href="/search/math?searchtype=author&query=Jasa%2C+H">Hajg Jasa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Differential Geometry (math.DG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We introduce the convex bundle method to solve convex, non-smooth
optimization problems on Riemannian manifolds. Each step of our method is based
on a model that involves the convex hull of previously collected subgradients,
parallely transported into the current serious iterate. This approach
generalizes the dual form of classical bundle subproblems in Euclidean space.
We prove that, under mild conditions, the convex bundle method converges to a
minimizer. Several numerical examples implemented using the Julia package
Manopt.jl illustrate the performance of the proposed method and compare it to
the subgradient method, the cyclic proximal point, as well as the proximal
bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13673" title="Abstract">arXiv:2402.13673</a> (cross-list from astro-ph.EP) [<a href="/pdf/2402.13673" title="Download PDF">pdf</a>, <a href="/format/2402.13673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Transiting Exoplanet Parameters with 1D Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=%C3%81lvarez%2C+S+I">Santiago Iglesias &#xc1;lvarez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Alonso%2C+E+D">Enrique D&#xed;ez Alonso</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rodr%C3%ADguez%2C+M+L+S">Mar&#xed;a Luisa S&#xe1;nchez Rodr&#xed;guez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rodr%C3%ADguez%2C+J+R">Javier Rodr&#xed;guez Rodr&#xed;guez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fern%C3%A1ndez%2C+S+P">Sa&#xfa;l P&#xe9;rez Fern&#xe1;ndez</a>, 
<a href="/search/astro-ph?searchtype=author&query=de+Cos+Juez%2C+F+J">Francisco Javier de Cos Juez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">The transit method allows the detection and characterization of planetary
systems by analyzing stellar light curves. Convolutional neural networks appear
to offer a viable solution for automating these analyses. In this research, two
1D convolutional neural network models, which work with simulated light curves
in which transit-like signals were injected, are presented. One model operates
on complete light curves and estimates the orbital period, and the other one
operates on phase-folded light curves and estimates the semimajor axis of the
orbit and the square of the planet-to-star radius ratio. Both models were
tested on real data from TESS light curves with confirmed planets to ensure
that they are able to work with real data. The results obtained show that 1D
CNNs are able to characterize transiting exoplanets from their host star's
detrended light curve and, furthermore, reducing both the required time and
computational costs compared with the current detection and characterization
algorithms.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13714" title="Abstract">arXiv:2402.13714</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.13714" title="Download PDF">pdf</a>, <a href="/format/2402.13714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Evaluation of Large Language Models in Bioinformatics Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yin%2C+H">Hengchuang Yin</a>, 
<a href="/search/q-bio?searchtype=author&query=Gu%2C+Z">Zhonghui Gu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+F">Fanhao Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Abuduhaibaier%2C+Y">Yiparemu Abuduhaibaier</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+Y">Yanqiao Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Tu%2C+X">Xinming Tu</a>, 
<a href="/search/q-bio?searchtype=author&query=Hua%2C+X">Xian-Sheng Hua</a>, 
<a href="/search/q-bio?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/q-bio?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) such as ChatGPT have gained considerable
interest across diverse research communities. Their notable ability for text
completion and generation has inaugurated a novel paradigm for
language-interfaced problem solving. However, the potential and efficacy of
these models in bioinformatics remain incompletely explored. In this work, we
study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.
These tasks include the identification of potential coding regions, extraction
of named entities for genes and proteins, detection of antimicrobial and
anti-cancer peptides, molecular optimization, and resolution of educational
bioinformatics problems. Our findings indicate that, given appropriate prompts,
LLMs like GPT variants can successfully handle most of these tasks. In
addition, we provide a thorough analysis of their limitations in the context of
complicated bioinformatics tasks. In conclusion, we believe that this work can
provide new perspectives and motivate future research in the field of LLMs
applications, AI for Science and bioinformatics.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13732" title="Abstract">arXiv:2402.13732</a> (cross-list from math.PR) [<a href="/pdf/2402.13732" title="Download PDF">pdf</a>, <a href="/ps/2402.13732" title="Download PostScript">ps</a>, <a href="/format/2402.13732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On optimal error rates for strong approximation of SDEs with a drift  coefficient of fractional Sobolev regularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ellinger%2C+S">Simon Ellinger</a>, 
<a href="/search/math?searchtype=author&query=M%C3%BCller-Gronbach%2C+T">Thomas M&#xfc;ller-Gronbach</a>, 
<a href="/search/math?searchtype=author&query=Yaroslavtseva%2C+L">Larisa Yaroslavtseva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We study strong approximation of scalar additive noise driven stochastic
differential equations (SDEs) at time point $1$ in the case that the drift
coefficient is bounded and has Sobolev regularity $s\in(0,1)$. Recently, it has
been shown in [<a href="/abs/2101.12185">arXiv:2101.12185v2</a> (2022)] that for such SDEs the equidistant
Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to
an arbitrary small $\varepsilon$, in terms of the number of evaluations of the
driving Brownian motion $W$. In the present article we prove a matching lower
error bound for $s\in(1/2,1)$. More precisely we show that, for every
$s\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term,
not be improved in general by no numerical method based on finitely many
evaluations of $W$ at fixed time points. Up to now, this result was known in
the literature only for the cases $s=1/2-$ and $s=1-$.
<br />For the proof we employ the coupling of noise technique recently introduced
in [<a href="/abs/2010.00915">arXiv:2010.00915</a> (2020)] to bound the $L^2$-error of an arbitrary
approximation from below by the $L^2$-distance of two occupation time
functionals provided by a specifically chosen drift coefficient with Sobolev
regularity $s$ and two solutions of the corresponding SDE with coupled driving
Brownian motions. For the analysis of the latter distance we employ a
transformation of the original SDE to overcome the problem of correlated
increments of the difference of the two coupled solutions, occupation time
estimates to cope with the lack of regularity of the chosen drift coefficient
around the point $0$ and scaling properties of the drift coefficient.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13754" title="Abstract">arXiv:2402.13754</a> (cross-list from quant-ph) [<a href="/pdf/2402.13754" title="Download PDF">pdf</a>, <a href="/format/2402.13754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement learning-assisted quantum architecture search for  variational quantum algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+A">Akash Kundu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> With 154 pages and 46 figures here lies my PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is
identifying functional quantum circuits. These circuits must also adhere to the
constraints imposed by current quantum hardware limitations. Variational
quantum algorithms (VQAs), a class of quantum-classical optimization
algorithms, were developed to address these challenges in the currently
available quantum devices. However, the overall performance of VQAs depends on
the initialization strategy of the variational circuit, the structure of the
circuit (also known as ansatz), and the configuration of the cost function.
Focusing on the structure of the circuit, in this thesis, we improve the
performance of VQAs by automating the search for an optimal structure for the
variational circuits using reinforcement learning (RL). Within the thesis, the
optimality of a circuit is determined by evaluating its depth, the overall
count of gates and parameters, and its accuracy in solving the given problem.
The task of automating the search for optimal quantum circuits is known as
quantum architecture search (QAS). The majority of research in QAS is primarily
focused on a noiseless scenario. Yet, the impact of noise on the QAS remains
inadequately explored. In this thesis, we tackle the issue by introducing a
tensor-based quantum circuit encoding, restrictions on environment dynamics to
explore the search space of possible circuits efficiently, an episode halting
scheme to steer the agent to find shorter circuits, a double deep Q-network
(DDQN) with an $\epsilon$-greedy policy for better stability. The numerical
experiments on noiseless and noisy quantum hardware show that in dealing with
various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods
we propose in the thesis can be readily adapted to address a wide range of
other VQAs.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13776" title="Abstract">arXiv:2402.13776</a> (cross-list from eess.IV) [<a href="/pdf/2402.13776" title="Download PDF">pdf</a>, <a href="/format/2402.13776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cas-DiffCom: Cascaded diffusion model for infant longitudinal  super-resolution 3D medical image completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+L">Lianghu Guo</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+T">Tianli Tao</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+X">Xinyi Cai</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Z">Zihao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lixuan Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+Z">Zhuoyang Gu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+H">Haifeng Tang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+R">Rui Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+S">Siyan Han</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+Y">Yan Liang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Han Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Early infancy is a rapid and dynamic neurodevelopmental period for behavior
and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an
effective tool to investigate such a crucial stage by capturing the
developmental trajectories of the brain structures. However, longitudinal MRI
acquisition always meets a serious data-missing problem due to participant
dropout and failed scans, making longitudinal infant brain atlas construction
and developmental trajectory delineation quite challenging. Thanks to the
development of an AI-based generative model, neuroimage completion has become a
powerful technique to retain as much available data as possible. However,
current image completion methods usually suffer from inconsistency within each
individual subject in the time dimension, compromising the overall quality. To
solve this problem, our paper proposed a two-stage cascaded diffusion model,
Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and
super-resolution. We applied our proposed method to the Baby Connectome Project
(BCP) dataset. The experiment results validate that Cas-DiffCom achieves both
individual consistency and high fidelity in longitudinal infant brain image
completion. We further applied the generated infant brain images to two
downstream tasks, brain tissue segmentation and developmental trajectory
delineation, to declare its task-oriented potential in the neuroscience field.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13867" title="Abstract">arXiv:2402.13867</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.13867" title="Download PDF">pdf</a>, <a href="/format/2402.13867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency  interference -- Application to pulsar observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cognard%2C+I">Isma&#xeb;l Cognard</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dobigeon%2C+N">Nicolas Dobigeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Radio frequency interference (RFI) have been an enduring concern in radio
astronomy, particularly for the observations of pulsars which require high
timing precision and data sensitivity. In most works of the literature, RFI
mitigation has been formulated as a detection task that consists of localizing
possible RFI in dynamic spectra. This strategy inevitably leads to a potential
loss of information since parts of the signal identified as possibly
RFI-corrupted are generally not considered in the subsequent data processing
pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint
detection and restoration that allows parts of the dynamic spectrum affected by
RFI to be not only identified but also recovered. The proposed supervised
method relies on a deep convolutional network whose architecture inherits the
performance reached by a recent yet popular image-denoising network. To train
this network, a whole simulation framework is built to generate large data sets
according to physics-inspired and statistical models of the pulsar signals and
of the RFI. The relevance of the proposed approach is quantitatively assessed
by conducting extensive experiments. In particular, the results show that the
restored dynamic spectra are sufficiently reliable to estimate pulsar
times-of-arrivals with an accuracy close to the one that would be obtained from
RFI-free signals.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13895" title="Abstract">arXiv:2402.13895</a> (cross-list from quant-ph) [<a href="/pdf/2402.13895" title="Download PDF">pdf</a>, <a href="/ps/2402.13895" title="Download PostScript">ps</a>, <a href="/format/2402.13895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grover&#x27;s oracle for the Shortest Vector Problem and its application in  hybrid classical-quantum solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Prokop%2C+M">Milos Prokop</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wallden%2C+P">Petros Wallden</a>, 
<a href="/search/quant-ph?searchtype=author&query=Joseph%2C+D">David Joseph</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Finding the shortest vector in a lattice is a problem that is believed to be
hard both for classical and quantum computers. Many major post-quantum secure
cryptosystems base their security on the hardness of the Shortest Vector
Problem (SVP). Finding the best classical, quantum or hybrid classical-quantum
algorithms for SVP is necessary to select cryptosystem parameters that offer
sufficient level of security. Grover's search quantum algorithm provides a
generic quadratic speed-up, given access to an oracle implementing some
function which describes when a solution is found. In this paper we provide
concrete implementation of such an oracle for the SVP. We define the circuit,
and evaluate costs in terms of number of qubits, number of gates, depth and
T-quantum cost. We then analyze how to combine Grover's quantum search for
small SVP instances with state-of-the-art classical solvers that use well known
algorithms, such as the BKZ, where the former is used as a subroutine. This
could enable solving larger instances of SVP with higher probability than
classical state-of-the-art records, but still very far from posing any threat
to cryptosystems being considered for standardization. Depending on the
technology available, there is a spectrum of trade-offs in creating this
combination.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13937" title="Abstract">arXiv:2402.13937</a> (cross-list from math.OC) [<a href="/pdf/2402.13937" title="Download PDF">pdf</a>, <a href="/format/2402.13937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifying message-passing neural networks via topology-based bounds  tightening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hojny%2C+C">Christopher Hojny</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+S">Shiqiang Zhang</a>, 
<a href="/search/math?searchtype=author&query=Campos%2C+J+S">Juan S. Campos</a>, 
<a href="/search/math?searchtype=author&query=Misener%2C+R">Ruth Misener</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Since graph neural networks (GNNs) are often vulnerable to attack, we need to
know when we can trust them. We develop a computationally effective approach
towards providing robust certificates for message-passing neural networks
(MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our
work builds on mixed-integer optimization, it encodes a wide variety of
subproblems, for example it admits (i) both adding and removing edges, (ii)
both global and local budgets, and (iii) both topological perturbations and
feature modifications. Our key technology, topology-based bounds tightening,
uses graph structure to tighten bounds. We also experiment with aggressive
bounds tightening to dynamically change the optimization constraints by
tightening variable bounds. To demonstrate the effectiveness of these
strategies, we implement an extension to the open-source branch-and-cut solver
SCIP. We test on both node and graph classification problems and consider
topological attacks that both add and remove edges.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13945" title="Abstract">arXiv:2402.13945</a> (cross-list from stat.ML) [<a href="/pdf/2402.13945" title="Download PDF">pdf</a>, <a href="/format/2402.13945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty  in Scientific Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pourkamali-Anaraki%2C+F">Farhad Pourkamali-Anaraki</a>, 
<a href="/search/stat?searchtype=author&query=Husseini%2C+J+F">Jamal F. Husseini</a>, 
<a href="/search/stat?searchtype=author&query=Stapleton%2C+S+E">Scott E. Stapleton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper investigates the use of probabilistic neural networks (PNNs) to
model aleatoric uncertainty, which refers to the inherent variability in the
input-output relationships of a system, often characterized by unequal variance
or heteroscedasticity. Unlike traditional neural networks that produce
deterministic outputs, PNNs generate probability distributions for the target
variable, allowing the determination of both predicted means and intervals in
regression scenarios. Contributions of this paper include the development of a
probabilistic distance metric to optimize PNN architecture, and the deployment
of PNNs in controlled data sets as well as a practical material science case
involving fiber-reinforced composites. The findings confirm that PNNs
effectively model aleatoric uncertainty, proving to be more appropriate than
the commonly employed Gaussian process regression for this purpose.
Specifically, in a real-world scientific machine learning context, PNNs yield
remarkably accurate output mean estimates with R-squared scores approaching
0.97, and their predicted intervals exhibit a high correlation coefficient of
nearly 0.80, closely matching observed data intervals. Hence, this research
contributes to the ongoing exploration of leveraging the sophisticated
representational capacity of neural networks to delineate complex input-output
relationships in scientific problems.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13999" title="Abstract">arXiv:2402.13999</a> (cross-list from stat.ML) [<a href="/pdf/2402.13999" title="Download PDF">pdf</a>, <a href="/format/2402.13999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotics of Learning with Deep Structured (Random) Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schr%C3%B6der%2C+D">Dominik Schr&#xf6;der</a>, 
<a href="/search/stat?searchtype=author&query=Dmitriev%2C+D">Daniil Dmitriev</a>, 
<a href="/search/stat?searchtype=author&query=Cui%2C+H">Hugo Cui</a>, 
<a href="/search/stat?searchtype=author&query=Loureiro%2C+B">Bruno Loureiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">For a large class of feature maps we provide a tight asymptotic
characterisation of the test error associated with learning the readout layer,
in the high-dimensional limit where the input dimension, hidden layer widths,
and number of training samples are proportionally large. This characterization
is formulated in terms of the population covariance of the features. Our work
is partially motivated by the problem of learning with Gaussian rainbow neural
networks, namely deep non-linear fully-connected networks with random but
structured weights, whose row-wise covariances are further allowed to depend on
the weights of previous layers. For such networks we also derive a closed-form
formula for the feature covariance in terms of the weight matrices. We further
find that in some cases our results can capture feature maps learned by deep,
finite-width neural networks trained under gradient descent.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu, 22 Feb 24</h3>
<dl>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1508.01167" title="Abstract">arXiv:1508.01167</a> (replaced) [<a href="/pdf/1508.01167" title="Download PDF">pdf</a>, <a href="/format/1508.01167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Divergence Index: A Decomposable Measure of Segregation and  Inequality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Roberto%2C+E">Elizabeth Roberto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Information Theory (cs.IT); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1804.05105" title="Abstract">arXiv:1804.05105</a> (replaced) [<a href="/pdf/1804.05105" title="Download PDF">pdf</a>, <a href="/format/1804.05105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recognizing Visibility Graphs of Polygons with Holes and  Internal-External Visibility Graphs of Polygons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boomari%2C+H">Hossein Boomari</a>, 
<a href="/search/cs?searchtype=author&query=Ostovari%2C+M">Mojtaba Ostovari</a>, 
<a href="/search/cs?searchtype=author&query=Zarei%2C+A">Alireza Zarei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1804.11222" title="Abstract">arXiv:1804.11222</a> (replaced) [<a href="/pdf/1804.11222" title="Download PDF">pdf</a>, <a href="/format/1804.11222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimized KE-tableau-based system for reasoning in the description  logic $\mathcal{DL}_{\mathbf{D}}^{4,\!\times}$ (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cantone%2C+D">Domenico Cantone</a>, 
<a href="/search/cs?searchtype=author&query=Nicolosi-Asmundo%2C+M">Marianna Nicolosi-Asmundo</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+D+F">Daniele Francesco Santamaria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85053216200">this https URL</a>&amp;origin=resultslist. arXiv admin note: substantial text overlap with <a href="/abs/1702.03096">arXiv:1702.03096</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1805.08606" title="Abstract">arXiv:1805.08606</a> (replaced) [<a href="/pdf/1805.08606" title="Download PDF">pdf</a>, <a href="/format/1805.08606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A set-based reasoner for the description logic  $\mathcal{DL}_{\mathbf{D}}^{4,\!\times}$ (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cantone%2C+D">Domenico Cantone</a>, 
<a href="/search/cs?searchtype=author&query=Nicolosi-Asmundo%2C+M">Marianna Nicolosi-Asmundo</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+D+F">Daniele Francesco Santamaria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85053670566">this https URL</a>&amp;origin=resultslist. arXiv admin note: text overlap with <a href="/abs/1804.11222">arXiv:1804.11222</a>, <a href="/abs/1707.07545">arXiv:1707.07545</a>, <a href="/abs/1702.03096">arXiv:1702.03096</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CEUR Workshop Proceedings, ISSN 1613-0073, Vol. 2199, pp. 52--66
  2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.04574" title="Abstract">arXiv:1910.04574</a> (replaced) [<a href="/pdf/1910.04574" title="Download PDF">pdf</a>, <a href="/format/1910.04574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmented Probability Simulation Methods for Sequential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ekin%2C+T">Tahir Ekin</a>, 
<a href="/search/stat?searchtype=author&query=Naveiro%2C+R">Roi Naveiro</a>, 
<a href="/search/stat?searchtype=author&query=Torres-Barr%C3%A1n%2C+A">Alberto Torres-Barr&#xe1;n</a>, 
<a href="/search/stat?searchtype=author&query=R%C3%ADos-Insua%2C+D">David R&#xed;os-Insua</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Journal of Operational Research, 306(1), 418-430 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.12267" title="Abstract">arXiv:2011.12267</a> (replaced) [<a href="/pdf/2011.12267" title="Download PDF">pdf</a>, <a href="/format/2011.12267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Fluid Motion Estimation using a Constraint-Based  Refinement Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doshi%2C+H">Hirak Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Kiran%2C+N+U">N. Uday Kiran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.01410" title="Abstract">arXiv:2012.01410</a> (replaced) [<a href="/pdf/2012.01410" title="Download PDF">pdf</a>, <a href="/format/2012.01410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontological Smart Contracts in OASIS: Ontology for Agents, Systems, and  Integration of Services (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cantone%2C+D">Domenico Cantone</a>, 
<a href="/search/cs?searchtype=author&query=Longo%2C+C+F">Carmelo Fabio Longo</a>, 
<a href="/search/cs?searchtype=author&query=Nicolosi-Asmundo%2C+M">Marianna Nicolosi-Asmundo</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+D+F">Daniele Francesco Santamaria</a>, 
<a href="/search/cs?searchtype=author&query=Santoro%2C+C">Corrado Santoro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85130258663">this https URL</a>&amp;origin=resultslist
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Intelligent Distributed Computing XIV, Studies in Computational
  Intelligence 1026, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.04566" title="Abstract">arXiv:2104.04566</a> (replaced) [<a href="/pdf/2104.04566" title="Download PDF">pdf</a>, <a href="/format/2104.04566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inapproximability of Unique Games in Fixed-Point Logic with Counting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tucker-Foltz%2C+J">Jamie Tucker-Foltz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2008.03115">arXiv:2008.03115</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.11808" title="Abstract">arXiv:2104.11808</a> (replaced) [<a href="/pdf/2104.11808" title="Download PDF">pdf</a>, <a href="/ps/2104.11808" title="Download PostScript">ps</a>, <a href="/format/2104.11808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying the Three Algebraic Approaches to the CSP via Minimal Taylor  Algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barto%2C+L">Libor Barto</a>, 
<a href="/search/cs?searchtype=author&query=Brady%2C+Z">Zarathustra Brady</a>, 
<a href="/search/cs?searchtype=author&query=Bulatov%2C+A">Andrei Bulatov</a>, 
<a href="/search/cs?searchtype=author&query=Kozik%2C+M">Marcin Kozik</a>, 
<a href="/search/cs?searchtype=author&query=Zhuk%2C+D">Dmitriy Zhuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.11277" title="Abstract">arXiv:2107.11277</a> (replaced) [<a href="/pdf/2107.11277" title="Download PDF">pdf</a>, <a href="/format/2107.11277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning with a Reject Option: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hendrickx%2C+K">Kilian Hendrickx</a>, 
<a href="/search/cs?searchtype=author&query=Perini%2C+L">Lorenzo Perini</a>, 
<a href="/search/cs?searchtype=author&query=Van+der+Plas%2C+D">Dries Van der Plas</a>, 
<a href="/search/cs?searchtype=author&query=Meert%2C+W">Wannes Meert</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">Jesse Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.08486" title="Abstract">arXiv:2110.08486</a> (replaced) [<a href="/pdf/2110.08486" title="Download PDF">pdf</a>, <a href="/format/2110.08486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Multimodal Procedural Knowledge by Sequencing Multimodal  Instructional Manuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Te-Lin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Spangher%2C+A">Alex Spangher</a>, 
<a href="/search/cs?searchtype=author&query=Alipoormolabashi%2C+P">Pegah Alipoormolabashi</a>, 
<a href="/search/cs?searchtype=author&query=Freedman%2C+M">Marjorie Freedman</a>, 
<a href="/search/cs?searchtype=author&query=Weischedel%2C+R">Ralph Weischedel</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the Conference of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14048" title="Abstract">arXiv:2110.14048</a> (replaced) [<a href="/pdf/2110.14048" title="Download PDF">pdf</a>, <a href="/format/2110.14048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conflict-Averse Gradient Descent for Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaojie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures, Conference on Neural Information Processing Systems, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14427" title="Abstract">arXiv:2110.14427</a> (replaced) [<a href="/pdf/2110.14427" title="Download PDF">pdf</a>, <a href="/format/2110.14427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ODE Method for Asymptotic Statistics in Stochastic Approximation and  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Borkar%2C+V">Vivek Borkar</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+S">Shuhang Chen</a>, 
<a href="/search/math?searchtype=author&query=Devraj%2C+A">Adithya Devraj</a>, 
<a href="/search/math?searchtype=author&query=Kontoyiannis%2C+I">Ioannis Kontoyiannis</a>, 
<a href="/search/math?searchtype=author&query=Meyn%2C+S">Sean Meyn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08108" title="Abstract">arXiv:2111.08108</a> (replaced) [<a href="/pdf/2111.08108" title="Download PDF">pdf</a>, <a href="/format/2111.08108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Control with Stochastic Models of Hamiltonian Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bajaj%2C+C">Chandrajit Bajaj</a>, 
<a href="/search/math?searchtype=author&query=Nguyen%2C+M">Minh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.10446" title="Abstract">arXiv:2111.10446</a> (replaced) [<a href="/pdf/2111.10446" title="Download PDF">pdf</a>, <a href="/format/2111.10446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiplicity structure of the arc space of a fat point
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Manssour%2C+R+A+E">Rida Ait El Manssour</a>, 
<a href="/search/math?searchtype=author&query=Pogudin%2C+G">Gleb Pogudin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Symbolic Computation (cs.SC); Commutative Algebra (math.AC); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.02002" title="Abstract">arXiv:2203.02002</a> (replaced) [<a href="/pdf/2203.02002" title="Download PDF">pdf</a>, <a href="/format/2203.02002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discord in the voter model for complex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vendeville%2C+A">Antoine Vendeville</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. E, 109(2), 024312 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.11160" title="Abstract">arXiv:2203.11160</a> (replaced) [<a href="/pdf/2203.11160" title="Download PDF">pdf</a>, <a href="/format/2203.11160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drive&amp;Segment: Unsupervised Semantic Segmentation of Urban Scenes via  Cross-modal Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vobecky%2C+A">Antonin Vobecky</a>, 
<a href="/search/cs?searchtype=author&query=Hurych%2C+D">David Hurych</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%A9oni%2C+O">Oriane Sim&#xe9;oni</a>, 
<a href="/search/cs?searchtype=author&query=Gidaris%2C+S">Spyros Gidaris</a>, 
<a href="/search/cs?searchtype=author&query=Bursuc%2C+A">Andrei Bursuc</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Sivic%2C+J">Josef Sivic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: improved quality of images. See the project webpage <a href="https://vobecant.github.io/DriveAndSegment/">this https URL</a> for the code and more
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02265" title="Abstract">arXiv:2204.02265</a> (replaced) [<a href="/pdf/2204.02265" title="Download PDF">pdf</a>, <a href="/format/2204.02265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fiat-Shamir for Proofs Lacks a Proof Even in the Presence of Shared  Entanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Dupuis%2C+F">Fr&#xe9;d&#xe9;ric Dupuis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lamontagne%2C+P">Philippe Lamontagne</a>, 
<a href="/search/quant-ph?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02643" title="Abstract">arXiv:2204.02643</a> (replaced) [<a href="/pdf/2204.02643" title="Download PDF">pdf</a>, <a href="/ps/2204.02643" title="Download PostScript">ps</a>, <a href="/format/2204.02643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional pre-processing for automated reasoning in dependent type  theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blot%2C+V">Valentin Blot</a>, 
<a href="/search/cs?searchtype=author&query=Cousineau%2C+D">Denis Cousineau</a>, 
<a href="/search/cs?searchtype=author&query=Crance%2C+E">Enzo Crance</a>, 
<a href="/search/cs?searchtype=author&query=de+Prisque%2C+L+D">Louise Dubois de Prisque</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+C">Chantal Keller</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">Assia Mahboubi</a>, 
<a href="/search/cs?searchtype=author&query=Vial%2C+P">Pierre Vial</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 12th ACM SIGPLAN International Conference on
  Certified Programs and Proofs (CPP '23), January 16-17, 2023, Boston, MA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03971" title="Abstract">arXiv:2204.03971</a> (replaced) [<a href="/pdf/2204.03971" title="Download PDF">pdf</a>, <a href="/format/2204.03971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No eleventh conditional Ingleton inequality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boege%2C+T">Tobias Boege</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure; v3: final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07424" title="Abstract">arXiv:2205.07424</a> (replaced) [<a href="/pdf/2205.07424" title="Download PDF">pdf</a>, <a href="/format/2205.07424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Graph Neural Networks: Aspects, Methods and Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingliang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jian Pei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 7 tables, 4 figures, double columns, accepted by Proceedings of the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04745" title="Abstract">arXiv:2206.04745</a> (replaced) [<a href="/pdf/2206.04745" title="Download PDF">pdf</a>, <a href="/format/2206.04745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mildly Conservative Q-Learning for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiafei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoteng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.06172" title="Abstract">arXiv:2206.06172</a> (replaced) [<a href="/pdf/2206.06172" title="Download PDF">pdf</a>, <a href="/format/2206.06172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-ADMM: A RIS and ADMM-Based Passive and Sparse Sensing Method With  Interference Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhimin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Miao%2C+P">Pu Miao</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications letters, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07341" title="Abstract">arXiv:2208.07341</a> (replaced) [<a href="/pdf/2208.07341" title="Download PDF">pdf</a>, <a href="/format/2208.07341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Assortment Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Golrezaei%2C+N">Negin Golrezaei</a>, 
<a href="/search/cs?searchtype=author&query=Susan%2C+F">Fransisca Susan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00655" title="Abstract">arXiv:2209.00655</a> (replaced) [<a href="/pdf/2209.00655" title="Download PDF">pdf</a>, <a href="/ps/2209.00655" title="Download PostScript">ps</a>, <a href="/format/2209.00655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Representation Learning on Electronic Health Records  with Graph Kernel Infomax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Hao-Ren Yao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+N">Nairen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+K">Katina Russell</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+D">Der-Chen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Frieder%2C+O">Ophir Frieder</a>, 
<a href="/search/cs?searchtype=author&query=Fineman%2C+J">Jeremy Fineman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Transactions on Computing for Healthcare (HEALTH)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09841" title="Abstract">arXiv:2209.09841</a> (replaced) [<a href="/pdf/2209.09841" title="Download PDF">pdf</a>, <a href="/format/2209.09841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Inconsistent Knowledge Distillation for Object Detection with  Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiawei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Ke Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACMMM 2023 Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10137" title="Abstract">arXiv:2209.10137</a> (replaced) [<a href="/pdf/2209.10137" title="Download PDF">pdf</a>, <a href="/format/2209.10137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank-Preserving Multidimensional Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Bikhchandani%2C+S">Sushil Bikhchandani</a>, 
<a href="/search/econ?searchtype=author&query=Mishra%2C+D">Debasis Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12235" title="Abstract">arXiv:2209.12235</a> (replaced) [<a href="/pdf/2209.12235" title="Download PDF">pdf</a>, <a href="/ps/2209.12235" title="Download PostScript">ps</a>, <a href="/format/2209.12235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Iterative Riemann Solvers for the Shallow Water  and Euler Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Moncayo%2C+C+M">Carlos Mu&#xf1;oz Moncayo</a>, 
<a href="/search/math?searchtype=author&query=de+Luna%2C+M+Q">Manuel Quezada de Luna</a>, 
<a href="/search/math?searchtype=author&query=Ketcheson%2C+D+I">David I. Ketcheson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications in Applied Mathematics and Computational Science
  18.1 (2023): 107-134
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13577" title="Abstract">arXiv:2209.13577</a> (replaced) [<a href="/pdf/2209.13577" title="Download PDF">pdf</a>, <a href="/format/2209.13577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Coupling Effect: Experimental Validation of the Fusion of Fossen and  Featherstone to Simulate UVMS Dynamics in Julia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolano%2C+H">Hannah Kolano</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+E">Evan Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+J+R">Joseph R. Davidson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages. Submitted to ICRA 2024. Major revision from previous paper. Predictive work with recurrent neural networks is excluded, replaced with a more in-depth description of the dynamic model, along with experimental validation of the model. Additional author added (Evan Palmer). The vehicle being examined is also changed from a Seabotix model to a BlueROV2, a more common platform
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00187" title="Abstract">arXiv:2210.00187</a> (replaced) [<a href="/pdf/2210.00187" title="Download PDF">pdf</a>, <a href="/format/2210.00187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Fuzzy Logic Controller for Washing Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dheerawat%2C+K">Kriti Dheerawat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05988" title="Abstract">arXiv:2210.05988</a> (replaced) [<a href="/pdf/2210.05988" title="Download PDF">pdf</a>, <a href="/format/2210.05988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lai%2C+P">Pin-Hua Lai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+B">Bo-Shan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+W">Wei-Chun Yang</a>, 
<a href="/search/eess?searchtype=author&query=Tsou%2C+H">Hsiang-Chieh Tsou</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+C">Chun-Shu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01062" title="Abstract">arXiv:2211.01062</a> (replaced) [<a href="/pdf/2211.01062" title="Download PDF">pdf</a>, <a href="/format/2211.01062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Identity Testing via Evaluation of Rational Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+I">Ivan Hu</a>, 
<a href="/search/cs?searchtype=author&query=van+Melkebeek%2C+D">Dieter van Melkebeek</a>, 
<a href="/search/cs?searchtype=author&query=Morgan%2C+A">Andrew Morgan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at ITCS 2022. This is an expanded version with an additional author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07245" title="Abstract">arXiv:2211.07245</a> (replaced) [<a href="/pdf/2211.07245" title="Download PDF">pdf</a>, <a href="/format/2211.07245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Uncertainty in Similarity Scoring: Performance &amp; Fairness in  Face Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conti%2C+J">Jean-R&#xe9;my Conti</a>, 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9men%C3%A7on%2C+S">St&#xe9;phan Cl&#xe9;men&#xe7;on</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08597" title="Abstract">arXiv:2211.08597</a> (replaced) [<a href="/pdf/2211.08597" title="Download PDF">pdf</a>, <a href="/format/2211.08597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SketchySGD: Reliable Stochastic Optimization via Randomized Curvature  Estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Frangella%2C+Z">Zachary Frangella</a>, 
<a href="/search/math?searchtype=author&query=Rathore%2C+P">Pratik Rathore</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+S">Shipu Zhao</a>, 
<a href="/search/math?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages, 43 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09221" title="Abstract">arXiv:2211.09221</a> (replaced) [<a href="/pdf/2211.09221" title="Download PDF">pdf</a>, <a href="/format/2211.09221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The non-overlapping statistical approximation to overlapping group lasso
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Qi%2C+M">Mingyu Qi</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+T">Tianxi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09771" title="Abstract">arXiv:2211.09771</a> (replaced) [<a href="/pdf/2211.09771" title="Download PDF">pdf</a>, <a href="/format/2211.09771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Object Representation Learning via Motion and Object Continuity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delfosse%2C+Q">Quentin Delfosse</a>, 
<a href="/search/cs?searchtype=author&query=Stammer%2C+W">Wolfgang Stammer</a>, 
<a href="/search/cs?searchtype=author&query=Rothenbacher%2C+T">Thomas Rothenbacher</a>, 
<a href="/search/cs?searchtype=author&query=Vittal%2C+D">Dwarak Vittal</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages main text, 32 tables, 21 Figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning and Knowledge Discovery in Databases: Research
  Track. ECML PKDD 2023. Lecture Notes in Computer Science(), vol 14172.
  Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09869" title="Abstract">arXiv:2211.09869</a> (replaced) [<a href="/pdf/2211.09869" title="Download PDF">pdf</a>, <a href="/format/2211.09869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anciukevi%C4%8Dius%2C+T">Titas Anciukevi&#x10d;ius</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zexiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+M">Matthew Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Paul Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Bilen%2C+H">Hakan Bilen</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+N+J">Niloy J. Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+P">Paul Guerrero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR 2023. Project page: <a href="https://github.com/Anciukevicius/RenderDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11460" title="Abstract">arXiv:2211.11460</a> (replaced) [<a href="/pdf/2211.11460" title="Download PDF">pdf</a>, <a href="/format/2211.11460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motor Imagery Decoding Using Ensemble Curriculum Learning and  Collaborative Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zoumpourlis%2C+G">Georgios Zoumpourlis</a>, 
<a href="/search/eess?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in 12th IEEE International Winter Conference on Brain-Computer Interface (BCI), 2024. Code: <a href="https://github.com/gzoumpourlis/Ensemble-MI">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04631" title="Abstract">arXiv:2212.04631</a> (replaced) [<a href="/pdf/2212.04631" title="Download PDF">pdf</a>, <a href="/format/2212.04631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Normalized Cross Density Functional: A Framework to Quantify  Statistical Dependence for Random Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Principe%2C+J+C">Jose C. Principe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14250" title="Abstract">arXiv:2212.14250</a> (replaced) [<a href="/pdf/2212.14250" title="Download PDF">pdf</a>, <a href="/format/2212.14250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheduling of Software-Defined Microgrids for Optimal Frequency  Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chu%2C+Z">Zhongda Chu</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+G">Guoxuan Cui</a>, 
<a href="/search/eess?searchtype=author&query=Teng%2C+F">Fei Teng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02998" title="Abstract">arXiv:2301.02998</a> (replaced) [<a href="/pdf/2301.02998" title="Download PDF">pdf</a>, <a href="/format/2301.02998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boytsov%2C+L">Leonid Boytsov</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+P">Preksha Patel</a>, 
<a href="/search/cs?searchtype=author&query=Sourabh%2C+V">Vivek Sourabh</a>, 
<a href="/search/cs?searchtype=author&query=Nisar%2C+R">Riddhi Nisar</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sayani Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Ramanathan%2C+R">Ramya Ramanathan</a>, 
<a href="/search/cs?searchtype=author&query=Nyberg%2C+E">Eric Nyberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06339" title="Abstract">arXiv:2301.06339</a> (replaced) [<a href="/pdf/2301.06339" title="Download PDF">pdf</a>, <a href="/format/2301.06339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation of optimization problems with constraints through kernel  Sum-Of-Squares
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aubin-Frankowski%2C+P">Pierre-Cyril Aubin-Frankowski</a>, 
<a href="/search/math?searchtype=author&query=Rudi%2C+A">Alessandro Rudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03963" title="Abstract">arXiv:2302.03963</a> (replaced) [<a href="/pdf/2302.03963" title="Download PDF">pdf</a>, <a href="/format/2302.03963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based Online Optimization for Autonomous Mobility-on-Demand  Fleet Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jungel%2C+K">Kai Jungel</a>, 
<a href="/search/math?searchtype=author&query=Parmentier%2C+A">Axel Parmentier</a>, 
<a href="/search/math?searchtype=author&query=Schiffer%2C+M">Maximilian Schiffer</a>, 
<a href="/search/math?searchtype=author&query=Vidal%2C+T">Thibaut Vidal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06717" title="Abstract">arXiv:2302.06717</a> (replaced) [<a href="/e-print/2302.06717" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Subgraph Isomorphism Problem for Port Graphs and Quantum Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mondada%2C+L">Luca Mondada</a>, 
<a href="/search/quant-ph?searchtype=author&query=Andr%C3%A9s-Mart%C3%ADnez%2C+P">Pablo Andr&#xe9;s-Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The main bound of thm 1 is asymptotically very close to previous work, significantly reducing the novelty and motivation for this work. A new approach to this problem is presented in <a href="/abs/2402.13065">2402.13065</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09852" title="Abstract">arXiv:2302.09852</a> (replaced) [<a href="/pdf/2302.09852" title="Download PDF">pdf</a>, <a href="/format/2302.09852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Layer-wise Score Aggregation for Textual OOD Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darrin%2C+M">Maxime Darrin</a>, 
<a href="/search/cs?searchtype=author&query=Staerman%2C+G">Guillaume Staerman</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+E+D+C">Eduardo Dadalto C&#xe2;mara Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+J+C">Jackie CK Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00666" title="Abstract">arXiv:2303.00666</a> (replaced) [<a href="/pdf/2303.00666" title="Download PDF">pdf</a>, <a href="/format/2303.00666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Space Efficient Two-Point Shortest Path Queries in a Polygonal  Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Berg%2C+S">Sarita de Berg</a>, 
<a href="/search/cs?searchtype=author&query=Miltzow%2C+T">Tillmann Miltzow</a>, 
<a href="/search/cs?searchtype=author&query=Staals%2C+F">Frank Staals</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03547" title="Abstract">arXiv:2303.03547</a> (replaced) [<a href="/pdf/2303.03547" title="Download PDF">pdf</a>, <a href="/format/2303.03547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small singular values can increase in lower precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boutsikas%2C+C">Christos Boutsikas</a>, 
<a href="/search/math?searchtype=author&query=Drineas%2C+P">Petros Drineas</a>, 
<a href="/search/math?searchtype=author&query=Ipsen%2C+I+C+F">Ilse C.F. Ipsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04376" title="Abstract">arXiv:2303.04376</a> (replaced) [<a href="/pdf/2303.04376" title="Download PDF">pdf</a>, <a href="/format/2303.04376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tsanet: Temporal and Scale Alignment for Unsupervised Video Object  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Suhwan Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dogyoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minhyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangyoun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICIP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 30th IEEE International Conference on Image Processing, ICIP 2023
  (Pages 1535 - 1539)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05352" title="Abstract">arXiv:2303.05352</a> (replaced) [<a href="/pdf/2303.05352" title="Download PDF">pdf</a>, <a href="/format/2303.05352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Accurate Materials Data from Research Papers with  Conversational Language Models and Prompt Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polak%2C+M+P">Maciej P. Polak</a>, 
<a href="/search/cs?searchtype=author&query=Morgan%2C+D">Dane Morgan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nature Communications (2024) 15:1569
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05376" title="Abstract">arXiv:2303.05376</a> (replaced) [<a href="/pdf/2303.05376" title="Download PDF">pdf</a>, <a href="/format/2303.05376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Leigh%2C+M">Matthew Leigh</a>, 
<a href="/search/hep-ph?searchtype=author&query=Sengupta%2C+D">Debajyoti Sengupta</a>, 
<a href="/search/hep-ph?searchtype=author&query=Qu%C3%A9tant%2C+G">Guillaume Qu&#xe9;tant</a>, 
<a href="/search/hep-ph?searchtype=author&query=Raine%2C+J+A">John Andrew Raine</a>, 
<a href="/search/hep-ph?searchtype=author&query=Zoch%2C+K">Knut Zoch</a>, 
<a href="/search/hep-ph?searchtype=author&query=Golling%2C+T">Tobias Golling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 25 figures, 5 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SciPost Phys. 16, 018 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06532" title="Abstract">arXiv:2303.06532</a> (replaced) [<a href="/pdf/2303.06532" title="Download PDF">pdf</a>, <a href="/format/2303.06532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Design of Metaheuristic Algorithms: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Q">Qiqi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research, 2024,
  https://openreview.net/forum?id=qhtHsvF5zj
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11602" title="Abstract">arXiv:2303.11602</a> (replaced) [<a href="/pdf/2303.11602" title="Download PDF">pdf</a>, <a href="/format/2303.11602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of variational Monte Carlo simulation and scale-invariant  pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrahamsen%2C+N">Nilin Abrahamsen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhiyan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Goldshlager%2C+G">Gil Goldshlager</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lin Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated presentation to unify notation and focus on the VMC setting. Added new numerics for scale-invariant supervised pre-training
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12644" title="Abstract">arXiv:2303.12644</a> (replaced) [<a href="/pdf/2303.12644" title="Download PDF">pdf</a>, <a href="/format/2303.12644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-Conditioned Cascaded Video Diffusion Models for Precise  Echocardiogram Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reynaud%2C+H">Hadrien Reynaud</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Mengyun Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dombrowski%2C+M">Mischa Dombrowski</a>, 
<a href="/search/cs?searchtype=author&query=Day%2C+T">Thomas Day</a>, 
<a href="/search/cs?searchtype=author&query=Razavi%2C+R">Reza Razavi</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+A">Alberto Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Leeson%2C+P">Paul Leeson</a>, 
<a href="/search/cs?searchtype=author&query=Kainz%2C+B">Bernhard Kainz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in MICCAI 2023 proceedings. <a href="https://link.springer.com/chapter/10.1007/978-3-031-43999-5_14">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13809" title="Abstract">arXiv:2303.13809</a> (replaced) [<a href="/pdf/2303.13809" title="Download PDF">pdf</a>, <a href="/format/2303.13809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Analysis Prompting Enables Human-Like Translation Evaluation in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qingyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+B">Baopu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kanjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kocmi%2C+T">Tom Kocmi</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14389" title="Abstract">arXiv:2303.14389</a> (replaced) [<a href="/pdf/2303.14389" title="Download PDF">pdf</a>, <a href="/format/2303.14389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MDTv2: Masked Diffusion Transformer is a Strong Image Synthesizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shanghua Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extension of ICCV 2023 work, source code: <a href="https://github.com/sail-sg/MDT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01580" title="Abstract">arXiv:2304.01580</a> (replaced) [<a href="/pdf/2304.01580" title="Download PDF">pdf</a>, <a href="/ps/2304.01580" title="Download PostScript">ps</a>, <a href="/format/2304.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untargeted Near-collision Attacks on Biometrics: Real-world Bounds and  Theoretical Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durbet%2C+A">Axel Durbet</a>, 
<a href="/search/cs?searchtype=author&query=Grollemund%2C+P">Paul-Marie Grollemund</a>, 
<a href="/search/cs?searchtype=author&query=Thiry-Atighehchi%2C+K">Kevin Thiry-Atighehchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Correction of typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03443" title="Abstract">arXiv:2304.03443</a> (replaced) [<a href="/pdf/2304.03443" title="Download PDF">pdf</a>, <a href="/format/2304.03443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiaping Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Feroskhan%2C+M">Mir Feroskhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Artificial Intelligence
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Artificial Intelligence (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09933" title="Abstract">arXiv:2304.09933</a> (replaced) [<a href="/pdf/2304.09933" title="Download PDF">pdf</a>, <a href="/ps/2304.09933" title="Download PostScript">ps</a>, <a href="/format/2304.09933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a Computational Framework for Bayesian Inverse Problems:  Ensemble Kalman Updates and MAP Estimators Under Mesh Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sanz-Alonso%2C+D">Daniel Sanz-Alonso</a>, 
<a href="/search/math?searchtype=author&query=Waniorek%2C+N">Nathan Waniorek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11643" title="Abstract">arXiv:2304.11643</a> (replaced) [<a href="/pdf/2304.11643" title="Download PDF">pdf</a>, <a href="/format/2304.11643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuecheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhenpeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+C">Chengyuan Mai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Youming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yanming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Ad Hoc Networks (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11783" title="Abstract">arXiv:2304.11783</a> (replaced) [<a href="/pdf/2304.11783" title="Download PDF">pdf</a>, <a href="/format/2304.11783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rip Current Detection in Nearshore Areas through UAV Video Analysis with  Almost Local-Isometric Embedding Techniques on Sphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+A">Anchen Sun</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+K">Kaiqi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04427" title="Abstract">arXiv:2305.04427</a> (replaced) [<a href="/pdf/2305.04427" title="Download PDF">pdf</a>, <a href="/ps/2305.04427" title="Download PostScript">ps</a>, <a href="/format/2305.04427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical discretizations of a convective Brinkman-Forchheimer model  under singular forcing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Allendes%2C+A">Alejandro Allendes</a>, 
<a href="/search/math?searchtype=author&query=Campa%C3%B1a%2C+G">Gilberto Campa&#xf1;a</a>, 
<a href="/search/math?searchtype=author&query=Otarola%2C+E">Enrique Otarola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08379" title="Abstract">arXiv:2305.08379</a> (replaced) [<a href="/pdf/2305.08379" title="Download PDF">pdf</a>, <a href="/format/2305.08379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TESS: Text-to-Text Self-Conditioned Simplex Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahabadi%2C+R+K">Rabeeh Karimi Mahabadi</a>, 
<a href="/search/cs?searchtype=author&query=Ivison%2C+H">Hamish Ivison</a>, 
<a href="/search/cs?searchtype=author&query=Tae%2C+J">Jaesung Tae</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+J">James Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Beltagy%2C+I">Iz Beltagy</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+M+E">Matthew E. Peters</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09526" title="Abstract">arXiv:2305.09526</a> (replaced) [<a href="/pdf/2305.09526" title="Download PDF">pdf</a>, <a href="/format/2305.09526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRSA-based Random Access over the Gaussian Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tralli%2C+V">Velio Tralli</a>, 
<a href="/search/cs?searchtype=author&query=Paolini%2C+E">Enrico Paolini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11351" title="Abstract">arXiv:2305.11351</a> (replaced) [<a href="/pdf/2305.11351" title="Download PDF">pdf</a>, <a href="/format/2305.11351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Redaction from Conditional Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Zhifeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SaTML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11527" title="Abstract">arXiv:2305.11527</a> (replaced) [<a href="/pdf/2305.11527" title="Download PDF">pdf</a>, <a href="/format/2305.11527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructIE: A Bilingual Instruction-based Information Extraction Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jintian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hongbin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengshu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; project homepage: <a href="https://www.zjukg.org/project/InstructIE/">this https URL</a> dataset: <a href="https://huggingface.co/datasets/zjunlp/InstructIE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11738" title="Abstract">arXiv:2305.11738</a> (replaced) [<a href="/pdf/2305.11738" title="Download PDF">pdf</a>, <a href="/format/2305.11738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRITIC: Large Language Models Can Self-Correct with Tool-Interactive  Critiquing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13269" title="Abstract">arXiv:2305.13269</a> (replaced) [<a href="/pdf/2305.13269" title="Download PDF">pdf</a>, <a href="/format/2305.13269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Knowledge: Grounding Large Language Models via Dynamic  Knowledge Adapting over Heterogeneous Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruochen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+Y+K">Yew Ken Chia</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bosheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13718" title="Abstract">arXiv:2305.13718</a> (replaced) [<a href="/pdf/2305.13718" title="Download PDF">pdf</a>, <a href="/format/2305.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+F">Fangkai Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bosheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14135" title="Abstract">arXiv:2305.14135</a> (replaced) [<a href="/pdf/2305.14135" title="Download PDF">pdf</a>, <a href="/format/2305.14135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reparo: Loss-Resilient Generative Codec for Video Conferencing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+V">Vibhaalakshmi Sivaraman</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+P">Pantea Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lijie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Katabi%2C+D">Dina Katabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15090" title="Abstract">arXiv:2305.15090</a> (replaced) [<a href="/pdf/2305.15090" title="Download PDF">pdf</a>, <a href="/format/2305.15090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAR: Boosting Low-Resource Information Extraction by Structure-to-Text  Data Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+P">Po-Nien Kung</a>, 
<a href="/search/cs?searchtype=author&query=Brantingham%2C+P+J">P. Jeffrey Brantingham</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at AAAI'24. More info is at <a href="https://derek.ma/STAR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15399" title="Abstract">arXiv:2305.15399</a> (replaced) [<a href="/pdf/2305.15399" title="Download PDF">pdf</a>, <a href="/format/2305.15399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Rundi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruoshi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vondrick%2C+C">Carl Vondrick</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Changxi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Project page: <a href="https://Sin3DM.github.io">this https URL</a>, Code: <a href="https://github.com/Sin3DM/Sin3DM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17212" title="Abstract">arXiv:2305.17212</a> (replaced) [<a href="/pdf/2305.17212" title="Download PDF">pdf</a>, <a href="/format/2305.17212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotational Equilibrium: How Weight Decay Balances Learning Across Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosson%2C+A">Atli Kosson</a>, 
<a href="/search/cs?searchtype=author&query=Messmer%2C+B">Bettina Messmer</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/epfml/REQ">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18010" title="Abstract">arXiv:2305.18010</a> (replaced) [<a href="/pdf/2305.18010" title="Download PDF">pdf</a>, <a href="/format/2305.18010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linchao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ICLR 2024, project page at <a href="https://mzhaoshuai.github.io/RLCF/">this https URL</a>, code is at <a href="https://github.com/mzhaoshuai/RLCF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19215" title="Abstract">arXiv:2305.19215</a> (replaced) [<a href="/pdf/2305.19215" title="Download PDF">pdf</a>, <a href="/format/2305.19215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> dotears: Scalable, consistent DAG estimation using observational and  interventional data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xue%2C+A">Albert Xue</a>, 
<a href="/search/stat?searchtype=author&query=Rao%2C+J">Jingyou Rao</a>, 
<a href="/search/stat?searchtype=author&query=Sankararaman%2C+S">Sriram Sankararaman</a>, 
<a href="/search/stat?searchtype=author&query=Pimentel%2C+H">Harold Pimentel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02586" title="Abstract">arXiv:2306.02586</a> (replaced) [<a href="/pdf/2306.02586" title="Download PDF">pdf</a>, <a href="/format/2306.02586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Internet of Things Meets Robotics: A Survey of Cloud-based Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eze%2C+C">Chrisantus Eze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04929" title="Abstract">arXiv:2306.04929</a> (replaced) [<a href="/pdf/2306.04929" title="Download PDF">pdf</a>, <a href="/format/2306.04929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical coupling of aerosol emissions, dry removal, and turbulent  mixing in the E3SM Atmosphere Model version 1 (EAMv1), part II: a  semi-discrete error analysis framework for assessing coupling schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vogl%2C+C+J">Christopher J. Vogl</a>, 
<a href="/search/math?searchtype=author&query=Wan%2C+H">Hui Wan</a>, 
<a href="/search/math?searchtype=author&query=Woodward%2C+C+S">Carol S. Woodward</a>, 
<a href="/search/math?searchtype=author&query=Bui%2C+Q+M">Quan M. Bui</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Geosci. Model Dev. 17, 3, 1409-1428 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05037" title="Abstract">arXiv:2306.05037</a> (replaced) [<a href="/pdf/2306.05037" title="Download PDF">pdf</a>, <a href="/format/2306.05037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalization-Equivariant Neural Networks with Application to Image  Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbreteau%2C+S">S&#xe9;bastien Herbreteau</a>, 
<a href="/search/cs?searchtype=author&query=Moebel%2C+E">Emmanuel Moebel</a>, 
<a href="/search/cs?searchtype=author&query=Kervrann%2C+C">Charles Kervrann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05857" title="Abstract">arXiv:2306.05857</a> (replaced) [<a href="/pdf/2306.05857" title="Download PDF">pdf</a>, <a href="/format/2306.05857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Sparse Can We Prune A Deep Network: A Fundamental Limit Viewpoint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Q">Qiaozhe Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R">Ruijie Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+J">Jun Sun</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yingzhuang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06088" title="Abstract">arXiv:2306.06088</a> (replaced) [<a href="/pdf/2306.06088" title="Download PDF">pdf</a>, <a href="/format/2306.06088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SENS: Part-Aware Sketch-based Implicit Neural Shape Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binninger%2C+A">Alexandre Binninger</a>, 
<a href="/search/cs?searchtype=author&query=Hertz%2C+A">Amir Hertz</a>, 
<a href="/search/cs?searchtype=author&query=Sorkine-Hornung%2C+O">Olga Sorkine-Hornung</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07196" title="Abstract">arXiv:2306.07196</a> (replaced) [<a href="/pdf/2306.07196" title="Download PDF">pdf</a>, <a href="/format/2306.07196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Enhanced Contrastive Vision-Text Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iscen%2C+A">Ahmet Iscen</a>, 
<a href="/search/cs?searchtype=author&query=Caron%2C+M">Mathilde Caron</a>, 
<a href="/search/cs?searchtype=author&query=Fathi%2C+A">Alireza Fathi</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+C">Cordelia Schmid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07541" title="Abstract">arXiv:2306.07541</a> (replaced) [<a href="/pdf/2306.07541" title="Download PDF">pdf</a>, <a href="/format/2306.07541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Unified Uncertainty-Guided Framework for Offline-to-Online  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jifeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sili Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hechang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+H">Haiyin Piao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08107" title="Abstract">arXiv:2306.08107</a> (replaced) [<a href="/pdf/2306.08107" title="Download PDF">pdf</a>, <a href="/format/2306.08107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoML in the Age of Large Language Models: Current Challenges, Future  Opportunities and Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tornede%2C+A">Alexander Tornede</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+D">Difan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Eimer%2C+T">Theresa Eimer</a>, 
<a href="/search/cs?searchtype=author&query=Giovanelli%2C+J">Joseph Giovanelli</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+A">Aditya Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Ruhkopf%2C+T">Tim Ruhkopf</a>, 
<a href="/search/cs?searchtype=author&query=Segel%2C+S">Sarah Segel</a>, 
<a href="/search/cs?searchtype=author&query=Theodorakopoulos%2C+D">Daphne Theodorakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Tornede%2C+T">Tanja Tornede</a>, 
<a href="/search/cs?searchtype=author&query=Wachsmuth%2C+H">Henning Wachsmuth</a>, 
<a href="/search/cs?searchtype=author&query=Lindauer%2C+M">Marius Lindauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted and accepted at TMLR: <a href="https://openreview.net/forum?id=cAthubStyG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09501" title="Abstract">arXiv:2306.09501</a> (replaced) [<a href="/pdf/2306.09501" title="Download PDF">pdf</a>, <a href="/format/2306.09501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ControlPULP: A RISC-V On-Chip Parallel Power Controller for Many-Core  HPC Processors with FPGA-Based Hardware-In-The-Loop Power and Thermal  Emulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ottaviano%2C+A">Alessandro Ottaviano</a>, 
<a href="/search/cs?searchtype=author&query=Balas%2C+R">Robert Balas</a>, 
<a href="/search/cs?searchtype=author&query=Bambini%2C+G">Giovanni Bambini</a>, 
<a href="/search/cs?searchtype=author&query=del+Vecchio%2C+A">Antonio del Vecchio</a>, 
<a href="/search/cs?searchtype=author&query=Ciani%2C+M">Maicol Ciani</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+D">Davide Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/cs?searchtype=author&query=Bartolini%2C+A">Andrea Bartolini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10061" title="Abstract">arXiv:2306.10061</a> (replaced) [<a href="/pdf/2306.10061" title="Download PDF">pdf</a>, <a href="/format/2306.10061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Ontology for Agents, Systems and Integration of Services: OASIS  version 2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bella%2C+G">Giampaolo Bella</a>, 
<a href="/search/cs?searchtype=author&query=Cantone%2C+D">Domenico Cantone</a>, 
<a href="/search/cs?searchtype=author&query=Longo%2C+C+F">Carmelo Fabio Longo</a>, 
<a href="/search/cs?searchtype=author&query=Nicolosi-Asmundo%2C+M">Marianna Nicolosi-Asmundo</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+D+F">Daniele Francesco Santamaria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165473819">this https URL</a>&amp;origin=resultslist
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Intelligenza Artificiale, Vol. 17, no 1, pp. 51-62, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10447" title="Abstract">arXiv:2306.10447</a> (replaced) [<a href="/pdf/2306.10447" title="Download PDF">pdf</a>, <a href="/format/2306.10447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Globally Interpretable Graph Learning via Distribution Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nian%2C+Y">Yi Nian</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yurui Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lu Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 page, 3 figures, 5 tables. Accepted by the ACM Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12584" title="Abstract">arXiv:2306.12584</a> (replaced) [<a href="/pdf/2306.12584" title="Download PDF">pdf</a>, <a href="/format/2306.12584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Neural Simulation-Based Inference Over Event Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Heinrich%2C+L">Lukas Heinrich</a>, 
<a href="/search/stat?searchtype=author&query=Mishra-Sharma%2C+S">Siddharth Mishra-Sharma</a>, 
<a href="/search/stat?searchtype=author&query=Pollard%2C+C">Chris Pollard</a>, 
<a href="/search/stat?searchtype=author&query=Windischhofer%2C+P">Philipp Windischhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15+2 pages, 7 figures; v2, version published in TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16248" title="Abstract">arXiv:2306.16248</a> (replaced) [<a href="/pdf/2306.16248" title="Download PDF">pdf</a>, <a href="/format/2306.16248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent SDEs on Homogeneous Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Sebastian Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Graf%2C+F">Florian Graf</a>, 
<a href="/search/cs?searchtype=author&query=Kwitt%2C+R">Roland Kwitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v3: updated experiments with results using the public source code (commit bc6edd1)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16869" title="Abstract">arXiv:2306.16869</a> (replaced) [<a href="/pdf/2306.16869" title="Download PDF">pdf</a>, <a href="/format/2306.16869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural  Network Inference in Low-Voltage Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao-Lun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hsiung%2C+L">Lei Hsiung</a>, 
<a href="/search/cs?searchtype=author&query=Chandramoorthy%2C+N">Nandhini Chandramoorthy</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00897" title="Abstract">arXiv:2307.00897</a> (replaced) [<a href="/pdf/2307.00897" title="Download PDF">pdf</a>, <a href="/format/2307.00897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixing confirmation bias in feature attribution methods via semantic  match
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cin%C3%A0%2C+G">Giovanni Cin&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Llaneza%2C+D">Daniel Fernandez-Llaneza</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+N">Nishant Mishra</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ber%2C+T+E">Tabea E. R&#xf6;ber</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>, 
<a href="/search/cs?searchtype=author&query=Calixto%2C+I">Iacer Calixto</a>, 
<a href="/search/cs?searchtype=author&query=Goedhart%2C+R">Rob Goedhart</a>, 
<a href="/search/cs?searchtype=author&query=Birbil%2C+%C5%9E+%C4%B0">&#x15e;. &#x130;lker Birbil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01470" title="Abstract">arXiv:2307.01470</a> (replaced) [<a href="/pdf/2307.01470" title="Download PDF">pdf</a>, <a href="/format/2307.01470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Driver Gaze Estimation and Application in Gaze Behavior  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P+K">Pavan Kumar Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+P">Pranamesh Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02654" title="Abstract">arXiv:2307.02654</a> (replaced) [<a href="/pdf/2307.02654" title="Download PDF">pdf</a>, <a href="/format/2307.02654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe &amp; Accurate at Speed with Tendons: A Robot Arm for Exploring Dynamic  Motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guist%2C+S">Simon Guist</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jan Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Le Chen</a>, 
<a href="/search/cs?searchtype=author&query=Berenz%2C+V">Vincent Berenz</a>, 
<a href="/search/cs?searchtype=author&query=Martus%2C+J">Julian Martus</a>, 
<a href="/search/cs?searchtype=author&query=Ott%2C+H">Heiko Ott</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%BCninger%2C+F">Felix Gr&#xfc;ninger</a>, 
<a href="/search/cs?searchtype=author&query=Muehlebach%2C+M">Michael Muehlebach</a>, 
<a href="/search/cs?searchtype=author&query=Fiene%2C+J">Jonathan Fiene</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCchler%2C+D">Dieter B&#xfc;chler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05209" title="Abstract">arXiv:2307.05209</a> (replaced) [<a href="/pdf/2307.05209" title="Download PDF">pdf</a>, <a href="/format/2307.05209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Pre-planning on Reward Machine Abstractions for Enhanced  Transfer in Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azran%2C+G">Guy Azran</a>, 
<a href="/search/cs?searchtype=author&query=Danesh%2C+M+H">Mohamad H. Danesh</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>, 
<a href="/search/cs?searchtype=author&query=Keren%2C+S">Sarah Keren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06629" title="Abstract">arXiv:2307.06629</a> (replaced) [<a href="/pdf/2307.06629" title="Download PDF">pdf</a>, <a href="/format/2307.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A well-balanced discontinuous Galerkin method for the first--order Z4  formulation of the Einstein--Euler system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Dumbser%2C+M">Michael Dumbser</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zanotti%2C+O">Olindo Zanotti</a>, 
<a href="/search/gr-qc?searchtype=author&query=Gaburro%2C+E">Elena Gaburro</a>, 
<a href="/search/gr-qc?searchtype=author&query=Peshkov%2C+I">Ilya Peshkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript matches the version accepted by Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07935" title="Abstract">arXiv:2307.07935</a> (replaced) [<a href="/pdf/2307.07935" title="Download PDF">pdf</a>, <a href="/format/2307.07935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from  Simulation to Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runsheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Q">Qin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submit the latest one, accepted by the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08256" title="Abstract">arXiv:2307.08256</a> (replaced) [<a href="/pdf/2307.08256" title="Download PDF">pdf</a>, <a href="/ps/2307.08256" title="Download PostScript">ps</a>, <a href="/format/2307.08256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URLLC in IRS-Aided MIMO Systems: Finite Blocklength Analysis and Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, accepted by Asilomar Conference on Signals, Systems, and Computers 2023. arXiv admin note: text overlap with <a href="/abs/2210.08832">arXiv:2210.08832</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15700" title="Abstract">arXiv:2307.15700</a> (replaced) [<a href="/pdf/2307.15700" title="Download PDF">pdf</a>, <a href="/format/2307.15700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruopeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023. In the latest version, we report the results on SportsMOT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15981" title="Abstract">arXiv:2307.15981</a> (replaced) [<a href="/pdf/2307.15981" title="Download PDF">pdf</a>, <a href="/format/2307.15981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaitASMS: Gait Recognition by Adaptive Structured Spatial Representation  and Multi-Scale Temporal Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+H">Hu Long</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xueling Feng</a>, 
<a href="/search/cs?searchtype=author&query=Nixon%2C+M">Mark Nixon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16797" title="Abstract">arXiv:2307.16797</a> (replaced) [<a href="/pdf/2307.16797" title="Download PDF">pdf</a>, <a href="/format/2307.16797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shepherding control and herdability in complex multiagent systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Lama%2C+A">Andrea Lama</a>, 
<a href="/search/cond-mat?searchtype=author&query=di+Bernardo%2C+M">Mario di Bernardo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01391" title="Abstract">arXiv:2308.01391</a> (replaced) [<a href="/pdf/2308.01391" title="Download PDF">pdf</a>, <a href="/ps/2308.01391" title="Download PostScript">ps</a>, <a href="/format/2308.01391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Machine Translation through Prompt Engineering: An  Investigation into ChatGPT&#x27;s Customizability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Masaru Yamada</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Machine Translation Summit XIX, Vol. 2: Users
  Track, pages 195-204, September 4-8, 2023, Macau SAR, China
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06797" title="Abstract">arXiv:2308.06797</a> (replaced) [<a href="/pdf/2308.06797" title="Download PDF">pdf</a>, <a href="/ps/2308.06797" title="Download PostScript">ps</a>, <a href="/format/2308.06797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Zero-Knowledge Revocable Credential Verification Protocol Using  Attribute-Based Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartolomeo%2C+G">Giovanni Bartolomeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> version #3 published on 17 February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06912" title="Abstract">arXiv:2308.06912</a> (replaced) [<a href="/pdf/2308.06912" title="Download PDF">pdf</a>, <a href="/format/2308.06912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CausalLM is not optimal for in-context learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Nan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Levinboim%2C+T">Tomer Levinboim</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+S">Sebastian Goodman</a>, 
<a href="/search/cs?searchtype=author&query=Soricut%2C+R">Radu Soricut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 conference paper. Code available at: <a href="https://github.com/google-research/causallm_icl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07915" title="Abstract">arXiv:2308.07915</a> (replaced) [<a href="/pdf/2308.07915" title="Download PDF">pdf</a>, <a href="/format/2308.07915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-threshold and low-overhead fault-tolerant quantum memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bravyi%2C+S">Sergey Bravyi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cross%2C+A+W">Andrew W. Cross</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gambetta%2C+J+M">Jay M. Gambetta</a>, 
<a href="/search/quant-ph?searchtype=author&query=Maslov%2C+D">Dmitri Maslov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rall%2C+P">Patrick Rall</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yoder%2C+T+J">Theodore J. Yoder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> numerical results are revised after fixing a bug in the simulation software
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09073" title="Abstract">arXiv:2308.09073</a> (replaced) [<a href="/pdf/2308.09073" title="Download PDF">pdf</a>, <a href="/format/2308.09073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Ying Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09905" title="Abstract">arXiv:2308.09905</a> (replaced) [<a href="/pdf/2308.09905" title="Download PDF">pdf</a>, <a href="/format/2308.09905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffusionTrack: Diffusion Model For Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Run Luo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zikai Song</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lintao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jinlin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10149" title="Abstract">arXiv:2308.10149</a> (replaced) [<a href="/pdf/2308.10149" title="Download PDF">pdf</a>, <a href="/format/2308.10149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Fairness in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingji Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Rui Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ying Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures, 2 tables, 175 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14116" title="Abstract">arXiv:2308.14116</a> (replaced) [<a href="/pdf/2308.14116" title="Download PDF">pdf</a>, <a href="/format/2308.14116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Kernel and Parameterized Algorithm for Almost Induced  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mingyu Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TAMC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14120" title="Abstract">arXiv:2308.14120</a> (replaced) [<a href="/pdf/2308.14120" title="Download PDF">pdf</a>, <a href="/ps/2308.14120" title="Download PostScript">ps</a>, <a href="/format/2308.14120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Streamline Automated Machine Learning for Clinical  Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lotfinia%2C+M">Mahshad Lotfinia</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/cs?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Nature Communications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nat Commun 15, 1603 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14456" title="Abstract">arXiv:2308.14456</a> (replaced) [<a href="/pdf/2308.14456" title="Download PDF">pdf</a>, <a href="/format/2308.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Self-Supervised Representations Benchmarking: a Case for Larger  Probing Heads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zaiem%2C+S">Salah Zaiem</a>, 
<a href="/search/eess?searchtype=author&query=Kemiche%2C+Y">Youcef Kemiche</a>, 
<a href="/search/eess?searchtype=author&query=Parcollet%2C+T">Titouan Parcollet</a>, 
<a href="/search/eess?searchtype=author&query=Essid%2C+S">Slim Essid</a>, 
<a href="/search/eess?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14500" title="Abstract">arXiv:2308.14500</a> (replaced) [<a href="/pdf/2308.14500" title="Download PDF">pdf</a>, <a href="/format/2308.14500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAC: Latent Action Composition for Skeleton-based Action Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Di Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dantcheva%2C+A">Antitza Dantcheva</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Quan Kong</a>, 
<a href="/search/cs?searchtype=author&query=Garattoni%2C+L">Lorenzo Garattoni</a>, 
<a href="/search/cs?searchtype=author&query=Francesca%2C+G">Gianpiero Francesca</a>, 
<a href="/search/cs?searchtype=author&query=Bremond%2C+F">Francois Bremond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15122" title="Abstract">arXiv:2308.15122</a> (replaced) [<a href="/pdf/2308.15122" title="Download PDF">pdf</a>, <a href="/format/2308.15122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikeBERT: A Language Spikformer Learned from BERT with Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Chenxi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zixuan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15478" title="Abstract">arXiv:2308.15478</a> (replaced) [<a href="/pdf/2308.15478" title="Download PDF">pdf</a>, <a href="/format/2308.15478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive Tangent Feature Perspective of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=LeJeune%2C+D">Daniel LeJeune</a>, 
<a href="/search/cs?searchtype=author&query=Alemohammad%2C+S">Sina Alemohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures. Appeared at the First Conference on Parsimony and Learning (CPAL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16198" title="Abstract">arXiv:2308.16198</a> (replaced) [<a href="/pdf/2308.16198" title="Download PDF">pdf</a>, <a href="/format/2308.16198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Information Dissemination with Graph-based Multi-Agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galliera%2C+R">Raffaele Galliera</a>, 
<a href="/search/cs?searchtype=author&query=Venable%2C+K+B">Kristen Brent Venable</a>, 
<a href="/search/cs?searchtype=author&query=Bassani%2C+M">Matteo Bassani</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+N">Niranjan Suri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16800" title="Abstract">arXiv:2308.16800</a> (replaced) [<a href="/pdf/2308.16800" title="Download PDF">pdf</a>, <a href="/format/2308.16800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Andreas Roth</a>, 
<a href="/search/cs?searchtype=author&query=Liebig%2C+T">Thomas Liebig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at LoG 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01513" title="Abstract">arXiv:2309.01513</a> (replaced) [<a href="/pdf/2309.01513" title="Download PDF">pdf</a>, <a href="/format/2309.01513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the  Absence of First-order Echoes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yeon%2C+I">Inmo Yeon</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+J">Jung-Woo Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01610" title="Abstract">arXiv:2309.01610</a> (replaced) [<a href="/pdf/2309.01610" title="Download PDF">pdf</a>, <a href="/format/2309.01610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Ranking under Disparate Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+R">Richa Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Joachims%2C+T">Thorsten Joachims</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A version of this paper was accepted as Spotlight (Oral) at UAI workshop on Epistemic AI, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03048" title="Abstract">arXiv:2309.03048</a> (replaced) [<a href="/pdf/2309.03048" title="Download PDF">pdf</a>, <a href="/format/2309.03048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Semantic Consistency in Unpaired Image Translation to Generate  Data for Surgical Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+D+K">Danush Kumar Venkatesh</a>, 
<a href="/search/cs?searchtype=author&query=Rivoir%2C+D">Dominik Rivoir</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+M">Micha Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Kolbinger%2C+F">Fiona Kolbinger</a>, 
<a href="/search/cs?searchtype=author&query=Distler%2C+M">Marius Distler</a>, 
<a href="/search/cs?searchtype=author&query=Weitz%2C+J">J&#xfc;rgen Weitz</a>, 
<a href="/search/cs?searchtype=author&query=Speidel%2C+S">Stefanie Speidel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IPCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03451" title="Abstract">arXiv:2309.03451</a> (replaced) [<a href="/pdf/2309.03451" title="Download PDF">pdf</a>, <a href="/format/2309.03451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-domain Sound Recognition for Efficient Underwater Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeongsoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Gyun Han</a>, 
<a href="/search/cs?searchtype=author&query=La%2C+H+S">Hyoung Sul La</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yoonchang Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Eun-Jin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to APSIPA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06365" title="Abstract">arXiv:2309.06365</a> (replaced) [<a href="/pdf/2309.06365" title="Download PDF">pdf</a>, <a href="/format/2309.06365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cited Text Spans for Citation Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangci Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yi-Hui Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+J">Jessica Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07524" title="Abstract">arXiv:2309.07524</a> (replaced) [<a href="/pdf/2309.07524" title="Download PDF">pdf</a>, <a href="/format/2309.07524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-scale Generalized Shrinkage Threshold Network for Image Blind  Deblurring in Remote Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yujie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaohong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages,Accepted to IEEE Transactions on Geoscience and Remote Sensing,2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Geoscience and Remote Sensing,2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Theory (cs.IT); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07808" title="Abstract">arXiv:2309.07808</a> (replaced) [<a href="/pdf/2309.07808" title="Download PDF">pdf</a>, <a href="/format/2309.07808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Matters to Enhance Traffic Rule Compliance of Imitation Learning  for Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+A">Aifen Sui</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08172" title="Abstract">arXiv:2309.08172</a> (replaced) [<a href="/pdf/2309.08172" title="Download PDF">pdf</a>, <a href="/format/2309.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LASER: LLM Agent with State-Space Exploration for Web Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kaixin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08415" title="Abstract">arXiv:2309.08415</a> (replaced) [<a href="/pdf/2309.08415" title="Download PDF">pdf</a>, <a href="/ps/2309.08415" title="Download PostScript">ps</a>, <a href="/format/2309.08415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new method of modeling the multi-stage decision-making process of CRT  using machine learning with uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Larsen%2C+K">Kristoffer Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Keyak%2C+J">Joyce Keyak</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Q">Qiuying Sha</a>, 
<a href="/search/cs?searchtype=author&query=Paez%2C+D">Diana Paez</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+G">Guang-Uei Hung</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jiangang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Peix%2C+A">Amalia Peix</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages,6 figures. arXiv admin note: text overlap with <a href="/abs/2305.02475">arXiv:2305.02475</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09101" title="Abstract">arXiv:2309.09101</a> (replaced) [<a href="/pdf/2309.09101" title="Download PDF">pdf</a>, <a href="/format/2309.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behavioral-based circular formation control for robot swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bautista%2C+J">Jes&#xfa;s Bautista</a>, 
<a href="/search/cs?searchtype=author&query=de+Marina%2C+H+G">H&#xe9;ctor Garc&#xed;a de Marina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09530" title="Abstract">arXiv:2309.09530</a> (replaced) [<a href="/pdf/2309.09530" title="Download PDF">pdf</a>, <a href="/format/2309.09530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Large Language Models via Reading Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Daixuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09697" title="Abstract">arXiv:2309.09697</a> (replaced) [<a href="/pdf/2309.09697" title="Download PDF">pdf</a>, <a href="/format/2309.09697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Gender Bias of Pre-trained Language Models in Natural  Language Inference by Considering All Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anantaprayoon%2C+P">Panatchakorn Anantaprayoon</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10400" title="Abstract">arXiv:2309.10400</a> (replaced) [<a href="/pdf/2309.10400" title="Download PDF">pdf</a>, <a href="/format/2309.10400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoSE: Efficient Context Window Extension of LLMs via Positional  Skip-wise Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dawei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sujian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10687" title="Abstract">arXiv:2309.10687</a> (replaced) [<a href="/pdf/2309.10687" title="Download PDF">pdf</a>, <a href="/format/2309.10687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EchoPrompt: Instructing the Model to Rephrase Queries for Improved  In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mekala%2C+R+R">Rajasekhar Reddy Mekala</a>, 
<a href="/search/cs?searchtype=author&query=Razeghi%2C+Y">Yasaman Razeghi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14681" title="Abstract">arXiv:2309.14681</a> (replaced) [<a href="/pdf/2309.14681" title="Download PDF">pdf</a>, <a href="/format/2309.14681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Human-generated Demonstrations Necessary for In-context Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16378" title="Abstract">arXiv:2309.16378</a> (replaced) [<a href="/pdf/2309.16378" title="Download PDF">pdf</a>, <a href="/format/2309.16378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meshless interface tracking for the simulation of dendrite envelope  growth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jan%C4%8Di%C4%8D%2C+M">Mitja Jan&#x10d;i&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Zalo%C5%BEnik%2C+M">Miha Zalo&#x17e;nik</a>, 
<a href="/search/math?searchtype=author&query=Kosec%2C+G">Gregor Kosec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version submitted to Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16935" title="Abstract">arXiv:2309.16935</a> (replaced) [<a href="/pdf/2309.16935" title="Download PDF">pdf</a>, <a href="/format/2309.16935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled  Prescriptive Maintenance Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Helin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17272" title="Abstract">arXiv:2309.17272</a> (replaced) [<a href="/pdf/2309.17272" title="Download PDF">pdf</a>, <a href="/format/2309.17272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Models in Coding Through Multi-Perspective  Self-Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17452" title="Abstract">arXiv:2309.17452</a> (replaced) [<a href="/pdf/2309.17452" title="Download PDF">pdf</a>, <a href="/format/2309.17452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; First two authors equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00297" title="Abstract">arXiv:2310.00297</a> (replaced) [<a href="/pdf/2310.00297" title="Download PDF">pdf</a>, <a href="/format/2310.00297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding In-Context Learning from Repetitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chiyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yafu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024. Updated with new experiments and results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01468" title="Abstract">arXiv:2310.01468</a> (replaced) [<a href="/pdf/2310.01468" title="Download PDF">pdf</a>, <a href="/format/2310.01468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing the Multi-turn Planning Capabilities of LLMs via 20 Question  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02296" title="Abstract">arXiv:2310.02296</a> (replaced) [<a href="/pdf/2310.02296" title="Download PDF">pdf</a>, <a href="/format/2310.02296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP Is Also a Good Teacher: A New Learning Framework for Inductive  Zero-shot Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+D">Daisuke Deguchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenkai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Murase%2C+H">Hiroshi Murase</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02529" title="Abstract">arXiv:2310.02529</a> (replaced) [<a href="/pdf/2310.02529" title="Download PDF">pdf</a>, <a href="/format/2310.02529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIDDAG: Where Does Our News Go? Investigating Information Diffusion via  Community-Level Information Pathways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+A+K">Alexander K. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+N">Nuan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+P">Po-Nien Kung</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenna Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shicheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Azure Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at AAAI'24. System demo video and more info: info-pathways.github.io
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02984" title="Abstract">arXiv:2310.02984</a> (replaced) [<a href="/pdf/2310.02984" title="Download PDF">pdf</a>, <a href="/format/2310.02984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws for Associative Memories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/stat?searchtype=author&query=Dohmatob%2C+E">Elvis Dohmatob</a>, 
<a href="/search/stat?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03054" title="Abstract">arXiv:2310.03054</a> (replaced) [<a href="/pdf/2310.03054" title="Download PDF">pdf</a>, <a href="/format/2310.03054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Sampling Based on Gradient Flows of the MMD with Negative  Distance Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>, 
<a href="/search/stat?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/stat?searchtype=author&query=Altekr%C3%BCger%2C+F">Fabian Altekr&#xfc;ger</a>, 
<a href="/search/stat?searchtype=author&query=Beinert%2C+R">Robert Beinert</a>, 
<a href="/search/stat?searchtype=author&query=Chemseddine%2C+J">Jannis Chemseddine</a>, 
<a href="/search/stat?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03240" title="Abstract">arXiv:2310.03240</a> (replaced) [<a href="/pdf/2310.03240" title="Download PDF">pdf</a>, <a href="/format/2310.03240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hierarchical Relational Representations through Relational  Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altabaa%2C+A">Awni Altabaa</a>, 
<a href="/search/cs?searchtype=author&query=Lafferty%2C+J">John Lafferty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03291" title="Abstract">arXiv:2310.03291</a> (replaced) [<a href="/pdf/2310.03291" title="Download PDF">pdf</a>, <a href="/format/2310.03291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expedited Training of Visual Conditioned Language Generation via  Redundancy Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Y">Yiren Jian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03661" title="Abstract">arXiv:2310.03661</a> (replaced) [<a href="/pdf/2310.03661" title="Download PDF">pdf</a>, <a href="/format/2310.03661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness-Guided Image Synthesis for Data-Free Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Huanpeng Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaoxuan He</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C">Chengfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03668" title="Abstract">arXiv:2310.03668</a> (replaced) [<a href="/pdf/2310.03668" title="Download PDF">pdf</a>, <a href="/format/2310.03668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sainz%2C+O">Oscar Sainz</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ferrero%2C+I">Iker Garc&#xed;a-Ferrero</a>, 
<a href="/search/cs?searchtype=author&query=Agerri%2C+R">Rodrigo Agerri</a>, 
<a href="/search/cs?searchtype=author&query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
<a href="/search/cs?searchtype=author&query=Rigau%2C+G">German Rigau</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05264" title="Abstract">arXiv:2310.05264</a> (replaced) [<a href="/pdf/2310.05264" title="Download PDF">pdf</a>, <a href="/format/2310.05264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Emergence of Reproducibility and Consistency in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huijie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinfan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minzhe Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Liyue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 23 figures, best paper award in NeurIPS Diffusion Model Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05440" title="Abstract">arXiv:2310.05440</a> (replaced) [<a href="/pdf/2310.05440" title="Download PDF">pdf</a>, <a href="/format/2310.05440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Modeling and Simulation of Chemo-Elasto-Plastically Coupled  Battery Active Particles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schoof%2C+R">Raphael Schoof</a>, 
<a href="/search/math?searchtype=author&query=Niermann%2C+J">Johannes Niermann</a>, 
<a href="/search/math?searchtype=author&query=Dyck%2C+A">Alexander Dyck</a>, 
<a href="/search/math?searchtype=author&query=B%C3%B6hlke%2C+T">Thomas B&#xf6;hlke</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+W">Willy D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07138" title="Abstract">arXiv:2310.07138</a> (replaced) [<a href="/pdf/2310.07138" title="Download PDF">pdf</a>, <a href="/format/2310.07138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Task Routing for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Byeongjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+S">Sangmin Woo</a>, 
<a href="/search/cs?searchtype=author&query=Go%2C+H">Hyojun Go</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin-Young Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changick Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07972" title="Abstract">arXiv:2310.07972</a> (replaced) [<a href="/pdf/2310.07972" title="Download PDF">pdf</a>, <a href="/format/2310.07972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Diffusion via Information Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xianghao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+O">Ollie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08041" title="Abstract">arXiv:2310.08041</a> (replaced) [<a href="/pdf/2310.08041" title="Download PDF">pdf</a>, <a href="/format/2310.08041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiuying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhiwei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jianfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08412" title="Abstract">arXiv:2310.08412</a> (replaced) [<a href="/pdf/2310.08412" title="Download PDF">pdf</a>, <a href="/format/2310.08412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sound and Complete Refinement Relation for Non-reducible Modal  Transition Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basile%2C+D">Davide Basile</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09470" title="Abstract">arXiv:2310.09470</a> (replaced) [<a href="/pdf/2310.09470" title="Download PDF">pdf</a>, <a href="/format/2310.09470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Aware Ergodic Search: Continuous Exploration for Multi-Agent  Systems with Battery Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seewald%2C+A">Adam Seewald</a>, 
<a href="/search/cs?searchtype=author&query=Lerch%2C+C+J">Cameron J. Lerch</a>, 
<a href="/search/cs?searchtype=author&query=Chanc%C3%A1n%2C+M">Marvin Chanc&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Dollar%2C+A+M">Aaron M. Dollar</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+I">Ian Abraham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, ICRA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10462" title="Abstract">arXiv:2310.10462</a> (replaced) [<a href="/pdf/2310.10462" title="Download PDF">pdf</a>, <a href="/format/2310.10462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Neural Ranking Framework: Toward Maximized Business Goal for  Cascade Ranking Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shiyang Wen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Dongying Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, Accepted by www2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10513" title="Abstract">arXiv:2310.10513</a> (replaced) [<a href="/pdf/2310.10513" title="Download PDF">pdf</a>, <a href="/format/2310.10513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Image Processing as Visual Prompting Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianzheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiantao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10534" title="Abstract">arXiv:2310.10534</a> (replaced) [<a href="/pdf/2310.10534" title="Download PDF">pdf</a>, <a href="/format/2310.10534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Comparators in Generalization Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hellstr%C3%B6m%2C+F">Fredrik Hellstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11198" title="Abstract">arXiv:2310.11198</a> (replaced) [<a href="/pdf/2310.11198" title="Download PDF">pdf</a>, <a href="/format/2310.11198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EEG motor imagery decoding: A framework for comparative analysis with  channel attention mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wimpff%2C+M">Martin Wimpff</a>, 
<a href="/search/cs?searchtype=author&query=Gizzi%2C+L">Leonardo Gizzi</a>, 
<a href="/search/cs?searchtype=author&query=Zerfowski%2C+J">Jan Zerfowski</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to: Journal of Neural Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12248" title="Abstract">arXiv:2310.12248</a> (replaced) [<a href="/pdf/2310.12248" title="Download PDF">pdf</a>, <a href="/format/2310.12248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perez%2C+M">Mateo Perez</a>, 
<a href="/search/cs?searchtype=author&query=Somenzi%2C+F">Fabio Somenzi</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Ashutosh Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13920" title="Abstract">arXiv:2310.13920</a> (replaced) [<a href="/pdf/2310.13920" title="Download PDF">pdf</a>, <a href="/format/2310.13920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New low-order mixed finite element methods for linear elasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xuehai Huang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yaqian Zhou</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+Y">Yangxing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14691" title="Abstract">arXiv:2310.14691</a> (replaced) [<a href="/pdf/2310.14691" title="Download PDF">pdf</a>, <a href="/ps/2310.14691" title="Download PostScript">ps</a>, <a href="/format/2310.14691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiability of total effects from abstractions of time series causal  graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Assaad%2C+C+K">Charles K. Assaad</a>, 
<a href="/search/math?searchtype=author&query=Devijver%2C+E">Emilie Devijver</a> (LIG, UGA), 
<a href="/search/math?searchtype=author&query=Gaussier%2C+E">Eric Gaussier</a> (LIG, UGA), 
<a href="/search/math?searchtype=author&query=G%C3%B6ssler%2C+G">Gregor G&#xf6;ssler</a> (LIG, SPADES), 
<a href="/search/math?searchtype=author&query=Meynaoui%2C+A">Anouar Meynaoui</a> (IRMAR, UR2)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16834" title="Abstract">arXiv:2310.16834</a> (replaced) [<a href="/pdf/2310.16834" title="Download PDF">pdf</a>, <a href="/format/2310.16834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Diffusion Modeling by Estimating the Ratios of the Data  Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lou%2C+A">Aaron Lou</a>, 
<a href="/search/stat?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/stat?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages. Code at <a href="https://github.com/louaaron/Score-Entropy-Discrete-Diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18948" title="Abstract">arXiv:2310.18948</a> (replaced) [<a href="/pdf/2310.18948" title="Download PDF">pdf</a>, <a href="/format/2310.18948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Feature Augmentation for AIS-Based Multi-Path Long-Term  Vessel Trajectory Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spadon%2C+G">Gabriel Spadon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+J">Jay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Eden%2C+D">Derek Eden</a>, 
<a href="/search/cs?searchtype=author&query=van+Berkel%2C+J">Josh van Berkel</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+T">Tom Foster</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+M">Matthew Smith</a>, 
<a href="/search/cs?searchtype=author&query=Vela%2C+S">Sarah Vela</a>, 
<a href="/search/cs?searchtype=author&query=Gehrmann%2C+R">Romina Gehrmann</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+A">Amilcar Soares</a>, 
<a href="/search/cs?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a>, 
<a href="/search/cs?searchtype=author&query=Matwin%2C+S">Stan Matwin</a>, 
<a href="/search/cs?searchtype=author&query=Pelot%2C+R">Ronald Pelot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19531" title="Abstract">arXiv:2310.19531</a> (replaced) [<a href="/pdf/2310.19531" title="Download PDF">pdf</a>, <a href="/format/2310.19531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties  in Generative Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenpeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xue Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+G">Guiguang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19673" title="Abstract">arXiv:2310.19673</a> (replaced) [<a href="/e-print/2310.19673" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Analysis of a Novel Radial Deployment Mechanism of Payloads  in Sounding Rockets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+T+P+G">Thakur Pranav G. Singh</a>, 
<a href="/search/eess?searchtype=author&query=Anand%2C+U">Utkarsh Anand</a>, 
<a href="/search/eess?searchtype=author&query=Agrawal%2C+T">Tanvi Agrawal</a>, 
<a href="/search/eess?searchtype=author&query=G%2C+S">Srinivas G</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The results in this paper have to be verified again and hence a new paper has to be written
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01200" title="Abstract">arXiv:2311.01200</a> (replaced) [<a href="/pdf/2311.01200" title="Download PDF">pdf</a>, <a href="/format/2311.01200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning Under Language Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gogoulou%2C+E">Evangelia Gogoulou</a>, 
<a href="/search/cs?searchtype=author&query=Lesort%2C+T">Timoth&#xe9;e Lesort</a>, 
<a href="/search/cs?searchtype=author&query=Boman%2C+M">Magnus Boman</a>, 
<a href="/search/cs?searchtype=author&query=Nivre%2C+J">Joakim Nivre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02847" title="Abstract">arXiv:2311.02847</a> (replaced) [<a href="/pdf/2311.02847" title="Download PDF">pdf</a>, <a href="/format/2311.02847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinematic-aware Prompting for Generalizable Articulated Object  Manipulation with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wenke Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+X">Xincheng Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhigang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Di Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03054" title="Abstract">arXiv:2311.03054</a> (replaced) [<a href="/pdf/2311.03054" title="Download PDF">pdf</a>, <a href="/format/2311.03054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnyText: Multilingual Visual Text Generation And Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuo%2C+Y">Yuxiang Tuo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wangmeng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun-Yan He</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yifeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04320" title="Abstract">arXiv:2311.04320</a> (replaced) [<a href="/pdf/2311.04320" title="Download PDF">pdf</a>, <a href="/format/2311.04320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proprioceptive Invariant Robot State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Yuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingjun Li</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Wenzhe Tong</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04590" title="Abstract">arXiv:2311.04590</a> (replaced) [<a href="/pdf/2311.04590" title="Download PDF">pdf</a>, <a href="/format/2311.04590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Cross-Domain Sequential Recommendation under Open-World  Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wujiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+M">Mingming Ha</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qiongxu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Linxun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bing Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06973" title="Abstract">arXiv:2311.06973</a> (replaced) [<a href="/pdf/2311.06973" title="Download PDF">pdf</a>, <a href="/ps/2311.06973" title="Download PostScript">ps</a>, <a href="/format/2311.06973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Verification of Deep Neural Network Performance for  Time-Synchronized Distribution System State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azimian%2C+B">Behrouz Azimian</a>, 
<a href="/search/cs?searchtype=author&query=Moshtagh%2C+S">Shiva Moshtagh</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Anamitra Pal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shanshan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06999" title="Abstract">arXiv:2311.06999</a> (replaced) [<a href="/pdf/2311.06999" title="Download PDF">pdf</a>, <a href="/format/2311.06999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum and classical query complexities of functions of matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Montanaro%2C+A">Ashley Montanaro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shao%2C+C">Changpeng Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, key results are enhanced
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07092" title="Abstract">arXiv:2311.07092</a> (replaced) [<a href="/pdf/2311.07092" title="Download PDF">pdf</a>, <a href="/format/2311.07092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Tell The Truth: Language of Deception and Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hazra%2C+S">Sanchaita Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07215" title="Abstract">arXiv:2311.07215</a> (replaced) [<a href="/pdf/2311.07215" title="Download PDF">pdf</a>, <a href="/format/2311.07215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungjun Moon</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yongho Song</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+H">Hyungjoo Chae</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongjin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taeyoon Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+K+T">Kai Tzu-iunn Ong</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Seung-won Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07945" title="Abstract">arXiv:2311.07945</a> (replaced) [<a href="/pdf/2311.07945" title="Download PDF">pdf</a>, <a href="/format/2311.07945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well begun is half done: Importance of Starting Right in Multi-Step Math  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Kushal Jain</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+K">Kumar Shridhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08596" title="Abstract">arXiv:2311.08596</a> (replaced) [<a href="/pdf/2311.08596" title="Download PDF">pdf</a>, <a href="/format/2311.08596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are You Sure? Challenging LLMs Leads to Performance Drops in The  FlipFlop Experiment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laban%2C+P">Philippe Laban</a>, 
<a href="/search/cs?searchtype=author&query=Murakhovs%27ka%2C+L">Lidiya Murakhovs&#x27;ka</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08648" title="Abstract">arXiv:2311.08648</a> (replaced) [<a href="/pdf/2311.08648" title="Download PDF">pdf</a>, <a href="/format/2311.08648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore Spurious Correlations at the Concept Level in Language Models  for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Paiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+W">Wei Ai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09483" title="Abstract">arXiv:2311.09483</a> (replaced) [<a href="/pdf/2311.09483" title="Download PDF">pdf</a>, <a href="/format/2311.09483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Interventions with User-Defined Goals for Health Behavior  Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandyam%2C+A">Aishwarya Mandyam</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B6rke%2C+M">Matthew J&#xf6;rke</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+B+E">Barbara E. Engelhardt</a>, 
<a href="/search/cs?searchtype=author&query=Brunskill%2C+E">Emma Brunskill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10475" title="Abstract">arXiv:2311.10475</a> (replaced) [<a href="/pdf/2311.10475" title="Download PDF">pdf</a>, <a href="/format/2311.10475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conway&#x27;s law, revised from a mathematical viewpoint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsutani%2C+S">Shigeki Matsutani</a>, 
<a href="/search/cs?searchtype=author&query=Ohmori%2C+S">Shousuke Ohmori</a>, 
<a href="/search/cs?searchtype=author&query=Hiranabe%2C+K">Kenji Hiranabe</a>, 
<a href="/search/cs?searchtype=author&query=Hanyuda%2C+E">Eiichi Hanyuda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> p.28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY); General Topology (math.GN)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10928" title="Abstract">arXiv:2311.10928</a> (replaced) [<a href="/pdf/2311.10928" title="Download PDF">pdf</a>, <a href="/format/2311.10928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMRA: Copilot for AMR Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J+Z">Jon Z. Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S+R">Shafiuddin Rehan Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Bonn%2C+J">Julia Bonn</a>, 
<a href="/search/cs?searchtype=author&query=Wright-Bettner%2C+K">Kristin Wright-Bettner</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+M">Martha Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+J+H">James H. Martin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 System Demonstration
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11343" title="Abstract">arXiv:2311.11343</a> (replaced) [<a href="/pdf/2311.11343" title="Download PDF">pdf</a>, <a href="/format/2311.11343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Model for Accelerated Inverse Modelling Using a Novel  Embedding for Continuous Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bompas%2C+S">S&#xe9;bastien Bompas</a>, 
<a href="/search/cs?searchtype=author&query=Sandfeld%2C+S">Stefan Sandfeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13277" title="Abstract">arXiv:2311.13277</a> (replaced) [<a href="/pdf/2311.13277" title="Download PDF">pdf</a>, <a href="/format/2311.13277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Matrix Factorization for Interpretable Collaborative  Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sugahara%2C+K">Kai Sugahara</a>, 
<a href="/search/cs?searchtype=author&query=Okamoto%2C+K">Kazushi Okamoto</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The paper is under consideration at Pattern Recognition Letters,
  Elsevier, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14024" title="Abstract">arXiv:2311.14024</a> (replaced) [<a href="/pdf/2311.14024" title="Download PDF">pdf</a>, <a href="/format/2311.14024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness  Measures for Cloud Detection in MSI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pirinen%2C+A">Aleksis Pirinen</a>, 
<a href="/search/cs?searchtype=author&query=Abid%2C+N">Nosheen Abid</a>, 
<a href="/search/cs?searchtype=author&query=Paszkowsky%2C+N+A">Nuria Agues Paszkowsky</a>, 
<a href="/search/cs?searchtype=author&query=Timoudas%2C+T+O">Thomas Ohlson Timoudas</a>, 
<a href="/search/cs?searchtype=author&query=Scheirer%2C+R">Ronald Scheirer</a>, 
<a href="/search/cs?searchtype=author&query=Ceccobello%2C+C">Chiara Ceccobello</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+G">Gy&#xf6;rgy Kov&#xe1;cs</a>, 
<a href="/search/cs?searchtype=author&query=Persson%2C+A">Anders Persson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the journal Remote Sensing (2024). Code, data and models available at <a href="https://github.com/aleksispi/ml-cloud-opt-thick">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00658" title="Abstract">arXiv:2312.00658</a> (replaced) [<a href="/pdf/2312.00658" title="Download PDF">pdf</a>, <a href="/ps/2312.00658" title="Download PostScript">ps</a>, <a href="/format/2312.00658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Safety Preserving Control Architecture for Constrained  Cyber-Physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Attar%2C+M">Mehran Attar</a>, 
<a href="/search/eess?searchtype=author&query=Lucia%2C+W">Walter Lucia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to the International Journal of Robust and Nonlinear Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02119" title="Abstract">arXiv:2312.02119</a> (replaced) [<a href="/pdf/2312.02119" title="Download PDF">pdf</a>, <a href="/format/2312.02119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+A">Anay Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Zampetakis%2C+M">Manolis Zampetakis</a>, 
<a href="/search/cs?searchtype=author&query=Kassianik%2C+P">Paul Kassianik</a>, 
<a href="/search/cs?searchtype=author&query=Nelson%2C+B">Blaine Nelson</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+H">Hyrum Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+Y">Yaron Singer</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An implementation of the presented method is available at <a href="https://github.com/RICommunity/TAP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02596" title="Abstract">arXiv:2312.02596</a> (replaced) [<a href="/pdf/2312.02596" title="Download PDF">pdf</a>, <a href="/format/2312.02596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSTSVR-PI: Least square twin support vector regression with privileged  information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumari%2C+A">Anuradha Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+M">M. Tanveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03871" title="Abstract">arXiv:2312.03871</a> (replaced) [<a href="/pdf/2312.03871" title="Download PDF">pdf</a>, <a href="/format/2312.03871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden yet quantifiable: A lower bound for confounding strength using  randomized trials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=De+Bartolomeis%2C+P">Piersilvio De Bartolomeis</a>, 
<a href="/search/stat?searchtype=author&query=Abad%2C+J">Javier Abad</a>, 
<a href="/search/stat?searchtype=author&query=Donhauser%2C+K">Konstantin Donhauser</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+F">Fanny Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04404" title="Abstract">arXiv:2312.04404</a> (replaced) [<a href="/pdf/2312.04404" title="Download PDF">pdf</a>, <a href="/format/2312.04404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Impact of Multi-dimensional Local Differential Privacy on  Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makhlouf%2C+K">Karima Makhlouf</a>, 
<a href="/search/cs?searchtype=author&query=Arcolezi%2C+H+H">Heber H. Arcolezi</a>, 
<a href="/search/cs?searchtype=author&query=Zhioua%2C+S">Sami Zhioua</a>, 
<a href="/search/cs?searchtype=author&query=Brahim%2C+G+B">Ghassen Ben Brahim</a>, 
<a href="/search/cs?searchtype=author&query=Palamidessi%2C+C">Catuscia Palamidessi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04690" title="Abstract">arXiv:2312.04690</a> (replaced) [<a href="/pdf/2312.04690" title="Download PDF">pdf</a>, <a href="/format/2312.04690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and  Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brade%2C+S">Stephen Brade</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bryan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sousa%2C+M">Mauricio Sousa</a>, 
<a href="/search/cs?searchtype=author&query=Newsome%2C+G+L">Gregory Lee Newsome</a>, 
<a href="/search/cs?searchtype=author&query=Oore%2C+S">Sageev Oore</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+T">Tovi Grossman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04792" title="Abstract">arXiv:2312.04792</a> (replaced) [<a href="/pdf/2312.04792" title="Download PDF">pdf</a>, <a href="/format/2312.04792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playing Large Games with Oracles and AI Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+D">Dean Foster</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06125" title="Abstract">arXiv:2312.06125</a> (replaced) [<a href="/pdf/2312.06125" title="Download PDF">pdf</a>, <a href="/format/2312.06125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Evolved Model for Complex Multi-objective Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Haokai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Min Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06892" title="Abstract">arXiv:2312.06892</a> (replaced) [<a href="/pdf/2312.06892" title="Download PDF">pdf</a>, <a href="/format/2312.06892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VitalLens: Take A Vital Selfie
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouast%2C+P+V">Philipp V. Rouast</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, accompanying iOS app available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07250" title="Abstract">arXiv:2312.07250</a> (replaced) [<a href="/pdf/2312.07250" title="Download PDF">pdf</a>, <a href="/format/2312.07250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Machine Translation of Clinical Text: An Empirical Investigation  into Multilingual Pre-Trained Language Models and Transfer-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Gladkoff%2C+S">Serge Gladkoff</a>, 
<a href="/search/cs?searchtype=author&query=Erofeev%2C+G">Gleb Erofeev</a>, 
<a href="/search/cs?searchtype=author&query=Sorokina%2C+I">Irina Sorokina</a>, 
<a href="/search/cs?searchtype=author&query=Galiano%2C+B">Betty Galiano</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Frontiers in Digital Health - Health Informatics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07266" title="Abstract">arXiv:2312.07266</a> (replaced) [<a href="/pdf/2312.07266" title="Download PDF">pdf</a>, <a href="/format/2312.07266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for  Open-Vocabulary Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Joonhyun Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Geondo Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jayeon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyungsik Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heesu Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI24. Code: <a href="https://github.com/clovaai/ProxyDet">this https URL</a> Project page: <a href="https://proxydet.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08821" title="Abstract">arXiv:2312.08821</a> (replaced) [<a href="/pdf/2312.08821" title="Download PDF">pdf</a>, <a href="/format/2312.08821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction of Sound Field through Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Miotello%2C+F">Federico Miotello</a>, 
<a href="/search/eess?searchtype=author&query=Comanducci%2C+L">Luca Comanducci</a>, 
<a href="/search/eess?searchtype=author&query=Pezzoli%2C+M">Mirco Pezzoli</a>, 
<a href="/search/eess?searchtype=author&query=Bernardini%2C+A">Alberto Bernardini</a>, 
<a href="/search/eess?searchtype=author&query=Antonacci%2C+F">Fabio Antonacci</a>, 
<a href="/search/eess?searchtype=author&query=Sarti%2C+A">Augusto Sarti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10107" title="Abstract">arXiv:2312.10107</a> (replaced) [<a href="/pdf/2312.10107" title="Download PDF">pdf</a>, <a href="/format/2312.10107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Context-Aware Domain Generalization: Understanding the Benefits  and Limits of Marginal Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J">Jens M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChmichel%2C+L">Lars K&#xfc;hmichel</a>, 
<a href="/search/cs?searchtype=author&query=Rohbeck%2C+M">Martin Rohbeck</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+S+T">Stefan T. Radev</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6the%2C+U">Ullrich K&#xf6;the</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11113" title="Abstract">arXiv:2312.11113</a> (replaced) [<a href="/pdf/2312.11113" title="Download PDF">pdf</a>, <a href="/format/2312.11113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interleaving Distance for Ordered Merge Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beurskens%2C+T">Thijs Beurskens</a>, 
<a href="/search/cs?searchtype=author&query=Ophelders%2C+T">Tim Ophelders</a>, 
<a href="/search/cs?searchtype=author&query=Speckmann%2C+B">Bettina Speckmann</a>, 
<a href="/search/cs?searchtype=author&query=Verbeek%2C+K">Kevin Verbeek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11529" title="Abstract">arXiv:2312.11529</a> (replaced) [<a href="/pdf/2312.11529" title="Download PDF">pdf</a>, <a href="/format/2312.11529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Scalable Graph Generation through Iterative Local  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bergmeister%2C+A">Andreas Bergmeister</a>, 
<a href="/search/cs?searchtype=author&query=Martinkus%2C+K">Karolis Martinkus</a>, 
<a href="/search/cs?searchtype=author&query=Perraudin%2C+N">Nathana&#xeb;l Perraudin</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12839" title="Abstract">arXiv:2312.12839</a> (replaced) [<a href="/pdf/2312.12839" title="Download PDF">pdf</a>, <a href="/format/2312.12839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Machine Learning Algorithms by Union-Free Generic Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blocher%2C+H">Hannah Blocher</a>, 
<a href="/search/cs?searchtype=author&query=Schollmeyer%2C+G">Georg Schollmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Nalenz%2C+M">Malte Nalenz</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+C">Christoph Jansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.09872">arXiv:2304.09872</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14063" title="Abstract">arXiv:2312.14063</a> (replaced) [<a href="/pdf/2312.14063" title="Download PDF">pdf</a>, <a href="/format/2312.14063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Time Convergence of the Iterative Evaluation of Datalogo  Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Im%2C+S">Sungjin Im</a>, 
<a href="/search/cs?searchtype=author&query=Moseley%2C+B">Benjamin Moseley</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+Q">Hung Q. Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Pruhs%2C+K">Kirk Pruhs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14647" title="Abstract">arXiv:2312.14647</a> (replaced) [<a href="/pdf/2312.14647" title="Download PDF">pdf</a>, <a href="/format/2312.14647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Message Brokers for Generative AI: Survey, Challenges, and  Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saleh%2C+A">Alaa Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Morabito%2C+R">Roberto Morabito</a>, 
<a href="/search/cs?searchtype=author&query=Tarkoma%2C+S">Sasu Tarkoma</a>, 
<a href="/search/cs?searchtype=author&query=Pirttikangas%2C+S">Susanna Pirttikangas</a>, 
<a href="/search/cs?searchtype=author&query=Lov%C3%A9n%2C+L">Lauri Lov&#xe9;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 181 references, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15474" title="Abstract">arXiv:2312.15474</a> (replaced) [<a href="/pdf/2312.15474" title="Download PDF">pdf</a>, <a href="/format/2312.15474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Conservative Approach for Few-Shot Transfer in Off-Dynamics  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daoudi%2C+P">Paul Daoudi</a>, 
<a href="/search/cs?searchtype=author&query=Prieur%2C+C">Christophe Prieur</a>, 
<a href="/search/cs?searchtype=author&query=Robu%2C+B">Bogdan Robu</a>, 
<a href="/search/cs?searchtype=author&query=Barlier%2C+M">Merwan Barlier</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+L+D">Ludovic Dos Santos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01512" title="Abstract">arXiv:2401.01512</a> (replaced) [<a href="/pdf/2401.01512" title="Download PDF">pdf</a>, <a href="/format/2401.01512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Syntactic Capabilities Are Statistically Learned by Masked  Language Models for Code?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velasco%2C+A">Alejandro Velasco</a>, 
<a href="/search/cs?searchtype=author&query=Palacio%2C+D+N">David N. Palacio</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez-Cardenas%2C+D">Daniel Rodriguez-Cardenas</a>, 
<a href="/search/cs?searchtype=author&query=Poshyvanyk%2C+D">Denys Poshyvanyk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02686" title="Abstract">arXiv:2401.02686</a> (replaced) [<a href="/pdf/2401.02686" title="Download PDF">pdf</a>, <a href="/format/2401.02686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Fidelity: Explaining Vulnerability Localization of Learning-based  Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Baijun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shengming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meizhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+G">Guangdong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+R">Ruitao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Tosem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02737" title="Abstract">arXiv:2401.02737</a> (replaced) [<a href="/pdf/2401.02737" title="Download PDF">pdf</a>, <a href="/format/2401.02737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Vulnerability Is in the Details: Locating Fine-grained Information  of Vulnerable Code Identified by Graph-based Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Baijun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiapu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yulei Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangqun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03149" title="Abstract">arXiv:2401.03149</a> (replaced) [<a href="/pdf/2401.03149" title="Download PDF">pdf</a>, <a href="/format/2401.03149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CaMML: Context-Aware Multimodal Learner for Large Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Boran Han</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03424" title="Abstract">arXiv:2401.03424</a> (replaced) [<a href="/pdf/2401.03424" title="Download PDF">pdf</a>, <a href="/format/2401.03424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pengcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03473" title="Abstract">arXiv:2401.03473</a> (replaced) [<a href="/pdf/2401.03473" title="Download PDF">pdf</a>, <a href="/ps/2401.03473" title="Download PostScript">ps</a>, <a href="/format/2401.03473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech  Recognition Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pengcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Ao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiayao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+H">Hui Bu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Binbin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longbiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+E+S">Eng Siong Chng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03555" title="Abstract">arXiv:2401.03555</a> (replaced) [<a href="/pdf/2401.03555" title="Download PDF">pdf</a>, <a href="/ps/2401.03555" title="Download PostScript">ps</a>, <a href="/format/2401.03555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMPaCT: Interval MDP Parallel Construction for Controller Synthesis of  Large-Scale Stochastic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wooding%2C+B">Ben Wooding</a>, 
<a href="/search/eess?searchtype=author&query=Lavaei%2C+A">Abolfazl Lavaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04829" title="Abstract">arXiv:2401.04829</a> (replaced) [<a href="/pdf/2401.04829" title="Download PDF">pdf</a>, <a href="/ps/2401.04829" title="Download PostScript">ps</a>, <a href="/format/2401.04829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNShap: Fast and Accurate GNN Explanations using Shapley Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akkas%2C+S">Selahattin Akkas</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+A">Ariful Azad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04898" title="Abstract">arXiv:2401.04898</a> (replaced) [<a href="/pdf/2401.04898" title="Download PDF">pdf</a>, <a href="/format/2401.04898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language  Models In Chinese Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bingchao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05930" title="Abstract">arXiv:2401.05930</a> (replaced) [<a href="/pdf/2401.05930" title="Download PDF">pdf</a>, <a href="/format/2401.05930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kai%2C+J">Jushi Kai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06233" title="Abstract">arXiv:2401.06233</a> (replaced) [<a href="/pdf/2401.06233" title="Download PDF">pdf</a>, <a href="/format/2401.06233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEGOBench: Scientific Leaderboard Generation Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shruti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shoaib Alam</a>, 
<a href="/search/cs?searchtype=author&query=Malwat%2C+H">Husain Malwat</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06255" title="Abstract">arXiv:2401.06255</a> (replaced) [<a href="/pdf/2401.06255" title="Download PDF">pdf</a>, <a href="/format/2401.06255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling and Predicting Online Vaccination Views using Bow-tie  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yueting Han</a>, 
<a href="/search/cs?searchtype=author&query=Bazzi%2C+M">Marya Bazzi</a>, 
<a href="/search/cs?searchtype=author&query=Turrini%2C+P">Paolo Turrini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SM update
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Royal Society Open Science, 11, 231792 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06432" title="Abstract">arXiv:2401.06432</a> (replaced) [<a href="/pdf/2401.06432" title="Download PDF">pdf</a>, <a href="/format/2401.06432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y+J">Yae Jee Cho</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Luyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fahrezi%2C+A">Aldi Fahrezi</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06561" title="Abstract">arXiv:2401.06561</a> (replaced) [<a href="/pdf/2401.06561" title="Download PDF">pdf</a>, <a href="/format/2401.06561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intention Analysis Makes LLMs A Good Jailbreak Defender
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06628" title="Abstract">arXiv:2401.06628</a> (replaced) [<a href="/pdf/2401.06628" title="Download PDF">pdf</a>, <a href="/format/2401.06628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OOP: Object-Oriented Programming Evaluation Benchmark for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06836" title="Abstract">arXiv:2401.06836</a> (replaced) [<a href="/pdf/2401.06836" title="Download PDF">pdf</a>, <a href="/format/2401.06836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Emotional Generation Capability of Large Language Models via  Emotional Chain-of-Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zaijing Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gongwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Rui Shao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dongmei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06954" title="Abstract">arXiv:2401.06954</a> (replaced) [<a href="/pdf/2401.06954" title="Download PDF">pdf</a>, <a href="/format/2401.06954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Preference Gap between Retrievers and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+Z">Zixuan Ke</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weize Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07995" title="Abstract">arXiv:2401.07995</a> (replaced) [<a href="/pdf/2401.07995" title="Download PDF">pdf</a>, <a href="/format/2401.07995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Pulse of Fileless Cryptojacking Attacks: Malicious PowerShell  Scripts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varlioglu%2C+S">Said Varlioglu</a>, 
<a href="/search/cs?searchtype=author&query=Elsayed%2C+N">Nelly Elsayed</a>, 
<a href="/search/cs?searchtype=author&query=Varlioglu%2C+E+R">Eva Ruhsar Varlioglu</a>, 
<a href="/search/cs?searchtype=author&query=Ozer%2C+M">Murat Ozer</a>, 
<a href="/search/cs?searchtype=author&query=ElSayed%2C+Z">Zag ElSayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08019" title="Abstract">arXiv:2401.08019</a> (replaced) [<a href="/pdf/2401.08019" title="Download PDF">pdf</a>, <a href="/ps/2401.08019" title="Download PostScript">ps</a>, <a href="/format/2401.08019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A polynomial algorithm for the most degree-central shortest path problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phosavanh%2C+J">Johnson Phosavanh</a>, 
<a href="/search/cs?searchtype=author&query=Matsypura%2C+D">Dmytro Matsypura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08865" title="Abstract">arXiv:2401.08865</a> (replaced) [<a href="/pdf/2401.08865" title="Download PDF">pdf</a>, <a href="/format/2401.08865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effect of Intrinsic Dataset Properties on Generalization: Unraveling  Learning Differences Between Natural and Medical Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Code: <a href="https://github.com/mazurowski-lab/intrinsic-properties">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12264" title="Abstract">arXiv:2401.12264</a> (replaced) [<a href="/pdf/2401.12264" title="Download PDF">pdf</a>, <a href="/format/2401.12264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model  for Multimodal Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yue%2C+X">Xianghu Yue</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+X">Xiaohai Tian</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Multimedia (cs.MM); Sound (cs.SD); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13170" title="Abstract">arXiv:2401.13170</a> (replaced) [<a href="/e-print/2401.13170" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert  Judgments For Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+I">Ishani Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yijun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Nghiem%2C+H">Huy Nghiem</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Another paper with almost duplicate and identical contents already exist. We do not want to keep duplicate copies. See <a href="/abs/2402.11161">arXiv:2402.11161</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16330" title="Abstract">arXiv:2401.16330</a> (replaced) [<a href="/pdf/2401.16330" title="Download PDF">pdf</a>, <a href="/ps/2401.16330" title="Download PostScript">ps</a>, <a href="/format/2401.16330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital requirements engineering with an INCOSE-derived SysML meta-model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wheaton%2C+J+S">James S. Wheaton</a>, 
<a href="/search/eess?searchtype=author&query=Herber%2C+D+R">Daniel R. Herber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages; 4 figures; 2 tables; to appear in Conference on Systems Engineering Research (CSER) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16468" title="Abstract">arXiv:2401.16468</a> (replaced) [<a href="/pdf/2401.16468" title="Download PDF">pdf</a>, <a href="/format/2401.16468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructIR: High-Quality Image Restoration Following Human Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conde%2C+M+V">Marcos V. Conde</a>, 
<a href="/search/cs?searchtype=author&query=Geigle%2C+G">Gregor Geigle</a>, 
<a href="/search/cs?searchtype=author&query=Timofte%2C+R">Radu Timofte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16708" title="Abstract">arXiv:2401.16708</a> (replaced) [<a href="/pdf/2401.16708" title="Download PDF">pdf</a>, <a href="/format/2401.16708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible  Cluster Shapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Yung-Peng Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hung-Hsuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17499" title="Abstract">arXiv:2401.17499</a> (replaced) [<a href="/pdf/2401.17499" title="Download PDF">pdf</a>, <a href="/format/2401.17499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvGPS: Adversarial GPS for Multi-Agent Perception Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jianwu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Juefei-Xu%2C+F">Felix Juefei-Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17580" title="Abstract">arXiv:2401.17580</a> (replaced) [<a href="/pdf/2401.17580" title="Download PDF">pdf</a>, <a href="/format/2401.17580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Contrastive Learning with Cohesive Subgraph Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yucheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Leye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Han-Jia Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17957" title="Abstract">arXiv:2401.17957</a> (replaced) [<a href="/pdf/2401.17957" title="Download PDF">pdf</a>, <a href="/ps/2401.17957" title="Download PostScript">ps</a>, <a href="/format/2401.17957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding breakdown in incomplete factorizations in low precision  arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scott%2C+J">Jennifer Scott</a>, 
<a href="/search/math?searchtype=author&query=T%C5%AFma%2C+M">Miroslav T&#x16f;ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00331" title="Abstract">arXiv:2402.00331</a> (replaced) [<a href="/pdf/2402.00331" title="Download PDF">pdf</a>, <a href="/ps/2402.00331" title="Download PostScript">ps</a>, <a href="/format/2402.00331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth and Proper Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anel%2C+M">Mathieu Anel</a>, 
<a href="/search/math?searchtype=author&query=Weinberger%2C+J">Jonathan Weinberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dedicated to Andr\'e Joyal to his 80th birthday; 13 pages, 4 tables. v2 simplified Table 3 and corrected the characterization of acyclic/localic maps in the corresponding examples. v3 add more examples
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO); Algebraic Geometry (math.AG); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00550" title="Abstract">arXiv:2402.00550</a> (replaced) [<a href="/pdf/2402.00550" title="Download PDF">pdf</a>, <a href="/format/2402.00550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical simulation of endovascular treatment options for cerebral  aneurysms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Frank%2C+M">Martin Frank</a>, 
<a href="/search/math?searchtype=author&query=Holzberger%2C+F">Fabian Holzberger</a>, 
<a href="/search/math?searchtype=author&query=Horvat%2C+M">Medeea Horvat</a>, 
<a href="/search/math?searchtype=author&query=Kirschke%2C+J">Jan Kirschke</a>, 
<a href="/search/math?searchtype=author&query=Mayr%2C+M">Matthias Mayr</a>, 
<a href="/search/math?searchtype=author&query=Muhr%2C+M">Markus Muhr</a>, 
<a href="/search/math?searchtype=author&query=Nebulishvili%2C+N">Natalia Nebulishvili</a>, 
<a href="/search/math?searchtype=author&query=Popp%2C+A">Alexander Popp</a>, 
<a href="/search/math?searchtype=author&query=Schwarting%2C+J">Julian Schwarting</a>, 
<a href="/search/math?searchtype=author&query=Wohlmuth%2C+B">Barbara Wohlmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02695" title="Abstract">arXiv:2402.02695</a> (replaced) [<a href="/pdf/2402.02695" title="Download PDF">pdf</a>, <a href="/format/2402.02695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Class Probabilities for Black-box Sentence-level Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03445" title="Abstract">arXiv:2402.03445</a> (replaced) [<a href="/pdf/2402.03445" title="Download PDF">pdf</a>, <a href="/format/2402.03445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion via Image-Based Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anciukevi%C4%8Dius%2C+T">Titas Anciukevi&#x10d;ius</a>, 
<a href="/search/cs?searchtype=author&query=Manhardt%2C+F">Fabian Manhardt</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Paul Henderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024. Project page: <a href="https://anciukevicius.github.io/generative-image-based-rendering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03762" title="Abstract">arXiv:2402.03762</a> (replaced) [<a href="/pdf/2402.03762" title="Download PDF">pdf</a>, <a href="/format/2402.03762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhetao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lechen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingrui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Compared to the previous version, we have fixed some problems in the encoding and the result has been improved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03781" title="Abstract">arXiv:2402.03781</a> (replaced) [<a href="/pdf/2402.03781" title="Download PDF">pdf</a>, <a href="/format/2402.03781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MolTC: Towards Molecular Relational Modeling In Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fang%2C+J">Junfeng Fang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+C">Chang Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Du%2C+W">Wenjie Du</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+X">Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03807" title="Abstract">arXiv:2402.03807</a> (replaced) [<a href="/pdf/2402.03807" title="Download PDF">pdf</a>, <a href="/format/2402.03807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEABO: A Simple Search-Based Method for Offline Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiafei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoteng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Le Wan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Runze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04032" title="Abstract">arXiv:2402.04032</a> (replaced) [<a href="/pdf/2402.04032" title="Download PDF">pdf</a>, <a href="/format/2402.04032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEAM : Hashed Embedding Acceleration using Processing-In-Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngsuk Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyuk-Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rhee%2C+C+E">Chae Eun Rhee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04273" title="Abstract">arXiv:2402.04273</a> (replaced) [<a href="/pdf/2402.04273" title="Download PDF">pdf</a>, <a href="/format/2402.04273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception  from Independent Private Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runsheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04536" title="Abstract">arXiv:2402.04536</a> (replaced) [<a href="/pdf/2402.04536" title="Download PDF">pdf</a>, <a href="/format/2402.04536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile-based Object Retrieval From Granular Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingxi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yinsen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+P">Patrick Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinyue Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zihan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>, 
<a href="/search/cs?searchtype=author&query=Ciocarlie%2C+M">Matei Ciocarlie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04863" title="Abstract">arXiv:2402.04863</a> (replaced) [<a href="/pdf/2402.04863" title="Download PDF">pdf</a>, <a href="/format/2402.04863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Smart Contract Summarization via LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yingjie Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenkai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05650" title="Abstract">arXiv:2402.05650</a> (replaced) [<a href="/pdf/2402.05650" title="Download PDF">pdf</a>, <a href="/format/2402.05650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation  of LLM-Supported SE Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+H">Huilong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by FSE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05804" title="Abstract">arXiv:2402.05804</a> (replaced) [<a href="/pdf/2402.05804" title="Download PDF">pdf</a>, <a href="/format/2402.05804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InkSight: Offline-to-Online Handwriting Conversion by Learning to Read  and Write
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitrevski%2C+B">Blagoj Mitrevski</a>, 
<a href="/search/cs?searchtype=author&query=Rak%2C+A">Arina Rak</a>, 
<a href="/search/cs?searchtype=author&query=Schnitzler%2C+J">Julian Schnitzler</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengkun Li</a>, 
<a href="/search/cs?searchtype=author&query=Maksai%2C+A">Andrii Maksai</a>, 
<a href="/search/cs?searchtype=author&query=Berent%2C+J">Jesse Berent</a>, 
<a href="/search/cs?searchtype=author&query=Musat%2C+C">Claudiu Musat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06147" title="Abstract">arXiv:2402.06147</a> (replaced) [<a href="/pdf/2402.06147" title="Download PDF">pdf</a>, <a href="/format/2402.06147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeAL: Decoding-time Alignment for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J+Y">James Y. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Sailik Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Bonadiman%2C+D">Daniele Bonadiman</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yi-an Lai</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arshit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+S">Saab Mansour</a>, 
<a href="/search/cs?searchtype=author&query=Kirchhoff%2C+K">Katrin Kirchhoff</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The appendix contains data that is offensive / disturbing in nature
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06919" title="Abstract">arXiv:2402.06919</a> (replaced) [<a href="/pdf/2402.06919" title="Download PDF">pdf</a>, <a href="/format/2402.06919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TREET: TRansfer Entropy Estimation via Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luxembourg%2C+O">Omer Luxembourg</a>, 
<a href="/search/cs?searchtype=author&query=Tsur%2C+D">Dor Tsur</a>, 
<a href="/search/cs?searchtype=author&query=Permuter%2C+H">Haim Permuter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07425" title="Abstract">arXiv:2402.07425</a> (replaced) [<a href="/pdf/2402.07425" title="Download PDF">pdf</a>, <a href="/format/2402.07425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Recommendation with Personal Popularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+W">Wentao Ning</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+B">Ben Kao</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+N">Nan Huo</a>, 
<a href="/search/cs?searchtype=author&query=Haldar%2C+N+A+H">Nur AI Hasan Haldar</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bo Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'24 as a research full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07588" title="Abstract">arXiv:2402.07588</a> (replaced) [<a href="/pdf/2402.07588" title="Download PDF">pdf</a>, <a href="/format/2402.07588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Scaling Laws for Learning in Strategic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Handina%2C+T">Tinashe Handina</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+E">Eric Mazumdar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07687" title="Abstract">arXiv:2402.07687</a> (replaced) [<a href="/pdf/2402.07687" title="Download PDF">pdf</a>, <a href="/format/2402.07687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual  Reality: Robustness and User Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+E">Ethan Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Ibragimov%2C+A">Azim Ibragimov</a>, 
<a href="/search/cs?searchtype=author&query=Proulx%2C+M+J">Michael J. Proulx</a>, 
<a href="/search/cs?searchtype=author&query=Tetali%2C+S+D">Sai Deep Tetali</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+K">Kevin Butler</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+E">Eakta Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07818" title="Abstract">arXiv:2402.07818</a> (replaced) [<a href="/pdf/2402.07818" title="Download PDF">pdf</a>, <a href="/format/2402.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Z Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">J Lou</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">W Bao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Z Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">K Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08113" title="Abstract">arXiv:2402.08113</a> (replaced) [<a href="/pdf/2402.08113" title="Download PDF">pdf</a>, <a href="/format/2402.08113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing cognitive bias in medical language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidgall%2C+S">Samuel Schmidgall</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+C">Carl Harris</a>, 
<a href="/search/cs?searchtype=author&query=Essien%2C+I">Ime Essien</a>, 
<a href="/search/cs?searchtype=author&query=Olshvang%2C+D">Daniel Olshvang</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tawsifur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J+W">Ji Woong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ziaei%2C+R">Rojin Ziaei</a>, 
<a href="/search/cs?searchtype=author&query=Eshraghian%2C+J">Jason Eshraghian</a>, 
<a href="/search/cs?searchtype=author&query=Abadir%2C+P">Peter Abadir</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08382" title="Abstract">arXiv:2402.08382</a> (replaced) [<a href="/pdf/2402.08382" title="Download PDF">pdf</a>, <a href="/format/2402.08382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Punctuation Restoration Improves Structure Understanding without  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+J">Junghyun Min</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Woochul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yeonsoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08397" title="Abstract">arXiv:2402.08397</a> (replaced) [<a href="/pdf/2402.08397" title="Download PDF">pdf</a>, <a href="/format/2402.08397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural-network Enhanced Video Coding Framework beyond ECM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenxuan He</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chuanmin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junru Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chaoyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Siwei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08484" title="Abstract">arXiv:2402.08484</a> (replaced) [<a href="/pdf/2402.08484" title="Download PDF">pdf</a>, <a href="/ps/2402.08484" title="Download PostScript">ps</a>, <a href="/format/2402.08484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Computational Complexity of the Housing Market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lock%2C+E">Edwin Lock</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zephyr Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Teytelboym%2C+A">Alexander Teytelboym</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08653" title="Abstract">arXiv:2402.08653</a> (replaced) [<a href="/pdf/2402.08653" title="Download PDF">pdf</a>, <a href="/format/2402.08653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wuxinlin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chenhui Deng</a>, 
<a href="/search/cs?searchtype=author&query=Aghdaei%2C+A">Ali Aghdaei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhuo Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09056" title="Abstract">arXiv:2402.09056</a> (replaced) [<a href="/pdf/2402.09056" title="Download PDF">pdf</a>, <a href="/format/2402.09056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Epistemic Uncertainty Faithfully Represented by Evidential Deep  Learning Methods?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=J%C3%BCrgens%2C+M">Mira J&#xfc;rgens</a>, 
<a href="/search/cs?searchtype=author&query=Meinert%2C+N">Nis Meinert</a>, 
<a href="/search/cs?searchtype=author&query=Bengs%2C+V">Viktor Bengs</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>, 
<a href="/search/cs?searchtype=author&query=Waegeman%2C+W">Willem Waegeman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09384" title="Abstract">arXiv:2402.09384</a> (replaced) [<a href="/pdf/2402.09384" title="Download PDF">pdf</a>, <a href="/ps/2402.09384" title="Download PostScript">ps</a>, <a href="/format/2402.09384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persuasion, Delegation, and Private Information in Algorithm-Assisted  Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Xu%2C+R">Ruqing Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09442" title="Abstract">arXiv:2402.09442</a> (replaced) [<a href="/pdf/2402.09442" title="Download PDF">pdf</a>, <a href="/ps/2402.09442" title="Download PostScript">ps</a>, <a href="/format/2402.09442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progress in artificial intelligence applications based on the  combination of self-driven sensors and deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wan%2C+W">Weixiang Wan</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+W">Wenjian Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+Q">Qiang Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+L">Linying Pan</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jingyu Xu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+B">Bo Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This aticle was accepted by ieee conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09660" title="Abstract">arXiv:2402.09660</a> (replaced) [<a href="/pdf/2402.09660" title="Download PDF">pdf</a>, <a href="/format/2402.09660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Modeling and User Profiling: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purificato%2C+E">Erasmo Purificato</a> (1), 
<a href="/search/cs?searchtype=author&query=Boratto%2C+L">Ludovico Boratto</a> (2), 
<a href="/search/cs?searchtype=author&query=De+Luca%2C+E+W">Ernesto William De Luca</a> (1) ((1) Otto von Guericke University Magdeburg, Germany, (2) University of Cagliari, Italy)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 71 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09697" title="Abstract">arXiv:2402.09697</a> (replaced) [<a href="/pdf/2402.09697" title="Download PDF">pdf</a>, <a href="/format/2402.09697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Three-Layer Data Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Fallah%2C+A">Alireza Fallah</a>, 
<a href="/search/econ?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/econ?searchtype=author&query=Makhdoumi%2C+A">Ali Makhdoumi</a>, 
<a href="/search/econ?searchtype=author&query=Malekian%2C+A">Azarakhsh Malekian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09742" title="Abstract">arXiv:2402.09742</a> (replaced) [<a href="/pdf/2402.09742" title="Download PDF">pdf</a>, <a href="/format/2402.09742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern  Doctors for Clinical Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhihao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jialong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+J">Jun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/LibertFan/AI_Hospital">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09764" title="Abstract">arXiv:2402.09764</a> (replaced) [<a href="/pdf/2402.09764" title="Download PDF">pdf</a>, <a href="/format/2402.09764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Crowd Feedback via Distributional Preference Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dexun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kuicai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Deik%2C+D+G+X">Derrick Goh Xin Deik</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10097" title="Abstract">arXiv:2402.10097</a> (replaced) [<a href="/pdf/2402.10097" title="Download PDF">pdf</a>, <a href="/format/2402.10097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Federated Learning in Heterogeneous Wireless Networks with  Independent Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jiaxiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yanzhao Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaofeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Juncheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, accepted for publication in IEEE International Conference on Communications (ICC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10232" title="Abstract">arXiv:2402.10232</a> (replaced) [<a href="/pdf/2402.10232" title="Download PDF">pdf</a>, <a href="/ps/2402.10232" title="Download PostScript">ps</a>, <a href="/format/2402.10232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple, unified analysis of Johnson-Lindenstrauss with applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yingru Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10567" title="Abstract">arXiv:2402.10567</a> (replaced) [<a href="/pdf/2402.10567" title="Download PDF">pdf</a>, <a href="/format/2402.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs  ready for the Indian Legal Domain?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+Y">Yogesh Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Donakanti%2C+R">Raghav Donakanti</a>, 
<a href="/search/cs?searchtype=author&query=Girhepuje%2C+S">Sahil Girhepuje</a>, 
<a href="/search/cs?searchtype=author&query=Kavathekar%2C+I">Ishan Kavathekar</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+B+H">Bhaskara Hanuma Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+G+S">Gokul S Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Shreya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Anmol Goel</a>, 
<a href="/search/cs?searchtype=author&query=Ravindran%2C+B">Balaraman Ravindran</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10773" title="Abstract">arXiv:2402.10773</a> (replaced) [<a href="/pdf/2402.10773" title="Download PDF">pdf</a>, <a href="/format/2402.10773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AIM: Automated Input Set Minimization for Metamorphic Security Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaleshtari%2C+N+B">Nazanin Bayati Chaleshtari</a>, 
<a href="/search/cs?searchtype=author&query=Marquer%2C+Y">Yoann Marquer</a>, 
<a href="/search/cs?searchtype=author&query=Pastore%2C+F">Fabrizio Pastore</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L+C">Lionel C. Briand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10790" title="Abstract">arXiv:2402.10790</a> (replaced) [<a href="/pdf/2402.10790" title="Download PDF">pdf</a>, <a href="/format/2402.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs  Miss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuratov%2C+Y">Yuri Kuratov</a>, 
<a href="/search/cs?searchtype=author&query=Bulatov%2C+A">Aydar Bulatov</a>, 
<a href="/search/cs?searchtype=author&query=Anokhin%2C+P">Petr Anokhin</a>, 
<a href="/search/cs?searchtype=author&query=Sorokin%2C+D">Dmitry Sorokin</a>, 
<a href="/search/cs?searchtype=author&query=Sorokin%2C+A">Artyom Sorokin</a>, 
<a href="/search/cs?searchtype=author&query=Burtsev%2C+M">Mikhail Burtsev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11M tokens, fix qa3 min facts per task in Table 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10853" title="Abstract">arXiv:2402.10853</a> (replaced) [<a href="/pdf/2402.10853" title="Download PDF">pdf</a>, <a href="/format/2402.10853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering and exploring cases of educational source code plagiarism  with Dolos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maertens%2C+R">Rien Maertens</a>, 
<a href="/search/cs?searchtype=author&query=Van+Neyghem%2C+M">Maarten Van Neyghem</a>, 
<a href="/search/cs?searchtype=author&query=Geldhof%2C+M">Maxiem Geldhof</a>, 
<a href="/search/cs?searchtype=author&query=Van+Petegem%2C+C">Charlotte Van Petegem</a>, 
<a href="/search/cs?searchtype=author&query=Strijbol%2C+N">Niko Strijbol</a>, 
<a href="/search/cs?searchtype=author&query=Dawyndt%2C+P">Peter Dawyndt</a>, 
<a href="/search/cs?searchtype=author&query=Mesuere%2C+B">Bart Mesuere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures; minor corrections, reference added for section 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10980" title="Abstract">arXiv:2402.10980</a> (replaced) [<a href="/pdf/2402.10980" title="Download PDF">pdf</a>, <a href="/format/2402.10980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChemReasoner: Heuristic Search over a Large Language Model&#x27;s Knowledge  Space using Quantum-Chemical Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sprueill%2C+H+W">Henry W. Sprueill</a>, 
<a href="/search/physics?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/physics?searchtype=author&query=Agarwal%2C+K">Khushbu Agarwal</a>, 
<a href="/search/physics?searchtype=author&query=Olarte%2C+M+V">Mariefel V. Olarte</a>, 
<a href="/search/physics?searchtype=author&query=Sanyal%2C+U">Udishnu Sanyal</a>, 
<a href="/search/physics?searchtype=author&query=Johnston%2C+C">Conrad Johnston</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/physics?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages; Added author institutions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10991" title="Abstract">arXiv:2402.10991</a> (replaced) [<a href="/pdf/2402.10991" title="Download PDF">pdf</a>, <a href="/ps/2402.10991" title="Download PostScript">ps</a>, <a href="/format/2402.10991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Semi-Asynchronous Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fanghao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jize Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11159" title="Abstract">arXiv:2402.11159</a> (replaced) [<a href="/pdf/2402.11159" title="Download PDF">pdf</a>, <a href="/format/2402.11159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding News Thumbnail Representativeness by Counterfactual  Text-Guided Contrastive Language-Image Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+Y">Yejun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Seunghyun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kunwoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11168" title="Abstract">arXiv:2402.11168</a> (replaced) [<a href="/pdf/2402.11168" title="Download PDF">pdf</a>, <a href="/format/2402.11168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust Regions for Explanations via Black-Box Probabilistic Certification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhurandhar%2C+A">Amit Dhurandhar</a>, 
<a href="/search/cs?searchtype=author&query=Haldar%2C+S">Swagatam Haldar</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Dennis Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11205" title="Abstract">arXiv:2402.11205</a> (replaced) [<a href="/pdf/2402.11205" title="Download PDF">pdf</a>, <a href="/format/2402.11205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Quantum Circuit for Block Encoding a Pairing Hamiltonian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nucl-th?searchtype=author&query=Liu%2C+D">Diyi Liu</a>, 
<a href="/search/nucl-th?searchtype=author&query=Du%2C+W">Weijie Du</a>, 
<a href="/search/nucl-th?searchtype=author&query=Lin%2C+L">Lin Lin</a>, 
<a href="/search/nucl-th?searchtype=author&query=Vary%2C+J+P">James P.Vary</a>, 
<a href="/search/nucl-th?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Nuclear Theory (nucl-th)</span>; Numerical Analysis (math.NA); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11330" title="Abstract">arXiv:2402.11330</a> (replaced) [<a href="/pdf/2402.11330" title="Download PDF">pdf</a>, <a href="/format/2402.11330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffuse Sound Field Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zotter%2C+F">Franz Zotter</a>, 
<a href="/search/eess?searchtype=author&query=Riedel%2C+S">Stefan Riedel</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%B6lles%2C+L">Lukas G&#xf6;lles</a>, 
<a href="/search/eess?searchtype=author&query=Frank%2C+M">Matthias Frank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 17 figures, submitted to acta acustica, including jan/feb 2024 upgrades while awaiting the reviews
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11352" title="Abstract">arXiv:2402.11352</a> (replaced) [<a href="/pdf/2402.11352" title="Download PDF">pdf</a>, <a href="/format/2402.11352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Capacity Results for Free-Space Optical Communication Systems  Over Gamma-Gamma Atmospheric Turbulence Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+H">Himani Verma</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kamal Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11398" title="Abstract">arXiv:2402.11398</a> (replaced) [<a href="/pdf/2402.11398" title="Download PDF">pdf</a>, <a href="/format/2402.11398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics  for Domain Specialized Text Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shaochen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huaqin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+P">Peng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenxiong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sikora%2C+A">Andrea Sikora</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11451" title="Abstract">arXiv:2402.11451</a> (replaced) [<a href="/pdf/2402.11451" title="Download PDF">pdf</a>, <a href="/format/2402.11451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SciAgent: Tool-augmented Language Models for Scientific Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yubo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Junheng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruochen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+H">Hany Awadalla</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11459" title="Abstract">arXiv:2402.11459</a> (replaced) [<a href="/pdf/2402.11459" title="Download PDF">pdf</a>, <a href="/format/2402.11459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion  Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+O">Odin Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/q-bio?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S+Z">Stan.Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11582" title="Abstract">arXiv:2402.11582</a> (replaced) [<a href="/pdf/2402.11582" title="Download PDF">pdf</a>, <a href="/format/2402.11582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Publicly auditable privacy-preserving electoral rolls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Prashant Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Jhanwar%2C+M+P">Mahabir Prasad Jhanwar</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S+V">Subodh Vishnu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Subhashis Banerjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11871" title="Abstract">arXiv:2402.11871</a> (replaced) [<a href="/pdf/2402.11871" title="Download PDF">pdf</a>, <a href="/format/2402.11871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions  and Models for Planning from Raw Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Naman Shah</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+J">Jayesh Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Pulkit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11942" title="Abstract">arXiv:2402.11942</a> (replaced) [<a href="/pdf/2402.11942" title="Download PDF">pdf</a>, <a href="/format/2402.11942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effect of Leaky ReLUs on the training and generalization of  overparameterized networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yinglong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+G">Gilad Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12041" title="Abstract">arXiv:2402.12041</a> (replaced) [<a href="/pdf/2402.12041" title="Download PDF">pdf</a>, <a href="/format/2402.12041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surround-View Fisheye Optics in Computer Vision and Simulation: Survey  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jakab%2C+D">Daniel Jakab</a>, 
<a href="/search/cs?searchtype=author&query=Deegan%2C+B+M">Brian Michael Deegan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sushil Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Grua%2C+E+M">Eoin Martino Grua</a>, 
<a href="/search/cs?searchtype=author&query=Horgan%2C+J">Jonathan Horgan</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+E">Enda Ward</a>, 
<a href="/search/cs?searchtype=author&query=Van+De+Ven%2C+P">Pepijn Van De Ven</a>, 
<a href="/search/cs?searchtype=author&query=Scanlan%2C+A">Anthony Scanlan</a>, 
<a href="/search/cs?searchtype=author&query=Eising%2C+C">Ciar&#xe1;n Eising</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 19 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12062" title="Abstract">arXiv:2402.12062</a> (replaced) [<a href="/pdf/2402.12062" title="Download PDF">pdf</a>, <a href="/ps/2402.12062" title="Download PostScript">ps</a>, <a href="/format/2402.12062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Equal Protection as Algorithmic Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Bello%2C+M">Marcello Di Bello</a>, 
<a href="/search/cs?searchtype=author&query=Cangiotti%2C+N">Nicol&#xf2; Cangiotti</a>, 
<a href="/search/cs?searchtype=author&query=Loi%2C+M">Michele Loi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12088" title="Abstract">arXiv:2402.12088</a> (replaced) [<a href="/pdf/2402.12088" title="Download PDF">pdf</a>, <a href="/format/2402.12088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniqueness, stability and algorithm for an inverse wave-number-dependent  source problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+M">Mengjie Zhao</a>, 
<a href="/search/math?searchtype=author&query=Si%2C+S">Suliang Si</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+G">Guanghui Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12343" title="Abstract">arXiv:2402.12343</a> (replaced) [<a href="/pdf/2402.12343" title="Download PDF">pdf</a>, <a href="/format/2402.12343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhichen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project web page: <a href="https://zhziszz.github.io/emulated-disalignment">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12391" title="Abstract">arXiv:2402.12391</a> (replaced) [<a href="/pdf/2402.12391" title="Download PDF">pdf</a>, <a href="/format/2402.12391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Team of AI-made Scientists for Scientific Discovery from Gene  Expression Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+H">Haoyang Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Y">Yijiang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Jian%2C+J">Jinglin Jian</a>, 
<a href="/search/q-bio?searchtype=author&query=Cheng%2C+Y">Yuxuan Cheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jianrong Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+S">Shuyi Guo</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+J">Jinglei Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Mianchen Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Miantong Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures; added contact
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12399" title="Abstract">arXiv:2402.12399</a> (replaced) [<a href="/pdf/2402.12399" title="Download PDF">pdf</a>, <a href="/format/2402.12399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiyuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhaoye Fei</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12503" title="Abstract">arXiv:2402.12503</a> (replaced) [<a href="/pdf/2402.12503" title="Download PDF">pdf</a>, <a href="/format/2402.12503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PARCv2: Physics-aware Recurrent Convolutional Neural Networks for  Spatiotemporal Dynamics Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P+C+H">Phong C.H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinlun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Azarfar%2C+S">Shahab Azarfar</a>, 
<a href="/search/cs?searchtype=author&query=Seshadri%2C+P">Pradeep Seshadri</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Y+T">Yen T. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Munho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sanghun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Udaykumar%2C+H+S">H.S. Udaykumar</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S">Stephen Baek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12636" title="Abstract">arXiv:2402.12636</a> (replaced) [<a href="/pdf/2402.12636" title="Download PDF">pdf</a>, <a href="/format/2402.12636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gaoxiang Cong</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuankai Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Beheshti%2C+A">Amin Beheshti</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhedong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Hengel%2C+A">Anton van den Hengel</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenggang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12688" title="Abstract">arXiv:2402.12688</a> (replaced) [<a href="/pdf/2402.12688" title="Download PDF">pdf</a>, <a href="/format/2402.12688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust-Wide: Robust Watermarking against Instruction-driven Image  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Runyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Ting Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12692" title="Abstract">arXiv:2402.12692</a> (replaced) [<a href="/pdf/2402.12692" title="Download PDF">pdf</a>, <a href="/format/2402.12692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormulaQA: A Question Answering Dataset for Formula-Based Numerical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sichen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bolin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Gong Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12832" title="Abstract">arXiv:2402.12832</a> (replaced) [<a href="/pdf/2402.12832" title="Download PDF">pdf</a>, <a href="/ps/2402.12832" title="Download PostScript">ps</a>, <a href="/format/2402.12832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Optimal Fault Tolerant Distance Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+D">Dipan Dey</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Manoj Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in STOC, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12974" title="Abstract">arXiv:2402.12974</a> (replaced) [<a href="/pdf/2402.12974" title="Download PDF">pdf</a>, <a href="/format/2402.12974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Style Prompting with Swapping Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jaeseok Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yunjey Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gayoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12994" title="Abstract">arXiv:2402.12994</a> (replaced) [<a href="/pdf/2402.12994" title="Download PDF">pdf</a>, <a href="/format/2402.12994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Graph-based Recommendation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bohao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qihao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13001" title="Abstract">arXiv:2402.13001</a> (replaced) [<a href="/pdf/2402.13001" title="Download PDF">pdf</a>, <a href="/ps/2402.13001" title="Download PostScript">ps</a>, <a href="/format/2402.13001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unifying primary framework for quantum graph neural networks from  quantum graph states
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Daskin%2C+A">Ammar Daskin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> short version 6 pages, a few important typos are corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13043" title="Abstract">arXiv:2402.13043</a> (replaced) [<a href="/pdf/2402.13043" title="Download PDF">pdf</a>, <a href="/format/2402.13043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Efficient Conversation Retrieval for Dialogue State  Tracking with Implicit Text Summaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jianpeng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Driesen%2C+J">Joris Driesen</a>, 
<a href="/search/cs?searchtype=author&query=Coca%2C+A">Alexandru Coca</a>, 
<a href="/search/cs?searchtype=author&query=Johannsen%2C+A">Anders Johannsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13092" title="Abstract">arXiv:2402.13092</a> (replaced) [<a href="/pdf/2402.13092" title="Download PDF">pdf</a>, <a href="/format/2402.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contractivity of neural ODEs: an eigenvalue optimization problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guglielmi%2C+N">Nicola Guglielmi</a>, 
<a href="/search/math?searchtype=author&query=De+Marinis%2C+A">Arturo De Marinis</a>, 
<a href="/search/math?searchtype=author&query=Savostianov%2C+A">Anton Savostianov</a>, 
<a href="/search/math?searchtype=author&query=Tudisco%2C+F">Francesco Tudisco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13104" title="Abstract">arXiv:2402.13104</a> (replaced) [<a href="/pdf/2402.13104" title="Download PDF">pdf</a>, <a href="/format/2402.13104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Perception Versus Objective Driving Behavior: Subject Study of  Lateral Vehicle Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haselberger%2C+J">Johann Haselberger</a>, 
<a href="/search/eess?searchtype=author&query=Schick%2C+B">Bernhard Schick</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+S">Steffen M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13145" title="Abstract">arXiv:2402.13145</a> (replaced) [<a href="/pdf/2402.13145" title="Download PDF">pdf</a>, <a href="/format/2402.13145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for  Boosting Metaphor Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yujie Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xinrong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13156" title="Abstract">arXiv:2402.13156</a> (replaced) [<a href="/pdf/2402.13156" title="Download PDF">pdf</a>, <a href="/ps/2402.13156" title="Download PostScript">ps</a>, <a href="/format/2402.13156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret-Minimizing Contracts: Agency Under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+M">Martino Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Marchesi%2C+A">Alberto Marchesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13184" title="Abstract">arXiv:2402.13184</a> (replaced) [<a href="/pdf/2402.13184" title="Download PDF">pdf</a>, <a href="/format/2402.13184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What if LLMs Have Different World Views: Simulating Alien Civilizations  with LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhaoqian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kai Mei</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13221" title="Abstract">arXiv:2402.13221</a> (replaced) [<a href="/pdf/2402.13221" title="Download PDF">pdf</a>, <a href="/format/2402.13221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset  for Advancing Graph Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friis-Jensen%2C+U">Ulrik Friis-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Johansen%2C+F+L">Frederik L. Johansen</a>, 
<a href="/search/cs?searchtype=author&query=Anker%2C+A+S">Andy S. Anker</a>, 
<a href="/search/cs?searchtype=author&query=Dam%2C+E+B">Erik B. Dam</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+K+M+%C3%98">Kirsten M. &#xd8;. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Selvan%2C+R">Raghavendra Selvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 15 figures, 8 tables. Dataset is available at <a href="https://github.com/UlrikFriisJensen/CHILI">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13224" title="Abstract">arXiv:2402.13224</a> (replaced) [<a href="/pdf/2402.13224" title="Download PDF">pdf</a>, <a href="/format/2402.13224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Large Electric Vehicle Charging Stations via User Behavior  Modeling and Stochastic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Puech%2C+A">Alban Puech</a>, 
<a href="/search/math?searchtype=author&query=Rigaut%2C+T">Tristan Rigaut</a>, 
<a href="/search/math?searchtype=author&query=Templier%2C+W">William Templier</a>, 
<a href="/search/math?searchtype=author&query=Tournoud%2C+M">Maud Tournoud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item396">Replacements</a></li>
</ul>
<small>[ total of 692 entries:  <b>1-692</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
