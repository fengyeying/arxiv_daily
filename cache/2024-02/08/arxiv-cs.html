<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue  6 Feb 24  to  Wed  7 Feb 24, announced Thu,  8 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item419">Replacements</a></li>
</ul>
<small>[ total of 674 entries:  <b>1-674</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu,  8 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04273" title="Abstract">arXiv:2402.04273</a> [<a href="/pdf/2402.04273" title="Download PDF">pdf</a>, <a href="/format/2402.04273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception  from Independent Private Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runsheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The diverse agents in multi-agent perception systems may be from different
companies. Each company might use the identical classic neural network
architecture based encoder for feature extraction. However, the data source to
train the various agents is independent and private in each company, leading to
the Distribution Gap of different private data for training distinct agents in
multi-agent perception system. The data silos by the above Distribution Gap
could result in a significant performance decline in multi-agent perception. In
this paper, we thoroughly examine the impact of the distribution gap on
existing multi-agent perception systems. To break the data silos, we introduce
the Feature Distribution-aware Aggregation (FDA) framework for cross-domain
learning to mitigate the above Distribution Gap in multi-agent perception. FDA
comprises two key components: Learnable Feature Compensation Module and
Distribution-aware Statistical Consistency Module, both aimed at enhancing
intermediate features to minimize the distribution gap among multi-agent
features. Intensive experiments on the public OPV2V and V2XSet datasets
underscore FDA's effectiveness in point cloud-based 3D object detection,
presenting it as an invaluable augmentation to existing multi-agent perception
systems.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04284" title="Abstract">arXiv:2402.04284</a> [<a href="/pdf/2402.04284" title="Download PDF">pdf</a>, <a href="/format/2402.04284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Difan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic
graph neural networks that leverage a memory module to extract, distill, and
memorize long-term temporal dependencies, leading to superior performance
compared to memory-less counterparts. However, training MDGNNs faces the
challenge of handling entangled temporal and structural dependencies, requiring
sequential and chronological processing of data sequences to capture accurate
temporal patterns. During the batch training, the temporal data points within
the same batch will be processed in parallel, while their temporal dependencies
are neglected. This issue is referred to as temporal discontinuity and
restricts the effective temporal batch size, limiting data parallelism and
reducing MDGNNs' flexibility in industrial applications. This paper studies the
efficient training of MDGNNs at scale, focusing on the temporal discontinuity
in training MDGNNs with large temporal batch sizes. We first conduct a
theoretical study on the impact of temporal batch size on the convergence of
MDGNN training. Based on the analysis, we propose PRES, an iterative
prediction-correction scheme combined with a memory coherence learning
objective to mitigate the effect of temporal discontinuity, enabling MDGNNs to
be trained with significantly larger temporal batches without sacrificing
generalization performance. Experimental results demonstrate that our approach
enables up to a 4x larger temporal batch (3.4x speed-up) during MDGNN training.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04290" title="Abstract">arXiv:2402.04290</a> [<a href="/pdf/2402.04290" title="Download PDF">pdf</a>, <a href="/format/2402.04290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded  Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Junchao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lei Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+P">Peng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanghan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Na Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jianhua Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Precipitation nowcasting based on radar data plays a crucial role in extreme
weather prediction and has broad implications for disaster management. Despite
progresses have been made based on deep learning, two key challenges of
precipitation nowcasting are not well-solved: (i) the modeling of complex
precipitation system evolutions with different scales, and (ii) accurate
forecasts for extreme precipitation. In this work, we propose CasCast, a
cascaded framework composed of a deterministic and a probabilistic part to
decouple the predictions for mesoscale precipitation distributions and
small-scale patterns. Then, we explore training the cascaded framework at the
high resolution and conducting the probabilistic modeling in a low dimensional
latent space with a frame-wise-guided diffusion transformer for enhancing the
optimization of extreme events while reducing computational costs. Extensive
experiments on three benchmark radar precipitation datasets show that CasCast
achieves competitive performance. Especially, CasCast significantly surpasses
the baseline (up to +91.8%) for regional extreme-precipitation nowcasting.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04291" title="Abstract">arXiv:2402.04291</a> [<a href="/pdf/2402.04291" title="Download PDF">pdf</a>, <a href="/format/2402.04291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yangdong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Pretrained large language models (LLMs) exhibit exceptional general language
processing capabilities but come with significant demands on memory and
computational resources. As a powerful compression technology, binarization can
extremely reduce model weights to a mere 1 bit, lowering the expensive
computation and memory requirements. However, existing quantization techniques
fall short of maintaining LLM performance under ultra-low bit-widths. In
response to this challenge, we present BiLLM, a groundbreaking 1-bit
post-training quantization scheme tailored for pretrained LLMs. Based on the
weight distribution of LLMs, BiLLM first identifies and structurally selects
salient weights, and minimizes the compression loss through an effective binary
residual approximation strategy. Moreover, considering the bell-shaped
distribution of the non-salient weights, we propose an optimal splitting search
to group and binarize them accurately. BiLLM achieving for the first time
high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit
weights across various LLMs families and evaluation metrics, outperforms SOTA
quantization methods of LLM by significant margins. Moreover, BiLLM enables the
binarization process of the LLM with 7 billion weights within 0.5 hours on a
single GPU, demonstrating satisfactory time efficiency.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04292" title="Abstract">arXiv:2402.04292</a> [<a href="/pdf/2402.04292" title="Download PDF">pdf</a>, <a href="/format/2402.04292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xixi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diffusion-based imitation learning improves Behavioral Cloning (BC) on
multi-modal decision-making, but comes at the cost of significantly slower
inference due to the recursion in the diffusion process. It urges us to design
efficient policy generators while keeping the ability to generate diverse
actions. To address this challenge, we propose AdaFlow, an imitation learning
framework based on flow-based generative modeling. AdaFlow represents the
policy with state-conditioned ordinary differential equations (ODEs), which are
known as probability flows. We reveal an intriguing connection between the
conditional variance of their training loss and the discretization error of the
ODEs. With this insight, we propose a variance-adaptive ODE solver that can
adjust its step size in the inference stage, making AdaFlow an adaptive
decision-maker, offering rapid inference without sacrificing diversity.
Interestingly, it automatically reduces to a one-step generator when the action
distribution is uni-modal. Our comprehensive empirical evaluation shows that
AdaFlow achieves high performance across all dimensions, including success
rate, behavioral diversity, and inference speed. The code is available at
https://github.com/hxixixh/AdaFlow
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04295" title="Abstract">arXiv:2402.04295</a> [<a href="/pdf/2402.04295" title="Download PDF">pdf</a>, <a href="/ps/2402.04295" title="Download PostScript">ps</a>, <a href="/format/2402.04295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructions of Abelian Codes multiplying dimension of cyclic codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernal%2C+J+J">Jos&#xe9; Joaqu&#xed;n Bernal</a>, 
<a href="/search/cs?searchtype=author&query=Bueno-Carre%C3%B1o%2C+D+H">Diana H. Bueno-Carre&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B3n%2C+J+J">Juan Jacobo Sim&#xf3;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.03938">arXiv:2402.03938</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this note, we apply some techniques developed in [1]-[3] to give a
particular construction of bivariate Abelian Codes from cyclic codes,
multiplying their dimension and preserving their apparent distance. We show
that, in the case of cyclic codes whose maximum BCH bound equals its minimum
distance the obtained abelian code verifies the same property; that is, the
strong apparent distance and the minimum distance coincide. We finally use this
construction to multiply Reed-Solomon codes to abelian codes
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04296" title="Abstract">arXiv:2402.04296</a> [<a href="/pdf/2402.04296" title="Download PDF">pdf</a>, <a href="/format/2402.04296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightHGNN: Distilling Hypergraph Neural Networks into MLPs for  $100\times$ Faster Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yifan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yihe Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+S">Shihui Ying</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hypergraph Neural Networks (HGNNs) have recently attracted much attention and
exhibited satisfactory performance due to their superiority in high-order
correlation modeling. However, it is noticed that the high-order modeling
capability of hypergraph also brings increased computation complexity, which
hinders its practical industrial deployment. In practice, we find that one key
barrier to the efficient deployment of HGNNs is the high-order structural
dependencies during inference. In this paper, we propose to bridge the gap
between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to
eliminate the hypergraph dependency of HGNNs and thus reduce computational
complexity as well as improve inference speed. Specifically, we introduce
LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN
directly distills the knowledge from teacher HGNNs to student MLPs via soft
labels, and LightHGNN$^+$ further explicitly injects reliable high-order
correlations into the student MLPs to achieve topology-aware distillation and
resistance to over-smoothing. Experiments on eight hypergraph datasets
demonstrate that even without hypergraph dependency, the proposed LightHGNNs
can still achieve competitive or even better performance than HGNNs and
outperform vanilla MLPs by $16.3$ on average. Extensive experiments on three
graph datasets further show the average best performance of our LightHGNNs
compared with all other methods. Experiments on synthetic hypergraphs with 5.5w
vertices indicate LightHGNNs can run $100\times$ faster than HGNNs, showcasing
their ability for latency-sensitive deployments.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04297" title="Abstract">arXiv:2402.04297</a> [<a href="/pdf/2402.04297" title="Download PDF">pdf</a>, <a href="/format/2402.04297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Road Surface Defect Detection -- From Image-based to Non-image-based: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jongmin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiaqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fichera%2C+S">Sebastiano Fichera</a>, 
<a href="/search/cs?searchtype=author&query=Paoletti%2C+P">Paolo Paoletti</a>, 
<a href="/search/cs?searchtype=author&query=Layzell%2C+L">Lisa Layzell</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+D">Devansh Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Survey papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ensuring traffic safety is crucial, which necessitates the detection and
prevention of road surface defects. As a result, there has been a growing
interest in the literature on the subject, leading to the development of
various road surface defect detection methods. The methods for detecting road
defects can be categorised in various ways depending on the input data types or
training methodologies. The predominant approach involves image-based methods,
which analyse pixel intensities and surface textures to identify defects.
Despite their popularity, image-based methods share the distinct limitation of
vulnerability to weather and lighting changes. To address this issue,
researchers have explored the use of additional sensors, such as laser scanners
or LiDARs, providing explicit depth information to enable the detection of
defects in terms of scale and volume. However, the exploration of data beyond
images has not been sufficiently investigated. In this survey paper, we provide
a comprehensive review of road surface defect detection studies, categorising
them based on input data types and methodologies used. Additionally, we review
recently proposed non-image-based methods and discuss several challenges and
open problems associated with these techniques.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04298" title="Abstract">arXiv:2402.04298</a> [<a href="/pdf/2402.04298" title="Download PDF">pdf</a>, <a href="/format/2402.04298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Symbolic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russeil%2C+E">Etienne Russeil</a>, 
<a href="/search/cs?searchtype=author&query=de+Fran%C3%A7a%2C+F+O">Fabr&#xed;cio Olivetti de Fran&#xe7;a</a>, 
<a href="/search/cs?searchtype=author&query=Malanchev%2C+K">Konstantin Malanchev</a>, 
<a href="/search/cs?searchtype=author&query=Burlacu%2C+B">Bogdan Burlacu</a>, 
<a href="/search/cs?searchtype=author&query=Ishida%2C+E+E+O">Emille E. O. Ishida</a>, 
<a href="/search/cs?searchtype=author&query=Leroux%2C+M">Marion Leroux</a>, 
<a href="/search/cs?searchtype=author&query=Michelin%2C+C">Cl&#xe9;ment Michelin</a>, 
<a href="/search/cs?searchtype=author&query=Moinard%2C+G">Guillaume Moinard</a>, 
<a href="/search/cs?searchtype=author&query=Gangler%2C+E">Emmanuel Gangler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to GECCO-2024. 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Applications (stat.AP)

</div>
<p class="mathjax">Symbolic regression (SR) searches for analytical expressions representing the
relationship between a set of explanatory and response variables. Current SR
methods assume a single dataset extracted from a single experiment.
Nevertheless, frequently, the researcher is confronted with multiple sets of
results obtained from experiments conducted with different setups. Traditional
SR methods may fail to find the underlying expression since the parameters of
each experiment can be different. In this work we present Multi-View Symbolic
Regression (MvSR), which takes into account multiple datasets simultaneously,
mimicking experimental environments, and outputs a general parametric solution.
This approach fits the evaluated expression to each independent dataset and
returns a parametric family of functions f(x; \theta) simultaneously capable of
accurately fitting all datasets. We demonstrate the effectiveness of MvSR using
data generated from known expressions, as well as real-world data from
astronomy, chemistry and economy, for which an a priori analytical expression
is not available. Results show that MvSR obtains the correct expression more
frequently and is robust to hyperparameters change. In real-world data, it is
able to grasp the group behaviour, recovering known expressions from the
literature as well as promising alternatives, thus enabling the use SR to a
large range of experimental scenarios.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04315" title="Abstract">arXiv:2402.04315</a> [<a href="/pdf/2402.04315" title="Download PDF">pdf</a>, <a href="/format/2402.04315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Language Models to Generate Text with Citations via  Fine-grained Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chengyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yushi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While recent Large Language Models (LLMs) have proven useful in answering
user queries, they are prone to hallucination, and their responses often lack
credibility due to missing references to reliable sources. An intuitive
solution to these issues would be to include in-text citations referring to
external documents as evidence. While previous works have directly prompted
LLMs to generate in-text citations, their performances are far from
satisfactory, especially when it comes to smaller LLMs. In this work, we
propose an effective training framework using fine-grained rewards to teach
LLMs to generate highly supportive and relevant citations, while ensuring the
correctness of their responses. We also conduct a systematic analysis of
applying these fine-grained rewards to common LLM training strategies,
demonstrating its advantage over conventional practices. We conduct extensive
experiments on Question Answering (QA) datasets taken from the ALCE benchmark
and validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the
incorporation of fine-grained rewards achieves the best performance among the
baselines, even surpassing that of GPT-3.5-turbo.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04318" title="Abstract">arXiv:2402.04318</a> [<a href="/pdf/2402.04318" title="Download PDF">pdf</a>, <a href="/format/2402.04318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Observation-Inspired Trajectory Prediction for Autonomous Driving  in Mixed-Autonomy Traffic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Haicheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shangqian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongkang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenning Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yanchen Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengzhong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the burgeoning field of autonomous vehicles (AVs), trajectory prediction
remains a formidable challenge, especially in mixed autonomy environments.
Traditional approaches often rely on computational methods such as time-series
analysis. Our research diverges significantly by adopting an interdisciplinary
approach that integrates principles of human cognition and observational
behavior into trajectory prediction models for AVs. We introduce a novel
"adaptive visual sector" mechanism that mimics the dynamic allocation of
attention human drivers exhibit based on factors like spatial orientation,
proximity, and driving speed. Additionally, we develop a "dynamic traffic
graph" using Convolutional Neural Networks (CNN) and Graph Attention Networks
(GAT) to capture spatio-temporal dependencies among agents. Benchmark tests on
the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms
state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively.
Our findings underscore the potential of leveraging human cognition principles
to enhance the proficiency and adaptability of trajectory prediction algorithms
in AVs. The code for the proposed model is available at our Github.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04319" title="Abstract">arXiv:2402.04319</a> [<a href="/pdf/2402.04319" title="Download PDF">pdf</a>, <a href="/format/2402.04319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modified de Casteljau Subdivision that Supports Smooth Stitching with  Hierarchically Organized Bicubic Bezier Patches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarrinmehr%2C+S">Saied Zarrinmehr</a>, 
<a href="/search/cs?searchtype=author&query=Akleman%2C+E">Ergun Akleman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianer Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Graphics (cs.GR); Human-Computer Interaction (cs.HC); Algebraic Geometry (math.AG); Algebraic Topology (math.AT)

</div>
<p class="mathjax">One of the theoretically intriguing problems in computer-aided geometric
modeling comes from the stitching of the tensor product Bezier patches. When
they share an extraordinary vertex, it is not possible to obtain continuity C1
or G1 along the edges emanating from that extraordinary vertex. Unfortunately,
this stitching problem cannot be solved by using higher degree or rational
polynomials. In this paper, we present a modified de Casteljau subdivision
algorithm that can provide a solution to this problem. Our modified de
Casteljau subdivision, when combined with topological modeling, provides a
framework for interactive real-time modeling of piecewise smooth manifold
meshes with arbitrary topology. The main advantage of the modified subdivision
is that the continuity C1 on a given boundary edge does not depend on the
positions of the control points on other boundary edges. The modified
subdivision allows us to obtain the desired C1 continuity along the edges
emanating from the extraordinary vertices along with the desired G1 continuity
in the extraordinary vertices.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04324" title="Abstract">arXiv:2402.04324</a> [<a href="/pdf/2402.04324" title="Download PDF">pdf</a>, <a href="/format/2402.04324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Weiming Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Harry Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Cong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinrun Du</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Stephen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://tiger-ai-lab.github.io/ConsistI2V/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image-to-video (I2V) generation aims to use the initial frame (alongside a
text prompt) to create a video sequence. A grand challenge in I2V generation is
to maintain visual consistency throughout the video: existing methods often
struggle to preserve the integrity of the subject, background, and style from
the first frame, as well as ensure a fluid and logical progression within the
video narrative. To mitigate these issues, we propose ConsistI2V, a
diffusion-based method to enhance visual consistency for I2V generation.
Specifically, we introduce (1) spatiotemporal attention over the first frame to
maintain spatial and motion consistency, (2) noise initialization from the
low-frequency band of the first frame to enhance layout consistency. These two
approaches enable ConsistI2V to generate highly consistent videos. We also
extend the proposed approaches to show their potential to improve consistency
in auto-regressive long video generation and camera motion control. To verify
the effectiveness of our method, we propose I2V-Bench, a comprehensive
evaluation benchmark for I2V generation. Our automatic and human evaluation
results demonstrate the superiority of ConsistI2V over existing methods.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04325" title="Abstract">arXiv:2402.04325</a> [<a href="/pdf/2402.04325" title="Download PDF">pdf</a>, <a href="/format/2402.04325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to  Non-Essential Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gagnon%2C+G">Garrett Gagnon</a>, 
<a href="/search/cs?searchtype=author&query=Venkataramani%2C+S">Swagath Venkataramani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Deep Neural Networks (DNNs) have revolutionized a wide range of industries,
from healthcare and finance to automotive, by offering unparalleled
capabilities in data analysis and decision-making. Despite their transforming
impact, DNNs face two critical challenges: the vulnerability to adversarial
attacks and the increasing computational costs associated with more complex and
larger models. In this paper, we introduce an effective method designed to
simultaneously enhance adversarial robustness and execution efficiency. Unlike
prior studies that enhance robustness via uniformly injecting noise, we
introduce a non-uniform noise injection algorithm, strategically applied at
each DNN layer to disrupt adversarial perturbations introduced in attacks. By
employing approximation techniques, our approach identifies and protects
essential neurons while strategically introducing noise into non-essential
neurons. Our experimental results demonstrate that our method successfully
enhances both robustness and efficiency across several attack scenarios, model
architectures, and datasets.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04326" title="Abstract">arXiv:2402.04326</a> [<a href="/pdf/2402.04326" title="Download PDF">pdf</a>, <a href="/format/2402.04326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personality Trait Recognition using ECG Spectrograms and Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altaf%2C+M+M">Muhammad Mohsin Altaf</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+U">Saadat Ullah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Majd%2C+M">Muhammad Majd</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+S+M">Syed Muhammad Anwar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents an innovative approach to recognizing personality traits
using deep learning (DL) methods applied to electrocardiogram (ECG) signals.
Within the framework of detecting the big five personality traits model
encompassing extra-version, neuroticism, agreeableness, conscientiousness, and
openness, the research explores the potential of ECG-derived spectrograms as
informative features. Optimal window sizes for spectrogram generation are
determined, and a convolutional neural network (CNN), specifically Resnet-18,
and visual transformer (ViT) are employed for feature extraction and
personality trait classification. The study utilizes the publicly available
ASCERTAIN dataset, which comprises various physiological signals, including ECG
recordings, collected from 58 participants during the presentation of video
stimuli categorized by valence and arousal levels. The outcomes of this study
demonstrate noteworthy performance in personality trait classification,
consistently achieving F1-scores exceeding 0.9 across different window sizes
and personality traits. These results emphasize the viability of ECG signal
spectrograms as a valuable modality for personality trait recognition, with
Resnet-18 exhibiting effectiveness in discerning distinct personality traits.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04328" title="Abstract">arXiv:2402.04328</a> [<a href="/pdf/2402.04328" title="Download PDF">pdf</a>, <a href="/ps/2402.04328" title="Download PostScript">ps</a>, <a href="/format/2402.04328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Production-Inventory games: a new class of totally balanced  combinatorial optimization games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Puerto%2C+J">Justo Puerto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In this paper we introduce a new class of cooperative games that arise from
production-inventory problems. Several agents have to cover their demand over a
finite time horizon and shortages are allowed. Each agent has its own unit
production, inventory-holding and backlogging cost. Cooperation among agents is
given by sharing production processes and warehouse facilities: agents in a
coalition produce with \ the cheapest production cost and store with the
cheapest inventory cost. We prove that the resulting cooperative game is
totally balanced and the Owen set reduces to a singleton: the Owen point. Based
on this type of allocation we find a population monotonic allocation scheme for
this class of games. Finally, we point out the relationship of the Owen point
with other well-known allocation rules such as the nucleolus and the Shapley
value.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04333" title="Abstract">arXiv:2402.04333</a> [<a href="/pdf/2402.04333" title="Download PDF">pdf</a>, <a href="/format/2402.04333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LESS: Selecting Influential Data for Targeted Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Malladi%2C+S">Sadhika Malladi</a>, 
<a href="/search/cs?searchtype=author&query=Gururangan%2C+S">Suchin Gururangan</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data are available at <a href="https://github.com/princeton-nlp/LESS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning has unlocked powerful capabilities in large language
models (LLMs), effectively using combined datasets to develop generalpurpose
chatbots. However, real-world applications often require a specialized suite of
skills (e.g., reasoning). The challenge lies in identifying the most relevant
data from these extensive datasets to effectively develop specific
capabilities, a setting we frame as targeted instruction tuning. We propose
LESS, an optimizer-aware and practically efficient algorithm to effectively
estimate data influences and perform Low-rank gradiEnt Similarity Search for
instruction data selection. Crucially, LESS adapts existing influence
formulations to work with the Adam optimizer and variable-length instruction
data. LESS first constructs a highly reusable and transferable gradient
datastore with low-dimensional gradient features and then selects examples
based on their similarity to few-shot examples embodying a specific capability.
Experiments show that training on a LESS-selected 5% of the data can often
outperform training on the full dataset across diverse downstream tasks.
Furthermore, the selected data is highly transferable: smaller models can be
leveraged to select useful data for larger models and models from different
families. Our qualitative analysis shows that our method goes beyond surface
form cues to identify data that exemplifies the necessary reasoning skills for
the intended downstream application.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04334" title="Abstract">arXiv:2402.04334</a> [<a href="/pdf/2402.04334" title="Download PDF">pdf</a>, <a href="/ps/2402.04334" title="Download PostScript">ps</a>, <a href="/format/2402.04334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Home Automation System based on Intelligent Transducer Enablers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Su%C3%A1rez-Albela%2C+M">Manuel Su&#xe1;rez-Albela</a>, 
<a href="/search/eess?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez-Caram%C3%A9s%2C+T+M">Tiago M. Fern&#xe1;ndez-Caram&#xe9;s</a>, 
<a href="/search/eess?searchtype=author&query=Dapena%2C+A">Adriana Dapena</a>, 
<a href="/search/eess?searchtype=author&query=Gonz%C3%A1lez-L%C3%B3pez%2C+M">Miguel Gonz&#xe1;lez-L&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 17 figures, accepted version of Sensors journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2016, 16(10), 1595
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper presents a novel home automation system named HASITE (Home
Automation System based on Intelligent Transducer Enablers), which has been
specifically designed to identify and configure transducers easily and quickly.
These features are especially useful in situations where many transducers are
deployed, since their setup becomes a cumbersome task that consumes a
significant amount of time and human resources. HASITE simplifies the
deployment of a home automation system by using wireless networks and both
self-configuration and self-registration protocols. Thanks to the application
of these three elements, HASITE is able to add new transducers by just powering
them up. According to the tests performed in different realistic scenarios, a
transducer is ready to be used in less than 13 s. Moreover, all HASITE
functionalities can be accessed through an API, which also allows for the
integration of third-party systems. As an example, an Android application based
on the API is presented. Remote users can use it to interact with transducers
by just using a regular smartphone or a tablet.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04335" title="Abstract">arXiv:2402.04335</a> [<a href="/pdf/2402.04335" title="Download PDF">pdf</a>, <a href="/format/2402.04335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LegalLens: Leveraging LLMs for Legal Violation Identification in  Unstructured Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernsohn%2C+D">Dor Bernsohn</a>, 
<a href="/search/cs?searchtype=author&query=Semo%2C+G">Gil Semo</a>, 
<a href="/search/cs?searchtype=author&query=Vazana%2C+Y">Yaron Vazana</a>, 
<a href="/search/cs?searchtype=author&query=Hayat%2C+G">Gila Hayat</a>, 
<a href="/search/cs?searchtype=author&query=Hagag%2C+B">Ben Hagag</a>, 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+J">Joel Niklaus</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+R">Rohit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Truskovskyi%2C+K">Kyryl Truskovskyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we focus on two main tasks, the first for detecting legal
violations within unstructured textual data, and the second for associating
these violations with potentially affected individuals. We constructed two
datasets using Large Language Models (LLMs) which were subsequently validated
by domain expert annotators. Both tasks were designed specifically for the
context of class-action cases. The experimental design incorporated fine-tuning
models from the BERT family and open-source LLMs, and conducting few-shot
experiments using closed-source LLMs. Our results, with an F1-score of 62.69\%
(violation identification) and 81.02\% (associating victims), show that our
datasets and setups can be used for both tasks. Finally, we publicly release
the datasets and the code used for the experiments in order to advance further
research in the area of legal natural language processing (NLP).
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04336" title="Abstract">arXiv:2402.04336</a> [<a href="/pdf/2402.04336" title="Download PDF">pdf</a>, <a href="/ps/2402.04336" title="Download PostScript">ps</a>, <a href="/format/2402.04336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> p-additive games: a class of totally balanced games arising from  inventory situations with temporary discounts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Toledo%2C+A">Andr&#xe9;s Toledo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We introduce a new class of totally balanced cooperative TU games, namely p
-additive games. It is inspired by the class of inventory games that arises
from inventory situations with temporary discounts (Toledo, 2002) and contains
the class of inventory cost games (Meca et al. 2003). It is shown that every
p-additive game and its corresponding subgames have a nonempty core. We also
focus on studying the character concave or convex and monotone of p-additive
games. In addition, the modified SOC-rule is proposed as a solution for
p-additive games. This solution is suitable for p-additive games since it is a
core-allocation which can be reached through a population monotonic allocation
scheme. Moreover, two characterizations of the modified SOC-rule are provided.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04338" title="Abstract">arXiv:2402.04338</a> [<a href="/pdf/2402.04338" title="Download PDF">pdf</a>, <a href="/ps/2402.04338" title="Download PostScript">ps</a>, <a href="/format/2402.04338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical recognition method for solving the problem of identification in  the Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saymanov%2C+I">Islambek Saymanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A new area of application of methods of algebra of logic and to valued logic,
which has emerged recently, is the problem of recognizing a variety of objects
and phenomena, medical or technical diagnostics, constructing modern machines,
checking test problems, etc., which can be reduced to constructing an optimal
extension of the logical function to the entire feature space. For example, in
logical recognition systems, logical methods based on discrete analysis and
propositional calculus based on it are used to build their own recognition
algorithms. In the general case, the use of a logical recognition method
provides for the presence of logical connections expressed by the optimal
continuation of a k-valued function over the entire feature space, in which the
variables are the logical features of the objects or phenomena being
recognized. The goal of this work is to develop a logical method for object
recognition consisting of a reference table with logical features and classes
of non-intersecting objects, which are specified as vectors from a given
feature space. The method consists of considering the reference table as a
logical function that is not defined everywhere and constructing an optimal
continuation of the logical function to the entire feature space, which
determines the extension of classes to the entire space.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04340" title="Abstract">arXiv:2402.04340</a> [<a href="/pdf/2402.04340" title="Download PDF">pdf</a>, <a href="/ps/2402.04340" title="Download PostScript">ps</a>, <a href="/format/2402.04340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skills in computational thinking of engineering students of the first  school year
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varela%2C+C">Concepcion Varela</a>, 
<a href="/search/cs?searchtype=author&query=Rebollar%2C+C">Carolina Rebollar</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+O">Olatz Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Bravo%2C+E">Eugenio Bravo</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+J">Javier Bilbao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In this world of the digital era, in which we are living, one of the
fundamental competences that students must acquire is the competence in
Computational Thinking (CT). Although there is no general consensus on a formal
definition, there is a general understanding of it as a set of skills and
attitudes necessary for the resolution, with or without a computer, of problems
that may arise in any area of life. Measuring and evaluating which of the CT
skills students have acquired is fundamental, and for this purpose, previously
validated measuring instruments must be used. In this study, a previously
validated instrument is applied to know if the new students in the Engineering
Degrees of the University of the Basque Country have the following skills in
CT: Critical Thinking, Algorithmic Thinking, Problem Solving, Cooperativity and
Creativity.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04344" title="Abstract">arXiv:2402.04344</a> [<a href="/pdf/2402.04344" title="Download PDF">pdf</a>, <a href="/format/2402.04344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Confidence Calibration Help Conformal Prediction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xi%2C+H">Huajun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianguo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Conformal prediction, as an emerging uncertainty qualification technique,
constructs prediction sets that are guaranteed to contain the true label with
high probability. Previous works usually employ temperature scaling to
calibrate the classifier, assuming that confidence calibration can benefit
conformal prediction. In this work, we first show that post-hoc calibration
methods surprisingly lead to larger prediction sets with improved calibration,
while over-confidence with small temperatures benefits the conformal prediction
performance instead. Theoretically, we prove that high confidence reduces the
probability of appending a new class in the prediction set. Inspired by the
analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$
(ConfTS), which rectifies the objective through the gap between the threshold
and the non-conformity score of the ground-truth label. In this way, the new
objective of ConfTS will optimize the temperature value toward an optimal set
that satisfies the $\textit{marginal coverage}$. Experiments demonstrate that
our method can effectively improve widely-used conformal prediction methods.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04347" title="Abstract">arXiv:2402.04347</a> [<a href="/pdf/2402.04347" title="Download PDF">pdf</a>, <a href="/format/2402.04347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hedgehog &amp; the Porcupine: Expressive Linear Attentions with Softmax  Mimicry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Michael Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+K">Kush Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Kumbong%2C+H">Hermann Kumbong</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 20 figures, 15 tables, ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Linear attentions have shown potential for improving Transformer efficiency,
reducing attention's quadratic complexity to linear in sequence length. This
holds exciting promise for (1) training linear Transformers from scratch, (2)
"finetuned-conversion" of task-specific Transformers into linear versions that
recover task performance, and (3) "pretrained-conversion" of Transformers such
as large language models into linear versions finetunable on downstream tasks.
However, linear attentions often underperform standard softmax attention in
quality. To close this performance gap, we find prior linear attentions lack
key properties of softmax attention tied to good performance: low-entropy (or
"spiky") weights and dot-product monotonicity. We further observe surprisingly
simple feature maps that retain these properties and match softmax performance,
but are inefficient to compute in linear attention. We thus propose Hedgehog, a
learnable linear attention that retains the spiky and monotonic properties of
softmax attention while maintaining linear complexity. Hedgehog uses simple
trainable MLPs to produce attention weights mimicking softmax attention.
Experiments show Hedgehog recovers over 99% of standard Transformer quality in
train-from-scratch and finetuned-conversion settings, outperforming prior
linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,
and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also
enables pretrained-conversion. Converting a pretrained GPT-2 into a linear
attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for
125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into
a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B
achieves 28.1 higher ROUGE-1 points over the base standard attention model,
where prior linear attentions lead to 16.5 point drops.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04348" title="Abstract">arXiv:2402.04348</a> [<a href="/pdf/2402.04348" title="Download PDF">pdf</a>, <a href="/format/2402.04348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inversion of the Laplace Transform of Point Masses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=McKenna%2C+M">Michael McKenna</a>, 
<a href="/search/math?searchtype=author&query=Mhaskar%2C+H+N">Hrushikesh N. Mhaskar</a>, 
<a href="/search/math?searchtype=author&query=Spencer%2C+R+G">Richard G. Spencer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 3 figures, 2 tables. Submitted to Inverse Problems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Motivated by applications in magnetic resonance relaxometry, we consider the
following problem: Given samples of a function $t\mapsto \sum_{k=1}^K
A_k\exp(-t\lambda_k)$, where $K\ge 2$ is an integer, $A_k\in\mathbb{R}$,
$\lambda_k&gt;0$ for $k=1,\cdots, K$, determine $K$, $A_k$'s and $\lambda_k$'s.
Our approach is to transform this function into another function of the same
form where $\lambda_k$'s are replaced by $i\lambda_k$. For this purpose, we
study the least square approximation using polynomials weighted by the Gaussian
weight, and use the fact that Hermite functions are eigenfunctions of the
Fourier transform. We provide a detailed analysis of the effect of noise in the
data.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04350" title="Abstract">arXiv:2402.04350</a> [<a href="/pdf/2402.04350" title="Download PDF">pdf</a>, <a href="/ps/2402.04350" title="Download PostScript">ps</a>, <a href="/format/2402.04350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Education and Sustainability: a model for different Engineering degrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bilbao%2C+J">Javier Bilbao</a>, 
<a href="/search/eess?searchtype=author&query=Bravo%2C+E">Eugenio Bravo</a>, 
<a href="/search/eess?searchtype=author&query=Garcia%2C+O">Olatz Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Rebollar%2C+C">Carolina Rebollar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Technologies related to the Internet of Things (IoT) have seen remarkable
growth in recent years. This has facilitated, among many other reasons, that
monitoring systems have spread in many everyday areas, including both industry
and the services and systems of the so-called smart home. These systems can
also be applied in Engineering and in Education for the different existing
engineering degrees; and one of the fields is sustainability. A project related
to sustainability and student practices has been launched at our university. In
this way, several objectives are achieved at the same time, such as the
transfer of knowledge from universities to society, and also the development of
sustainable education, in line with the sustainable development goals. In this
framework, we want to apply the ideas of monitoring through IoT applications,
by means of the measurement of certain environmental factors that occur both in
an urban garden and in a composting process. Only open hardware-based devices
have been used in the project. The proposed model can be applied in other areas
of knowledge, having considered different alternatives and having chosen the
best elements, based on sustainability criteria, for each section of the
project. Specifically, in the system that has been created, the environmental
factors of a small urban garden and also of a composting box can be measured.
Both sections of the project, garden and composting, are located at the
university. The factors to be measured are the following: air temperature, air
humidity, soil moisture, ultraviolet radiation and amount of light (luminosity)
received in the urban garden; and temperature and humidity in the composting
process.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04353" title="Abstract">arXiv:2402.04353</a> [<a href="/pdf/2402.04353" title="Download PDF">pdf</a>, <a href="/format/2402.04353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Interval Scheduling of Indivisible Chores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Equbal%2C+S">Sarfaraz Equbal</a>, 
<a href="/search/cs?searchtype=author&query=Gurjar%2C+R">Rohit Gurjar</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+Y">Yatharth Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaprava Nath</a>, 
<a href="/search/cs?searchtype=author&query=Vaish%2C+R">Rohit Vaish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the problem of fairly assigning a set of discrete tasks (or chores)
among a set of agents with additive valuations. Each chore is associated with a
start and finish time, and each agent can perform at most one chore at any
given time. The goal is to find a fair and efficient schedule of the chores,
where fairness pertains to satisfying envy-freeness up to one chore (EF1) and
efficiency pertains to maximality (i.e., no unallocated chore can be feasibly
assigned to any agent). Our main result is a polynomial-time algorithm for
computing an EF1 and maximal schedule for two agents under monotone valuations
when the conflict constraints constitute an arbitrary interval graph. The
algorithm uses a coloring technique in interval graphs that may be of
independent interest. For an arbitrary number of agents, we provide an
algorithm for finding a fair schedule under identical dichotomous valuations
when the constraints constitute a path graph. We also show that stronger
fairness and efficiency properties, including envy-freeness up to any chore
(EFX) along with maximality and EF1 along with Pareto optimality, cannot be
achieved.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04354" title="Abstract">arXiv:2402.04354</a> [<a href="/pdf/2402.04354" title="Download PDF">pdf</a>, <a href="/ps/2402.04354" title="Download PostScript">ps</a>, <a href="/format/2402.04354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D printer-controlled syringe pumps for dual, active, regulable and  simultaneous dispensing of reagents. Manufacturing of immunochromatographic  test strips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siano%2C+G">Gabriel Siano</a>, 
<a href="/search/cs?searchtype=author&query=Peretti%2C+L">Leandro Peretti</a>, 
<a href="/search/cs?searchtype=author&query=Marquez%2C+J+M">Juan Manuel Marquez</a>, 
<a href="/search/cs?searchtype=author&query=Pujato%2C+N">Nazarena Pujato</a>, 
<a href="/search/cs?searchtype=author&query=Giovanini%2C+L">Leonardo Giovanini</a>, 
<a href="/search/cs?searchtype=author&query=Berli%2C+C">Claudio Berli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lateral flow immunoassays (LFIA) are widely used worldwide for the detection
of different analytes because they combine multiple advantages such as low
production cost, simplicity, and portability, which allows biomarkers detection
without requiring infrastructure or highly trained personnel. Here we propose
to provide solutions to the manufacturing process of LFIA at laboratory-scale,
particularly to the controlled and active dispensing of the reagents in the
form the Test Lines (TL) and the Control Lines (CL). To accomplish this task,
we adapted a 3D printer to also control Syringe Pumps (SP), since the proposed
adaptation of a 3D printer is easy, free and many laboratories already have it
in their infrastructure. In turn, the standard function of the 3D printer can
be easily restored by disconnecting the SPs and reconnecting the extruder.
Additionally, the unified control of the 3D printer enables dual, active,
regulable and simultaneous dispensing, four features that are typically found
only in certain high-cost commercial equipment. With the proposed setup, the
challenge of dispensing simultaneously at least 2 lines (CL and TL) with SPs
controlled by a 3D printer was addressed, including regulation in the width of
dispensed lines within experimental limits. Also, the construction of a LFIA
for the detection of leptospirosis is shown as a practical example of
automatized reagent dispensing.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04356" title="Abstract">arXiv:2402.04356</a> [<a href="/pdf/2402.04356" title="Download PDF">pdf</a>, <a href="/format/2402.04356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Autoregressive Diffusion Model for Dance Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Canyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Youbao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Ruei-Sung Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Mei Han</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Dance serves as a powerful medium for expressing human emotions, but the
lifelike generation of dance is still a considerable challenge. Recently,
diffusion models have showcased remarkable generative abilities across various
domains. They hold promise for human motion generation due to their adaptable
many-to-many nature. Nonetheless, current diffusion-based motion generation
models often create entire motion sequences directly and unidirectionally,
lacking focus on the motion with local and bidirectional enhancement. When
choreographing high-quality dance movements, people need to take into account
not only the musical context but also the nearby music-aligned dance motions.
To authentically capture human behavior, we propose a Bidirectional
Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a
bidirectional encoder is built to enforce that the generated dance is
harmonious in both the forward and backward directions. To make the generated
dance motion smoother, a local information decoder is built for local motion
enhancement. The proposed framework is able to generate new motions based on
the input conditions and nearby motions, which foresees individual motion
slices iteratively and consolidates all predictions. To further refine the
synchronicity between the generated dance and the beat, the beat information is
incorporated as an input to generate better music-aligned dance movements.
Experimental results demonstrate that the proposed model achieves
state-of-the-art performance compared to existing unidirectional approaches on
the prominent benchmark for music-to-dance generation.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04357" title="Abstract">arXiv:2402.04357</a> [<a href="/pdf/2402.04357" title="Download PDF">pdf</a>, <a href="/format/2402.04357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Retrieval Systems for the ClueWeb22-B Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+H">Harshit Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Callan%2C+J">Jamie Callan</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhen Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The ClueWeb22 dataset containing nearly 10 billion documents was released in
2022 to support academic and industry research. The goal of this project was to
build retrieval baselines for the English section of the "super head" part
(category B) of this dataset. These baselines can then be used by the research
community to compare their systems and also to generate data to train/evaluate
new retrieval and ranking algorithms. The report covers sparse and dense first
stage retrievals as well as neural rerankers that were implemented for this
dataset. These systems are available as a service on a Carnegie Mellon
University cluster.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04359" title="Abstract">arXiv:2402.04359</a> [<a href="/pdf/2402.04359" title="Download PDF">pdf</a>, <a href="/format/2402.04359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Inference: Theoretical Limits and Unexplored Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hor%2C+S">Soheil Hor</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Ying Qian</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>, 
<a href="/search/cs?searchtype=author&query=Arbabian%2C+A">Amin Arbabian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces the first theoretical framework for quantifying the
efficiency and performance gain opportunity size of adaptive inference
algorithms. We provide new approximate and exact bounds for the achievable
efficiency and performance gains, supported by empirical evidence demonstrating
the potential for 10-100x efficiency improvements in both Computer Vision and
Natural Language Processing tasks without incurring any performance penalties.
Additionally, we offer insights on improving achievable efficiency gains
through the optimal selection and design of adaptive inference state spaces.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04362" title="Abstract">arXiv:2402.04362</a> [<a href="/pdf/2402.04362" title="Download PDF">pdf</a>, <a href="/format/2402.04362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks Learn Statistics of Increasing Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belrose%2C+N">Nora Belrose</a>, 
<a href="/search/cs?searchtype=author&query=Pope%2C+Q">Quintin Pope</a>, 
<a href="/search/cs?searchtype=author&query=Quirke%2C+L">Lucia Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Fern%2C+X">Xiaoli Fern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The distributional simplicity bias (DSB) posits that neural networks learn
low-order moments of the data distribution first, before moving on to
higher-order correlations. In this work, we present compelling new evidence for
the DSB by showing that networks automatically learn to perform well on
maximum-entropy distributions whose low-order statistics match those of the
training set early in training, then lose this ability later. We also extend
the DSB to discrete domains by proving an equivalence between token $n$-gram
frequencies and the moments of embedding vectors, and by finding empirical
evidence for the bias in LLMs. Finally we use optimal transport methods to
surgically edit the low-order statistics of one class to match those of
another, and show that early-training networks treat the edited samples as if
they were drawn from the target class. Code is available at
https://github.com/EleutherAI/features-across-time.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04364" title="Abstract">arXiv:2402.04364</a> [<a href="/pdf/2402.04364" title="Download PDF">pdf</a>, <a href="/format/2402.04364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Separation Between Powers of Regular and General Resolution  Over Parities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S+K">Sreejata Kishor Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+A">Arkadev Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+P">Pavel Dvo&#x159;&#xe1;k</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">Proving super-polynomial lower bounds on the size of proofs of
unsatisfiability of Boolean formulas using resolution over parities, is an
outstanding problem that has received a lot of attention after its introduction
by Raz and Tzamaret [Ann. Pure Appl. Log.'08]. Very recently, Efremenko,
Garl\'ik and Itsykson [ECCC'23] proved the first exponential lower bounds on
the size of ResLin proofs that were additionally restricted to be
bottom-regular. We show that there are formulas for which such regular ResLin
proofs of unsatisfiability continue to have exponential size even though there
exists short proofs of their unsatisfiability in ordinary, non-regular
resolution. This is the first super-polynomial separation between the power of
general ResLin and and that of regular ResLin for any natural notion of
regularity.
<br />Our argument, while building upon the work of Efremenko et al, uses
additional ideas from the literature on lifting theorems.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04367" title="Abstract">arXiv:2402.04367</a> [<a href="/pdf/2402.04367" title="Download PDF">pdf</a>, <a href="/ps/2402.04367" title="Download PostScript">ps</a>, <a href="/format/2402.04367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merkle Trees in Blockchain: A Study of Collision Probability and  Security Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+O">Oleksandr Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Rusnak%2C+A">Alex Rusnak</a>, 
<a href="/search/cs?searchtype=author&query=Yezhov%2C+A">Anton Yezhov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+K">Kateryna Kuznetsova</a>, 
<a href="/search/cs?searchtype=author&query=Kanonik%2C+D">Dzianis Kanonik</a>, 
<a href="/search/cs?searchtype=author&query=Domin%2C+O">Oleksandr Domin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the rapidly evolving landscape of blockchain technology, ensuring the
integrity and security of data is paramount. This study delves into the
security aspects of Merkle Trees, a fundamental component in blockchain
architectures, such as Ethereum. We critically examine the susceptibility of
Merkle Trees to hash collisions, a potential vulnerability that poses
significant risks to data security within blockchain systems. Despite their
widespread application, the collision resistance of Merkle Trees and their
robustness against preimage attacks have not been thoroughly investigated,
leading to a notable gap in the comprehensive understanding of blockchain
security mechanisms. Our research endeavors to bridge this gap through a
meticulous blend of theoretical analysis and empirical validation. We
scrutinize the probability of root collisions in Merkle Trees, considering
various factors such as hash length and path length within the tree. Our
findings reveal a direct correlation between the increase in path length and
the heightened probability of root collisions, thereby underscoring potential
security vulnerabilities. Conversely, we observe that an increase in hash
length significantly reduces the likelihood of collisions, highlighting its
critical role in fortifying security. The insights garnered from our research
offer valuable guidance for blockchain developers and researchers, aiming to
bolster the security and operational efficacy of blockchain-based systems.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04370" title="Abstract">arXiv:2402.04370</a> [<a href="/pdf/2402.04370" title="Download PDF">pdf</a>, <a href="/format/2402.04370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pedestrian crossing decisions can be explained by bounded optimal  decision-making under noisy visual perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A+R">Aravinda Ramakrishnan Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Jokinen%2C+J+P+P">Jussi P.P. Jokinen</a>, 
<a href="/search/cs?searchtype=author&query=Oulasvirta%2C+A">Antti Oulasvirta</a>, 
<a href="/search/cs?searchtype=author&query=Markkula%2C+G">Gustav Markkula</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper presents a model of pedestrian crossing decisions, based on the
theory of computational rationality. It is assumed that crossing decisions are
boundedly optimal, with bounds on optimality arising from human cognitive
limitations. While previous models of pedestrian behaviour have been either
'black-box' machine learning models or mechanistic models with explicit
assumptions about cognitive factors, we combine both approaches. Specifically,
we model mechanistically noisy human visual perception and assumed rewards in
crossing, but we use reinforcement learning to learn bounded optimal behaviour
policy. The model reproduces a larger number of known empirical phenomena than
previous models, in particular: (1) the effect of the time to arrival of an
approaching vehicle on whether the pedestrian accepts the gap, the effect of
the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of
crossing in front of yielding vehicles, and (4) the effect on this crossing
timing of the stopping distance of the yielding vehicle. Notably, our findings
suggest that behaviours previously framed as 'biases' in decision-making, such
as speed-dependent gap acceptance, might instead be a product of rational
adaptation to the constraints of visual perception. Our approach also permits
fitting the parameters of cognitive constraints and rewards per individual, to
better account for individual differences. To conclude, by leveraging both RL
and mechanistic modelling, our model offers novel insights about pedestrian
behaviour, and may provide a useful foundation for more accurate and scalable
pedestrian models.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04373" title="Abstract">arXiv:2402.04373</a> [<a href="/pdf/2402.04373" title="Download PDF">pdf</a>, <a href="/format/2402.04373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The World of Generative AI: Deepfakes and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Alakananda Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Mohanty%2C+S+P">Saraju P. Mohanty</a>, 
<a href="/search/cs?searchtype=author&query=Kougianos%2C+E">Elias Kougianos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes
and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in
particular, pose an alarming threat to society as they are capable of spreading
misinformation and changing the truth. LLMs are powerful language models that
generate general-purpose language. However due to its generative aspect, it can
also be a risk for people if used with ill intentions. The ethical use of these
technologies is a big concern. This short article tries to find out the
interrelationship between them.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04374" title="Abstract">arXiv:2402.04374</a> [<a href="/pdf/2402.04374" title="Download PDF">pdf</a>, <a href="/format/2402.04374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SKOOTR: A SKating, Omni-Oriented, Tripedal Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hung%2C+A+J">Adam Joshua Hung</a>, 
<a href="/search/cs?searchtype=author&query=Adu%2C+C+E">Challen Enninful Adu</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+T+Y">Talia Y. Moore</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In both animals and robots, locomotion capabilities are determined by the
physical structure of the system. The majority of legged animals and robots are
bilaterally symmetric, which facilitates locomotion with consistent headings
and obstacle traversal, but leads to constraints in their turning ability. On
the other hand, radially symmetric animals have demonstrated rapid turning
abilities enabled by their omni-directional body plans. Radially symmetric
tripedal robots are able to turn instantaneously, but are commonly constrained
by needing to change direction with every step, resulting in inefficient and
less stable locomotion. We address these challenges by introducing a novel
design for a tripedal robot that has both frictional and rolling contacts.
Additionally, a freely rotating central sphere provides an added contact point
so the robot can retain a stable tripod base of support while lifting and
pushing with any one of its legs. The SKating, Omni-Oriented, Tripedal Robot
(SKOOTR) is more versatile and stable than other existing tripedal robots. It
is capable of multiple forward gaits, multiple turning maneuvers, obstacle
traversal, and stair climbing. SKOOTR has been designed to facilitate
customization for diverse applications: it is fully open-source, is constructed
with 3D printed or off-the-shelf parts, and costs approximately $500 USD to
build.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04375" title="Abstract">arXiv:2402.04375</a> [<a href="/pdf/2402.04375" title="Download PDF">pdf</a>, <a href="/format/2402.04375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding the Excess Risk for Linear Models Trained on  Marginal-Preserving, Differentially-Private, Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yvonne Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Mingyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Brugere%2C+I">Ivan Brugere</a>, 
<a href="/search/cs?searchtype=author&query=Dachman-Soled%2C+D">Dana Dachman-Soled</a>, 
<a href="/search/cs?searchtype=author&query=Dervovic%2C+D">Danial Dervovic</a>, 
<a href="/search/cs?searchtype=author&query=Polychroniadou%2C+A">Antigoni Polychroniadou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The growing use of machine learning (ML) has raised concerns that an ML model
may reveal private information about an individual who has contributed to the
training dataset. To prevent leakage of sensitive data, we consider using
differentially-private (DP), synthetic training data instead of real training
data to train an ML model. A key desirable property of synthetic data is its
ability to preserve the low-order marginals of the original distribution. Our
main contribution comprises novel upper and lower bounds on the excess
empirical risk of linear models trained on such synthetic data, for continuous
and Lipschitz loss functions. We perform extensive experimentation alongside
our theoretical results.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04376" title="Abstract">arXiv:2402.04376</a> [<a href="/pdf/2402.04376" title="Download PDF">pdf</a>, <a href="/format/2402.04376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling laws for learning with real and surrogate data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Ayush Jain</a>, 
<a href="/search/cs?searchtype=author&query=Montanari%2C+A">Andrea Montanari</a>, 
<a href="/search/cs?searchtype=author&query=Sasoglu%2C+E">Eren Sasoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Collecting large quantities of high-quality data is often prohibitively
expensive or impractical, and a crucial bottleneck in machine learning. One may
instead augment a small set of $n$ data points from the target distribution
with data from more accessible sources like public datasets, data collected
under different circumstances, or synthesized by generative models. Blurring
distinctions, we refer to such data as `surrogate data'.
<br />We define a simple scheme for integrating surrogate data into training and
use both theoretical models and empirical studies to explore its behavior. Our
main findings are: $(i)$ Integrating surrogate data can significantly reduce
the test error on the original distribution; $(ii)$ In order to reap this
benefit, it is crucial to use optimally weighted empirical risk minimization;
$(iii)$ The test error of models trained on mixtures of real and surrogate data
is well described by a scaling law. This can be used to predict the optimal
weighting and the gain from surrogate data.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04377" title="Abstract">arXiv:2402.04377</a> [<a href="/pdf/2402.04377" title="Download PDF">pdf</a>, <a href="/format/2402.04377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient  Distributed Prediction Serving Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moradi%2C+P">Parsa Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Maddah-Ali%2C+M+A">Mohammad Ali Maddah-Ali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT)

</div>
<p class="mathjax">Resilience against stragglers is a critical element of prediction serving
systems, tasked with executing inferences on input data for a pre-trained
machine-learning model. In this paper, we propose NeRCC, as a general
straggler-resistant framework for approximate coded computing. NeRCC includes
three layers: (1) encoding regression and sampling, which generates coded data
points, as a combination of original data points, (2) computing, in which a
cluster of workers run inference on the coded data points, (3) decoding
regression and sampling, which approximately recovers the predictions of the
original data points from the available predictions on the coded data points.
We argue that the overall objective of the framework reveals an underlying
interconnection between two regression models in the encoding and decoding
layers. We propose a solution to the nested regressions problem by summarizing
their dependence on two regularization terms that are jointly optimized. Our
extensive experiments on different datasets and various machine learning
models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate
that NeRCC accurately approximates the original predictions in a wide range of
stragglers, outperforming the state-of-the-art by up to 23%.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04379" title="Abstract">arXiv:2402.04379</a> [<a href="/pdf/2402.04379" title="Download PDF">pdf</a>, <a href="/format/2402.04379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuned Language Models Generate Stable Inorganic Materials as Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruver%2C+N">Nate Gruver</a>, 
<a href="/search/cs?searchtype=author&query=Sriram%2C+A">Anuroop Sriram</a>, 
<a href="/search/cs?searchtype=author&query=Madotto%2C+A">Andrea Madotto</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Zitnick%2C+C+L">C. Lawrence Zitnick</a>, 
<a href="/search/cs?searchtype=author&query=Ulissi%2C+Z">Zachary Ulissi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Code available at: <a href="https://github.com/facebookresearch/crystal-llm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">We propose fine-tuning large language models for generation of stable
materials. While unorthodox, fine-tuning large language models on text-encoded
atomistic data is simple to implement yet reliable, with around 90% of sampled
structures obeying physical constraints on atom positions and charges. Using
energy above hull calculations from both learned ML potentials and
gold-standard DFT calculations, we show that our strongest model (fine-tuned
LLaMA-2 70B) can generate materials predicted to be metastable at about twice
the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text
prompting's inherent flexibility, our models can simultaneously be used for
unconditional generation of stable material, infilling of partial structures
and text-conditional generation. Finally, we show that language models' ability
to capture key symmetries of crystal structures improves with model scale,
suggesting that the biases of pretrained LLMs are surprisingly well-suited for
atomistic data.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04380" title="Abstract">arXiv:2402.04380</a> [<a href="/pdf/2402.04380" title="Download PDF">pdf</a>, <a href="/format/2402.04380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assured LLM-Based Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alshahwan%2C+N">Nadia Alshahwan</a>, 
<a href="/search/cs?searchtype=author&query=Harman%2C+M">Mark Harman</a>, 
<a href="/search/cs?searchtype=author&query=Harper%2C+I">Inna Harper</a>, 
<a href="/search/cs?searchtype=author&query=Marginean%2C+A">Alexandru Marginean</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Shubho Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+E">Eddy Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, InteNSE 24: ACM International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, April, 2024, Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In this paper we address the following question: How can we use Large
Language Models (LLMs) to improve code independently of a human, while ensuring
that the improved code
<br />- does not regress the properties of the original code?
<br />- improves the original in a verifiable and measurable way?
<br />To address this question, we advocate Assured LLM-Based Software Engineering;
a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE
applies a series of semantic filters that discard code that fails to meet these
twin guarantees. This overcomes the potential problem of LLM's propensity to
hallucinate. It allows us to generate code using LLMs, independently of any
human. The human plays the role only of final code reviewer, as they would do
with code generated by other human engineers.
<br />This paper is an outline of the content of the keynote by Mark Harman at the
International Workshop on Interpretability, Robustness, and Benchmarking in
Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04382" title="Abstract">arXiv:2402.04382</a> [<a href="/pdf/2402.04382" title="Download PDF">pdf</a>, <a href="/format/2402.04382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Generation with Answer Set Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+S">Sopam Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Shakerin%2C+F">Farhad Shakerin</a>, 
<a href="/search/cs?searchtype=author&query=Arias%2C+J">Joaqu&#xed;n Arias</a>, 
<a href="/search/cs?searchtype=author&query=Salazar%2C+E">Elmer Salazar</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gopal Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Machine learning models that automate decision-making are increasingly being
used in consequential areas such as loan approvals, pretrial bail approval,
hiring, and many more. Unfortunately, most of these models are black-boxes,
i.e., they are unable to reveal how they reach these prediction decisions. A
need for transparency demands justification for such predictions. An affected
individual might also desire explanations to understand why a decision was
made. Ethical and legal considerations may further require informing the
individual of changes in the input attribute that could be made to produce a
desirable outcome. This paper focuses on the latter problem of automatically
generating counterfactual explanations. We propose a framework Counterfactual
Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and
the s(CASP) goal-directed ASP system to automatically generate counterfactual
explanations from rules generated by rule-based machine learning (RBML)
algorithms. In our framework, we show how counterfactual explanations are
computed and justified by imagining worlds where some or all factual
assumptions are altered/changed. More importantly, we show how we can navigate
between these worlds, namely, go from our original world/scenario where we
obtain an undesired outcome to the imagined world/scenario where we obtain a
desired/favourable outcome.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04383" title="Abstract">arXiv:2402.04383</a> [<a href="/pdf/2402.04383" title="Download PDF">pdf</a>, <a href="/format/2402.04383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairWire: Fair Graph Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kose%2C+O+D">O. Deniz Kose</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanning Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Machine learning over graphs has recently attracted growing attention due to
its ability to analyze and learn complex relations within critical
interconnected systems. However, the disparate impact that is amplified by the
use of biased graph structures in these algorithms has raised significant
concerns for the deployment of them in real-world decision systems. In
addition, while synthetic graph generation has become pivotal for privacy and
scalability considerations, the impact of generative learning algorithms on the
structural bias has not yet been investigated. Motivated by this, this work
focuses on the analysis and mitigation of structural bias for both real and
synthetic graphs. Specifically, we first theoretically analyze the sources of
structural bias that result in disparity for the predictions of dyadic
relations. To alleviate the identified bias factors, we design a novel fairness
regularizer that offers a versatile use. Faced with the bias amplification in
graph generation models that is brought to light in this work, we further
propose a fair graph generation framework, FairWire, by leveraging our fair
regularizer design in a generative model. Experimental results on real-world
networks validate that the proposed tools herein deliver effective structural
bias mitigation for both real and synthetic graphs.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04384" title="Abstract">arXiv:2402.04384</a> [<a href="/pdf/2402.04384" title="Download PDF">pdf</a>, <a href="/format/2402.04384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Probabilistic Models in Six Simple Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Diaconu%2C+C">Cristiana-Diana Diaconu</a>, 
<a href="/search/cs?searchtype=author&query=Markou%2C+S">Stratis Markou</a>, 
<a href="/search/cs?searchtype=author&query=Shysheya%2C+A">Aliaksandra Shysheya</a>, 
<a href="/search/cs?searchtype=author&query=Foong%2C+A+Y+K">Andrew Y. K. Foong</a>, 
<a href="/search/cs?searchtype=author&query=Mlodozeniec%2C+B">Bruno Mlodozeniec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of
deep generative model that have been successfully applied to a diverse range of
problems including image and video generation, protein and material synthesis,
weather forecasting, and neural surrogates of partial differential equations.
Despite their ubiquity it is hard to find an introduction to DDPMs which is
simple, comprehensive, clean and clear. The compact explanations necessary in
research papers are not able to elucidate all of the different design steps
taken to formulate the DDPM and the rationale of the steps that are presented
is often omitted to save space. Moreover, the expositions are typically
presented from the variational lower bound perspective which is unnecessary and
arguably harmful as it obfuscates why the method is working and suggests
generalisations that do not perform well in practice. On the other hand,
perspectives that take the continuous time-limit are beautiful and general, but
they have a high barrier-to-entry as they require background knowledge of
stochastic differential equations and probability flow. In this note, we
distill down the formulation of the DDPM into six simple steps each of which
comes with a clear rationale. We assume that the reader is familiar with
fundamental topics in machine learning including basic probabilistic modelling,
Gaussian distributions, maximum likelihood estimation, and deep learning.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04385" title="Abstract">arXiv:2402.04385</a> [<a href="/pdf/2402.04385" title="Download PDF">pdf</a>, <a href="/ps/2402.04385" title="Download PostScript">ps</a>, <a href="/format/2402.04385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locating the roots of a quadratic equation in one variable through a  Line-Circumference (LC) geometric construction in the plane of complex  numbers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alba-Cuellar%2C+D">Daniel Alba-Cuellar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Complex Variables (math.CV)

</div>
<p class="mathjax">This paper describes a geometrical method for finding the roots $r_1$, $r_2$
of a quadratic equation in one complex variable of the form $x^2+c_1 x+c_2=0$,
by means of a Line $L$ and a Circumference $C$ in the complex plane,
constructed from known coefficients $c_1$, $c_2$. This Line-Circumference (LC)
geometric structure contains the sought roots $r_1$, $r_2$ at the intersections
of its component elements $L$ and $C$. Line $L$ in the LC structure is mapped
onto Circumference $C$ by a Mobius transformation. The location and inclination
angle of Line $L$ can be computed directly from coefficients $c_1$, $c_2$,
while Circumference $C$ is constructed by dividing the constant term $c_2$ by
each point from Line $L$. This paper describes and develops the technical
details for the LC Method, and then shows how the LC Method works through a
numerical example. The quadratic LC method described here can be extended to
polynomials in one variable of degree greater than two, in order to find
initial approximations to their roots. As an additional feature, this paper
also studies an interesting property of the rectilinear segments connecting key
points in a quadratic LC structure.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04386" title="Abstract">arXiv:2402.04386</a> [<a href="/pdf/2402.04386" title="Download PDF">pdf</a>, <a href="/format/2402.04386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel Methods for Load Estimation in Cell Switching in HAPS-Assisted  Sustainable 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salamatmoghadasi%2C+M">Maryam Salamatmoghadasi</a>, 
<a href="/search/cs?searchtype=author&query=Ozturk%2C+M">Metin Ozturk</a>, 
<a href="/search/cs?searchtype=author&query=Yanikomeroglu%2C+H">Halim Yanikomeroglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, ICC Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In the evolving landscape of vertical heterogeneous networks, the practice of
cell switching particularly for small base stations faces a significant
challenge due to the lack of accurate data on the traffic load of sleeping
SBSs. This information gap is crucial as it hinders the feasibility and
applicability of existing power consumption optimization methods; however, the
studies in the literature predominantly assume perfect knowledge about the
traffic load of sleeping SBSs. Addressing this critical issue, our study
introduces innovative methodologies for estimating the traffic load of sleeping
SBSs in a vHetNet including the integration of a high altitude platform as a
super macro base station into the terrestrial network. We propose three
distinct spatial interpolation-based estimation schemes: clustering-based,
distance based, and random neighboring selection. Employing a real data set for
empirical validations, we compare the estimation performance of the developed
traffic load estimation schemes and assess the impact of estimation errors. Our
findings demonstrate that accurate estimation of sleeping SBSs' traffic loads
is essential for making network power consumption optimization methods both
feasible and applicable in vHetNets.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04390" title="Abstract">arXiv:2402.04390</a> [<a href="/pdf/2402.04390" title="Download PDF">pdf</a>, <a href="/ps/2402.04390" title="Download PostScript">ps</a>, <a href="/format/2402.04390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Densely Multiplied Physics Informed Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feilong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+X">Xiaonan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Min Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although physics-informed neural networks (PINNs) have shown great potential
in dealing with nonlinear partial differential equations (PDEs), it is common
that PINNs will suffer from the problem of insufficient precision or obtaining
incorrect outcomes. Unlike most of the existing solutions trying to enhance the
ability of PINN by optimizing the training process, this paper improved the
neural network architecture to improve the performance of PINN. We propose a
densely multiply PINN (DM-PINN) architecture, which multiplies the output of a
hidden layer with the outputs of all the behind hidden layers. Without
introducing more trainable parameters, this effective mechanism can
significantly improve the accuracy of PINNs. The proposed architecture is
evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation,
Burgers equation and 1D convection equation). Comparisons between the proposed
architecture and different PINN structures demonstrate the superior performance
of the DM-PINN in both accuracy and efficiency.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04391" title="Abstract">arXiv:2402.04391</a> [<a href="/pdf/2402.04391" title="Download PDF">pdf</a>, <a href="/ps/2402.04391" title="Download PostScript">ps</a>, <a href="/format/2402.04391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Howard-Harvard effect: Institutional reproduction of intersectional  inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kozlowski%2C+D">Diego Kozlowski</a>, 
<a href="/search/cs?searchtype=author&query=Monroe-White%2C+T">Thema Monroe-White</a>, 
<a href="/search/cs?searchtype=author&query=Larivi%C3%A8re%2C+V">Vincent Larivi&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Sugimoto%2C+C+R">Cassidy R. Sugimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The US higher education system concentrates the production of science and
scientists within a few institutions. This has implications for minoritized
scholars and the topics with which they are disproportionately associated. This
paper examines topical alignment between institutions and authors of varying
intersectional identities, and the relationship with prestige and scientific
impact. We observe a Howard-Harvard effect, in which the topical profile of
minoritized scholars are amplified in mission-driven institutions and decreased
in prestigious institutions. Results demonstrate a consistent pattern of
inequality in topics and research impact. Specifically, we observe
statistically significant differences between minoritized scholars and White
men in citations and journal impact. The aggregate research profile of
prestigious US universities is highly correlated with the research profile of
White men, and highly negatively correlated with the research profile of
minoritized women. Furthermore, authors affiliated with more prestigious
institutions are associated with increasing inequalities in both citations and
journal impact. Academic institutions and funders are called to create policies
to mitigate the systemic barriers that prevent the United States from achieving
a fully robust scientific ecosystem.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04392" title="Abstract">arXiv:2402.04392</a> [<a href="/pdf/2402.04392" title="Download PDF">pdf</a>, <a href="/ps/2402.04392" title="Download PostScript">ps</a>, <a href="/format/2402.04392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factorial Basis Method for q-Series Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jim%C3%A9nez-Pastor%2C+A">Antonio Jim&#xe9;nez-Pastor</a>, 
<a href="/search/cs?searchtype=author&query=Uncu%2C+A+K">Ali Kemal Uncu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 double-column pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The Factorial Basis method, initially designed for quasi-triangular,
shift-compatible factorial bases, provides solutions to linear recurrence
equations in the form of definite-sums. This paper extends the Factorial Basis
method to its q-analog, enabling its application in q-calculus. We demonstrate
the adaptation of the method to q-sequences and its utility in the realm of
q-combinatorics. The extended technique is employed to automatically prove
established identities and unveil novel ones, particularly some associated with
the Rogers-Ramanujan identities.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04396" title="Abstract">arXiv:2402.04396</a> [<a href="/pdf/2402.04396" title="Download PDF">pdf</a>, <a href="/format/2402.04396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuIP#: Even Better LLM Quantization with Hadamard Incoherence and  Lattice Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseng%2C+A">Albert Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Chee%2C+J">Jerry Chee</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingyao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>, 
<a href="/search/cs?searchtype=author&query=De+Sa%2C+C">Christopher De Sa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Post-training quantization (PTQ) reduces the memory footprint of LLMs by
quantizing their weights to low-precision. In this work, we introduce QuIP#, a
weight-only PTQ method that achieves state-of-the-art results in extreme
compression regimes ($\le$ 4 bits per weight) using three novel techniques.
First, QuIP# improves the incoherence processing from QuIP by using the
randomized Hadamard transform, which is faster and has better theoretical
properties. Second, QuIP# uses vector quantization techniques to take advantage
of the ball-shaped sub-Gaussian distribution that incoherent weights possess:
specifically, we introduce a set of hardware-efficient codebooks based on the
highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit
ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original
model. Our experiments show that QuIP# outperforms existing PTQ methods,
enables new behaviors in PTQ scaling, and supports fast inference.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04398" title="Abstract">arXiv:2402.04398</a> [<a href="/pdf/2402.04398" title="Download PDF">pdf</a>, <a href="/format/2402.04398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Time Series under Temporal Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+S">Sujay Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Gerych%2C+W">Walter Gerych</a>, 
<a href="/search/cs?searchtype=author&query=Tonekaboni%2C+S">Sana Tonekaboni</a>, 
<a href="/search/cs?searchtype=author&query=Goldenberg%2C+A">Anna Goldenberg</a>, 
<a href="/search/cs?searchtype=author&query=Ustun%2C+B">Berk Ustun</a>, 
<a href="/search/cs?searchtype=author&query=Hartvigsen%2C+T">Thomas Hartvigsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Many sequential classification tasks are affected by label noise that varies
over time. Such noise can cause label quality to improve, worsen, or
periodically change over time. We first propose and formalize temporal label
noise, an unstudied problem for sequential classification of time series. In
this setting, multiple labels are recorded in sequence while being corrupted by
a time-dependent noise function. We first demonstrate the importance of
modelling the temporal nature of the label noise function and how existing
methods will consistently underperform. We then propose methods that can train
noise-tolerant classifiers by estimating the temporal label noise function
directly from data. We show that our methods lead to state-of-the-art
performance in the presence of diverse temporal label noise functions using
real and synthetic data.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04399" title="Abstract">arXiv:2402.04399</a> [<a href="/pdf/2402.04399" title="Download PDF">pdf</a>, <a href="/ps/2402.04399" title="Download PostScript">ps</a>, <a href="/format/2402.04399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Repeated Auction Model for Load-Aware Dynamic Resource Allocation in  Multi-Access Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habiba%2C+U">Ummy Habiba</a>, 
<a href="/search/cs?searchtype=author&query=Maghsudi%2C+S">Setareh Maghsudi</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+E">Ekram Hossain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Mobile Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Multi-access edge computing (MEC) is one of the enabling technologies for
high-performance computing at the edge of the 6 G networks, supporting high
data rates and ultra-low service latency. Although MEC is a remedy to meet the
growing demand for computation-intensive applications, the scarcity of
resources at the MEC servers degrades its performance. Hence, effective
resource management is essential; nevertheless, state-of-the-art research lacks
efficient economic models to support the exponential growth of the MEC-enabled
applications market. We focus on designing a MEC offloading service market
based on a repeated auction model with multiple resource sellers (e.g., network
operators and service providers) that compete to sell their computing resources
to the offloading users. We design a computationally-efficient modified
Generalized Second Price (GSP)-based algorithm that decides on pricing and
resource allocation by considering the dynamic offloading requests arrival and
the servers' computational workloads. Besides, we propose adaptive
best-response bidding strategies for the resource sellers, satisfying the
symmetric Nash equilibrium (SNE) and individual rationality properties.
Finally, via intensive numerical results, we show the effectiveness of our
proposed resource allocation mechanism.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04400" title="Abstract">arXiv:2402.04400</a> [<a href="/pdf/2402.04400" title="Download PDF">pdf</a>, <a href="/format/2402.04400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEHR-GPT: Generating Electronic Health Records with Chronological  Patient Timelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Chao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinzhuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pavinkurve%2C+N+P">Nishanth Parameshwar Pavinkurve</a>, 
<a href="/search/cs?searchtype=author&query=Kalluri%2C+K+S">Krishna S. Kalluri</a>, 
<a href="/search/cs?searchtype=author&query=Minto%2C+E+L">Elise L. Minto</a>, 
<a href="/search/cs?searchtype=author&query=Patterson%2C+J">Jason Patterson</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hripcsak%2C+G">George Hripcsak</a>, 
<a href="/search/cs?searchtype=author&query=Elhadad%2C+N">No&#xe9;mie Elhadad</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+K">Karthik Natarajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in
advancing healthcare applications and machine learning models, particularly for
researchers without direct access to healthcare data. Although existing
methods, like rule-based approaches and generative adversarial networks (GANs),
generate synthetic data that resembles real-world EHR data, these methods often
use a tabular format, disregarding temporal dependencies in patient histories
and limiting data replication. Recently, there has been a growing interest in
leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables
applications like disease progression analysis, population estimation,
counterfactual reasoning, and synthetic data generation. In this work, we focus
on synthetic data generation and demonstrate the capability of training a GPT
model using a particular patient representation derived from CEHR-BERT,
enabling us to generate patient sequences that can be seamlessly converted to
the Observational Medical Outcomes Partnership (OMOP) data format.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04401" title="Abstract">arXiv:2402.04401</a> [<a href="/pdf/2402.04401" title="Download PDF">pdf</a>, <a href="/format/2402.04401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Democratizing Large Language Models via Personalized Parameter-Efficient  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingkai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bing Yin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Personalization in large language models (LLMs) is increasingly important,
aiming to align LLM's interactions, content, and recommendations with
individual user preferences. Recent advances in LLM personalization have
spotlighted effective prompt design, by enriching user queries with
non-parametric knowledge through behavior history retrieval and textual
profiles. However, these approaches were limited due to a lack of model
ownership, resulting in constrained customization and privacy issues. Moreover,
they often failed to accurately capture user behavior patterns, especially in
cases where user data were complex and dynamic. To address these shortcomings,
we introduce One PEFT Per User (OPPU), which employs personalized
parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior
patterns and preferences. By plugging in users' personal PEFT parameters, they
can own and use their LLMs personally. OPPU integrates parametric user
knowledge in the personal PEFT parameters with the non-parametric knowledge
acquired through retrieval and profile. This integration adapts individual LLMs
to user behavior shifts. Experimental results demonstrate that OPPU
significantly outperforms existing prompt-based methods across seven diverse
tasks in the LaMP benchmark. Further in-depth studies reveal OPPU's enhanced
capabilities in handling user behavior shifts, modeling users at different
active levels, maintaining robustness across various user history formats, and
displaying versatility with different PEFT methods.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04403" title="Abstract">arXiv:2402.04403</a> [<a href="/pdf/2402.04403" title="Download PDF">pdf</a>, <a href="/format/2402.04403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-Parallel Graph Encoder Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lubonja%2C+A">Ariel Lubonja</a> (1), 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Cencheng Shen</a> (2), 
<a href="/search/cs?searchtype=author&query=Priebe%2C+C">Carey Priebe</a> (1), 
<a href="/search/cs?searchtype=author&query=Burns%2C+R">Randal Burns</a> (1) ((1) Johns Hopkins University, (2) University of Delaware)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">New algorithms for embedding graphs have reduced the asymptotic complexity of
finding low-dimensional representations. One-Hot Graph Encoder Embedding (GEE)
uses a single, linear pass over edges and produces an embedding that converges
asymptotically to the spectral embedding. The scaling and performance benefits
of this approach have been limited by a serial implementation in an interpreted
language. We refactor GEE into a parallel program in the Ligra graph engine
that maps functions over the edges of the graph and uses lock-free atomic
instrutions to prevent data races. On a graph with 1.8B edges, this results in
a 500 times speedup over the original implementation and a 17 times speedup
over a just-in-time compiled version.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04405" title="Abstract">arXiv:2402.04405</a> [<a href="/pdf/2402.04405" title="Download PDF">pdf</a>, <a href="/ps/2402.04405" title="Download PostScript">ps</a>, <a href="/format/2402.04405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable domain knowledge enhanced machine learning framework on  axial capacity prediction of circular CFST columns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhigang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Kondo%2C+G">Gen Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peipeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal Research Article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This study introduces a novel machine learning framework, integrating domain
knowledge, to accurately predict the bearing capacity of CFSTs, bridging the
gap between traditional engineering and machine learning techniques. Utilizing
a comprehensive database of 2621 experimental data points on CFSTs, we
developed a Domain Knowledge Enhanced Neural Network (DKNN) model. This model
incorporates advanced feature engineering techniques, including Pearson
correlation, XGBoost, and Random tree algorithms. The DKNN model demonstrated a
marked improvement in prediction accuracy, with a Mean Absolute Percentage
Error (MAPE) reduction of over 50% compared to existing models. Its robustness
was confirmed through extensive performance assessments, maintaining high
accuracy even in noisy environments. Furthermore, sensitivity and SHAP analysis
were conducted to assess the contribution of each effective parameter to axial
load capacity and propose design recommendations for the diameter of
cross-section, material strength range and material combination. This research
advances CFST predictive modelling, showcasing the potential of integrating
machine learning with domain expertise in structural engineering. The DKNN
model sets a new benchmark for accuracy and reliability in the field.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04407" title="Abstract">arXiv:2402.04407</a> [<a href="/pdf/2402.04407" title="Download PDF">pdf</a>, <a href="/ps/2402.04407" title="Download PostScript">ps</a>, <a href="/format/2402.04407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Lower Bounds on the Manifold Widths of Sobolev and Besov Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Siegel%2C+J+W">Jonathan W. Siegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the problem of determining the asymptotics of the manifold
$n$-widths of Sobolev and Besov spaces with error measured in the $L_p$-norm.
The manifold widths control how efficiently these spaces can be approximated by
general non-linear parametric methods with the restriction that the parameter
selection and parameterization maps must be continuous. Existing upper and
lower bounds only match when the Sobolev or Besov smoothness index $q$
satisfies $q\leq p$. We close this gap and extend the existing lower bounds to
all $1\leq p,q\leq \infty$. In the process, we show that the Bernstein widths,
which are typically used to lower bound the manifold widths, may decay
asymptotically slower than the manifold widths.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04408" title="Abstract">arXiv:2402.04408</a> [<a href="/pdf/2402.04408" title="Download PDF">pdf</a>, <a href="/ps/2402.04408" title="Download PostScript">ps</a>, <a href="/format/2402.04408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection Transformer for Teeth Detection, Segmentation, and Numbering  in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadi%2C+H">Hocine Kadi</a>, 
<a href="/search/cs?searchtype=author&query=Sourget%2C+T">Th&#xe9;o Sourget</a>, 
<a href="/search/cs?searchtype=author&query=Kawczynski%2C+M">Marzena Kawczynski</a>, 
<a href="/search/cs?searchtype=author&query=Bendjama%2C+S">Sara Bendjama</a>, 
<a href="/search/cs?searchtype=author&query=Grollemund%2C+B">Bruno Grollemund</a>, 
<a href="/search/cs?searchtype=author&query=Bloch-Zupan%2C+A">Agn&#xe8;s Bloch-Zupan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we focused on deep learning image processing in the context of
oral rare diseases, which pose challenges due to limited data availability. A
crucial step involves teeth detection, segmentation and numbering in panoramic
radiographs. To this end, we used a dataset consisting of 156 panoramic
radiographs from individuals with rare oral diseases and labeled by experts. We
trained the Detection Transformer (DETR) neural network for teeth detection,
segmentation, and numbering the 52 teeth classes. In addition, we used data
augmentation techniques, including geometric transformations. Finally, we
generated new panoramic images using inpainting techniques with stable
diffusion, by removing teeth from a panoramic radiograph and integrating teeth
into it. The results showed a mAP exceeding 0,69 for DETR without data
augmentation. The mAP was improved to 0,82 when data augmentation techniques
are used. Furthermore, we observed promising performances when using new
panoramic radiographs generated with inpainting technique, with mAP of 0,76.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04409" title="Abstract">arXiv:2402.04409</a> [<a href="/pdf/2402.04409" title="Download PDF">pdf</a>, <a href="/format/2402.04409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fair, Robust and Efficient Client Contribution Evaluation in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meiying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ebron%2C+S">Sheldon Ebron</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The performance of clients in Federated Learning (FL) can vary due to various
reasons. Assessing the contributions of each client is crucial for client
selection and compensation. It is challenging because clients often have
non-independent and identically distributed (non-iid) data, leading to
potentially noisy or divergent updates. The risk of malicious clients amplifies
the challenge especially when there's no access to clients' local data or a
benchmark root dataset. In this paper, we introduce a novel method called Fair,
Robust, and Efficient Client Assessment (FRECA) for quantifying client
contributions in FL. FRECA employs a framework called FedTruth to estimate the
global model's ground truth update, balancing contributions from all clients
while filtering out impacts from malicious ones. This approach is robust
against Byzantine attacks and incorporates a Byzantine-resilient aggregation
algorithm. FRECA is also efficient, as it operates solely on local model
updates and requires no validation operations or datasets. Our experimental
results show that FRECA can accurately and efficiently quantify client
contributions in a robust manner.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04411" title="Abstract">arXiv:2402.04411</a> [<a href="/pdf/2402.04411" title="Download PDF">pdf</a>, <a href="/format/2402.04411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chatbot Meets Pipeline: Augment Large Language Model with Definite  Finite Automaton
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiyou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces the Definite Finite Automaton augmented large language
model (DFA-LLM), a novel framework designed to enhance the capabilities of
conversational agents using large language models (LLMs). Traditional LLMs face
challenges in generating regulated and compliant responses in special scenarios
with predetermined response guidelines, like emotional support and customer
service. Our framework addresses these challenges by embedding a Definite
Finite Automaton (DFA), learned from training dialogues, within the LLM. This
structured approach enables the LLM to adhere to a deterministic response
pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable
structure through human-readable DFA, context-aware retrieval for responses in
conversations, and plug-and-play compatibility with existing LLMs. Extensive
benchmarks validate DFA-LLM's effectiveness, indicating its potential as a
valuable contribution to the conversational agent.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04412" title="Abstract">arXiv:2402.04412</a> [<a href="/pdf/2402.04412" title="Download PDF">pdf</a>, <a href="/format/2402.04412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The VampPrior Mixture Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stirn%2C+A">Andrew Stirn</a>, 
<a href="/search/cs?searchtype=author&query=Knowles%2C+D+A">David A. Knowles</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Current clustering priors for deep latent variable models (DLVMs) require
defining the number of clusters a-priori and are susceptible to poor
initializations. Addressing these deficiencies could greatly benefit deep
learning-based scRNA-seq analysis by performing integration and clustering
simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a
Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture
Model (VMM), a novel prior for DLVMs. We propose an inference procedure that
alternates between variational inference and Empirical Bayes to cleanly
distinguish variational and prior parameters. Using the VMM in a Variational
Autoencoder attains highly competitive clustering performance on benchmark
datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration
method, with the VMM significantly improves its performance and automatically
arranges cells into biologically meaningful clusters.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04416" title="Abstract">arXiv:2402.04416</a> [<a href="/pdf/2402.04416" title="Download PDF">pdf</a>, <a href="/format/2402.04416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data Centric Approach for Unsupervised Domain Generalization via  Retrieval from Web Scale Multimodal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Christopher Liao</a>, 
<a href="/search/cs?searchtype=author&query=Tsiligkaridis%2C+T">Theodoros Tsiligkaridis</a>, 
<a href="/search/cs?searchtype=author&query=Kulis%2C+B">Brian Kulis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Domain generalization (DG) is an important problem that learns a model that
can generalize to unseen test domains leveraging one or more source domains,
under the assumption of shared label spaces. However, most DG methods assume
access to abundant source data in the target label space, a requirement that
proves overly stringent for numerous real-world applications, where acquiring
the same label space as the target task is prohibitively expensive. For this
setting, we tackle the multimodal version of the unsupervised domain
generalization (UDG) problem, which uses a large task-agnostic unlabeled source
dataset, such as LAION-2B during finetuning. Our framework does not explicitly
assume any relationship between the source dataset and target task. Instead, it
relies only on the premise that the source dataset can be efficiently searched
in a joint vision-language space. For this multimodal UDG setting, we propose a
novel method to build a small ($&lt;$100K) subset of the source data in three
simple steps: (1) diversified retrieval using label names as queries, (2) rank
pseudo-labeling, and (3) clustering to find representative samples. To
demonstrate the value of studying the multimodal UDG problem, we compare our
results against state-of-the-art source-free DG and zero-shot (ZS) methods on
their respective benchmarks and show up to 10% improvement in accuracy on 20
diverse target datasets. Additionally, our multi-stage dataset construction
method achieves 3% improvement on average over nearest neighbors retrieval.
Code is available: https://github.com/Chris210634/mudg
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04417" title="Abstract">arXiv:2402.04417</a> [<a href="/pdf/2402.04417" title="Download PDF">pdf</a>, <a href="/ps/2402.04417" title="Download PostScript">ps</a>, <a href="/format/2402.04417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengfan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Klabjan%2C+D">Diego Klabjan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We study a robust multi-agent multi-armed bandit problem where multiple
clients or participants are distributed on a fully decentralized blockchain,
with the possibility of some being malicious. The rewards of arms are
homogeneous among the clients, following time-invariant stochastic
distributions that are revealed to the participants only when the system is
secure enough. The system's objective is to efficiently ensure the cumulative
rewards gained by the honest participants. To this end and to the best of our
knowledge, we are the first to incorporate advanced techniques from
blockchains, as well as novel mechanisms, into the system to design optimal
strategies for honest participants. This allows various malicious behaviors and
the maintenance of participant privacy. More specifically, we randomly select a
pool of validators who have access to all participants, design a brand-new
consensus mechanism based on digital signatures for these validators, invent a
UCB-based strategy that requires less information from participants through
secure multi-party computation, and design the chain-participant interaction
and an incentive mechanism to encourage participants' participation. Notably,
we are the first to prove the theoretical guarantee of the proposed algorithms
by regret analyses in the context of optimality in blockchains. Unlike existing
work that integrates blockchains with learning problems such as federated
learning which mainly focuses on numerical optimality, we demonstrate that the
regret of honest participants is upper bounded by $log{T}$. This is consistent
with the multi-agent multi-armed bandit problem without malicious participants
and the robust multi-agent multi-armed bandit problem with purely Byzantine
attacks.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04418" title="Abstract">arXiv:2402.04418</a> [<a href="/pdf/2402.04418" title="Download PDF">pdf</a>, <a href="/format/2402.04418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Offline and Online Learning-Based Algorithms for Multirotor  UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%B6nmez%2C+S">Serhat S&#xf6;nmez</a>, 
<a href="/search/cs?searchtype=author&query=Rutherford%2C+M+J">Matthew J. Rutherford</a>, 
<a href="/search/cs?searchtype=author&query=Valavanis%2C+K+P">Kimon P. Valavanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures, 4 tables, Survey Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Multirotor UAVs are used for a wide spectrum of civilian and public domain
applications. Navigation controllers endowed with different attributes and
onboard sensor suites enable multirotor autonomous or semi-autonomous, safe
flight, operation, and functionality under nominal and detrimental conditions
and external disturbances, even when flying in uncertain and dynamically
changing environments. During the last decade, given the
faster-than-exponential increase of available computational power, different
learning-based algorithms have been derived, implemented, and tested to
navigate and control, among other systems, multirotor UAVs. Learning algorithms
have been, and are used to derive data-driven based models, to identify
parameters, to track objects, to develop navigation controllers, and to learn
the environment in which multirotors operate. Learning algorithms combined with
model-based control techniques have been proven beneficial when applied to
multirotors. This survey summarizes published research since 2015, dividing
algorithms, techniques, and methodologies into offline and online learning
categories, and then, further classifying them into machine learning, deep
learning, and reinforcement learning sub-categories. An integral part and focus
of this survey are on online learning algorithms as applied to multirotors with
the aim to register the type of learning techniques that are either hard or
almost hard real-time implementable, as well as to understand what information
is learned, why, and how, and how fast. The outcome of the survey offers a
clear understanding of the recent state-of-the-art and of the type and kind of
learning-based algorithms that may be implemented, tested, and executed in
real-time.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04420" title="Abstract">arXiv:2402.04420</a> [<a href="/pdf/2402.04420" title="Download PDF">pdf</a>, <a href="/format/2402.04420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring machine learning harms from stereotypes: requires  understanding who is being harmed by which errors in what ways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Angelina Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xuechunzi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Barocas%2C+S">Solon Barocas</a>, 
<a href="/search/cs?searchtype=author&query=Blodgett%2C+S+L">Su Lin Blodgett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> earlier draft non-archival at EAAMO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As machine learning applications proliferate, we need an understanding of
their potential for harm. However, current fairness metrics are rarely grounded
in human psychological experiences of harm. Drawing on the social psychology of
stereotypes, we use a case study of gender stereotypes in image search to
examine how people react to machine learning errors. First, we use survey
studies to show that not all machine learning errors reflect stereotypes nor
are equally harmful. Then, in experimental studies we randomly expose
participants to stereotype-reinforcing, -violating, and -neutral machine
learning errors. We find stereotype-reinforcing errors induce more
experientially (i.e., subjectively) harmful experiences, while having minimal
changes to cognitive beliefs, attitudes, or behaviors. This experiential harm
impacts women more than men. However, certain stereotype-violating errors are
more experientially harmful for men, potentially due to perceived threats to
masculinity. We conclude that harm cannot be the sole guide in fairness
mitigation, and propose a nuanced perspective depending on who is experiencing
what harm and why.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04421" title="Abstract">arXiv:2402.04421</a> [<a href="/pdf/2402.04421" title="Download PDF">pdf</a>, <a href="/format/2402.04421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying Vulnerable Code Entities in R
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zixiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+M+M">Millon Madhur Das</a>, 
<a href="/search/cs?searchtype=author&query=Fard%2C+F+H">Fatemeh H. Fard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, and 2 tables. to be published in ICPC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained Code Language Models (Code-PLMs) have shown many advancements and
achieved state-of-the-art results for many software engineering tasks in the
past few years. These models are mainly targeted for popular programming
languages such as Java and Python, leaving out many other ones like R. Though R
has a wide community of developers and users, there is little known about the
applicability of Code-PLMs for R. In this preliminary study, we aim to
investigate the vulnerability of Code-PLMs for code entities in R. For this
purpose, we use an R dataset of code and comment pairs and then apply
CodeAttack, a black-box attack model that uses the structure of code to
generate adversarial code samples. We investigate how the model can attack
different entities in R. This is the first step towards understanding the
importance of R token types, compared to popular programming languages (e.g.,
Java). We limit our study to code summarization. Our results show that the most
vulnerable code entity is the identifier, followed by some syntax tokens
specific to R. The results can shed light on the importance of token types and
help in developing models for code summarization and method name prediction for
the R language.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04423" title="Abstract">arXiv:2402.04423</a> [<a href="/pdf/2402.04423" title="Download PDF">pdf</a>, <a href="/format/2402.04423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Pipe System for a Shipyard 4.0
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/eess?searchtype=author&query=Noceda-Davila%2C+D">Diego Noceda-Davila</a>, 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez-Caram%C3%A9s%2C+T+M">Tiago M. Fern&#xe1;ndez-Caram&#xe9;s</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%ADaz-Bouza%2C+M+A">Manuel A. D&#xed;az-Bouza</a>, 
<a href="/search/eess?searchtype=author&query=Vilar-Montesinos%2C+M">Miguel Vilar-Montesinos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 25 figures, accepted version of Sensors journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2016, 16(12), 2186
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">As a result of the progressive implantation of the Industry 4.0 paradigm,
many industries are experimenting a revolution that shipyards cannot ignore.
Therefore, the application of the principles of Industry 4.0 to shipyards are
leading to the creation of Shipyards 4.0. Due to this, Navantia, one of the 10
largest shipbuilders in the world, is updating its whole inner workings to keep
up with the near-future challenges that a Shipyard 4.0 will have to face. Such
challenges can be divided into three groups: the vertical integration of
production systems, the horizontal integration of a new generation of value
creation networks, and the re-engineering of the entire production chain,
making changes that affect the entire life cycle of each piece of a ship.
Pipes, which exist in a huge number and varied typology on a ship, are one of
the key pieces, and its monitoring constitutes a prospective cyber-physical
system. Their improved identification, traceability, and indoor location, from
production and through their life, can enhance shipyard productivity and
safety. In order to perform such tasks, this article first conducts a thorough
analysis of the shipyard environment. From this analysis, the essential
hardware and software technical requirements are determined. Next, the concept
of smart pipe is presented and defined as an object able to transmit signals
periodically that allows for providing enhanced services in a shipyard. In
order to build a smart pipe system, different technologies are selected and
evaluated, concluding that passive and active RFID are currently the most
appropriate technologies to create it. Furthermore, some promising indoor
positioning results obtained in a pipe workshop are presented, showing that
multi-antenna algorithms and Kalman filtering can help to stabilize Received
Signal Strength (RSS) and improve the overall accuracy of the system.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04424" title="Abstract">arXiv:2402.04424</a> [<a href="/pdf/2402.04424" title="Download PDF">pdf</a>, <a href="/format/2402.04424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Binary Signaling for a Two Sensor Gaussian MAC Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sardellitti%2C+L">Luca Sardellitti</a>, 
<a href="/search/cs?searchtype=author&query=Takahara%2C+G">Glen Takahara</a>, 
<a href="/search/cs?searchtype=author&query=Alajaji%2C+F">Fady Alajaji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider a two sensor distributed detection system transmitting a binary
non-uniform source over a Gaussian multiple access channel (MAC). We model the
network via binary sensors whose outputs are generated by binary symmetric
channels of different noise levels. We prove an optimal one dimensional
constellation design under individual sensor power constraints which minimizes
the error probability of detecting the source. Three distinct cases arise for
this optimization based on the parameters in the problem setup. In the most
notable case (Case III), the optimal signaling design is to not necessarily use
all of the power allocated to the more noisy sensor (with less correlation to
the source). We compare the error performance of the optimal one dimensional
constellation to orthogonal signaling. The results show that the optimal one
dimensional constellation achieves lower error probability than using
orthogonal channels.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04431" title="Abstract">arXiv:2402.04431</a> [<a href="/pdf/2402.04431" title="Download PDF">pdf</a>, <a href="/ps/2402.04431" title="Download PostScript">ps</a>, <a href="/format/2402.04431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for  Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sedaghatgoo%2C+A">Ali Sedaghatgoo</a>, 
<a href="/search/cs?searchtype=author&query=Hajisadeghi%2C+A+M">Amir M. Hajisadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Momtazpour%2C+M">Mahmoud Momtazpour</a>, 
<a href="/search/cs?searchtype=author&query=Bagherzadeh%2C+N">Nader Bagherzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The Convolutional Neural Network (CNN) has emerged as a powerful and
versatile tool for artificial intelligence (AI) applications. Conventional
computing architectures face challenges in meeting the demanding processing
requirements of compute-intensive CNN applications, as they suffer from limited
throughput and low utilization. To this end, specialized accelerators have been
developed to speed up CNN computations. However, as we demonstrate in this
paper via extensive design space exploration, different neural network models
have different characteristics, which calls for different accelerator
architectures and configurations to match their computing demand. We show that
a one-size-fits-all fixed architecture does not guarantee optimal
power/energy/performance trade-off. To overcome this challenge, this paper
proposes ARMAN, a novel reconfigurable systolic-array-based accelerator
architecture based on Monolithic 3D (M3D) technology for CNN inference. The
proposed accelerator offers the flexibility to reconfigure among different
scale-up or scale-out arrangements depending on the neural network structure,
providing the optimal trade-off across power, energy, and performance for
various neural network models. We demonstrate the effectiveness of our approach
through evaluations of multiple benchmarks. The results demonstrate that the
proposed accelerator exhibits up to 2x, 2.24x, 1.48x, and 2x improvements in
terms of execution cycles, power, energy, and EDP respectively, over the
non-configurable architecture.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04435" title="Abstract">arXiv:2402.04435</a> [<a href="/pdf/2402.04435" title="Download PDF">pdf</a>, <a href="/format/2402.04435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep  Intellectual Property Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+E">Enyan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Minhua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretraining on Graph Neural Networks (GNNs) has shown great power in
facilitating various downstream tasks. As pretraining generally requires huge
amount of data and computational resources, the pretrained GNNs are high-value
Intellectual Properties (IP) of the legitimate owner. However, adversaries may
illegally copy and deploy the pretrained GNN models for their downstream tasks.
Though initial efforts have been made to watermark GNN classifiers for IP
protection, these methods require the target classification task for
watermarking, and thus are not applicable to self-supervised pretraining of GNN
models. Hence, in this work, we propose a novel framework named PreGIP to
watermark the pretraining of GNN encoder for IP protection while maintain the
high-quality of the embedding space. PreGIP incorporates a task-free
watermarking loss to watermark the embedding space of pretrained GNN encoder. A
finetuning-resistant watermark injection is further deployed. Theoretical
analysis and extensive experiments show the effectiveness of {\method} in IP
protection and maintaining high-performance for downstream tasks.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04437" title="Abstract">arXiv:2402.04437</a> [<a href="/pdf/2402.04437" title="Download PDF">pdf</a>, <a href="/format/2402.04437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Entity Extraction Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mikaelyan%2C+L">Liana Mikaelyan</a>, 
<a href="/search/cs?searchtype=author&query=Meulemans%2C+A">Alexander Meulemans</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hensman%2C+J">James Hensman</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+B">Bhaskar Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in machine learning have significantly impacted the field of
information extraction, with Large Language Models (LLMs) playing a pivotal
role in extracting structured information from unstructured text. This paper
explores the challenges and limitations of current methodologies in structured
entity extraction and introduces a novel approach to address these issues. We
contribute to the field by first introducing and formalizing the task of
Structured Entity Extraction (SEE), followed by proposing Approximate Entity
Set OverlaP (AESOP) Metric designed to appropriately assess model performance
on this task. Later, we propose a new model that harnesses the power of LLMs
for enhanced effectiveness and efficiency through decomposing the entire
extraction task into multiple stages. Quantitative evaluation and human
side-by-side evaluation confirm that our model outperforms baselines, offering
promising directions for future advancements in structured entity extraction.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04438" title="Abstract">arXiv:2402.04438</a> [<a href="/pdf/2402.04438" title="Download PDF">pdf</a>, <a href="/format/2402.04438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The domino problem is decidable for robust tilesets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aubrun%2C+N">Nathalie Aubrun</a>, 
<a href="/search/cs?searchtype=author&query=Blanc%2C+M">Manon Blanc</a>, 
<a href="/search/cs?searchtype=author&query=Bournez%2C+O">Olivier Bournez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Logic in Computer Science (cs.LO); Combinatorics (math.CO); Dynamical Systems (math.DS); Logic (math.LO)

</div>
<p class="mathjax">One of the most fundamental problems in tiling theory is the domino problem:
given a set of tiles and tiling rules, decide if there exists a way to tile the
plane using copies of tiles and following their rules. The problem is known to
be undecidable in general and even for sets of Wang tiles, which are unit
square tiles wearing colours on their edges which can be assembled provided
they share the same colour on their common edge, as proven by Berger in the
1960s. In this paper, we focus on Wang tilesets. We prove that the domino
problem is decidable for robust tilesets, i.e. tilesets that either cannot tile
the plane or can but, if so, satisfy some particular invariant provably. We
establish that several famous tilesets considered in the literature are robust.
We give arguments that this is true for all tilesets unless they are produced
from non-robust Turing machines: a Turing machine is said to be non-robust if
it does not halt and furthermore does so non-provably.
<br />As a side effect of our work, we provide a sound and relatively complete
method for proving that a tileset can tile the plane.
<br />Our analysis also provides explanations for the observed similarities between
proofs in the literature for various tilesets, as well as of phenomena that
have been observed experimentally in the systematic study of tilesets using
computer methods.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04440" title="Abstract">arXiv:2402.04440</a> [<a href="/pdf/2402.04440" title="Download PDF">pdf</a>, <a href="/format/2402.04440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring higher-order neural network node interactions with total  correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerby%2C+T">Thomas Kerby</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+T">Teresa White</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+K">Kevin Moon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In domains such as ecological systems, collaborations, and the human brain
the variables interact in complex ways. Yet accurately characterizing
higher-order variable interactions (HOIs) is a difficult problem that is
further exacerbated when the HOIs change across the data. To solve this problem
we propose a new method called Local Correlation Explanation (CorEx) to capture
HOIs at a local scale by first clustering data points based on their proximity
on the data manifold. We then use a multivariate version of the mutual
information called the total correlation, to construct a latent factor
representation of the data within each cluster to learn the local HOIs. We use
Local CorEx to explore HOIs in synthetic and real world data to extract hidden
insights about the data structure. Lastly, we demonstrate Local CorEx's
suitability to explore and interpret the inner workings of trained neural
networks.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04442" title="Abstract">arXiv:2402.04442</a> [<a href="/pdf/2402.04442" title="Download PDF">pdf</a>, <a href="/format/2402.04442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Embeddings for One-Shot Classification of Doctor-AI  Consultations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojo%2C+O+E">Olumide Ebenezer Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Adebanji%2C+O+O">Olaronke Oluwayemisi Adebanji</a>, 
<a href="/search/cs?searchtype=author&query=Gelbukh%2C+A">Alexander Gelbukh</a>, 
<a href="/search/cs?searchtype=author&query=Calvo%2C+H">Hiram Calvo</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+A">Anna Feldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Effective communication between healthcare providers and patients is crucial
to providing high-quality patient care. In this work, we investigate how
Doctor-written and AI-generated texts in healthcare consultations can be
classified using state-of-the-art embeddings and one-shot classification
systems. By analyzing embeddings such as bag-of-words, character n-grams,
Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our
one-shot classification systems capture semantic information within medical
consultations. Results show that the embeddings are capable of capturing
semantic features from text in a reliable and adaptable manner. Overall,
Word2Vec, GloVe and Character n-grams embeddings performed well, indicating
their suitability for modeling targeted to this task. GPT2 embedding also shows
notable performance, indicating its suitability for models tailored to this
task as well. Our machine learning architectures significantly improved the
quality of health conversations when training data are scarce, improving
communication between patients and healthcare providers.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04444" title="Abstract">arXiv:2402.04444</a> [<a href="/pdf/2402.04444" title="Download PDF">pdf</a>, <a href="/format/2402.04444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk  Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yuqi Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zamzam%2C+A">Ahmed Zamzam</a>, 
<a href="/search/eess?searchtype=author&query=Bernstein%2C+A">Andrey Bernstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Increasing amount of wildfires in recent years consistently challenges the
safe and reliable operations of power systems. To prevent power lines and other
electrical components from causing wildfires under extreme conditions, electric
utilities often deploy public safety power shutoffs (PSPS) to mitigate the
wildfire risks therein. Although PSPS are effective countermeasures against
wildfires, uncoordinated strategies can cause disruptions in electricity supply
and even lead to cascading failures. Meanwhile, it is extremely important to
consider mitigating biased decisions on different communities and populations
during the implementation of shutoff actions. In this work, we primarily focus
on the dynamic reconfiguration problem of networked microgrids with distributed
energy resources. In particular, we formulate a rolling horizon optimization
problem allowing for flexible network reconfiguration at each time interval to
mitigate wildfire risks. To promote equity and fairness during the span of
shutoffs, we further enforce a range of constraints associated with load
shedding to discourage disproportionate impact on individual load blocks.
Numerical studies on the modified IEEE 13-bus system and a larger-sized
Smart-DS system demonstrate the performance of the proposed algorithm towards
more equitable power shutoff operations.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04447" title="Abstract">arXiv:2402.04447</a> [<a href="/pdf/2402.04447" title="Download PDF">pdf</a>, <a href="/format/2402.04447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Spectrum Coexistence of Terrestrial Beyond 5G Networks in  Satellite Bands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niloy%2C+T+S+R">Ta Seen Reaz Niloy</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+Z">Zoheb Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+R">Rob Smith</a>, 
<a href="/search/cs?searchtype=author&query=Anapana%2C+V+R">Vikram R. Anapana</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Spectrum sharing between terrestrial 5G and incumbent networks in the
satellite bands presents a promising avenue to satisfy the ever-increasing
bandwidth demand of the next-generation wireless networks. However, protecting
incumbent operations from harmful interference poses a fundamental challenge in
accommodating terrestrial broadband cellular networks in the satellite bands.
State-of-the-art spectrum-sharing policies usually consider several worst-case
assumptions and ignore site-specific contextual factors in making
spectrum-sharing decisions, and thus, often results in under-utilization of the
shared band for the secondary licensees. To address such limitations, this
paper introduces CAT3S (Context-Aware Terrestrial-Satellite Spectrum Sharing)
framework that empowers the coexisting terrestrial 5G network to maximize
utilization of the shared satellite band without creating harmful interference
to the incumbent links by exploiting the contextual factors. CAT3S consists of
the following two components: (i) context-acquisition unit to collect and
process essential contextual information for spectrum sharing and (ii)
context-aware base station (BS) control unit to optimize the set of operational
BSs and their operation parameters (i.e., transmit power and active beams per
sector). To evaluate the performance of the CAT3S, a realistic spectrum
coexistence case study over the 12 GHz band is considered. Experiment results
demonstrate that the proposed CAT3S achieves notably higher spectrum
utilization than state-of-the-art spectrum-sharing policies in different
weather contexts.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04448" title="Abstract">arXiv:2402.04448</a> [<a href="/pdf/2402.04448" title="Download PDF">pdf</a>, <a href="/format/2402.04448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Failure Analysis in Next-Generation Critical Cellular Communication  Infrastructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bi%2C+S">Siguo Bi</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+S">Shuyan Hu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+W">Wei Ni</a>, 
<a href="/search/eess?searchtype=author&query=Hossain%2C+E">Ekram Hossain</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The advent of communication technologies marks a transformative phase in
critical infrastructure construction, where the meticulous analysis of failures
becomes paramount in achieving the fundamental objectives of continuity,
security, and availability. This survey enriches the discourse on failures,
failure analysis, and countermeasures in the context of the next-generation
critical communication infrastructures. Through an exhaustive examination of
existing literature, we discern and categorize prominent research orientations
with focuses on, namely resource depletion, security vulnerabilities, and
system availability concerns. We also analyze constructive countermeasures
tailored to address identified failure scenarios and their prevention.
Furthermore, the survey emphasizes the imperative for standardization in
addressing failures related to Artificial Intelligence (AI) within the ambit of
the sixth-generation (6G) networks, accounting for the forward-looking
perspective for the envisioned intelligence of 6G network architecture. By
identifying new challenges and delineating future research directions, this
survey can help guide stakeholders toward unexplored territories, fostering
innovation and resilience in critical communication infrastructure development
and failure prevention.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04451" title="Abstract">arXiv:2402.04451</a> [<a href="/pdf/2402.04451" title="Download PDF">pdf</a>, <a href="/format/2402.04451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-guided Swarms: Impedance Control-inspired Influence in Virtual  Reality Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barclay%2C+S">Spencer Barclay</a>, 
<a href="/search/cs?searchtype=author&query=Jerath%2C+K">Kshitij Jerath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Prior works in human-swarm interaction (HSI) have sought to guide swarm
behavior towards established objectives, but may be unable to handle specific
scenarios that require finer human supervision, variable autonomy, or
application to large-scale swarms. In this paper, we present an approach that
enables human supervisors to tune the level of swarm control, and guide a large
swarm using an assistive control mechanism that does not significantly restrict
emergent swarm behaviors. We develop this approach in a virtual reality (VR)
environment, using the HTC Vive and Unreal Engine 4 with AirSim plugin. The
novel combination of an impedance control-inspired influence mechanism and a VR
test bed enables and facilitates the rapid design and test iterations to
examine trade-offs between swarming behavior and macroscopic-scale human
influence, while circumventing flight duration limitations associated with
battery-powered small unmanned aerial system (sUAS) systems. The impedance
control-inspired mechanism was tested by a human supervisor to guide a virtual
swarm consisting of 16 sUAS agents. Each test involved moving the swarm's
center of mass through narrow canyons, which were not feasible for a swarm to
traverse autonomously. Results demonstrate that integration of the influence
mechanism enabled the successful manipulation of the macro-scale behavior of
the swarm towards task completion, while maintaining the innate swarming
behavior.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04452" title="Abstract">arXiv:2402.04452</a> [<a href="/pdf/2402.04452" title="Download PDF">pdf</a>, <a href="/format/2402.04452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Computation of Sequential Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graf%2C+M">Moritz Graf</a>, 
<a href="/search/cs?searchtype=author&query=Engesser%2C+T">Thorsten Engesser</a>, 
<a href="/search/cs?searchtype=author&query=Nebel%2C+B">Bernhard Nebel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper at AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The sequential equilibrium is a standard solution concept for extensive-form
games with imperfect information that includes an explicit representation of
the players' beliefs. An assessment consisting of a strategy and a belief is a
sequential equilibrium if it satisfies the properties of sequential rationality
and consistency.
<br />Our main result is that both properties together can be written as a single
finite system of polynomial equations and inequalities. The solutions to this
system are exactly the sequential equilibria of the game. We construct this
system explicitly and describe an implementation that solves it using
cylindrical algebraic decomposition. To write consistency as a finite system of
equations, we need to compute the extreme directions of a set of polyhedral
cones. We propose a modified version of the double description method,
optimized for this specific purpose. To the best of our knowledge, our
implementation is the first to symbolically solve general finite imperfect
information games for sequential equilibria.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04453" title="Abstract">arXiv:2402.04453</a> [<a href="/pdf/2402.04453" title="Download PDF">pdf</a>, <a href="/format/2402.04453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Potential of AutoML for Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vente%2C+T">Tobias Vente</a>, 
<a href="/search/cs?searchtype=author&query=Beel%2C+J">Joeran Beel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Automated Machine Learning (AutoML) has greatly advanced applications of
Machine Learning (ML) including model compression, machine translation, and
computer vision. Recommender Systems (RecSys) can be seen as an application of
ML. Yet, AutoML has found little attention in the RecSys community; nor has
RecSys found notable attention in the AutoML community. Only few and relatively
simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt
AutoML techniques. However, these libraries are based on student projects and
do not offer the features and thorough development of AutoML libraries. We set
out to determine how AutoML libraries perform in the scenario of an
inexperienced user who wants to implement a recommender system. We compared the
predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from
15 libraries, including a mean predictor baseline, on 14 explicit feedback
RecSys datasets. To simulate the perspective of an inexperienced user, the
algorithms were evaluated with default hyperparameters. We found that AutoML
and AutoRecSys libraries performed best. AutoML libraries performed best for
six of the 14 datasets (43%), but it was not always the same AutoML library
performing best. The single-best library was the AutoRecSys library
Auto-Surprise, which performed best on five datasets (36%). On three datasets
(21%), AutoML libraries performed poorly, and RecSys libraries with default
parameters performed best. Although, while obtaining 50% of all placements in
the top five per dataset, RecSys algorithms fall behind AutoML on average. ML
algorithms generally performed the worst.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04454" title="Abstract">arXiv:2402.04454</a> [<a href="/pdf/2402.04454" title="Download PDF">pdf</a>, <a href="/format/2402.04454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving Mobile Cloud Gaming with 5G Standalone Network Telemetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Haoran Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kyle Jamieson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Mobile cloud gaming places the simultaneous demands of high capacity and low
latency on the wireless network, demands that Private and Metropolitan-Area
Standalone 5G networks are poised to meet. However, lacking introspection into
the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope
with the vagaries of the wireless last hop to a mobile client, while 5G network
operators run mostly closed networks, limiting their potential for co-design
with the wider internet and user applications. This paper presents Telesa, a
passive, incrementally-deployable, and independently-deployable Standalone 5G
network telemetry system that streams fine-grained RAN capacity, latency, and
retransmission information to application servers to enable better millisecond
scale, application-level decisions on offered load and bit rate adaptation than
end-to-end latency measurements or end-to-end packet losses currently permit.
We design, implement, and evaluate a Telesa telemetry-enhanced game streaming
platform, demonstrating exact congestion-control that can better adapt game
video bitrate while simultaneously controlling end-to-end latency, thus
maximizing game quality of experience. Our experimental evaluation on a
production 5G Standalone network demonstrates a 178-249% Quality of Experience
improvement versus two state-of-the-art cloud gaming applications.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04457" title="Abstract">arXiv:2402.04457</a> [<a href="/pdf/2402.04457" title="Download PDF">pdf</a>, <a href="/format/2402.04457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliability quality measures for recommender systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bobadilla%2C+J">Jes&#xfa;s Bobadilla</a>, 
<a href="/search/cs?searchtype=author&query=Gutierrez%2C+A">Abraham Gutierrez</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+F">Fernando Ortega</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bo Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Sciences 442-443, 145-157 (2018)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Users want to know the reliability of the recommendations; they do not accept
high predictions if there is no reliability evidence. Recommender systems
should provide reliability values associated with the predictions. Research
into reliability measures requires the existence of simple, plausible and
universal reliability quality measures. Research into recommender system
quality measures has focused on accuracy. Moreover, novelty, serendipity and
diversity have been studied; nevertheless there is an important lack of
research into reliability/confidence quality measures.
<br />This paper proposes a reliability quality prediction measure (RPI) and a
reliability quality recommendation measure (RRI). Both quality measures are
based on the hypothesis that the more suitable a reliability measure is, the
better accuracy results it will provide when applied. These reliability quality
measures show accuracy improvements when appropriated reliability values are
associated with their predictions (i.e. high reliability values associated with
correct predictions or low reliability values associated with incorrect
predictions).
<br />The proposed reliability quality metrics will lead to the design of brand new
recommender system reliability measures. These measures could be applied to
different matrix factorization techniques and to content-based, context-aware
and social recommendation approaches. The recommender system reliability
measures designed could be tested, compared and improved using the proposed
reliability quality metrics.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04464" title="Abstract">arXiv:2402.04464</a> [<a href="/pdf/2402.04464" title="Download PDF">pdf</a>, <a href="/ps/2402.04464" title="Download PostScript">ps</a>, <a href="/format/2402.04464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ten Hard Problems in Artificial Intelligence We Must Get Right
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leech%2C+G">Gavin Leech</a>, 
<a href="/search/cs?searchtype=author&query=Garfinkel%2C+S">Simson Garfinkel</a>, 
<a href="/search/cs?searchtype=author&query=Yagudin%2C+M">Misha Yagudin</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+A">Alexander Briand</a>, 
<a href="/search/cs?searchtype=author&query=Zhuravlev%2C+A">Aleksandr Zhuravlev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 71 + 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We explore the AI2050 "hard problems" that block the promise of AI and cause
AI risks: (1) developing general capabilities of the systems; (2) assuring the
performance of AI systems and their training processes; (3) aligning system
goals with human goals; (4) enabling great applications of AI in real life; (5)
addressing economic disruptions; (6) ensuring the participation of all; (7) at
the same time ensuring socially responsible deployment; (8) addressing any
geopolitical disruptions that AI causes; (9) promoting sound governance of the
technology; and (10) managing the philosophical disruptions for humans living
in the age of AI. For each problem, we outline the area, identify significant
recent work, and suggest ways forward. [Note: this paper reviews literature
through January 2023.]
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04465" title="Abstract">arXiv:2402.04465</a> [<a href="/pdf/2402.04465" title="Download PDF">pdf</a>, <a href="/format/2402.04465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BAdaCost: Multi-class Boosting with Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Baldera%2C+A">Antonio Fern&#xe1;ndez-Baldera</a>, 
<a href="/search/cs?searchtype=author&query=Buenaposada%2C+J+M">Jos&#xe9; M. Buenaposada</a>, 
<a href="/search/cs?searchtype=author&query=Baumela%2C+L">Luis Baumela</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition. Volume 79, July 2018, Pages 467-479
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present BAdaCost, a multi-class cost-sensitive classification algorithm.
It combines a set of cost-sensitive multi-class weak learners to obtain a
strong classification rule within the Boosting framework. To derive the
algorithm we introduce CMEL, a Cost-sensitive Multi-class Exponential Loss that
generalizes the losses optimized in various classification algorithms such as
AdaBoost, SAMME, Cost-sensitive AdaBoost and PIBoost. Hence unifying them under
a common theoretical framework. In the experiments performed we prove that
BAdaCost achieves significant gains in performance when compared to previous
multi-class cost-sensitive approaches. The advantages of the proposed algorithm
in asymmetric multi-class classification are also evaluated in practical
multi-view face and car detection problems.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04466" title="Abstract">arXiv:2402.04466</a> [<a href="/pdf/2402.04466" title="Download PDF">pdf</a>, <a href="/format/2402.04466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Deterministic End-to-end Latency for Medical AI Systems in  NVIDIA Holoscan
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Soham Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+S">Shekhar Dwivedi</a>, 
<a href="/search/cs?searchtype=author&query=Azizian%2C+M">Mahdi Azizian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Operating Systems (cs.OS)

</div>
<p class="mathjax">The introduction of AI and ML technologies into medical devices has
revolutionized healthcare diagnostics and treatments. Medical device
manufacturers are keen to maximize the advantages afforded by AI and ML by
consolidating multiple applications onto a single platform. However, concurrent
execution of several AI applications, each with its own visualization
components, leads to unpredictable end-to-end latency, primarily due to GPU
resource contentions. To mitigate this, manufacturers typically deploy separate
workstations for distinct AI applications, thereby increasing financial,
energy, and maintenance costs. This paper addresses these challenges within the
context of NVIDIA's Holoscan platform, a real-time AI system for streaming
sensor data and images. We propose a system design optimized for heterogeneous
GPU workloads, encompassing both compute and graphics tasks. Our design
leverages CUDA MPS for spatial partitioning of compute workloads and isolates
compute and graphics processing onto separate GPUs. We demonstrate significant
performance improvements across various end-to-end latency determinism metrics
through empirical evaluation with real-world Holoscan medical device
applications. For instance, the proposed design reduces maximum latency by
21-30% and improves latency distribution flatness by 17-25% for up to five
concurrent endoscopy tool tracking AI applications, compared to a single-GPU
baseline. Against a default multi-GPU setup, our optimizations decrease maximum
latency by 35% for up to six concurrent applications by improving GPU
utilization by 42%. This paper provides clear design insights for AI
applications in the edge-computing domain including medical systems, where
performance predictability of concurrent and heterogeneous GPU workloads is a
critical requirement.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04467" title="Abstract">arXiv:2402.04467</a> [<a href="/pdf/2402.04467" title="Download PDF">pdf</a>, <a href="/format/2402.04467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiff%2C+Y">Yair Schiff</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z+Y">Zhong Yi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Parker%2C+J+B">Jeffrey B. Parker</a>, 
<a href="/search/cs?searchtype=author&query=Hoyer%2C+S">Stephan Hoyer</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Zepeda-N%C3%BA%C3%B1ez%2C+L">Leonardo Zepeda-N&#xfa;&#xf1;ez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Learning dynamics from dissipative chaotic systems is notoriously difficult
due to their inherent instability, as formalized by their positive Lyapunov
exponents, which exponentially amplify errors in the learned dynamics. However,
many of these systems exhibit ergodicity and an attractor: a compact and highly
complex manifold, to which trajectories converge in finite-time, that supports
an invariant measure, i.e., a probability distribution that is invariant under
the action of the dynamics, which dictates the long-term statistical behavior
of the system. In this work, we leverage this structure to propose a new
framework that targets learning the invariant measure as well as the dynamics,
in contrast with typical methods that only target the misfit between
trajectories, which often leads to divergence as the trajectories' length
increases. We use our framework to propose a tractable and sample efficient
objective that can be used with any existing learning objectives. Our Dynamics
Stable Learning by Invariant Measures (DySLIM) objective enables model training
that achieves better point-wise tracking and long-term statistical accuracy
relative to other learning objectives. By targeting the distribution with a
scalable regularization term, we hope that this approach can be extended to
more complex systems exhibiting slowly-variant distributions, such as weather
and climate models.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04469" title="Abstract">arXiv:2402.04469</a> [<a href="/pdf/2402.04469" title="Download PDF">pdf</a>, <a href="/format/2402.04469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IoT Network Traffic Analysis with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Leon Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PerCom 2024 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">As IoT networks become more complex and generate massive amounts of dynamic
data, it is difficult to monitor and detect anomalies using traditional
statistical methods and machine learning methods. Deep learning algorithms can
process and learn from large amounts of data and can also be trained using
unsupervised learning techniques, meaning they don't require labelled data to
detect anomalies. This makes it possible to detect new and unknown anomalies
that may not have been detected before. Also, deep learning algorithms can be
automated and highly scalable; thereby, they can run continuously in the
backend and make it achievable to monitor large IoT networks instantly. In this
work, we conduct a literature review on the most recent works using deep
learning techniques and implement a model using ensemble techniques on the KDD
Cup 99 dataset. The experimental results showcase the impressive performance of
our deep anomaly detection model, achieving an accuracy of over 98\%.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04470" title="Abstract">arXiv:2402.04470</a> [<a href="/pdf/2402.04470" title="Download PDF">pdf</a>, <a href="/ps/2402.04470" title="Download PostScript">ps</a>, <a href="/format/2402.04470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI language models as role-playing tools, not human participants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhicheng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Advances in AI invite misuse of language models as replacements for human
participants. We argue that treating their responses as glimpses into an
average human mind fundamentally mischaracterizes these statistical algorithms
and that language models should be embraced as flexible simulation tools, able
to mimic diverse behaviors without possessing human traits themselves.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04476" title="Abstract">arXiv:2402.04476</a> [<a href="/pdf/2402.04476" title="Download PDF">pdf</a>, <a href="/format/2402.04476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-View Visual Contextualization for Web Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kil%2C+J">Jihyung Kil</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C+H">Chan Hee Song</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Boyuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Automatic web navigation aims to build a web agent that can follow language
instructions to execute complex and diverse tasks on real-world websites.
Existing work primarily takes HTML documents as input, which define the
contents and action spaces (i.e., actionable elements and operations) of
webpages. Nevertheless, HTML documents may not provide a clear task-related
context for each element, making it hard to select the right (sequence of)
actions. In this paper, we propose to contextualize HTML elements through their
"dual views" in webpage screenshots: each HTML element has its corresponding
bounding box and visual content in the screenshot. We build upon the insight --
web developers tend to arrange task-related elements nearby on webpages to
enhance user experiences -- and propose to contextualize each element with its
neighbor elements, using both textual and visual features. The resulting
representations of HTML elements are more informative for the agent to take
action. We validate our method on the recently released Mind2Web dataset, which
features diverse navigation domains and tasks on real-world websites. Our
method consistently outperforms the baseline in all the scenarios, including
cross-task, cross-website, and cross-domain ones.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04477" title="Abstract">arXiv:2402.04477</a> [<a href="/pdf/2402.04477" title="Download PDF">pdf</a>, <a href="/format/2402.04477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Mode Collapse in Language Models via Narration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamilton%2C+S">Sil Hamilton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the proceedings of the first Workshop on the Scaling Behavior of Large Language Models (EACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">No two authors write alike. Personal flourishes invoked in written
narratives, from lexicon to rhetorical devices, imply a particular author--what
literary theorists label the implied or virtual author; distinct from the real
author or narrator of a text. Early large language models trained on unfiltered
training sets drawn from a variety of discordant sources yielded incoherent
personalities, problematic for conversational tasks but proving useful for
sampling literature from multiple perspectives. Successes in alignment research
in recent years have allowed researchers to impose subjectively consistent
personae on language models via instruction tuning and reinforcement learning
from human feedback (RLHF), but whether aligned models retain the ability to
model an arbitrary virtual author has received little scrutiny. By studying
4,374 stories sampled from three OpenAI language models, we show successive
versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby
overfitting the model during alignment constrains it from generalizing over
authorship: models suffering from mode collapse become unable to assume a
multiplicity of perspectives. Our method and results are significant for
researchers seeking to employ language models in sociological simulations.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04479" title="Abstract">arXiv:2402.04479</a> [<a href="/pdf/2402.04479" title="Download PDF">pdf</a>, <a href="/ps/2402.04479" title="Download PostScript">ps</a>, <a href="/format/2402.04479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Potential of LTE for Next Generation Railway  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fraga-Lamas%2C+P">P. Fraga-Lamas</a>, 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez-Pi%C3%B1eiro%2C+J">J. Rodr&#xed;guez-Pi&#xf1;eiro</a>, 
<a href="/search/eess?searchtype=author&query=Garc%C3%ADa-Naya%2C+J+A">J.A. Garc&#xed;a-Naya</a>, 
<a href="/search/eess?searchtype=author&query=Castedo%2C+L">L. Castedo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a portion of the ACCEPTED VERSION of the published document in: Kassab, M., Berbineau,M., Vinel, A., Jonsson, M., Garcia, F., Soler, J. (eds) Communication Technologies for Vehicles. Nets4Cars/Nets4Trains/Nets4Aircraft 2015. Lecture Notes in Computer Science, vol 9066. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-17765-6_14">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science, vol 9066. Springer, Cham, 2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computers and Society (cs.CY); Signal Processing (eess.SP)

</div>
<p class="mathjax">In an increasingly demanding marketplace that will put great strain on
railway services, research on broadband wireless communication must continue to
strive for improvement. Based on the mature narrowband GSM technology, Global
System for Mobile Communications-Railways (GSM-R) has been deployed both for
operational and voice communications. Although GSM-R fulfills the requirements
of current railway services, it imposes limited capacity and high costs that
restrict enhancements of operational efficiency, passenger security and
transport quality. 4G Long Term Evolution (LTE) is expected to be the natural
successor of GSM-R not only for its technical advantages and increasing
performance, but also due to the current evolution of general-purpose
communication systems. This paper examines the key features of LTE as well as
its technical ability to support both the migration of current railway services
and the provisioning of future ones.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04482" title="Abstract">arXiv:2402.04482</a> [<a href="/pdf/2402.04482" title="Download PDF">pdf</a>, <a href="/format/2402.04482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEBLID: Boosted efficient binary local image descriptor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%C3%A1rez%2C+I">Iago Su&#xe1;rez</a>, 
<a href="/search/cs?searchtype=author&query=Sfeir%2C+G">Ghesn Sfeir</a>, 
<a href="/search/cs?searchtype=author&query=Buenaposada%2C+J+M">Jos&#xe9; M. Buenaposada</a>, 
<a href="/search/cs?searchtype=author&query=Baumela%2C+L">Luis Baumela</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition Letters. Volume 133, May 2020, Pages 366-372
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Efficient matching of local image features is a fundamental task in many
computer vision applications. However, the real-time performance of top
matching algorithms is compromised in computationally limited devices, such as
mobile phones or drones, due to the simplicity of their hardware and their
finite energy supply. In this paper we introduce BEBLID, an efficient learned
binary image descriptor. It improves our previous real-valued descriptor,
BELID, making it both more efficient for matching and more accurate. To this
end we use AdaBoost with an improved weak-learner training scheme that produces
better local descriptions. Further, we binarize our descriptor by forcing all
weak-learners to have the same weight in the strong learner combination and
train it in an unbalanced data set to address the asymmetries arising in
matching and retrieval tasks. In our experiments BEBLID achieves an accuracy
close to SIFT and better computational efficiency than ORB, the fastest
algorithm in the literature.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04485" title="Abstract">arXiv:2402.04485</a> [<a href="/pdf/2402.04485" title="Download PDF">pdf</a>, <a href="/format/2402.04485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivized Truthful Communication for Federated Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhepei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuanhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+T">Tianze Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 2 figures. Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">To enhance the efficiency and practicality of federated bandit learning,
recent advances have introduced incentives to motivate communication among
clients, where a client participates only when the incentive offered by the
server outweighs its participation cost. However, existing incentive mechanisms
naively assume the clients are truthful: they all report their true cost and
thus the higher cost one participating client claims, the more the server has
to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming
to optimize their own utility by misreporting. To address this issue, we
propose an incentive compatible (i.e., truthful) communication protocol, named
Truth-FedBan, where the incentive for each participant is independent of its
self-reported cost, and reporting the true cost is the only way to achieve the
best utility. More importantly, Truth-FedBan still guarantees the sub-linear
regret and communication cost without any overheads. In other words, the core
conceptual contribution of this paper is, for the first time, demonstrating the
possibility of simultaneously achieving incentive compatibility and nearly
optimal regret in federated bandit learning. Extensive numerical studies
further validate the effectiveness of our proposed solution.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04486" title="Abstract">arXiv:2402.04486</a> [<a href="/pdf/2402.04486" title="Download PDF">pdf</a>, <a href="/format/2402.04486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outer Code Designs for Augmented and Local-Global Polar Code  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Siegel%2C+P+H">Paul H. Siegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper, we introduce two novel methods to design outer polar codes for
two previously proposed concatenated polar code architectures: augmented polar
codes and local-global polar codes. These methods include a stopping set (SS)
construction and a nonstationary density evolution (NDE) construction.
Simulation results demonstrate the advantage of these methods over previously
proposed constructions based on density evolution (DE) and LLR evolution.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04489" title="Abstract">arXiv:2402.04489</a> [<a href="/pdf/2402.04489" title="Download PDF">pdf</a>, <a href="/format/2402.04489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-amplifying Bias from Differential Privacy in Language Model  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Sanjari Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Mardziel%2C+P">Piotr Mardziel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhikhun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ahlawat%2C+A">Archana Ahlawat</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anupam Datta</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+J+C">John C Mitchell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY); Methodology (stat.ME)

</div>
<p class="mathjax">Fairness and privacy are two important values machine learning (ML)
practitioners often seek to operationalize in models. Fairness aims to reduce
model bias for social/demographic sub-groups. Privacy via differential privacy
(DP) mechanisms, on the other hand, limits the impact of any individual's
training data on the resulting model. The trade-offs between privacy and
fairness goals of trustworthy ML pose a challenge to those wishing to address
both. We show that DP amplifies gender, racial, and religious bias when
fine-tuning large language models (LLMs), producing models more biased than
ones fine-tuned without DP. We find the cause of the amplification to be a
disparity in convergence of gradients across sub-groups. Through the case of
binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),
a known method for addressing bias, also mitigates bias amplification by DP. As
a consequence, DP and CDA together can be used to fine-tune models while
maintaining both fairness and privacy.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04491" title="Abstract">arXiv:2402.04491</a> [<a href="/pdf/2402.04491" title="Download PDF">pdf</a>, <a href="/ps/2402.04491" title="Download PostScript">ps</a>, <a href="/format/2402.04491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Characterizing Service Interference in Dynamic  Infrastructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medel%2C+V">V&#xcd;ctor Medel</a>, 
<a href="/search/cs?searchtype=author&query=Arronategui%2C+U">Unai Arronategui</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+O">Omer Rana</a>, 
<a href="/search/cs?searchtype=author&query=Ba%C3%91ares%2C+J+%C3%81">Jos&#xc9; &#xc1;ngel Ba&#xd1;ares</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana-Calasanz%2C+R">Rafael Tolosana-Calasanz</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access 11: 21387-21403 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Performance interference can occur when various services are executed over
the same physical infrastructure in a cloud system. This can lead to
performance degradation compared to the execution of services in isolation.
This work proposes a Confirmatory Factor Analysis (CFA)-based model to estimate
performance interference across containers, caused by the use of CPU, memory
and IO across a number of co-hosted applications. The approach provides
resource characterization through human comprehensible indices expressed as
time series, so the interference in the entire execution lifetime of a service
can be analyzed. Our experiments, based on the combination of real services
with different profiles executed in Docker containers, suggest that our model
can accurately predict the overall execution time, for different service
combinations. The approach can be used by a service designer to identify
phases, during the execution life-cycle of a service, that are likely to lead
to a greater degree of interference, and to ensure that only complementary
services are hosted on the same physical machine. Interference-awareness of
this kind will enable more intelligent resource management and scheduling for
cloud systems, and may be used to dynamically modify scheduling decisions.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04492" title="Abstract">arXiv:2402.04492</a> [<a href="/pdf/2402.04492" title="Download PDF">pdf</a>, <a href="/format/2402.04492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burapacheep%2C+J">Jirayu Burapacheep</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+I">Ishan Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+A">Agam Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Thrush%2C+T">Tristan Thrush</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper introduces the ColorSwap dataset, designed to assess and improve
the proficiency of multimodal models in matching objects with their colors. The
dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000
examples. Each example includes a caption-image pair, along with a
``color-swapped'' pair. We follow the Winoground schema: the two captions in an
example have the same words, but the color words have been rearranged to modify
different objects. The dataset was created through a novel blend of automated
caption and image generation with humans in the loop. We evaluate image-text
matching (ITM) and visual language models (VLMs) and find that even the latest
ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on
our main VLM metric, although they may improve with more advanced prompting
techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP
perform close to chance (at 12% and 30%, respectively), although the
non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning
on fewer than 2,000 examples yields significant performance gains on this
out-of-distribution word-order understanding task. The dataset is here:
https://github.com/Top34051/colorswap.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04494" title="Abstract">arXiv:2402.04494</a> [<a href="/pdf/2402.04494" title="Download PDF">pdf</a>, <a href="/format/2402.04494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grandmaster-Level Chess Without Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruoss%2C+A">Anian Ruoss</a>, 
<a href="/search/cs?searchtype=author&query=Del%C3%A9tang%2C+G">Gr&#xe9;goire Del&#xe9;tang</a>, 
<a href="/search/cs?searchtype=author&query=Medapati%2C+S">Sourabh Medapati</a>, 
<a href="/search/cs?searchtype=author&query=Grau-Moya%2C+J">Jordi Grau-Moya</a>, 
<a href="/search/cs?searchtype=author&query=Wenliang%2C+L+K">Li Kevin Wenliang</a>, 
<a href="/search/cs?searchtype=author&query=Catt%2C+E">Elliot Catt</a>, 
<a href="/search/cs?searchtype=author&query=Reid%2C+J">John Reid</a>, 
<a href="/search/cs?searchtype=author&query=Genewein%2C+T">Tim Genewein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The recent breakthrough successes in machine learning are mainly attributed
to scale: namely large-scale attention-based architectures and datasets of
unprecedented scale. This paper investigates the impact of training at scale
for chess. Unlike traditional chess engines that rely on complex heuristics,
explicit search, or a combination of both, we train a 270M parameter
transformer model with supervised learning on a dataset of 10 million chess
games. We annotate each board in the dataset with action-values provided by the
powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our
largest model reaches a Lichess blitz Elo of 2895 against humans, and
successfully solves a series of challenging chess puzzles, without any
domain-specific tweaks or explicit search algorithms. We also show that our
model outperforms AlphaZero's policy and value networks (without MCTS) and
GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size
shows that strong chess performance only arises at sufficient scale. To
validate our results, we perform an extensive series of ablations of design
choices and hyperparameters.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04497" title="Abstract">arXiv:2402.04497</a> [<a href="/pdf/2402.04497" title="Download PDF">pdf</a>, <a href="/ps/2402.04497" title="Download PostScript">ps</a>, <a href="/format/2402.04497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fine-Grained Complexity of Gradient Computation for Training Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alman%2C+J">Josh Alman</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Large language models (LLMs) have made fundamental contributions over the
last a few years. To train an LLM, one needs to alternatingly run `forward'
computations and `backward' computations. The forward computation can be viewed
as attention function evaluation, and the backward computation can be viewed as
a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it
was proved that the forward step can be performed in almost-linear time in
certain parameter regimes, but that there is no truly sub-quadratic time
algorithm in the remaining parameter regimes unless the popular hypothesis SETH
is false. In this work, we show nearly identical results for the harder-seeming
problem of computing the gradient of loss function of one layer attention
network, and thus for the entire process of LLM training. This completely
characterizes the fine-grained complexity of every step of LLM training.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04504" title="Abstract">arXiv:2402.04504</a> [<a href="/pdf/2402.04504" title="Download PDF">pdf</a>, <a href="/format/2402.04504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2Street: Controllable Text-to-image Generation for Street Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinming Su</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Songen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yiting Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junfeng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image generation has made remarkable progress with the emergence of
diffusion models. However, it is still a difficult task to generate images for
street views based on text, mainly because the road topology of street scenes
is complex, the traffic status is diverse and the weather condition is various,
which makes conventional text-to-image models difficult to deal with. To
address these challenges, we propose a novel controllable text-to-image
framework, named \textbf{Text2Street}. In the framework, we first introduce the
lane-aware road topology generator, which achieves text-to-map generation with
the accurate road structure and lane lines armed with the counting adapter,
realizing the controllable road topology generation. Then, the position-based
object layout generator is proposed to obtain text-to-layout generation through
an object-level bounding box diffusion strategy, realizing the controllable
traffic object layout generation. Finally, the multiple control image generator
is designed to integrate the road topology, object layout and weather
description to realize controllable street-view image generation. Extensive
experiments show that the proposed approach achieves controllable street-view
text-to-image generation and validates the effectiveness of the Text2Street
framework for street views.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04505" title="Abstract">arXiv:2402.04505</a> [<a href="/pdf/2402.04505" title="Download PDF">pdf</a>, <a href="/format/2402.04505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developments in Sheaf-Theoretic Models of Natural Language Ambiguities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+K+I">Kin Ian Lo</a>, 
<a href="/search/cs?searchtype=author&query=Sadrzadeh%2C+M">Mehrnoosh Sadrzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Mansfield%2C+S">Shane Mansfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.16498">arXiv:2308.16498</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Sheaves are mathematical objects consisting of a base which constitutes a
topological space and the data associated with each open set thereof, e.g.
continuous functions defined on the open sets. Sheaves have originally been
used in algebraic topology and logic. Recently, they have also modelled events
such as physical experiments and natural language disambiguation processes. We
extend the latter models from lexical ambiguities to discourse ambiguities
arising from anaphora. To begin, we calculated a new measure of contextuality
for a dataset of basic anaphoric discourses, resulting in a higher proportion
of contextual models--82.9%--compared to previous work which only yielded 3.17%
contextual models. Then, we show how an extension of the natural language
processing challenge, known as the Winograd Schema, which involves anaphoric
ambiguities can be modelled on the Bell-CHSH scenario with a contextual
fraction of 0.096.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04507" title="Abstract">arXiv:2402.04507</a> [<a href="/pdf/2402.04507" title="Download PDF">pdf</a>, <a href="/ps/2402.04507" title="Download PostScript">ps</a>, <a href="/format/2402.04507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Digital Pixel Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Udoy%2C+M+R+I">Md Rahatul Islam Udoy</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shamiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Mazharul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Akhilesh Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+A">Ahmedullah Aziz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Digital pixel sensor (DPS) has evolved as a pivotal component in modern
imaging systems and has the potential to revolutionize various fields such as
medical imaging, astronomy, surveillance, IoT devices, etc. Compared to analog
pixel sensors, the DPS offers high speed and good image quality. However, the
introduced intrinsic complexity within each pixel, primarily attributed to the
accommodation of the ADC circuit, engenders a substantial increase in the pixel
pitch. Unfortunately, such a pronounced escalation in pixel pitch drastically
undermines the feasibility of achieving high-density integration, which is an
obstacle that significantly narrows down the field of potential applications.
Nonetheless, designing compact conversion circuits along with strategic
integration of 3D architectural paradigms can be a potential remedy to the
prevailing situation. This review article presents a comprehensive overview of
the vast area of DPS technology. The operating principles, advantages, and
challenges of different types of DPS circuits have been analyzed. We categorize
the schemes into several categories based on ADC operation. A comparative study
based on different performance metrics has also been showcased for a
well-rounded understanding.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04513" title="Abstract">arXiv:2402.04513</a> [<a href="/pdf/2402.04513" title="Download PDF">pdf</a>, <a href="/format/2402.04513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Cascade Learning for Efficient Inference over Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Lunyiu Nie</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhimin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+E">Erdong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jermaine%2C+C">Christopher Jermaine</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Swarat Chaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have a natural role in answering complex queries
about data streams, but the high computational cost of LLM inference makes them
infeasible in many such tasks. We propose online cascade learning, the first
approach to addressing this challenge. The objective here is to learn a
"cascade" of models, starting with lower-capacity models (such as logistic
regressors) and ending with a powerful LLM, along with a deferral policy that
determines the model that is used on a given input. We formulate the task of
learning cascades online as an imitation-learning problem and give a no-regret
algorithm for the problem. Experimental results across four benchmarks show
that our method parallels LLMs in accuracy while cutting down inference costs
by as much as 90%, underscoring its efficacy and adaptability in stream
processing.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04514" title="Abstract">arXiv:2402.04514</a> [<a href="/pdf/2402.04514" title="Download PDF">pdf</a>, <a href="/format/2402.04514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based methods for hyperbolic systems of conservation laws using  discontinuous space discretizations, Part I: building blocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kronbichler%2C+M">Martin Kronbichler</a>, 
<a href="/search/math?searchtype=author&query=Maier%2C+M">Matthias Maier</a>, 
<a href="/search/math?searchtype=author&query=Tomas%2C+I">Ignacio Tomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a graph-based discretization method for solving hyperbolic systems
of conservation laws using discontinuous finite elements. The method is based
on the convex limiting technique technique introduced by Guermond et al. (SIAM
J. Sci. Comput. 40, A3211--A3239, 2018). As such, these methods are
mathematically guaranteed to be invariant-set preserving and to satisfy
discrete pointwise entropy inequalities. In this paper we extend the theory for
the specific case of discontinuous finite elements, incorporating the effect of
boundary conditions into the formulation. From a practical point of view, the
implementation of these methods is algebraic, meaning, that they operate
directly on the stencil of the spatial discretization.
<br />This first paper in a sequence of two papers introduces and verifies
essential building blocks for the convex limiting procedure using discontinuous
Galerkin discretizations. In particular, we discuss a minimally stabilized
high-order discontinuous Galerkin method that exhibits optimal convergence
rates comparable to linear stabilization techniques for cell-based methods. In
addition, we discuss a proper choice of local bounds for the convex limiting
procedure. A follow-up contribution will focus on the high-performance
implementation, benchmarking and verification of the method.
<br />We verify convergence rates on a sequence of one- and two-dimensional tests
with differing regularity. In particular, we obtain optimal convergence rates
for single rarefaction waves. We also propose a simple test in order to verify
the implementation of boundary conditions and their convergence rates.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04515" title="Abstract">arXiv:2402.04515</a> [<a href="/pdf/2402.04515" title="Download PDF">pdf</a>, <a href="/format/2402.04515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in  Next-gen Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrol%2C+A">Akshita Abrol</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+P+M">Purnima Murali Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Truong-Huu%2C+T">Tram Truong-Huu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the Proceedings of the IEEE International Conference on Communications (IEEE ICC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Next-gen networks require significant evolution of management to enable
automation and adaptively adjust network configuration based on traffic
dynamics. The advent of software-defined networking (SDN) and programmable
switches enables flexibility and programmability. However, traditional
techniques that decide traffic policies are usually based on hand-crafted
programming optimization and heuristic algorithms. These techniques make
non-realistic assumptions, e.g., considering static network load and topology,
to obtain tractable solutions, which are inadequate for next-gen networks. In
this paper, we design and develop a deep reinforcement learning (DRL) approach
for adaptive traffic routing. We design a deep graph convolutional neural
network (DGCNN) integrated into the DRL framework to learn the traffic behavior
from not only the network topology but also link and node attributes. We adopt
the Deep Q-Learning technique to train the DGCNN model in the DRL framework
without the need for a labeled training dataset, enabling the framework to
quickly adapt to traffic dynamics. The model leverages q-value estimates to
select the routing path for every traffic flow request, balancing exploration
and exploitation. We perform extensive experiments with various traffic
patterns and compare the performance of the proposed approach with the Open
Shortest Path First (OSPF) protocol. The experimental results show the
effectiveness and adaptiveness of the proposed framework by increasing the
network throughput by up to 7.8% and reducing the traffic delay by up to 16.1%
compared to OSPF.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04517" title="Abstract">arXiv:2402.04517</a> [<a href="/pdf/2402.04517" title="Download PDF">pdf</a>, <a href="/ps/2402.04517" title="Download PostScript">ps</a>, <a href="/format/2402.04517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating the audit of electronic invoices with a soft robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T+J">Tian Jun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+J">Chia Jung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Y+L">Yao Lin Ong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y+F">Yi Fang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sheu%2C+G+Y">Guang Yih Sheu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Taiwan's Chi Mei Medical Center has completed four challenges mentioned in
published robotic process automation (RPA) studies including automating a
dynamic process, designing feasible human-robot collaboration, incorporating
other emerging technologies, and bringing positive business impacts. Its
executives called a committee to implement the electronic invoicing. This
implementation includes the creation of a software robot to download
automatically cloud electronic invoice (E-invoice) data from Taiwan's E-invoice
platform and detect the inconsistency between them and on-premise data. This
bot operates when internal auditors are off their office. They satisfied this
software robot since the remaining work is only verifying the resulting
inconsistency. The Chi Mei Medical Center measured the time and costs before
and after adopting software robots to audit E-invoice; consequently, it
welcomed more bots automating other business processes. In conclusion,
integrating a software robot with other emerging technologies mitigates the
possible errors provided by this bot. A good human-robot collaboration relies
on the consideration of human perspective in choosing RPA tasks. Free bot
creators are sufficient to verify that automating a business process using a
bot is a reasonable investment.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04518" title="Abstract">arXiv:2402.04518</a> [<a href="/pdf/2402.04518" title="Download PDF">pdf</a>, <a href="/format/2402.04518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLAGRED -- Fuzzy Logic-based Algorithm Generalizing Risk Estimation for  Drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hovington%2C+S">Samuel Hovington</a>, 
<a href="/search/cs?searchtype=author&query=Petit%2C+L">Louis Petit</a>, 
<a href="/search/cs?searchtype=author&query=Stratford%2C+S">Sophie Stratford</a>, 
<a href="/search/cs?searchtype=author&query=Hamelin%2C+P">Philippe Hamelin</a>, 
<a href="/search/cs?searchtype=author&query=Lussier-Desbiens%2C+A">Alexis Lussier-Desbiens</a>, 
<a href="/search/cs?searchtype=author&query=Ferland%2C+F">Francois Ferland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurately estimating risk in real-time is essential for ensuring the safety
and efficiency of many applications involving autonomous robot systems. This
paper presents a novel, generalizable algorithm for the real-time estimation of
risks created by external disturbances on multirotors. Unlike conventional
approaches, our method requires no additional sensors, accurate drone models,
or large datasets. It employs motor command data in a fuzzy logic system,
overcoming barriers to real-world implementation. Inherently adaptable, it
utilizes fundamental drone characteristics, making it applicable to diverse
drone models. The efficiency of the algorithm has been confirmed through
comprehensive real-world testing on various platforms. It proficiently
discerned between high and low-risk scenarios resulting from diverse wind
disturbances and varying thrust-to-weight ratios. The algorithm surpassed the
widely-recognized ArduCopter wind estimation algorithm in performance and
demonstrated its capability to promptly detect brief gusts.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04519" title="Abstract">arXiv:2402.04519</a> [<a href="/pdf/2402.04519" title="Download PDF">pdf</a>, <a href="/format/2402.04519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for  Robust Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shiyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yipei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yimin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rongshuai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Renshu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiadong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is published in IJCV (refer to DOI). Please cite the published IJCV
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Int J Comput Vis (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Single object tracking (SOT) is a fundamental problem in computer vision,
with a wide range of applications, including autonomous driving, augmented
reality, and robot navigation. The robustness of SOT faces two main challenges:
tiny target and fast motion. These challenges are especially manifested in
videos captured by unmanned aerial vehicles (UAV), where the target is usually
far away from the camera and often with significant motion relative to the
camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the
first bionic drone-based visual benchmark for SOT. Unlike existing UAV
datasets, BioDrone features videos captured from a flapping-wing UAV system
with a major camera shake due to its aerodynamics. BioDrone hence highlights
the tracking of tiny targets with drastic changes between consecutive frames,
providing a new robust vision benchmark for SOT. To date, BioDrone offers the
largest UAV-based SOT benchmark with high-quality fine-grained manual
annotations and automatically generates frame-level labels, designed for robust
vision analyses. Leveraging our proposed BioDrone, we conduct a systematic
evaluation of existing SOT methods, comparing the performance of 20
representative models and studying novel means of optimizing a SOTA method
(KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and
insights for robust SOT. Moving forward, we hope that BioDrone will not only
serve as a high-quality benchmark for robust SOT, but also invite future
research into robust computer vision. The database, toolkits, evaluation
server, and baseline results are available at <a href="http://biodrone.aitestunion.com.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04520" title="Abstract">arXiv:2402.04520</a> [<a href="/pdf/2402.04520" title="Download PDF">pdf</a>, <a href="/ps/2402.04520" title="Download PostScript">ps</a>, <a href="/format/2402.04520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computational Limits of Modern Hopfield Models: A Fine-Grained  Complexity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J+Y">Jerry Yao-Chieh Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Thomas Lin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We investigate the computational limits of the memory retrieval dynamics of
modern Hopfield models from the fine-grained complexity analysis. Our key
contribution is the characterization of a phase transition behavior in the
efficiency of all possible modern Hopfield models based on the norm of
patterns. Specifically, we establish an upper bound criterion for the norm of
input query patterns and memory patterns. Only below this criterion,
sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming
the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we
provide a formal example of efficient constructions of modern Hopfield models
using low-rank approximation when the efficient criterion holds. This includes
a derivation of a lower bound on the computational time, scaling linearly with
$\Max\{$# of stored memory patterns, length of input query sequence$\}$. In
addition, we prove its memory retrieval error bound and exponential memory
capacity.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04522" title="Abstract">arXiv:2402.04522</a> [<a href="/pdf/2402.04522" title="Download PDF">pdf</a>, <a href="/format/2402.04522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H-EYE: Holistic Resource Modeling and Management for Diversely Scaled  Edge-Cloud Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagli%2C+I">Ismet Dagli</a>, 
<a href="/search/cs?searchtype=author&query=Morshedlou%2C+A">Amid Morshedlou</a>, 
<a href="/search/cs?searchtype=author&query=Rostami%2C+J">Jamal Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Belviranli%2C+M+E">Mehmet E. Belviranli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Computing systems have been evolving to be more pervasive, heterogeneous, and
dynamic. An increasing number of emerging domains now rely on diverse edge to
cloud continuum where the execution of applications often spans various tiers
of systems with significantly heterogeneous computational capabilities.
Resources in each tier are often handled in isolation due to scalability and
privacy concerns. However, better overall resource utilization could be
achieved if different tiers of systems had the means to communicate their
computational capabilities.
<br />In this paper, we propose H-EYE, a universal approach to holistically capture
diverse computational characteristics of edge-cloud systems with arbitrary
topologies and to manage the assignment of tasks to the computational resources
with the whole continuum in the scope. Our proposed work introduces two
significant innovations: (1) We present a multi-layer, graph-based hardware
(HW) representation and a modular performance modeling interface that could
capture interactions and inference between different computing and
communication resources in the system at desired level of detail. (2) We
introduce a novel orchestrator mechanism that leverages the graph-based HW
representation to hierarchically locate target devices that a given set of
tasks could be mapped to. Orchestrator provides isolation for various device
groups and allows hierarchical abstraction to scalably find mappings that
satisfy system deadlines. The orchestrator internally relies on a novel
traverser that takes shared resource slowdown into account. We demonstrate the
utility and flexibility of H-EYE on edge-server systems that are deployed on
the field in two different disciplines, improving up to 47% latency over
baselines with less than 2% scheduling overhead
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04523" title="Abstract">arXiv:2402.04523</a> [<a href="/pdf/2402.04523" title="Download PDF">pdf</a>, <a href="/format/2402.04523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SumRec: A Framework for Recommendation using Open-Domain Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asahara%2C+R">Ryutaro Asahara</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+M">Masaki Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Iwahashi%2C+C">Chiho Iwahashi</a>, 
<a href="/search/cs?searchtype=author&query=Inaba%2C+M">Michimasa Inaba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PACLIC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Chat dialogues contain considerable useful information about a speaker's
interests, preferences, and experiences.Thus, knowledge from open-domain chat
dialogue can be used to personalize various systems and offer recommendations
for advanced information.This study proposed a novel framework SumRec for
recommending information from open-domain chat dialogue.The study also examined
the framework using ChatRec, a newly constructed dataset for training and
evaluation. To extract the speaker and item characteristics, the SumRec
framework employs a large language model (LLM) to generate a summary of the
speaker information from a dialogue and to recommend information about an item
according to the type of user.The speaker and item information are then input
into a score estimation model, generating a recommendation score.Experimental
results show that the SumRec framework provides better recommendations than the
baseline method of using dialogues and item descriptions in their original
form. Our dataset and code is publicly available at
https://github.com/Ryutaro-A/SumRec
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04527" title="Abstract">arXiv:2402.04527</a> [<a href="/pdf/2402.04527" title="Download PDF">pdf</a>, <a href="/format/2402.04527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaohan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhongrui Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLM) have recently emerged as a powerful tool for a
variety of natural language processing tasks, bringing a new surge of combining
LLM with recommendation systems, termed as LLM-based RS. Current approaches
generally fall into two main paradigms, the ID direct usage paradigm and the ID
translation paradigm, noting their core weakness stems from lacking
recommendation knowledge and uniqueness. To address this limitation, we propose
a new paradigm, ID representation, which incorporates pre-trained ID embeddings
into LLMs in a complementary manner. In this work, we present RA-Rec, an
efficient ID representation alignment framework for LLM-based recommendation,
which is compatible with multiple ID-based methods and LLM architectures.
Specifically, we treat ID embeddings as soft prompts and design an innovative
alignment module and an efficient tuning method with tailored data construction
for alignment. Extensive experiments demonstrate RA-Rec substantially
outperforms current state-of-the-art methods, achieving up to 3.0% absolute
HitRate@100 improvements while utilizing less than 10x training data.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04533" title="Abstract">arXiv:2402.04533</a> [<a href="/pdf/2402.04533" title="Download PDF">pdf</a>, <a href="/ps/2402.04533" title="Download PostScript">ps</a>, <a href="/format/2402.04533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing Block Incentive Volatility Through Verkle Tree-Based Dynamic  Transaction Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiongfei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gerui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+H">Hou-Wan Long</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+Y">Yain-Whar Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Transaction fees are a crucial revenue source for miners in public and
consortium blockchains. However, while public blockchains have additional
revenue streams, transaction fees serve as the primary income for miners in
consortium blockchains formed by various financial institutions. These miners
allocate different levels of computing resources to process transactions and
earn corresponding fees. Nonetheless, relying solely on transaction fees can
lead to significant volatility and encourage non-standard mining behaviors,
thereby posing threats to the blockchain's security and integrity. Despite
previous attempts to mitigate the impact of transaction fees on illicit mining
behaviors, a comprehensive solution to this vulnerability is yet to be
established. To address this gap, we introduce a novel approach that leverages
Dynamic Transaction Storage (DTS) strategies to effectively minimize block
incentive volatility. Our solution implements a Verkle tree-based storage
mechanism to reduce bandwidth consumption. Moreover, to configure the DTS
strategies, we evaluate several optimization algorithms and formulate the
challenge as a Vehicle Routing Problem. Our experiments conducted using
historical transactions from Bitcoin and remittance data from the Industrial
and Commercial Bank of China reveal that the strategy focusing on time-based
transaction incorporation priority, while excluding a designated space for
small-fee transactions, as discovered by the gradient-based optimizer
algorithm, proves most effective in reducing volatility. Hence, the DTS
strategy can sustain stable block incentives irrespective of transaction types
or user bidding behavior. Furthermore, the inclusion of higher-fee
transactions, often smaller in size, can alleviate propagation delays and the
occurrence of forks.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04534" title="Abstract">arXiv:2402.04534</a> [<a href="/pdf/2402.04534" title="Download PDF">pdf</a>, <a href="/format/2402.04534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yawen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Su Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tansi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuewen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+S">Songlin Fei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Victor Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Forest monitoring and education are key to forest protection, education and
management, which is an effective way to measure the progress of a country's
forest and climate commitments. Due to the lack of a large-scale wild forest
monitoring benchmark, the common practice is to train the model on a common
outdoor benchmark (e.g., KITTI) and evaluate it on real forest datasets (e.g.,
CanaTree100). However, there is a large domain gap in this setting, which makes
the evaluation and deployment difficult. In this paper, we propose a new
photorealistic virtual forest dataset and a multimodal transformer-based
algorithm for tree detection and instance segmentation. To the best of our
knowledge, it is the first time that a multimodal detection and segmentation
algorithm is applied to large-scale forest scenes. We believe that the proposed
dataset and method will inspire the simulation, computer vision, education, and
forestry communities towards a more comprehensive multi-modal understanding.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04535" title="Abstract">arXiv:2402.04535</a> [<a href="/pdf/2402.04535" title="Download PDF">pdf</a>, <a href="/format/2402.04535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuNES: Multifloor Navigation Including Elevators and Stairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+D">Donghwi Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jae-Kyung Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Woo Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a scheme called MuNES for single mapping and trajectory planning
including elevators and stairs. Optimized multifloor trajectories are important
for optimal interfloor movements of robots. However, given two or more options
of moving between floors, it is difficult to select the best trajectory because
there are no suitable indoor multifloor maps in the existing methods. To solve
this problem, MuNES creates a single multifloor map including elevators and
stairs by estimating altitude changes based on pressure data. In addition, the
proposed method performs floor-based loop detection for faster and more
accurate loop closure. The single multifloor map is then voxelized leaving only
the parts needed for trajectory planning. An optimal and realistic multifloor
trajectory is generated by exploring the voxels using an A* algorithm based on
the proposed cost function, which affects realistic factors. We tested this
algorithm using data acquired from around a campus and note that a single
accurate multifloor map could be created. Furthermore, optimal and realistic
multifloor trajectory could be found by selecting the means of motion between
floors between elevators and stairs according to factors such as the starting
point, ending point, and elevator waiting time. The code and data used in this
work are available at https://github.com/donghwijung/MuNES.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04536" title="Abstract">arXiv:2402.04536</a> [<a href="/pdf/2402.04536" title="Download PDF">pdf</a>, <a href="/format/2402.04536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile-based Object Retrieval From Granular Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingxi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yinsen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+P">Patrick Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinyue Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zihan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>, 
<a href="/search/cs?searchtype=author&query=Ciocarlie%2C+M">Matei Ciocarlie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce GEOTACT, a robotic manipulation method capable of retrieving
objects buried in granular media. This is a challenging task due to the need to
interact with granular media, and doing so based exclusively on tactile
feedback, since a buried object can be completely hidden from vision. Tactile
feedback is in itself challenging in this context, due to ubiquitous contact
with the surrounding media, and the inherent noise level induced by the tactile
readings. To address these challenges, we use a learning method trained
end-to-end with simulated sensor noise. We show that our problem formulation
leads to the natural emergence of learned pushing behaviors that the
manipulator uses to reduce uncertainty and funnel the object to a stable grasp
despite spurious and noisy tactile readings. We also introduce a training
curriculum that enables learning these behaviors in simulation, followed by
zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is
the first method to reliably retrieve a number of different objects from a
granular environment, doing so on real hardware and with integrated tactile
sensing. Videos and additional information can be found at
https://jxu.ai/geotact.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04538" title="Abstract">arXiv:2402.04538</a> [<a href="/pdf/2402.04538" title="Download PDF">pdf</a>, <a href="/format/2402.04538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triplet Interaction Improves Graph Transformers: Accurate Molecular  Graph Learning with Triplet Graph Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussain%2C+M+S">Md Shamim Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+M+J">Mohammed J. Zaki</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+D">Dharmashankar Subramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First preprint, 24 pages, 10 figures, 18 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph transformers typically lack direct pair-to-pair communication, instead
forcing neighboring pairs to exchange information via a common node. We propose
the Triplet Graph Transformer (TGT) that enables direct communication between
two neighboring pairs in a graph via novel triplet attention and aggregation
mechanisms. TGT is applied to molecular property prediction by first predicting
interatomic distances from 2D graphs and then using these distances for
downstream tasks. A novel three-stage training procedure and stochastic
inference further improve training efficiency and model performance. Our model
achieves new state-of-the-art (SOTA) results on open challenge benchmarks
PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and
LIT-PCBA molecular property prediction benchmarks via transfer learning. We
also demonstrate the generality of TGT with SOTA results on the traveling
salesman problem (TSP).
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04539" title="Abstract">arXiv:2402.04539</a> [<a href="/pdf/2402.04539" title="Download PDF">pdf</a>, <a href="/format/2402.04539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Diverse Policies with Soft Self-Generated Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guojian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Faguo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianxiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 19 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Intelligent Systems, Volume 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) with sparse and deceptive rewards is challenging
because non-zero rewards are rarely obtained. Hence, the gradient calculated by
the agent can be stochastic and without valid information. Recent studies that
utilize memory buffers of previous experiences can lead to a more efficient
learning process. However, existing methods often require these experiences to
be successful and may overly exploit them, which can cause the agent to adopt
suboptimal behaviors. This paper develops an approach that uses diverse past
trajectories for faster and more efficient online RL, even if these
trajectories are suboptimal or not highly rewarded. The proposed algorithm
combines a policy improvement step with an additional exploration step using
offline demonstration data. The main contribution of this paper is that by
regarding diverse past trajectories as guidance, instead of imitating them, our
method directs its policy to follow and expand past trajectories while still
being able to learn without rewards and approach optimality. Furthermore, a
novel diversity measurement is introduced to maintain the team's diversity and
regulate exploration. The proposed algorithm is evaluated on discrete and
continuous control tasks with sparse and deceptive rewards. Compared with the
existing RL methods, the experimental results indicate that our proposed
algorithm is significantly better than the baseline methods regarding diverse
exploration and avoiding local optima.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04541" title="Abstract">arXiv:2402.04541</a> [<a href="/pdf/2402.04541" title="Download PDF">pdf</a>, <a href="/format/2402.04541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BRI3L: A Brightness Illusion Image Dataset for Identification and  Localization of Regions of Illusory Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Aniket Roy</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Anirban Roy</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Soma Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+K">Kuntal Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual illusions play a significant role in understanding visual perception.
Current methods in understanding and evaluating visual illusions are mostly
deterministic filtering based approach and they evaluate on a handful of visual
illusions, and the conclusions therefore, are not generic. To this end, we
generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion
Image dataset for Identification and Localization of illusory perception) of
the five types of brightness illusions and benchmark the dataset using
data-driven neural network based approaches. The dataset contains label
information - (1) whether a particular image is illusory/nonillusory, (2) the
segmentation mask of the illusory region of the image. Hence, both the
classification and segmentation task can be evaluated using this dataset. We
follow the standard psychophysical experiments involving human subjects to
validate the dataset. To the best of our knowledge, this is the first attempt
to develop a dataset of visual illusions and benchmark using data-driven
approach for illusion classification and localization. We consider five
well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous
Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced
Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in
illusion identification and 84.37% pixel accuracy in illusion localization. The
application of deep learning model, it is shown, also generalizes over unseen
brightness illusions like brightness assimilation to contrast transitions. We
also test the ability of state-of-theart diffusion models to generate
brightness illusions. We have provided all the code, dataset, instructions etc
in the github repo: https://github.com/aniket004/BRI3L
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04542" title="Abstract">arXiv:2402.04542</a> [<a href="/pdf/2402.04542" title="Download PDF">pdf</a>, <a href="/format/2402.04542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Share What You Already Know: Cross-Language-Script Transfer and  Alignment for Sentiment Detection in Code-Mixed Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahari%2C+N">Niraj Pahari</a>, 
<a href="/search/cs?searchtype=author&query=Shimada%2C+K">Kazutaka Shimada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Code-switching entails mixing multiple languages. It is an increasingly
occurring phenomenon in social media texts. Usually, code-mixed texts are
written in a single script, even though the languages involved have different
scripts. Pre-trained multilingual models primarily utilize the data in the
native script of the language. In existing studies, the code-switched texts are
utilized as they are. However, using the native script for each language can
generate better representations of the text owing to the pre-trained knowledge.
Therefore, a cross-language-script knowledge sharing architecture utilizing the
cross attention and alignment of the representations of text in individual
language scripts was proposed in this study. Experimental results on two
different datasets containing Nepali-English and Hindi-English code-switched
texts, demonstrate the effectiveness of the proposed method. The interpretation
of the model using model explainability technique illustrates the sharing of
language-specific knowledge between language-specific representations.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04546" title="Abstract">arXiv:2402.04546</a> [<a href="/pdf/2402.04546" title="Download PDF">pdf</a>, <a href="/format/2402.04546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry  Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yawen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhuoyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jinyuan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+S">Songlin Fei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Victor Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The popularity of LiDAR devices and sensor technology has gradually empowered
users from autonomous driving to forest monitoring, and research on 3D LiDAR
has made remarkable progress over the years. Unlike 2D images, whose focused
area is visible and rich in texture information, understanding the point
distribution can help companies and researchers find better ways to develop
point-based 3D applications. In this work, we contribute an unreal-based LiDAR
simulation tool and a 3D simulation dataset named LiDAR-Forest, which can be
used by various studies to evaluate forest reconstruction, tree DBH estimation,
and point cloud compression for easy visualization. The simulation is
customizable in tree species, LiDAR types and scene generation, with low cost
and high efficiency.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04548" title="Abstract">arXiv:2402.04548</a> [<a href="/pdf/2402.04548" title="Download PDF">pdf</a>, <a href="/format/2402.04548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NORMY: Non-Uniform History Modeling for Open Retrieval Conversational  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+S">Muhammad Shihab Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Meem%2C+J+A">Jannat Ara Meem</a>, 
<a href="/search/cs?searchtype=author&query=Hristidis%2C+V">Vagelis Hristidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at IEEE ICSC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Open Retrieval Conversational Question Answering (OrConvQA) answers a
question given a conversation as context and a document collection. A typical
OrConvQA pipeline consists of three modules: a Retriever to retrieve relevant
documents from the collection, a Reranker to rerank them given the question and
the context, and a Reader to extract an answer span. The conversational turns
can provide valuable context to answer the final query. State-of-the-art
OrConvQA systems use the same history modeling for all three modules of the
pipeline. We hypothesize this as suboptimal. Specifically, we argue that a
broader context is needed in the first modules of the pipeline to not miss
relevant documents, while a narrower context is needed in the last modules to
identify the exact answer span. We propose NORMY, the first unsupervised
non-uniform history modeling pipeline which generates the best conversational
history for each module. We further propose a novel Retriever for NORMY, which
employs keyphrase extraction on the conversation history, and leverages
passages retrieved in previous turns as additional context. We also created a
new dataset for OrConvQA, by expanding the doc2dial dataset. We implemented
various state-of-the-art history modeling techniques and comprehensively
evaluated them separately for each module of the pipeline on three datasets:
OR-QUAC, our doc2dial extension, and ConvMix. Our extensive experiments show
that NORMY outperforms the state-of-the-art in the individual modules and in
the end-to-end system.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04553" title="Abstract">arXiv:2402.04553</a> [<a href="/pdf/2402.04553" title="Download PDF">pdf</a>, <a href="/format/2402.04553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curvature-Informed SGD via General Purpose Lie-Group Preconditioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pooladzandi%2C+O">Omead Pooladzandi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi-Lin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a novel approach to accelerate stochastic gradient descent (SGD)
by utilizing curvature information obtained from Hessian-vector products or
finite differences of parameters and gradients, similar to the BFGS algorithm.
Our approach involves two preconditioners: a matrix-free preconditioner and a
low-rank approximation preconditioner. We update both preconditioners online
using a criterion that is robust to stochastic gradient noise and does not
require line search or damping. To preserve the corresponding symmetry or
invariance, our preconditioners are constrained to certain connected Lie
groups. The Lie group's equivariance property simplifies the preconditioner
fitting process, while its invariance property eliminates the need for damping,
which is commonly required in second-order optimizers. As a result, the
learning rate for parameter updating and the step size for preconditioner
fitting are naturally normalized, and their default values work well in most
scenarios. Our proposed approach offers a promising direction for improving the
convergence of SGD with low computational overhead. We demonstrate that
Preconditioned SGD (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across
multiple modern deep-learning architectures. We have provided code for
reproducing toy and large scale experiments in this paper.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04554" title="Abstract">arXiv:2402.04554</a> [<a href="/pdf/2402.04554" title="Download PDF">pdf</a>, <a href="/format/2402.04554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yifei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Ming Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Y">Yizhen Lao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields
(NeRF) designed specifically for reconstructing large-scale scenes using aerial
imagery. Unlike previous research focused on small-scale and object-centric
NeRF reconstruction, our approach addresses multiple challenges, including (1)
Addressing the issue of slow training and rendering associated with large
models. (2) Meeting the computational demands necessitated by modeling a
substantial number of images, requiring extensive resources such as
high-performance GPUs. (3) Overcoming significant artifacts and low visual
fidelity commonly observed in large-scale reconstruction tasks due to limited
model capacity. Specifically, we present a novel bird-view pose-based spatial
decomposition algorithm that decomposes a large aerial image set into multiple
small sets with appropriately sized overlaps, allowing us to train individual
NeRFs of sub-scene. This decomposition approach not only decouples rendering
time from the scene size but also enables rendering to scale seamlessly to
arbitrarily large environments. Moreover, it allows for per-block updates of
the environment, enhancing the flexibility and adaptability of the
reconstruction process. Additionally, we propose a projection-guided novel view
re-rendering strategy, which aids in effectively utilizing the independently
trained sub-scenes to generate superior rendering results. We evaluate our
approach on existing datasets as well as against our own drone footage,
improving reconstruction speed by 10x over classical photogrammetry software
and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with
similar rendering quality.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04555" title="Abstract">arXiv:2402.04555</a> [<a href="/pdf/2402.04555" title="Download PDF">pdf</a>, <a href="/format/2402.04555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Z">Zhijian Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Semantic mapping based on the supervised object detectors is sensitive to
image distribution. In real-world environments, the object detection and
segmentation performance can lead to a major drop, preventing the use of
semantic mapping in a wider domain. On the other hand, the development of
vision-language foundation models demonstrates a strong zero-shot
transferability across data distribution. It provides an opportunity to
construct generalizable instance-aware semantic maps. Hence, this work explores
how to boost instance-aware semantic mapping from object detection generated
from foundation models. We propose a probabilistic label fusion method to
predict close-set semantic classes from open-set label measurements. An
instance refinement module merges the over-segmented instances caused by
inconsistent segmentation. We integrate all the modules into a unified semantic
mapping system. Reading a sequence of RGB-D input, our work incrementally
reconstructs an instance-aware semantic map. We evaluate the zero-shot
performance of our method in ScanNet and SceneNN datasets. Our method achieves
40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation
task. It outperforms the traditional semantic mapping method significantly.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04558" title="Abstract">arXiv:2402.04558</a> [<a href="/pdf/2402.04558" title="Download PDF">pdf</a>, <a href="/format/2402.04558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guoqiang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiahao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shizhou Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human de-occlusion, which aims to infer the appearance of invisible human
parts from an occluded image, has great value in many human-related tasks, such
as person re-id, and intention inference. To address this task, this paper
proposes a dynamic mask-aware transformer (DMAT), which dynamically augments
information from human regions and weakens that from occlusion. First, to
enhance token representation, we design an expanded convolution head with
enlarged kernels, which captures more local valid context and mitigates the
influence of surrounding occlusion. To concentrate on the visible human parts,
we propose a novel dynamic multi-head human-mask guided attention mechanism
through integrating multiple masks, which can prevent the de-occluded regions
from assimilating to the background. Besides, a region upsampling strategy is
utilized to alleviate the impact of occlusion on interpolated images. During
model learning, an amodal loss is developed to further emphasize the recovery
effect of human regions, which also refines the model's convergence. Extensive
experiments on the AHP dataset demonstrate its superior performance compared to
recent state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04559" title="Abstract">arXiv:2402.04559</a> [<a href="/pdf/2402.04559" title="Download PDF">pdf</a>, <a href="/format/2402.04559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Model Agents Simulate Human Trust Behaviors?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chengxing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Canyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Feiran Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Ziyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guohao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally. Project website: <a href="https://www.camel-ai.org/research/agent-trust">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Model (LLM) agents have been increasingly adopted as
simulation tools to model humans in applications such as social science.
However, one fundamental question remains: can LLM agents really simulate human
behaviors? In this paper, we focus on one of the most critical behaviors in
human interactions, trust, and aim to investigate whether or not LLM agents can
simulate human trust behaviors. We first find that LLM agents generally exhibit
trust behaviors, referred to as agent trust, under the framework of Trust
Games, which are widely recognized in behavioral economics. Then, we discover
that LLM agents can have high behavioral alignment with humans regarding trust
behaviors, indicating the feasibility to simulate human trust behaviors with
LLM agents. In addition, we probe into the biases in agent trust and the
differences in agent trust towards agents and humans. We also explore the
intrinsic properties of agent trust under conditions including advanced
reasoning strategies and external manipulations. We further offer important
implications for various scenarios where trust is paramount. Our study
represents a significant step in understanding the behaviors of LLM agents and
the LLM-human analogy.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04563" title="Abstract">arXiv:2402.04563</a> [<a href="/pdf/2402.04563" title="Download PDF">pdf</a>, <a href="/format/2402.04563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Guided CAM: Visual Explanations of Vision Transformer Guided  by Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leem%2C+S">Saebom Leem</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">Hyunseok Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI2024. Code available at <a href="https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision Transformer(ViT) is one of the most widely used models in the computer
vision field with its great performance on various tasks. In order to fully
utilize the ViT-based architecture in various applications, proper
visualization methods with a decent localization performance are necessary, but
these methods employed in CNN-based models are still not available in ViT due
to its unique structure. In this work, we propose an attention-guided
visualization method applied to ViT that provides a high-level semantic
explanation for its decision. Our method selectively aggregates the gradients
directly propagated from the classification output to each self-attention,
collecting the contribution of image features extracted from each location of
the input image. These gradients are additionally guided by the normalized
self-attention scores, which are the pairwise patch correlation scores. They
are used to supplement the gradients on the patch-level context information
efficiently detected by the self-attention mechanism. This approach of our
method provides elaborate high-level semantic explanations with great
localization performance only with the class labels. As a result, our method
outperforms the previous leading explainability methods of ViT in the
weakly-supervised localization task and presents great capability in capturing
the full instances of the target class object. Meanwhile, our method provides a
visualization that faithfully explains the model, which is demonstrated in the
perturbation comparison test.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04567" title="Abstract">arXiv:2402.04567</a> [<a href="/pdf/2402.04567" title="Download PDF">pdf</a>, <a href="/format/2402.04567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Erfani%2C+S">Sarah Erfani</a>, 
<a href="/search/cs?searchtype=author&query=Alpcan%2C+T">Tansu Alpcan</a>, 
<a href="/search/cs?searchtype=author&query=Leckie%2C+C">Christopher Leckie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Anomaly detection in decision-making sequences is a challenging problem due
to the complexity of normality representation learning and the sequential
nature of the task. Most existing methods based on Reinforcement Learning (RL)
are difficult to implement in the real world due to unrealistic assumptions,
such as having access to environment dynamics, reward signals, and online
interactions with the environment. To address these limitations, we propose an
unsupervised method named Offline Imitation Learning based Anomaly Detection
(OIL-AD), which detects anomalies in decision-making sequences using two
extracted behaviour features: action optimality and sequential association. Our
offline learning model is an adaptation of behavioural cloning with a
transformer policy network, where we modify the training process to learn a Q
function and a state value function from normal trajectories. We propose that
the Q function and the state value function can provide sufficient information
about agents' behavioural data, from which we derive two features for anomaly
detection. The intuition behind our method is that the action optimality
feature derived from the Q function can differentiate the optimal action from
others at each local state, and the sequential association feature derived from
the state value function has the potential to maintain the temporal
correlations between decisions (state-action pairs). Our experiments show that
OIL-AD can achieve outstanding online anomaly detection performance with up to
34.8% improvement in F1 score over comparable baselines.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04568" title="Abstract">arXiv:2402.04568</a> [<a href="/pdf/2402.04568" title="Download PDF">pdf</a>, <a href="/format/2402.04568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing User Interaction in ChatGPT: Characterizing and Consolidating  Multiple Prompts for Issue Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Saikat Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Bappon%2C+S+D">Suborno Deb Bappon</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+C+K">Chanchal K. Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted at the 21st International Conference on Mining Software Repositories (MSR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Prompt design plays a crucial role in shaping the efficacy of ChatGPT,
influencing the model's ability to extract contextually accurate responses.
Thus, optimal prompt construction is essential for maximizing the utility and
performance of ChatGPT. However, sub-optimal prompt design may necessitate
iterative refinement, as imprecise or ambiguous instructions can lead to
undesired responses from ChatGPT. Existing studies explore several prompt
patterns and strategies to improve the relevance of responses generated by
ChatGPT. However, the exploration of constraints that necessitate the
submission of multiple prompts is still an unmet attempt. In this study, our
contributions are twofold. First, we attempt to uncover gaps in prompt design
that demand multiple iterations. In particular, we manually analyze 686 prompts
that were submitted to resolve issues related to Java and Python programming
languages and identify eleven prompt design gaps (e.g., missing
specifications). Such gap exploration can enhance the efficacy of single
prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by
consolidating multiple prompts into a single one. We can completely consolidate
prompts with four gaps (e.g., missing context) and partially consolidate
prompts with three gaps (e.g., additional functionality). Such an effort
provides concrete evidence to users to design more optimal prompts mitigating
these gaps. Our study findings and evidence can - (a) save users time, (b)
reduce costs, and (c) increase user satisfaction.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04570" title="Abstract">arXiv:2402.04570</a> [<a href="/pdf/2402.04570" title="Download PDF">pdf</a>, <a href="/format/2402.04570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-NOMA integrated low-complexity transceiver architecture: Sum rate  and energy efficiency perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kota%2C+K+K">Kali Krishna Kota</a>, 
<a href="/search/cs?searchtype=author&query=Mankar%2C+P+D">Praful D. Mankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper aims to explore reconfigurable intelligent surface (RIS)
integration in a millimeter wave (mmWave) communication system with
low-complexity transceiver architecture under imperfect CSI assumption. Towards
this, we propose a RIS-aided system with a fully analog (FA) architecture at
the base station. However, to overcome the disadvantage of single-user
transmission due to the single RF-chain, we employ NOMA. For such a system, we
formulate sum rate (SR) and energy efficiency (EE) maximization problems to
obtain the joint transmit beamformer, RIS phase shift matrix, and power
allocation solutions under minimum rate constraint. We first tackle the
fractional objectives of both problems by reformulating the SR and EE
maximization problems into equivalent quadratic forms using the quadratic
transform. On the other hand, we employ successive convex approximation and the
semi-definite relaxation technique to handle the non-convex minimum rate and
unit modulus constraint of the RIS phase shifts, respectively. Next, we propose
an alternating optimization-based algorithm that iterates over the transmit
beamformer, power allocation, and RIS phase shift subproblems. Further, we also
show that the quadratic reformulation is equivalent to the WMSE-based
reformulation for the case of SR maximization problem. Our numerical results
show that the proposed RIS-NOMA integrated FA architecture system outperforms
the optimally configured fully digital architecture in terms of SR at low SNR
and EE for a wide range of SNR while still maintaining low hardware complexity
and cost. Finally, we present the numerical performance analysis of the
RIS-NOMA integrated low-complexity system for various system configuration
parameters.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04573" title="Abstract">arXiv:2402.04573</a> [<a href="/pdf/2402.04573" title="Download PDF">pdf</a>, <a href="/format/2402.04573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Conservative Adaptation for Evolving Target Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gangming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaoqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenhao He</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengwei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chaowei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conventional domain adaptation typically transfers knowledge from a source
domain to a stationary target domain. However, in many real-world cases, target
data usually emerge sequentially and have continuously evolving distributions.
Restoring and adapting to such target data results in escalating computational
and resource consumption over time. Hence, it is vital to devise algorithms to
address the evolving domain adaptation (EDA) problem, \emph{i.e.,} adapting
models to evolving target domains without access to historic target domains. To
achieve this goal, we propose a simple yet effective approach, termed
progressive conservative adaptation (PCAda). To manage new target data that
diverges from previous distributions, we fine-tune the classifier head based on
the progressively updated class prototypes. Moreover, as adjusting to the most
recent target domain can interfere with the features learned from previous
target domains, we develop a conservative sparse attention mechanism. This
mechanism restricts feature adaptation within essential dimensions, thus easing
the inference related to historical knowledge. The proposed PCAda is
implemented with a meta-learning framework, which achieves the fast adaptation
of the classifier with the help of the progressively updated class prototypes
in the inner loop and learns a generalized feature without severely interfering
with the historic knowledge via the conservative sparse attention in the outer
loop. Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate
the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04575" title="Abstract">arXiv:2402.04575</a> [<a href="/pdf/2402.04575" title="Download PDF">pdf</a>, <a href="/format/2402.04575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Identify Stack Overflow Questions Requiring Code Snippets?  Investigating the Cause &amp; Effect of Missing Code Snippets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Saikat Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Mohammad Masudur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+C+K">Chanchal K. Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for inclusion in the International Conference on Software Analysis, Evolution, and Reengineering (SANER 2024) technical program
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">On the Stack Overflow (SO) Q&amp;A site, users often request solutions to their
code-related problems (e.g., errors, unexpected behavior). Unfortunately, they
often miss required code snippets during their question submission, which could
prevent their questions from getting prompt and appropriate answers. In this
study, we conduct an empirical study investigating the cause &amp; effect of
missing code snippets in SO questions whenever required. Here, our
contributions are threefold. First, we analyze how the presence or absence of
required code snippets affects the correlation between question types (missed
code, included code after requests &amp; had code snippets during submission) and
corresponding answer meta-data (e.g., presence of an accepted answer).
According to our analysis, the chance of getting accepted answers is three
times higher for questions that include required code snippets during their
question submission than those that missed the code. We also investigate
whether the confounding factors (e.g., user reputation) affect questions
receiving answers besides the presence or absence of required code snippets. We
found that such factors do not hurt the correlation between the presence or
absence of required code snippets and answer meta-data. Second, we surveyed 64
practitioners to understand why users miss necessary code snippets. About 60%
of them agree that users are unaware of whether their questions require any
code snippets. Third, we thus extract four text-based features (e.g., keywords)
and build six ML models to identify the questions that need code snippets. Our
models can predict the target questions with 86.5% precision, 90.8% recall,
85.3% F1-score, and 85.2% overall accuracy. Our work has the potential to save
significant time in programming question-answering and improve the quality of
the valuable knowledge base by decreasing unanswered and unresolved questions.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04578" title="Abstract">arXiv:2402.04578</a> [<a href="/pdf/2402.04578" title="Download PDF">pdf</a>, <a href="/format/2402.04578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-Agents: self-organizing agents in open-ended environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preview, 23 pages, 12 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Leveraging large language models (LLMs), autonomous agents have significantly
improved, gaining the ability to handle a variety of tasks. In open-ended
settings, optimizing collaboration for efficiency and effectiveness demands
flexible adjustments. Despite this, current research mainly emphasizes fixed,
task-oriented workflows and overlooks agent-centric organizational structures.
Drawing inspiration from human organizational behavior, we introduce a
self-organizing agent system (S-Agents) with a "tree of agents" structure for
dynamic workflow, an "hourglass agent architecture" for balancing information
priorities, and a "non-obstructive collaboration" method to allow asynchronous
task execution among agents. This structure can autonomously coordinate a group
of agents, efficiently addressing the challenges of an open and dynamic
environment without human intervention. Our experiments demonstrate that
S-Agents proficiently execute collaborative building tasks and resource
collection in the Minecraft environment, validating their effectiveness.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04579" title="Abstract">arXiv:2402.04579</a> [<a href="/pdf/2402.04579" title="Download PDF">pdf</a>, <a href="/format/2402.04579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collective Counterfactual Explanations via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehyaei%2C+A">Ahmad-Reza Ehyaei</a>, 
<a href="/search/cs?searchtype=author&query=Shirali%2C+A">Ali Shirali</a>, 
<a href="/search/cs?searchtype=author&query=Samadi%2C+S">Samira Samadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Counterfactual explanations provide individuals with cost-optimal actions
that can alter their labels to desired classes. However, if substantial
instances seek state modification, such individual-centric methods can lead to
new competitions and unanticipated costs. Furthermore, these recommendations,
disregarding the underlying data distribution, may suggest actions that users
perceive as outliers. To address these issues, our work proposes a collective
approach for formulating counterfactual explanations, with an emphasis on
utilizing the current density of the individuals to inform the recommended
actions. Our problem naturally casts as an optimal transport problem.
Leveraging the extensive literature on optimal transport, we illustrate how
this collective method improves upon the desiderata of classical counterfactual
explanations. We support our proposal with numerical simulations, illustrating
the effectiveness of the proposed approach and its relation to classic methods.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04580" title="Abstract">arXiv:2402.04580</a> [<a href="/pdf/2402.04580" title="Download PDF">pdf</a>, <a href="/format/2402.04580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+H">Haoyi Niu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jianming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xianyuan Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The burgeoning fields of robot learning and embodied AI have triggered an
increasing demand for large quantities of data. However, collecting sufficient
unbiased data from the target domain remains a challenge due to costly data
collection processes and stringent safety requirements. Consequently,
researchers often resort to data from easily accessible source domains, such as
simulation and laboratory environments, for cost-effective data acquisition and
rapid model iteration. Nevertheless, the environments and embodiments of these
source domains can be quite different from their target domain counterparts,
underscoring the need for effective cross-domain policy transfer approaches. In
this paper, we conduct a systematic review of existing cross-domain policy
transfer methods. Through a nuanced categorization of domain gaps, we
encapsulate the overarching insights and design considerations of each problem
setting. We also provide a high-level discussion about the key methodologies
used in cross-domain policy transfer problems. Lastly, we summarize the open
challenges that lie beyond the capabilities of current paradigms and discuss
potential future directions in this field.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04581" title="Abstract">arXiv:2402.04581</a> [<a href="/pdf/2402.04581" title="Download PDF">pdf</a>, <a href="/format/2402.04581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Reinforcement Learning Algorithms in Continuous Robotic  Reaching Tasks using Adaptive Potential Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schomaker%2C+L">Lambert Schomaker</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+F">Francisco Cruz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In reinforcement learning, reward shaping is an efficient way to guide the
learning process of an agent, as the reward can indicate the optimal policy of
the task. The potential-based reward shaping framework was proposed to
guarantee policy invariance after reward shaping, where a potential function is
used to calculate the shaping reward. In former work, we proposed a novel
adaptive potential function (APF) method to learn the potential function
concurrently with training the agent based on information collected by the
agent during the training process, and examined the APF method in discrete
action space scenarios. This paper investigates the feasibility of using APF in
solving continuous-reaching tasks in a real-world robotic scenario with
continuous action space. We combine the Deep Deterministic Policy Gradient
(DDPG) algorithm and our proposed method to form a new algorithm called
APF-DDPG. To compare APF-DDPG with DDPG, we designed a task where the agent
learns to control Baxter's right arm to reach a goal position. The experimental
results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both
learning speed and robustness.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04583" title="Abstract">arXiv:2402.04583</a> [<a href="/pdf/2402.04583" title="Download PDF">pdf</a>, <a href="/format/2402.04583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Psychological Study: Importance of Contrast and Luminance in Color to  Grayscale Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ambalathankandy%2C+P">Prasoon Ambalathankandy</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yafei Ou</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+S">Sae Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Ikebe%2C+M">Masayuki Ikebe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Grayscale images are essential in image processing and computer vision tasks.
They effectively emphasize luminance and contrast, highlighting important
visual features, while also being easily compatible with other algorithms.
Moreover, their simplified representation makes them efficient for storage and
transmission purposes. While preserving contrast is important for maintaining
visual quality, other factors such as preserving information relevant to the
specific application or task at hand may be more critical for achieving optimal
performance. To evaluate and compare different decolorization algorithms, we
designed a psychological experiment. During the experiment, participants were
instructed to imagine color images in a hypothetical "colorless world" and
select the grayscale image that best resembled their mental visualization. We
conducted a comparison between two types of algorithms: (i) perceptual-based
simple color space conversion algorithms, and (ii) spatial contrast-based
algorithms, including iteration-based methods. Our experimental findings
indicate that CIELAB exhibited superior performance on average, providing
further evidence for the effectiveness of perception-based decolorization
algorithms. On the other hand, the spatial contrast-based algorithms showed
relatively poorer performance, possibly due to factors such as DC-offset and
artificial contrast generation. However, these algorithms demonstrated shorter
selection times. Notably, no single algorithm consistently outperformed the
others across all test images. In this paper, we will delve into a
comprehensive discussion on the significance of contrast and luminance in
color-to-grayscale mapping based on our experimental results and analysis.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04586" title="Abstract">arXiv:2402.04586</a> [<a href="/pdf/2402.04586" title="Download PDF">pdf</a>, <a href="/format/2402.04586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient anytime algorithms to solve the bi-objective Next Release  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dom%C3%ADnguez-R%C3%ADos%2C+M+%C3%81">Miguel &#xc1;ngel Dom&#xed;nguez-R&#xed;os</a>, 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>, 
<a href="/search/cs?searchtype=author&query=Alba%2C+E">Enrique Alba</a>, 
<a href="/search/cs?searchtype=author&query=del+%C3%81guila%2C+I+M">Isabel Mar&#xed;a del &#xc1;guila</a>, 
<a href="/search/cs?searchtype=author&query=del+Sagrado%2C+J">Jos&#xe9; del Sagrado</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Sys. Soft. 156: 217-231 (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The Next Release Problem consists in selecting a subset of requirements to
develop in the next release of a software product. The selection should be done
in a way that maximizes the satisfaction of the stakeholders while the
development cost is minimized and the constraints of the requirements are
fulfilled. Recent works have solved the problem using exact methods based on
Integer Linear Programming. In practice, there is no need to compute all the
efficient solutions of the problem; a well-spread set in the objective space is
more convenient for the decision maker. The exact methods used in the past to
find the complete Pareto front explore the objective space in a lexicographic
order or use a weighted sum of the objectives to solve a single-objective
problem, finding only supported solutions. In this work, we propose five new
methods that maintain a well-spread set of solutions at any time during the
search, so that the decision maker can stop the algorithm when a large enough
set of solutions is found. The methods are called anytime due to this feature.
They find both supported and non-supported solutions, and can complete the
whole Pareto front if the time provided is long enough.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04587" title="Abstract">arXiv:2402.04587</a> [<a href="/pdf/2402.04587" title="Download PDF">pdf</a>, <a href="/format/2402.04587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image  Modeling for CBCT Tooth Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+P">Pengyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yafei Ou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate tooth identification and segmentation in Cone Beam Computed
Tomography (CBCT) dental images can significantly enhance the efficiency and
precision of manual diagnoses performed by dentists. However, existing
segmentation methods are mainly developed based on large data volumes training,
on which their annotations are extremely time-consuming. Meanwhile, the teeth
of each class in CBCT dental images being closely positioned, coupled with
subtle inter-class differences, gives rise to the challenge of indistinct
boundaries when training model with limited data. To address these challenges,
this study aims to propose a tasked-oriented Masked Auto-Encoder paradigm to
effectively utilize large amounts of unlabeled data to achieve accurate tooth
segmentation with limited labeled data. Specifically, we first construct a
self-supervised pre-training framework of masked auto encoder to efficiently
utilize unlabeled data to enhance the network performance. Subsequently, we
introduce a sparse masked prompt mechanism based on graph attention to
incorporate boundary information of the teeth, aiding the network in learning
the anatomical structural features of teeth. To the best of our knowledge, we
are pioneering the integration of the mask pre-training paradigm into the CBCT
tooth segmentation task. Extensive experiments demonstrate both the feasibility
of our proposed method and the potential of the boundary prompt mechanism.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04588" title="Abstract">arXiv:2402.04588</a> [<a href="/pdf/2402.04588" title="Download PDF">pdf</a>, <a href="/format/2402.04588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised  Fine-tuning Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xujia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuzhuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Open-source large language models (LLMs) have gained significant strength
across diverse fields. Nevertheless, the majority of studies primarily
concentrate on English, with only limited exploration into the realm of
multilingual supervised fine-tuning. In this work, we therefore construct an
open-source multilingual supervised fine-tuning dataset. Different from
previous works that simply translate English instructions, we consider both the
language-specific and language-agnostic abilities of LLMs. For
language-specific abilities, we introduce a knowledge-grounded data
augmentation approach to elicit more culture-specific knowledge of LLMs,
improving their ability to serve users from different countries. For
language-agnostic abilities, we find through experiments that modern LLMs
exhibit strong cross-lingual transfer capabilities, thus repeatedly learning
identical content in various languages is not necessary. Consequently, we can
substantially prune the language-agnostic SFT data without any performance
degradation, making the SFT process more efficient. The resulting UltraLink
dataset comprises approximately 1 million samples across five languages, and
the proposed data construction method can also be easily extended to other
languages. UltraLink-LM, which is trained on UltraLink, outperforms several
representative baselines across many tasks.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04590" title="Abstract">arXiv:2402.04590</a> [<a href="/pdf/2402.04590" title="Download PDF">pdf</a>, <a href="/ps/2402.04590" title="Download PostScript">ps</a>, <a href="/format/2402.04590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concurrent Strategies on Games with Algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huriot-Tattegrain%2C+S">Sacha Huriot-Tattegrain</a>, 
<a href="/search/cs?searchtype=author&query=Winskel%2C+G">Glynn Winskel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages report as part of a master's degree internship
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Probabilistic concurrent/distributed strategies have so far not been
investigated thoroughly in the context of imperfect information, where the
Player has only partial knowledge of the moves made by the Opponent. In a
situation where the Player and Opponent can make concurrent moves according to
the game, and the Player cannot see the move of the Opponent, the move of the
Player should be probabilistically independent of the move of the Opponent.
What has been achieved is showing a bijection between strategies on a game with
algebra and strategies on a regular (albeit more complex) game. We also
succeeded in showing the results holds with neutral events. However it is still
unclear if a well-formed bicategory of concurrent games with algebras can be
defined. Our attempts to compose these strategies while managing the added
structure didn't pan out. Concerning the other classic extensions of concurrent
games the first results we presented show promise of a more general usage of
games with algebra.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04594" title="Abstract">arXiv:2402.04594</a> [<a href="/pdf/2402.04594" title="Download PDF">pdf</a>, <a href="/ps/2402.04594" title="Download PostScript">ps</a>, <a href="/format/2402.04594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ransomware Detection Dynamics: Insights and Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkongolo%2C+M">Mike Nkongolo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computing and Digital Systems, 15(1),
  pp.893-927. http://137.117.138.59/handle/123456789/5410
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The rise of ransomware attacks has necessitated the development of effective
strategies for identifying and mitigating these threats. This research
investigates the utilization of a feature selection algorithm for
distinguishing ransomware-related and benign transactions in both Bitcoin (BTC)
and United States Dollar (USD). Leveraging the UGRansome dataset, a
comprehensive repository of ransomware related BTC and USD transactions, we
propose a set of novel features designed to capture the distinct
characteristics of ransomware activity within the cryptocurrency ecosystem.
These features encompass transaction metadata, ransom analysis, and behavioral
patterns, offering a multifaceted view of ransomware-related financial
transactions. Through rigorous experimentation and evaluation, we demonstrate
the effectiveness of our feature set in accurately extracting BTC and USD
transactions, thereby aiding in the early detection and prevention of
ransomware-related financial flows. We introduce a Ransomware Feature Selection
Algorithm (RFSA) based on Gini Impurity and Mutual Information (MI) for
selecting crucial ransomware features from the UGRansome dataset. Insights from
the visualization highlight the potential of Gini Impurity and MI-based feature
selection to enhance ransomware detection systems by effectively discriminating
between ransomware classes. The analysis reveals that approximately 68% of
ransomware incidents involve BTC transactions within the range of 1.46 to 2.56,
with an average of 2.01 BTC transactions per attack. The findings emphasize the
dynamic and adaptable nature of ransomware demands, suggesting that there is no
fixed amount for specific cyberattacks, highlighting the evolving landscape of
ransomware threats.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04596" title="Abstract">arXiv:2402.04596</a> [<a href="/pdf/2402.04596" title="Download PDF">pdf</a>, <a href="/format/2402.04596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Improved Imbalance Robustness in Continual Multi-Label Learning  with Dual Output Spiking Architecture (DOSA)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Sourav Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Dora%2C+S">Shirin Dora</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+S">Suresh Sundaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 4 tables, 45 references. Submitted to IJCNN 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Algorithms designed for addressing typical supervised classification problems
can only learn from a fixed set of samples and labels, making them unsuitable
for the real world, where data arrives as a stream of samples often associated
with multiple labels over time. This motivates the study of task-agnostic
continual multi-label learning problems. While algorithms using deep learning
approaches for continual multi-label learning have been proposed in the recent
literature, they tend to be computationally heavy. Although spiking neural
networks (SNNs) offer a computationally efficient alternative to artificial
neural networks, existing literature has not used SNNs for continual
multi-label learning. Also, accurately determining multiple labels with SNNs is
still an open research problem. This work proposes a dual output spiking
architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss
function is also proposed, improving the multi-label classification performance
of the model by making it more robust to data imbalance. A modified F1 score is
presented to evaluate the effectiveness of the proposed loss function in
handling imbalance. Experiments on several benchmark multi-label datasets show
that DOSA trained with the proposed loss function shows improved robustness to
data imbalance and obtains better continual multi-label learning performance
than CIFDM, a previous state-of-the-art algorithm.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04597" title="Abstract">arXiv:2402.04597</a> [<a href="/pdf/2402.04597" title="Download PDF">pdf</a>, <a href="/format/2402.04597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMSA algorithm for solving the prioritized pairwise test data generation  problem in software product lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+J">Javier Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>, 
<a href="/search/cs?searchtype=author&query=Toro%2C+J+A+O">Jos&#xe9; Antonio Ortega Toro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the submitted version of the article in Journal of Heuristics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Heuristics 27(1-2): 229-249 (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">In Software Product Lines (SPLs) it may be difficult or even impossible to
test all the products of the family because of the large number of valid
feature combinations that may exist. Thus, we want to find a minimal subset of
the product family that allows us to test all these possible combinations
(pairwise). Furthermore, when testing a single product is a great effort, it is
desirable to first test products composed of a set of priority features. This
problem is called Prioritized Pairwise Test Data Generation Problem.
<br />State-of-the-art algorithms based on Integer Linear Programming for this
problema are faster enough for small and medium instances. However, there
exists some real instances that are too large to be computed with these
algorithms in a reasonable time because of the exponential growth of the number
of candidate solutions. Also, these heuristics not always lead us to the best
solutions. In this work we propose a new approach based on a hybrid
metaheuristic algorithm called Construct, Merge, Solve &amp; Adapt. We compare this
matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear
Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming
(HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm
called prioritized-ICPL. The analysis reveals that CMSA results in
statistically significantly better quality solutions in most instances and for
most levels of weighted coverage, although it requires more execution time.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04598" title="Abstract">arXiv:2402.04598</a> [<a href="/pdf/2402.04598" title="Download PDF">pdf</a>, <a href="/format/2402.04598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Data Agency and Autonomous Agents as Embodied Data  Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6mbs%2C+S">Sarah Sch&#xf6;mbs</a>, 
<a href="/search/cs?searchtype=author&query=Goncalves%2C+J">Jorge Goncalves</a>, 
<a href="/search/cs?searchtype=author&query=Johal%2C+W">Wafa Johal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 1 figure, Presented as poster at 2023 IEEE Visualization Conference (VIS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In the light of recent advances in embodied data visualizations, we aim to
shed light on agency in the context of data visualization. To do so, we
introduce Data Agency and Data-Agent Interplay as potential terms and research
focus. Furthermore, we exemplify the former in the context of human-robot
interaction, and identify future challenges and research questions.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04599" title="Abstract">arXiv:2402.04599</a> [<a href="/pdf/2402.04599" title="Download PDF">pdf</a>, <a href="/format/2402.04599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via  Temporal-Viewpoint Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under minor revision with IJCV. An extension of our ACCV'22 paper [arXiv:<a href="/abs/2210.16820">arXiv:2210.16820</a>] which was distinguished by the Sang Uk Lee Best Student Paper Award. arXiv admin note: text overlap with <a href="/abs/2112.12668">arXiv:2112.12668</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Video sequences exhibit significant nuisance variations (undesired effects)
of speed of actions, temporal locations, and subjects' poses, leading to
temporal-viewpoint misalignment when comparing two sets of frames or evaluating
the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera
viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D
skeleton sequences whose camera and subjects' poses can be easily manipulated
in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where
matching well temporal blocks (temporal chunks that make up a sequence) of
support-query sequence pairs (by factoring out nuisance variations) is
essential due to limited samples of novel classes. Given a query sequence, we
create its several views by simulating several camera locations. For a support
sequence, we match it with view-simulated query sequences, as in the popular
Dynamic Time Warping (DTW). Specifically, each support temporal block can be
matched to the query temporal block with the same or adjacent (next) temporal
index, and adjacent camera views to achieve joint local temporal-viewpoint
warping. JEANIE selects the smallest distance among matching paths with
different temporal-viewpoint warping patterns, an advantage over DTW which only
performs temporal alignment. We also propose an unsupervised FSAR akin to
clustering of sequences with JEANIE as a distance measure. JEANIE achieves
state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D
Multiview Activity II on supervised and unsupervised FSAR, and their
meta-learning inspired fusion.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04601" title="Abstract">arXiv:2402.04601</a> [<a href="/pdf/2402.04601" title="Download PDF">pdf</a>, <a href="/format/2402.04601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haihui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chinese grammatical error correction (CGEC) faces serious overcorrection
challenges when employing autoregressive generative models such as
sequence-to-sequence (Seq2Seq) models and decoder-only large language models
(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,
they are difficult to adapt to decoder-only LLMs. In this paper, we propose an
alignment-enhanced corrector for the overcorrection problem that applies to
both Seq2Seq models and decoder-only LLMs. Our method first trains a correction
model to generate an initial correction of the source sentence. Then, we
combine the source sentence with the initial correction and feed it through an
alignment model for another round of correction, aiming to enforce the
alignment model to focus on potential overcorrection. Moreover, to enhance the
model's ability to identify nuances, we further explore the reverse alignment
of the source sentence and the initial correction. Finally, we transfer the
alignment knowledge from two alignment models to the correction model,
instructing it on how to avoid overcorrection. Experimental results on three
CGEC datasets demonstrate the effectiveness of our approach in alleviating
overcorrection and improving overall performance.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04607" title="Abstract">arXiv:2402.04607</a> [<a href="/pdf/2402.04607" title="Download PDF">pdf</a>, <a href="/format/2402.04607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Google Scholar is manipulatable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+H">Hazem Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>, 
<a href="/search/cs?searchtype=author&query=Rahwan%2C+T">Talal Rahwan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Digital Libraries (cs.DL); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Citations are widely considered in scientists' evaluation. As such,
scientists may be incentivized to inflate their citation counts. While previous
literature has examined self-citations and citation cartels, it remains unclear
whether scientists can purchase citations. Here, we compile a dataset of ~1.6
million profiles on Google Scholar to examine instances of citation fraud on
the platform. We survey faculty at highly-ranked universities, and confirm that
Google Scholar is widely used when evaluating scientists. Intrigued by a
citation-boosting service that we unravelled during our investigation, we
contacted the service while undercover as a fictional author, and managed to
purchase 50 citations. These findings provide conclusive evidence that
citations can be bought in bulk, and highlight the need to look beyond citation
counts.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04609" title="Abstract">arXiv:2402.04609</a> [<a href="/pdf/2402.04609" title="Download PDF">pdf</a>, <a href="/format/2402.04609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Cross-Domain Low-Resource Text Generation through LLM  Post-Editing: A Programmer-Interpreter Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Haroutunian%2C+L">Levon Haroutunian</a>, 
<a href="/search/cs?searchtype=author&query=Tumuluri%2C+R">Raj Tumuluri</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+P">Philip Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 (findings), short paper, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Post-editing has proven effective in improving the quality of text generated
by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when
direct updating of their parameters to enhance text quality is infeasible or
expensive. However, relying solely on smaller language models for post-editing
can limit the LLMs' ability to generalize across domains. Moreover, the editing
strategies in these methods are not optimally designed for text-generation
tasks. To address these limitations, we propose a neural programmer-interpreter
approach that preserves the domain generalization ability of LLMs when editing
their output. The editing actions in this framework are specifically devised
for text generation. Extensive experiments demonstrate that the
programmer-interpreter significantly enhances GPT-3.5's performance in logical
form-to-text conversion and low-resource machine translation, surpassing other
state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04610" title="Abstract">arXiv:2402.04610</a> [<a href="/pdf/2402.04610" title="Download PDF">pdf</a>, <a href="/format/2402.04610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early Stopping of Untrained Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jahn%2C+T">Tim Jahn</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+B">Bangti Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In recent years, new regularization methods based on (deep) neural networks
have shown very promising empirical performance for the numerical solution of
ill-posed problems, such as in medical imaging and imaging science. Due to the
nonlinearity of neural networks, these methods often lack satisfactory
theoretical justification. In this work, we rigorously discuss the convergence
of a successful unsupervised approach that utilizes untrained convolutional
neural networks to represent solutions to linear ill-posed problems. Untrained
neural networks have particular appeal for many applications because they do
not require paired training data. The regularization property of the approach
relies solely on the architecture of the neural network instead. Due to the
vast over-parameterization of the employed neural network, suitable early
stopping is essential for the success of the method. We establish that the
classical discrepancy principle is an adequate method for early stopping of
two-layer untrained convolutional neural networks learned by gradient descent,
and furthermore, it yields an approximation with minimax optimal convergence
rates. Numerical results are also presented to illustrate the theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04614" title="Abstract">arXiv:2402.04614</a> [<a href="/pdf/2402.04614" title="Download PDF">pdf</a>, <a href="/format/2402.04614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations  from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+C">Chirag Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Tanneru%2C+S+H">Sree Harsha Tanneru</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we urge the community
to identify the faithfulness requirements of real-world applications and ensure
explanations meet those needs. Finally, we propose some directions for future
work, emphasizing the need for novel methodologies and frameworks that can
enhance the faithfulness of self-explanations without compromising their
plausibility, essential for the transparent deployment of LLMs in diverse
high-stakes domains.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04615" title="Abstract">arXiv:2402.04615</a> [<a href="/pdf/2402.04615" title="Download PDF">pdf</a>, <a href="/format/2402.04615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScreenAI: A Vision-Language Model for UI and Infographics Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baechler%2C+G">Gilles Baechler</a>, 
<a href="/search/cs?searchtype=author&query=Sunkara%2C+S">Srinivas Sunkara</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maria Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zubach%2C+F">Fedir Zubach</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+H">Hassan Mansoor</a>, 
<a href="/search/cs?searchtype=author&query=Etter%2C+V">Vincent Etter</a>, 
<a href="/search/cs?searchtype=author&query=C%C4%83rbune%2C+V">Victor C&#x103;rbune</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jason Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhanshu Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages main tex with 5 figures, 2 page bib, 6 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Screen user interfaces (UIs) and infographics, sharing similar visual
language and design principles, play important roles in human communication and
human-machine interaction. We introduce ScreenAI, a vision-language model that
specializes in UI and infographics understanding. Our model improves upon the
PaLI architecture with the flexible patching strategy of pix2struct and is
trained on a unique mixture of datasets. At the heart of this mixture is a
novel screen annotation task in which the model has to identify the type and
location of UI elements. We use these text annotations to describe screens to
Large Language Models and automatically generate question-answering (QA), UI
navigation, and summarization training datasets at scale. We run ablation
studies to demonstrate the impact of these design choices. At only 5B
parameters, ScreenAI achieves new state-of-the-artresults on UI- and
infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget
Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and
InfographicVQA) compared to models of similar size. Finally, we release three
new datasets: one focused on the screen annotation task and two others focused
on question answering.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04616" title="Abstract">arXiv:2402.04616</a> [<a href="/pdf/2402.04616" title="Download PDF">pdf</a>, <a href="/format/2402.04616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TinyLLM: Learning a Small Student from Multiple Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yikun Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Transferring the reasoning capability from stronger large language models
(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more
flexible to deploy with less expense. Among the existing solutions, knowledge
distillation stands out due to its outstanding efficiency and generalization.
However, existing methods suffer from several drawbacks, including limited
knowledge diversity and the lack of rich contextual information. To solve the
problems and facilitate the learning of compact language models, we propose
TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM
from multiple large teacher LLMs. In particular, we encourage the student LLM
to not only generate the correct answers but also understand the rationales
behind these answers. Given that different LLMs possess diverse reasoning
skills, we guide the student model to assimilate knowledge from various teacher
LLMs. We further introduce an in-context example generator and a
teacher-forcing Chain-of-Thought strategy to ensure that the rationales are
accurate and grounded in contextually appropriate scenarios. Extensive
experiments on six datasets across two reasoning tasks demonstrate the
superiority of our method. Results show that TinyLLM can outperform large
teacher LLMs significantly, despite having a considerably smaller model size.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04617" title="Abstract">arXiv:2402.04617</a> [<a href="/pdf/2402.04617" title="Download PDF">pdf</a>, <a href="/format/2402.04617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding  Extremely Long Sequences with Training-Free Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Guangxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have emerged as a cornerstone in real-world
applications with lengthy streaming inputs, such as LLM-driven agents. However,
existing LLMs, pre-trained on sequences with restricted maximum length, cannot
generalize to longer sequences due to the out-of-domain and distraction issues.
To alleviate these issues, existing efforts employ sliding attention windows
and discard distant tokens to achieve the processing of extremely long
sequences. Unfortunately, these approaches inevitably fail to capture
long-distance dependencies within sequences to deeply understand semantics.
This paper introduces a training-free memory-based method, InfLLM, to unveil
the intrinsic ability of LLMs to process streaming long sequences.
Specifically, InfLLM stores distant contexts into additional memory units and
employs an efficient mechanism to lookup token-relevant units for attention
computation. Thereby, InfLLM allows LLMs to efficiently process long sequences
while maintaining the ability to capture long-distance dependencies. Without
any training, InfLLM enables LLMs pre-trained on sequences of a few thousand
tokens to achieve superior performance than competitive baselines continually
training these LLMs on long sequences. Even when the sequence length is scaled
to $1,024$K, InfLLM still effectively captures long-distance dependencies.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04618" title="Abstract">arXiv:2402.04618</a> [<a href="/pdf/2402.04618" title="Download PDF">pdf</a>, <a href="/format/2402.04618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Scale Semantic Segmentation with Modified MBConv Blocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Bo Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+T">Taesung Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, MBConv blocks, initially designed for efficiency in
resource-limited settings and later adapted for cutting-edge image
classification performances, have demonstrated significant potential in image
classification tasks. Despite their success, their application in semantic
segmentation has remained relatively unexplored. This paper introduces a novel
adaptation of MBConv blocks specifically tailored for semantic segmentation.
Our modification stems from the insight that semantic segmentation requires the
extraction of more detailed spatial information than image classification. We
argue that to effectively perform multi-scale semantic segmentation, each
branch of a U-Net architecture, regardless of its resolution, should possess
equivalent segmentation capabilities. By implementing these changes, our
approach achieves impressive mean Intersection over Union (IoU) scores of 84.5%
and 84.0% on the Cityscapes test and validation datasets, respectively,
demonstrating the efficacy of our proposed modifications in enhancing semantic
segmentation performance.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04620" title="Abstract">arXiv:2402.04620</a> [<a href="/pdf/2402.04620" title="Download PDF">pdf</a>, <a href="/format/2402.04620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract  Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramjee%2C+P">Pragnya Ramjee</a>, 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+B">Bhuvan Sachdeva</a>, 
<a href="/search/cs?searchtype=author&query=Golechha%2C+S">Satvik Golechha</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shreyas Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Fulari%2C+G">Geeta Fulari</a>, 
<a href="/search/cs?searchtype=author&query=Murali%2C+K">Kaushik Murali</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+M">Mohit Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The healthcare landscape is evolving, with patients seeking more reliable
information about their health conditions, treatment options, and potential
risks. Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
doctors and hospital staff, highlighting the need for expert-endorsed health
information. However, the pressure on experts has led to reduced communication
time, impacting information sharing. To address this gap, we propose
CataractBot, an experts-in-the-loop chatbot powered by large language models
(LLMs). Developed in collaboration with a tertiary eye hospital in India,
CataractBot answers cataract surgery related questions instantly by querying a
curated knowledge base, and provides expert-verified responses asynchronously.
CataractBot features multimodal support and multilingual capabilities. In an
in-the-wild deployment study with 49 participants, CataractBot proved valuable,
providing anytime accessibility, saving time, and accommodating diverse
literacy levels. Trust was established through expert verification. Broadly,
our results could inform future work on designing expert-mediated LLM bots.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04621" title="Abstract">arXiv:2402.04621</a> [<a href="/pdf/2402.04621" title="Download PDF">pdf</a>, <a href="/format/2402.04621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Distribution on Graph Topology Mediates the Effect of Graph  Convolution: Homophily Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+Y">Soo Yong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+F">Fanchen Bu</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jaemin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">How would randomly shuffling feature vectors among nodes from the same class
affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs
the dependence between graph topology and features (A-X dependence) for GNNs to
learn from. Surprisingly, we observe a consistent and significant improvement
in GNN performance following the feature shuffle. Having overlooked the impact
of A-X dependence on GNNs, the prior literature does not provide a satisfactory
understanding of the phenomenon. Thus, we raise two research questions. First,
how should A-X dependence be measured, while controlling for potential
confounds? Second, how does A-X dependence affect GNNs? In response, we (i)
propose a principled measure for A-X dependence, (ii) design a random graph
model that controls A-X dependence, (iii) establish a theory on how A-X
dependence relates to graph convolution, and (iv) present empirical analysis on
real-world graphs that aligns with the theory. We conclude that A-X dependence
mediates the effect of graph convolution, such that smaller dependence improves
GNN-based node classification.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04623" title="Abstract">arXiv:2402.04623</a> [<a href="/pdf/2402.04623" title="Download PDF">pdf</a>, <a href="/format/2402.04623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validity-Preserving Delta Debugging via Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Luyao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Z">Ziyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yanyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiao He</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Reducing test inputs that trigger bugs is crucial for efficient debugging.
Delta debugging is the most popular approach for this purpose. When test inputs
need to conform to certain specifications, existing delta debugging practice
encounters a validity problem: it blindly applies reduction rules, producing a
large number of invalid test inputs that do not satisfy the required
specifications. This overall diminishing effectiveness and efficiency becomes
even more pronounced when the specifications extend beyond syntactical
structures. Our key insight is that we should leverage input generators, which
are aware of these specifications, to generate valid reduced inputs, rather
than straightforwardly performing reduction on test inputs. In this paper, we
propose a generator-based delta debugging method, namely GReduce, which derives
validity-preserving reducers. Specifically, given a generator and its
execution, demonstrating how the bug-inducing test input is generated, GReduce
searches for other executions on the generator that yield reduced, valid test
inputs. To evaluate the effectiveness, efficiency, and versatility of GReduce,
we apply GReduce and the state-of-the-art reducer Perses in three domains:
graphs, deep learning models, and JavaScript programs. The results of GReduce
are 28.5%, 34.6%, 75.6% in size of those from Perses, and GReduce takes 17.5%,
0.6%, 65.4% time taken by Perses.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04624" title="Abstract">arXiv:2402.04624</a> [<a href="/pdf/2402.04624" title="Download PDF">pdf</a>, <a href="/format/2402.04624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEMORYLLM: Towards Self-Updatable Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing Large Language Models (LLMs) usually remain static after deployment,
which might make it hard to inject new knowledge into the model. We aim to
build models containing a considerable portion of self-updatable parameters,
enabling the model to integrate new knowledge effectively and efficiently. To
this end, we introduce MEMORYLLM, a model that comprises a transformer and a
fixed-size memory pool within the latent space of the transformer. MEMORYLLM
can self-update with text knowledge and memorize the knowledge injected
earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively
incorporate new knowledge, as evidenced by its performance on model editing
benchmarks. Meanwhile, the model exhibits long-term information retention
capacity, which is validated through our custom-designed evaluations and
long-context benchmarks. MEMORYLLM also shows operational integrity without any
sign of performance degradation even after nearly a million memory updates.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04625" title="Abstract">arXiv:2402.04625</a> [<a href="/pdf/2402.04625" title="Download PDF">pdf</a>, <a href="/format/2402.04625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise Map Guidance: Inversion with Spatial Context for Real Image  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hansam Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+B">Seoung Bum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+T">Tae-Hyun Oh</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+Y">Yonghyun Jeong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-guided diffusion models have become a popular tool in image synthesis,
known for producing high-quality and diverse images. However, their application
to editing real images often encounters hurdles primarily due to the text
condition deteriorating the reconstruction quality and subsequently affecting
editing fidelity. Null-text Inversion (NTI) has made strides in this area, but
it fails to capture spatial context and requires computationally intensive
per-timestep optimization. Addressing these challenges, we present Noise Map
Guidance (NMG), an inversion method rich in a spatial context, tailored for
real-image editing. Significantly, NMG achieves this without necessitating
optimization, yet preserves the editing quality. Our empirical investigations
highlight NMG's adaptability across various editing techniques and its
robustness to variants of DDIM inversions.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04627" title="Abstract">arXiv:2402.04627</a> [<a href="/pdf/2402.04627" title="Download PDF">pdf</a>, <a href="/format/2402.04627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question  Answering over a Life Science Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rangel%2C+J+C">Julio C. Rangel</a>, 
<a href="/search/cs?searchtype=author&query=de+Farias%2C+T+M">Tarcisio Mendes de Farias</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+A+C">Ana Claudia Sima</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+N">Norio Kobayashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proceedings of SWAT4HCLS 2024: Semantic Web Tools and Applications for Healthcare and Life Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Databases (cs.DB); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The recent success of Large Language Models (LLM) in a wide range of Natural
Language Processing applications opens the path towards novel Question
Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the
main obstacles preventing their implementation is the scarcity of training data
for the task of translating questions into corresponding SPARQL queries,
particularly in the case of domain-specific KGs. To overcome this challenge, in
this study, we evaluate several strategies for fine-tuning the OpenLlama LLM
for question answering over life science knowledge graphs. In particular, we
propose an end-to-end data augmentation approach for extending a set of
existing queries over a given knowledge graph towards a larger dataset of
semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even
for datasets where these pairs are scarce. In this context, we also investigate
the role of semantic "clues" in the queries, such as meaningful variable names
and inline comments. Finally, we evaluate our approach over the real-world Bgee
gene expression knowledge graph and we show that semantic clues can improve
model performance by up to 33% compared to a baseline with random variable
names and no comments included.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04630" title="Abstract">arXiv:2402.04630</a> [<a href="/pdf/2402.04630" title="Download PDF">pdf</a>, <a href="/format/2402.04630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained  Descriptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Sheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xueying Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaxing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lewei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Inspired by the outstanding zero-shot capability of vision language models
(VLMs) in image classification tasks, open-vocabulary object detection has
attracted increasing interest by distilling the broad VLM knowledge into
detector training. However, most existing open-vocabulary detectors learn by
aligning region embeddings with categorical labels (e.g., bicycle) only,
disregarding the capability of VLMs on aligning visual embeddings with
fine-grained text description of object parts (e.g., pedals and bells). This
paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that
introduces conditional context prompts and hierarchical textual descriptors
that enable precise region-text alignment as well as open-vocabulary detection
training in general. Specifically, the conditional context prompt transforms
regional embeddings into image-like representations that can be directly
integrated into general open vocabulary detection training. In addition, we
introduce large language models as an interactive and implicit knowledge
repository which enables iterative mining and refining visually oriented
textual descriptors for precise region-text alignment. Extensive experiments
over multiple large-scale benchmarks show that DVDet outperforms the
state-of-the-art consistently by large margins.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04631" title="Abstract">arXiv:2402.04631</a> [<a href="/pdf/2402.04631" title="Download PDF">pdf</a>, <a href="/format/2402.04631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents:  New Perspectives and Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+B">Bin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yasan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiwen Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Persuasion, as one of the crucial abilities in human communication, has
garnered extensive attention from researchers within the field of intelligent
dialogue systems. We humans tend to persuade others to change their viewpoints,
attitudes or behaviors through conversations in various scenarios (e.g.,
persuasion for social good, arguing in online platforms). Developing dialogue
agents that can persuade others to accept certain standpoints is essential to
achieving truly intelligent and anthropomorphic dialogue system. Benefiting
from the substantial progress of Large Language Models (LLMs), dialogue agents
have acquired an exceptional capability in context understanding and response
generation. However, as a typical and complicated cognitive psychological
system, persuasive dialogue agents also require knowledge from the domain of
cognitive psychology to attain a level of human-like persuasion. Consequently,
the cognitive strategy-enhanced persuasive dialogue agent (defined as
CogAgent), which incorporates cognitive strategies to achieve persuasive
targets through conversation, has become a predominant research paradigm. To
depict the research trends of CogAgent, in this paper, we first present several
fundamental cognitive psychology theories and give the formalized definition of
three typical cognitive strategies, including the persuasion strategy, the
topic path planning strategy, and the argument structure prediction strategy.
Then we propose a new system architecture by incorporating the formalized
definition to lay the foundation of CogAgent. Representative works are detailed
and investigated according to the combined cognitive strategy, followed by the
summary of authoritative benchmarks and evaluation metrics. Finally, we
summarize our insights on open issues and future directions of CogAgent for
upcoming researchers.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04632" title="Abstract">arXiv:2402.04632</a> [<a href="/pdf/2402.04632" title="Download PDF">pdf</a>, <a href="/format/2402.04632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GSN: Generalisable Segmentation in Neural Radiance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vinayak Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+R">Rahul Goel</a>, 
<a href="/search/cs?searchtype=author&query=Dhawal%2C+S">Sirikonda Dhawal</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+P+J">P. J. Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Main Technical Track of AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Traditional Radiance Field (RF) representations capture details of a specific
scene and must be trained afresh on each scene. Semantic feature fields have
been added to RFs to facilitate several segmentation tasks. Generalised RF
representations learn the principles of view interpolation. A generalised RF
can render new views of an unknown and untrained scene, given a few views. We
present a way to distil feature fields into the generalised GNT representation.
Our GSN representation generates new views of unseen scenes on the fly along
with consistent, per-pixel semantic features. This enables multi-view
segmentation of arbitrary new scenes. We show different semantic features being
distilled into generalised RFs. Our multi-view segmentation results are on par
with methods that use traditional RFs. GSN closes the gap between standard and
generalisable RF methods significantly. Project Page:
https://vinayak-vg.github.io/GSN/
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04634" title="Abstract">arXiv:2402.04634</a> [<a href="/pdf/2402.04634" title="Download PDF">pdf</a>, <a href="/format/2402.04634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Transaction Fees? No Problem! Achieving Fairness in Transaction Fee  Mechanism Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Damle%2C+S">Sankarshan Damle</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+V">Varul Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Gujar%2C+S">Sujit Gujar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract (AAMAS '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The recently proposed Transaction Fee Mechanism (TFM) literature studies the
strategic interaction between the miner of a block and the transaction creators
(or users) in a blockchain. In a TFM, the miner includes transactions that
maximize its utility while users submit fees for a slot in the block. The
existing TFM literature focuses on satisfying standard incentive properties --
which may limit widespread adoption. We argue that a TFM is "fair" to the
transaction creators if it satisfies specific notions, namely Zero-fee
Transaction Inclusion and Monotonicity. First, we prove that one generally
cannot ensure both these properties and prevent a miner's strategic
manipulation. We also show that existing TFMs either do not satisfy these
notions or do so at a high cost to the miners' utility. As such, we introduce a
novel TFM using on-chain randomness -- rTFM. We prove that rTFM guarantees
incentive compatibility for miners and users while satisfying our novel
fairness constraints.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04636" title="Abstract">arXiv:2402.04636</a> [<a href="/pdf/2402.04636" title="Download PDF">pdf</a>, <a href="/format/2402.04636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransLLaMa: LLM-based Simultaneous Translation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koshkin%2C+R">Roman Koshkin</a>, 
<a href="/search/cs?searchtype=author&query=Sudoh%2C+K">Katsuhito Sudoh</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Satoshi Nakamura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Decoder-only large language models (LLMs) have recently demonstrated
impressive capabilities in text generation and reasoning. Nonetheless, they
have limited applications in simultaneous machine translation (SiMT), currently
dominated by encoder-decoder transformers. This study demonstrates that, after
fine-tuning on a small dataset comprising causally aligned source and target
sentence pairs, a pre-trained open-source LLM can control input segmentation
directly by generating a special "wait" token. This obviates the need for a
separate policy and enables the LLM to perform English-German and
English-Russian SiMT tasks with BLEU scores that are comparable to those of
specific state-of-the-art baselines. We also evaluated closed-source models
such as GPT-4, which displayed encouraging results in performing the SiMT task
without prior training (zero-shot), indicating a promising avenue for enhancing
future SiMT systems.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04638" title="Abstract">arXiv:2402.04638</a> [<a href="/pdf/2402.04638" title="Download PDF">pdf</a>, <a href="/format/2402.04638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient unconditional energy stable scheme for the simulation of  droplet formation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jinpeng Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Changjuan Zhang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaoping Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">We have developed an efficient and unconditionally energy-stable method for
simulating droplet formation dynamics. Our approach involves a novel
time-marching scheme based on the scalar auxiliary variable technique,
specifically designed for solving the Cahn-Hilliard-Navier-Stokes phase field
model with variable density and viscosity. We have successfully applied this
method to simulate droplet formation in scenarios where a Newtonian fluid is
injected through a vertical tube into another immiscible Newtonian fluid. To
tackle the challenges posed by nonhomogeneous Dirichlet boundary conditions at
the tube entrance, we have introduced additional nonlocal auxiliary variables
and associated ordinary differential equations. These additions effectively
eliminate the influence of boundary terms. Moreover, we have incorporated
stabilization terms into the scheme to enhance its numerical effectiveness.
Notably, our resulting scheme is fully decoupled, requiring the solution of
only linear systems at each time step. We have also demonstrated the energy
decaying property of the scheme, with suitable modifications. To assess the
accuracy and stability of our algorithm, we have conducted extensive numerical
simulations. Additionally, we have examined the dynamics of droplet formation
and explored the impact of dimensionless parameters on the process. Overall,
our work presents a refined method for simulating droplet formation dynamics,
offering improved efficiency, energy stability, and accuracy.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04640" title="Abstract">arXiv:2402.04640</a> [<a href="/pdf/2402.04640" title="Download PDF">pdf</a>, <a href="/format/2402.04640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Bridge: Generative model-based domain forensic for black-box  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ee-Chien Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In forensic investigations of machine learning models, techniques that
determine a model's data domain play an essential role, with prior work relying
on large-scale corpora like ImageNet to approximate the target model's domain.
Although such methods are effective in finding broad domains, they often
struggle in identifying finer-grained classes within those domains. In this
paper, we introduce an enhanced approach to determine not just the general data
domain (e.g., human face) but also its specific attributes (e.g., wearing
glasses). Our approach uses an image embedding model as the encoder and a
generative model as the decoder. Beginning with a coarse-grained description,
the decoder generates a set of images, which are then presented to the unknown
target model. Successful classifications by the model guide the encoder to
refine the description, which in turn, are used to produce a more specific set
of images in the subsequent iteration. This iterative refinement narrows down
the exact class of interest. A key strength of our approach lies in leveraging
the expansive dataset, LAION-5B, on which the generative model Stable Diffusion
is trained. This enlarges our search space beyond traditional corpora, such as
ImageNet. Empirical results showcase our method's performance in identifying
specific attributes of a model's input domain, paving the way for more detailed
forensic analyses of deep learning models.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04644" title="Abstract">arXiv:2402.04644</a> [<a href="/pdf/2402.04644" title="Download PDF">pdf</a>, <a href="/format/2402.04644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different  Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roh%2C+Y">Yuji Roh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Huan Gui</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhe Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yujin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Whang%2C+S+E">Steven Euijong Whang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Shuchao Bi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhe Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fine-tuning is becoming widely used for leveraging the power of pre-trained
foundation models in new downstream tasks. While there are many successes of
fine-tuning on various tasks, recent studies have observed challenges in the
generalization of fine-tuned models to unseen distributions (i.e.,
out-of-distribution; OOD). To improve OOD generalization, some previous studies
identify the limitations of fine-tuning data and regulate fine-tuning to
preserve the general representation learned from pre-training data. However,
potential limitations in the pre-training data and models are often ignored. In
this paper, we contend that overly relying on the pre-trained representation
may hinder fine-tuning from learning essential representations for downstream
tasks and thus hurt its OOD generalization. It can be especially catastrophic
when new tasks are from different (sub)domains compared to pre-training data.
To address the issues in both pre-training and fine-tuning data, we propose a
novel generalizable fine-tuning method LEVI, where the pre-trained model is
adaptively ensembled layer-wise with a small task-specific model, while
preserving training and inference efficiencies. By combining two complementing
models, LEVI effectively suppresses problematic features in both the
fine-tuning data and pre-trained model and preserves useful features for new
tasks. Broad experiments with large language and vision models show that LEVI
greatly improves fine-tuning generalization via emphasizing different views
from fine-tuning data and pre-trained features.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04645" title="Abstract">arXiv:2402.04645</a> [<a href="/pdf/2402.04645" title="Download PDF">pdf</a>, <a href="/format/2402.04645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity Modification in the Stable Matching Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+S">Salil Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+S">Shivika Narang</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Samarth Singla</a>, 
<a href="/search/cs?searchtype=author&query=Vaish%2C+R">Rohit Vaish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the problem of capacity modification in the many-to-one stable
matching of workers and firms. Our goal is to systematically study how the set
of stable matchings changes when some seats are added to or removed from the
firms. We make three main contributions: First, we examine whether firms and
workers can improve or worsen upon changing the capacities under
worker-proposing and firm-proposing deferred acceptance algorithms. Second, we
study the computational problem of adding or removing seats to either match a
fixed worker-firm pair in some stable matching or make a fixed matching stable
with respect to the modified problem. We develop polynomial-time algorithms for
these problems when only the overall change in the firms' capacities is
restricted, and show NP-hardness when there are additional constraints for
individual firms. Lastly, we compare capacity modification with the classical
model of preference manipulation by firms and identify scenarios under which
one mode of manipulation outperforms the other. We find that a threshold on a
given firm's capacity, which we call its peak, crucially determines the
effectiveness of different manipulation actions.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04646" title="Abstract">arXiv:2402.04646</a> [<a href="/pdf/2402.04646" title="Download PDF">pdf</a>, <a href="/format/2402.04646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Diversification from Block Sparse Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yong Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper introduces a novel prior called Diversified Block Sparse Prior to
characterize the widespread block sparsity phenomenon in real-world data. By
allowing diversification on variance and correlation matrix, we effectively
address the sensitivity issue of existing block sparse learning methods to
pre-defined block information, which enables adaptive block estimation while
mitigating the risk of overfitting. Based on this, a diversified block sparse
Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual
ascent method for hyperparameter estimation. Moreover, we establish the global
and local optimality theory of our model. Experiments validate the advantages
of DivSBL over existing algorithms.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04647" title="Abstract">arXiv:2402.04647</a> [<a href="/pdf/2402.04647" title="Download PDF">pdf</a>, <a href="/format/2402.04647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Plan Transformer: Planning as Latent Variable Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deqian Kong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dehong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Minglu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+B">Bo Pang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jianwen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lizarraga%2C+A">Andrew Lizarraga</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sirui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In tasks aiming for long-term returns, planning becomes necessary. We study
generative modeling for planning with datasets repurposed from offline
reinforcement learning. Specifically, we identify temporal consistency in the
absence of step-wise rewards as one key technical challenge. We introduce the
Latent Plan Transformer (LPT), a novel model that leverages a latent space to
connect a Transformer-based trajectory generator and the final return. LPT can
be learned with maximum likelihood estimation on trajectory-return pairs. In
learning, posterior sampling of the latent variable naturally gathers
sub-trajectories to form a consistent abstraction despite the finite context.
During test time, the latent variable is inferred from an expected return
before policy execution, realizing the idea of planning as inference. It then
guides the autoregressive policy throughout the episode, functioning as a plan.
Our experiments demonstrate that LPT can discover improved decisions from
suboptimal trajectories. It achieves competitive performance across several
benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting
capabilities of nuanced credit assignments, trajectory stitching, and
adaptation to environmental contingencies. These results validate that latent
variable inference can be a strong alternative to step-wise reward prompting.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04648" title="Abstract">arXiv:2402.04648</a> [<a href="/pdf/2402.04648" title="Download PDF">pdf</a>, <a href="/format/2402.04648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+G">Guibiao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaichen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Z">Zhenyu Bao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kanglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The development of Neural Radiance Fields (NeRFs) has provided a potent
representation for encapsulating the geometric and appearance characteristics
of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D
semantic perception tasks has been a recent focus. However, current methods
that extract semantics directly from Contrastive Language-Image Pretraining
(CLIP) for semantic field learning encounter difficulties due to noisy and
view-inconsistent semantics provided by CLIP. To tackle these limitations, we
propose OV-NeRF, which exploits the potential of pre-trained vision and
language foundation models to enhance semantic field learning through proposed
single-view and cross-view strategies. First, from the single-view perspective,
we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask
proposals derived from SAM to rectify the noisy semantics of each training
view, facilitating accurate semantic field learning. Second, from the
cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy
to address the challenge raised by view-inconsistent semantics. Rather than
invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the
3D consistent semantics generated from the well-trained semantic field itself
for semantic field training, aiming to reduce ambiguity and enhance overall
semantic consistency across different views. Extensive experiments validate our
OV-NeRF outperforms current state-of-the-art methods, achieving a significant
improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet,
respectively. Furthermore, our approach exhibits consistent superior results
across various CLIP configurations, further verifying its robustness.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04653" title="Abstract">arXiv:2402.04653</a> [<a href="/pdf/2402.04653" title="Download PDF">pdf</a>, <a href="/format/2402.04653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Over Complete Deep Learning Method for Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eliasof%2C+M">Moshe Eliasof</a>, 
<a href="/search/cs?searchtype=author&query=Haber%2C+E">Eldad Haber</a>, 
<a href="/search/cs?searchtype=author&query=Treister%2C+E">Eran Treister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Obtaining meaningful solutions for inverse problems has been a major
challenge with many applications in science and engineering. Recent machine
learning techniques based on proximal and diffusion-based methods have shown
promising results. However, as we show in this work, they can also face
challenges when applied to some exemplary problems. We show that similar to
previous works on over-complete dictionaries, it is possible to overcome these
shortcomings by embedding the solution into higher dimensions. The novelty of
the work proposed is that we jointly design and learn the embedding and the
regularizer for the embedding vector. We demonstrate the merit of this approach
on several exemplary and common inverse problems.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04655" title="Abstract">arXiv:2402.04655</a> [<a href="/pdf/2402.04655" title="Download PDF">pdf</a>, <a href="/format/2402.04655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Vocabulary Calibration for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bob Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprrint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Vision-language models (VLMs) have emerged as formidable tools, showing their
strong capability in handling various open-vocabulary tasks in image
recognition, text-driven visual content generation, and visual chatbots, to
name a few. In recent years, considerable efforts and resources have been
devoted to adaptation methods for improving downstream performance of VLMs,
particularly on parameter-efficient fine-tuning methods like prompt learning.
However, a crucial aspect that has been largely overlooked is the confidence
calibration problem in fine-tuned VLMs, which could greatly reduce reliability
when deploying such models in the real world. This paper bridges the gap by
systematically investigating the confidence calibration problem in the context
of prompt learning and reveals that existing calibration methods are
insufficient to address the problem, especially in the open-vocabulary setting.
To solve the problem, we present a simple and effective approach called
Distance-Aware Calibration (DAC), which is based on scaling the temperature
using as guidance the distance between predicted text labels and base classes.
The experiments with 7 distinct prompt learning methods applied across 11
diverse downstream datasets demonstrate the effectiveness of DAC, which
achieves high efficacy without sacrificing the inference speed.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04660" title="Abstract">arXiv:2402.04660</a> [<a href="/pdf/2402.04660" title="Download PDF">pdf</a>, <a href="/format/2402.04660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness Through Artifact Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shua%2C+T">Tsufit Shua</a>, 
<a href="/search/cs?searchtype=author&query=Sharif%2C+M">Mahmood Sharif</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Adversarial examples arose as a challenge for machine learning. To hinder
them, most defenses alter how models are trained (e.g., adversarial training)
or inference is made (e.g., randomized smoothing). Still, while these
approaches markedly improve models' adversarial robustness, models remain
highly susceptible to adversarial examples. Identifying that, in certain
domains such as traffic-sign recognition, objects are implemented per standards
specifying how artifacts (e.g., signs) should be designed, we propose a novel
approach for improving adversarial robustness. Specifically, we offer a method
to redefine standards, making minor changes to existing ones, to defend against
adversarial examples. We formulate the problem of artifact design as a robust
optimization problem, and propose gradient-based and greedy search methods to
solve it. We evaluated our approach in the domain of traffic-sign recognition,
allowing it to alter traffic-sign pictograms (i.e., symbols within the signs)
and their colors. We found that, combined with adversarial training, our
approach led to up to 25.18\% higher robust accuracy compared to
state-of-the-art methods against two adversary types, while further increasing
accuracy on benign inputs.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04663" title="Abstract">arXiv:2402.04663</a> [<a href="/pdf/2402.04663" title="Download PDF">pdf</a>, <a href="/format/2402.04663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yulong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaopeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongwei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zunchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haotian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Biao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bojun Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Spiking neural networks (SNNs) are promising brain-inspired energy-efficient
models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs
exhibit superior efficiency and capability to process temporal information.
However, it remains a challenge to train SNNs due to their undifferentiable
spiking mechanism. The surrogate gradients method is commonly used to train
SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We
link the degraded accuracy to the vanishing of gradient on the temporal
dimension through the analytical and experimental study of the training process
of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the
Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths
to facilitate the backpropagation in computing temporal gradient while keeping
binary output. CLIF is hyperparameter-free and features broad applicability.
Extensive experiments on a variety of datasets demonstrate CLIF's clear
performance advantage over other neuron models. Moreover, the CLIF's
performance even slightly surpasses superior ANNs with identical network
structure and training conditions.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04665" title="Abstract">arXiv:2402.04665</a> [<a href="/pdf/2402.04665" title="Download PDF">pdf</a>, <a href="/format/2402.04665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Process-Based Nonlinear Moving Horizon Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wolff%2C+T+M">Tobias M. Wolff</a>, 
<a href="/search/eess?searchtype=author&query=Lopez%2C+V+G">Victor G. Lopez</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+M+A">Matthias A. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we propose a novel Gaussian process-based moving horizon
estimation (MHE) framework for unknown nonlinear systems. In the proposed
scheme, we take advantage of the properties of Gaussian processes. On the one
hand, we approximate the system dynamics by the posterior means of the learned
Gaussian processes (GPs). On the other hand, we exploit the posterior variances
of the Gaussian processes to design the weighting matrices in the MHE cost
function and account for the uncertainty in the learned system dynamics. The
data collection and the tuning of the hyperparameters are done offline. We
prove robust stability of the GP-based MHE scheme using a Lyapunov-based proof
technique. Furthermore, as additional contribution, we analyze under which
conditions incremental input/output-to-state stability (a nonlinear
detectability notion) is preserved when approximating the system dynamics
using, e.g., machine learning techniques. Finally, we illustrate the
performance of the GP-based MHE scheme in a simulation case study and show how
the chosen weighting matrices can lead to an improved performance compared to
standard cost functions.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04667" title="Abstract">arXiv:2402.04667</a> [<a href="/pdf/2402.04667" title="Download PDF">pdf</a>, <a href="/ps/2402.04667" title="Download PostScript">ps</a>, <a href="/format/2402.04667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Sensitivity Computations in ESDIRK-Based Optimal  Control Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Andersen%2C+A+H+D">Anders Hilmar Damm Andersen</a>, 
<a href="/search/eess?searchtype=author&query=J%C3%B8rgensen%2C+J+B">John Bagterp J&#xf8;rgensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 2 tables. Submitted for European Control Conference 2024 (ECC2024). Stockholm, Sweden
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we compare the impact of iterated and direct approaches to
sensitivity computation in fixed-step explicit singly diagonally-implicit
Runge-Kutta (ESDIRK) methods when applied to optimal control problems (OCPs).
We use the principle of internal numerical differentiation (IND) strictly for
the iterated approach, i.e., reusing the iteration matrix factorizations, the
number of Newton-type iterations, and Newton iterates, to compute the
sensitivities. The direct method computes the sensitivities without using the
Newton schemes. We compare the impact of the iterated and direct sensitivity
computations in OCPs for the quadruple tank system. We benchmark the iterated
and direct approaches with a base case. This base case is an OCP that applies
an ESDIRK method that refactorizes the iteration matrix in every Newton
iteration and uses a direct approach for sensitivity computations. In these
OCPs, we vary the number of integration steps between control intervals and we
evaluate the performance based on the number of SQP and QPs iterations, KKT
violations, and the total number of function evaluations, Jacobian updates, and
iteration matrix factorizations. The results indicate that the iterated
approach outperforms the direct approach but yields similar performance to the
base case.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04668" title="Abstract">arXiv:2402.04668</a> [<a href="/pdf/2402.04668" title="Download PDF">pdf</a>, <a href="/format/2402.04668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Perspective on Individualized Treatment Effects Estimation from  Time-series Health Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosheh%2C+G+O">Ghadeer O. Ghosheh</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6gl%2C+M">Moritz G&#xf6;gl</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tingting Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The burden of diseases is rising worldwide, with unequal treatment efficacy
for patient populations that are underrepresented in clinical trials.
Healthcare, however, is driven by the average population effect of medical
treatments and, therefore, operates in a "one-size-fits-all" approach, not
necessarily what best fits each patient. These facts suggest a pressing need
for methodologies to study individualized treatment effects (ITE) to drive
personalized treatment. Despite the increased interest in
machine-learning-driven ITE estimation models, the vast majority focus on
tabular data with limited review and understanding of methodologies proposed
for time-series electronic health records (EHRs). To this end, this work
provides an overview of ITE works for time-series data and insights into future
research. The work summarizes the latest work in the literature and reviews it
in light of theoretical assumptions, types of treatment settings, and
computational frameworks. Furthermore, this work discusses challenges and
future research directions for ITEs in a time-series setting. We hope this work
opens new directions and serves as a resource for understanding one of the
exciting yet under-studied research areas.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04671" title="Abstract">arXiv:2402.04671</a> [<a href="/pdf/2402.04671" title="Download PDF">pdf</a>, <a href="/format/2402.04671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V2VSSC: A 3D Semantic Scene Completion Benchmark for Perception with  Vehicle to Vehicle Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanfang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kaiqing Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiying Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiayi Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+D">Denghui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Peng Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengpei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic scene completion (SSC) has recently gained popularity because it can
provide both semantic and geometric information that can be used directly for
autonomous vehicle navigation. However, there are still challenges to overcome.
SSC is often hampered by occlusion and short-range perception due to sensor
limitations, which can pose safety risks. This paper proposes a fundamental
solution to this problem by leveraging vehicle-to-vehicle (V2V) communication.
We propose the first generalized collaborative SSC framework that allows
autonomous vehicles to share sensing information from different sensor views to
jointly perform SSC tasks. To validate the proposed framework, we further build
V2VSSC, the first V2V SSC benchmark, on top of the large-scale V2V perception
dataset OPV2V. Extensive experiments demonstrate that by leveraging V2V
communication, the SSC performance can be increased by 8.3% on geometric metric
IoU and 6.0% mIOU.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04672" title="Abstract">arXiv:2402.04672</a> [<a href="/pdf/2402.04672" title="Download PDF">pdf</a>, <a href="/format/2402.04672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-NAS: Generalizable Neural Architecture Search for Single Domain  Generalization Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jinling Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenghu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+N">Nanyang Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we focus on a realistic yet challenging task, Single Domain
Generalization Object Detection (S-DGOD), where only one source domain's data
can be used for training object detectors, but have to generalize multiple
distinct target domains. In S-DGOD, both high-capacity fitting and
generalization abilities are needed due to the task's complexity.
Differentiable Neural Architecture Search (NAS) is known for its high capacity
for complex data fitting and we propose to leverage Differentiable NAS to solve
S-DGOD. However, it may confront severe over-fitting issues due to the feature
imbalance phenomenon, where parameters optimized by gradient descent are biased
to learn from the easy-to-learn features, which are usually non-causal and
spuriously correlated to ground truth labels, such as the features of
background in object detection data. Consequently, this leads to serious
performance degradation, especially in generalizing to unseen target domains
with huge domain gaps between the source domain and target domains. To address
this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware
objective, preventing NAS from over-fitting by using gradient descent to
optimize parameters not only on a subset of easy-to-learn features but also the
remaining predictive features for generalization, and the overall framework is
named G-NAS. Experimental results on the S-DGOD urban-scene datasets
demonstrate that the proposed G-NAS achieves SOTA performance compared to
baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04676" title="Abstract">arXiv:2402.04676</a> [<a href="/pdf/2402.04676" title="Download PDF">pdf</a>, <a href="/format/2402.04676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Distributionally Robust Dataset Distillation with Risk  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vahidian%2C+S">Saeed Vahidian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jianyang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Kungurtsev%2C+V">Vyacheslav Kungurtsev</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Dataset distillation (DD) has emerged as a widely adopted technique for
crafting a synthetic dataset that captures the essential information of a
training dataset, facilitating the training of accurate neural models. Its
applications span various domains, including transfer learning, federated
learning, and neural architecture search. The most popular methods for
constructing the synthetic data rely on matching the convergence properties of
training the model with the synthetic dataset and the training dataset.
However, targeting the training dataset must be thought of as auxiliary in the
same sense that the training set is an approximate substitute for the
population distribution, and the latter is the data of interest. Yet despite
its popularity, an aspect that remains unexplored is the relationship of DD to
its generalization, particularly across uncommon subgroups. That is, how can we
ensure that a model trained on the synthetic dataset performs well when faced
with samples from regions with low population density? Here, the
representativeness and coverage of the dataset become salient over the
guaranteed training error at inference. Drawing inspiration from
distributionally robust optimization, we introduce an algorithm that combines
clustering with the minimization of a risk measure on the loss to conduct DD.
We provide a theoretical rationale for our approach and demonstrate its
effective generalization and robustness across subgroups through numerical
experiments.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04677" title="Abstract">arXiv:2402.04677</a> [<a href="/pdf/2402.04677" title="Download PDF">pdf</a>, <a href="/format/2402.04677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source Identification in Abstractive Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suhara%2C+Y">Yoshi Suhara</a>, 
<a href="/search/cs?searchtype=author&query=Alikaniotis%2C+D">Dimitris Alikaniotis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Neural abstractive summarization models make summaries in an end-to-end
manner, and little is known about how the source information is actually
converted into summaries. In this paper, we define input sentences that contain
essential information in the generated summary as $\textit{source sentences}$
and study how abstractive summaries are made by analyzing the source sentences.
To this end, we annotate source sentences for reference summaries and system
summaries generated by PEGASUS on document-summary pairs sampled from the
CNN/DailyMail and XSum datasets. We also formulate automatic source sentence
detection and compare multiple methods to establish a strong baseline for the
task. Experimental results show that the perplexity-based method performs well
in highly abstractive settings, while similarity-based methods perform robustly
in relatively extractive settings. Our code and data are available at
https://github.com/suhara/sourcesum.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04678" title="Abstract">arXiv:2402.04678</a> [<a href="/pdf/2402.04678" title="Download PDF">pdf</a>, <a href="/format/2402.04678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models As Faithful Explainers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yu-Neng Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanchu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chia-Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruixiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xuanting Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have recently become proficient in addressing
complex tasks by utilizing their rich internal knowledge and reasoning ability.
Consequently, this complexity hinders traditional input-focused explanation
algorithms for explaining the complex decision-making processes of LLMs. Recent
advancements have thus emerged for self-explaining their predictions through a
single feed-forward inference in a natural language format. However, natural
language explanations are often criticized for lack of faithfulness since these
explanations may not accurately reflect the decision-making behaviors of the
LLMs. In this work, we introduce a generative explanation framework, xLLM, to
improve the faithfulness of the explanations provided in natural language
formats for LLMs. Specifically, we propose an evaluator to quantify the
faithfulness of natural language explanation and enhance the faithfulness by an
iterative optimization process of xLLM, with the goal of maximizing the
faithfulness scores. Experiments conducted on three NLU datasets demonstrate
that xLLM can significantly improve the faithfulness of generated explanations,
which are in alignment with the behaviors of LLMs.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04681" title="Abstract">arXiv:2402.04681</a> [<a href="/pdf/2402.04681" title="Download PDF">pdf</a>, <a href="/format/2402.04681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectural Design Decisions for Self-Serve Data Platforms in Data  Meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Eijk%2C+T">Tom van Eijk</a>, 
<a href="/search/cs?searchtype=author&query=Kumara%2C+I">Indika Kumara</a>, 
<a href="/search/cs?searchtype=author&query=Di+Nucci%2C+D">Dario Di Nucci</a>, 
<a href="/search/cs?searchtype=author&query=Tamburri%2C+D+A">Damian Andrew Tamburri</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Heuvel%2C+W">Willem-Jan van den Heuvel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21st IEEE International Conference on Software Architecture (ICSA 2024), 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Data mesh is an emerging decentralized approach to managing and generating
value from analytical enterprise data at scale. It shifts the ownership of the
data to the business domains closest to the data, promotes sharing and managing
data as autonomous products, and uses a federated and automated data governance
model. The data mesh relies on a managed data platform that offers services to
domain and governance teams to build, share, and manage data products
efficiently. However, designing and implementing a self-serve data platform is
challenging, and the platform engineers and architects must understand and
choose the appropriate design options to ensure the platform will enhance the
experience of domain and governance teams. For these reasons, this paper
proposes a catalog of architectural design decisions and their corresponding
decision options by systematically reviewing 43 industrial gray literature
articles on self-serve data platforms in data mesh. Moreover, we used
semi-structured interviews with six data engineering experts with data mesh
experience to validate, refine, and extend the findings from the literature.
Such a catalog of design decisions and options drawn from the state of practice
shall aid practitioners in building data meshes while providing a baseline for
further research on data mesh architectures.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04686" title="Abstract">arXiv:2402.04686</a> [<a href="/pdf/2402.04686" title="Download PDF">pdf</a>, <a href="/ps/2402.04686" title="Download PostScript">ps</a>, <a href="/format/2402.04686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Influence of Autofocus Lenses in the Camera Calibration Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ricolfe-Viala%2C+C">Carlos Ricolfe-Viala</a>, 
<a href="/search/cs?searchtype=author&query=Esparza%2C+A">Alicia Esparza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Camera calibration is a crucial step in robotics and computer vision.
Accurate camera parameters are necessary to achieve robust applications.
Nowadays, camera calibration process consists of adjusting a set of data to a
pin-hole model, assuming that with a reprojection error close to cero, camera
parameters are correct. Since all camera parameters are unknown, computed
results are considered true. However, the pin-hole model does not represent the
camera behavior accurately if the focus is considered. Real cameras change the
focal length slightly to obtain sharp objects in the image and this feature
skews the calibration result if a unique pin-hole model is computed with a
constant focal length. In this paper, a deep analysis of the camera calibration
process is done to detect and strengthen its weaknesses. The camera is mounted
in a robot arm to known extrinsic camera parameters with accuracy and to be
able to compare computed results with the true ones. Based on the bias that
exist between computed results and the true ones, a modification of the widely
accepted camera calibration method using images of a planar template is
presented. A pin-hole model with distance dependent focal length is proposed to
improve the calibration process substantially
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04696" title="Abstract">arXiv:2402.04696</a> [<a href="/pdf/2402.04696" title="Download PDF">pdf</a>, <a href="/ps/2402.04696" title="Download PostScript">ps</a>, <a href="/format/2402.04696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nash Equilibria in Reverse Temporal Voronoi Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pawlowski%2C+S">Simeon Pawlowski</a>, 
<a href="/search/cs?searchtype=author&query=Froese%2C+V">Vincent Froese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study Voronoi games on temporal graphs as introduced by Boehmer et al.
(IJCAI 2021) where two players each select a vertex in a temporal graph with
the goal of reaching the other vertices earlier than the other player. In this
work, we consider the reverse temporal Voronoi game, that is, a player wants to
maximize the number of vertices reaching her earlier than the other player.
Since temporal distances in temporal graphs are not symmetric in general, this
yields a different game. We investigate the difference between the two games
with respect to the existence of Nash equilibria in various temporal graph
classes including temporal trees, cycles, grids, cliques and split graphs. Our
extensive results show that the two games indeed behave quite differently
depending on the considered temporal graph class.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04699" title="Abstract">arXiv:2402.04699</a> [<a href="/pdf/2402.04699" title="Download PDF">pdf</a>, <a href="/format/2402.04699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World  Illusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotyan%2C+S">Shashank Kotyan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+P">PoYuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+D+V">Danilo Vasconcellos Vargas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Deep neural networks are exploited using natural adversarial samples, which
have no impact on human perception but are misclassified. Current approaches
often rely on the white-box nature of deep neural networks to generate these
adversarial samples or alter the distribution of adversarial samples compared
to training distribution. To alleviate the limitations of current approaches,
we propose EvoSeed, a novel evolutionary strategy-based search algorithmic
framework to generate natural adversarial samples. Our EvoSeed framework uses
auxiliary Diffusion and Classifier models to operate in a model-agnostic
black-box setting. We employ CMA-ES to optimize the search for an adversarial
seed vector, which, when processed by the Conditional Diffusion Model, results
in an unrestricted natural adversarial sample misclassified by the Classifier
Model. Experiments show that generated adversarial images are of high image
quality and are transferable to different classifiers. Our approach
demonstrates promise in enhancing the quality of adversarial samples using
evolutionary algorithms. We hope our research opens new avenues to enhance the
robustness of deep neural networks in real-world scenarios. Project Website can
be accessed at \url{https://shashankkotyan.github.io/EvoSeed}.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04701" title="Abstract">arXiv:2402.04701</a> [<a href="/pdf/2402.04701" title="Download PDF">pdf</a>, <a href="/format/2402.04701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exhaustive Classification and Quantification of Coupling Modes in Power  Systems with Power Electronics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zoghby%2C+P">Pamela Zoghby</a>, 
<a href="/search/eess?searchtype=author&query=Marinescu%2C+B">Bogdan Marinescu</a>, 
<a href="/search/eess?searchtype=author&query=Rosse%2C+A">Antoine Rosse</a>, 
<a href="/search/eess?searchtype=author&query=Prime%2C+G">Gregoire Prime</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Due to the energy transition, today's electrical networks include synchronous
machines and inverter-based resources interfacing renewable energies such as
wind turbines, solar panels, and Battery Energy Storage Systems to the grid. In
such systems, interactions known as coupling modes or dynamic interactions,
between synchronous machines and inverter-based resources may arise. This paper
conducts a clear and exhaustive study on a proposed benchmark, in order to
analyze, quantify and classify these new types of modes. Detailed models
representing electromagnetic transient phenomena are developed and linearized,
then used for conducting modal analysis to fully characterize the small-signal
stability of the system. Also, a sensitivity analysis is presented to evaluate
the impact of key parameters on the detected modes of oscillation. Besides the
exhaustive classification of the possible coupling modes, the proposed
benchmark and methodology can be used to study any given power system in a
minimal order modeling. The case of a fully detailed power grid based on the
IEEE 39 bus system was studied as an illustrative example.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04710" title="Abstract">arXiv:2402.04710</a> [<a href="/pdf/2402.04710" title="Download PDF">pdf</a>, <a href="/format/2402.04710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Retrieval-based Causal Learning with Information  Bottlenecks for Interpretable Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+J">Jiahua Rao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiancong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hanjing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuangjia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuedong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have gained considerable traction for their
capability to effectively process topological data, yet their interpretability
remains a critical concern. Current interpretation methods are dominated by
post-hoc explanations to provide a transparent and intuitive understanding of
GNNs. However, they have limited performance in interpreting complicated
subgraphs and can't utilize the explanation to advance GNN predictions. On the
other hand, transparent GNN models are proposed to capture critical subgraphs.
While such methods could improve GNN predictions, they usually don't perform
well on explanations. Thus, it is desired for a new strategy to better couple
GNN explanation and prediction. In this study, we have developed a novel
interpretable causal GNN framework that incorporates retrieval-based causal
learning with Graph Information Bottleneck (GIB) theory. The framework could
semi-parametrically retrieve crucial subgraphs detected by GIB and compress the
explanatory subgraphs via a causal module. The framework was demonstrated to
consistently outperform state-of-the-art methods, and to achieve 32.71\% higher
precision on real-world explanation scenarios with diverse explanation types.
More importantly, the learned explanations were shown able to also improve GNN
prediction performance.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04713" title="Abstract">arXiv:2402.04713</a> [<a href="/pdf/2402.04713" title="Download PDF">pdf</a>, <a href="/format/2402.04713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical and Empirical Analysis of Adaptive Entry Point Selection for  Graph-based Approximate Nearest Neighbor Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oguri%2C+Y">Yutaro Oguri</a>, 
<a href="/search/cs?searchtype=author&query=Matsui%2C+Y">Yusuke Matsui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a theoretical and empirical analysis of the adaptive entry point
selection for graph-based approximate nearest neighbor search (ANNS). We
introduce novel concepts: $b\textit{-monotonic path}$ and $B\textit{-MSNET}$,
which better capture an actual graph in practical algorithms than existing
concepts like MSNET. We prove that adaptive entry point selection offers better
performance upper bound than the fixed central entry point under more general
conditions than previous work. Empirically, we validate the method's
effectiveness in accuracy, speed, and memory usage across various datasets,
especially in challenging scenarios with out-of-distribution data and hard
instances. Our comprehensive study provides deeper insights into optimizing
entry points for graph-based ANNS for real-world high-dimensional data
applications.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04717" title="Abstract">arXiv:2402.04717</a> [<a href="/pdf/2402.04717" title="Download PDF">pdf</a>, <a href="/format/2402.04717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with  Semantic Graph Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenguo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yadong Mu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024 for spotlight presentation; Project page: <a href="https://chenguolin.github.io/projects/InstructScene">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Comprehending natural language instructions is a charming property for 3D
indoor scene synthesis systems. Existing methods directly model object joint
distributions and express object relations implicitly within a scene, thereby
hindering the controllability of generation. We introduce InstructScene, a
novel generative framework that integrates a semantic graph prior and a layout
decoder to improve controllability and fidelity for 3D scene synthesis. The
proposed semantic graph prior jointly learns scene appearances and layout
distributions, exhibiting versatility across various downstream tasks in a
zero-shot manner. To facilitate the benchmarking for text-driven 3D scene
synthesis, we curate a high-quality dataset of scene-instruction pairs with
large language and multimodal models. Extensive experimental results reveal
that the proposed method surpasses existing state-of-the-art approaches by a
large margin. Thorough ablation studies confirm the efficacy of crucial design
components. Project page: https://chenguolin.github.io/projects/InstructScene.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04718" title="Abstract">arXiv:2402.04718</a> [<a href="/pdf/2402.04718" title="Download PDF">pdf</a>, <a href="/format/2402.04718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for  Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jeon%2C+S">Soobin Jeon</a>, 
<a href="/search/eess?searchtype=author&query=Cho%2C+H">Hancheol Cho</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+S">Sang-Young Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper investigates the efficiency of nonsingular fast terminal sliding
mode and adaptive smooth control method for the distributed space telescope
demonstration mission. The distributed space telescope has a flexible focal
length that corresponds to the relative position of the formation flying
concept. The precise formation flying technology by CubeSats enhances the
utility of distributed space systems with low costs. The propulsion systems for
CubeSats usually have restricted degrees of freedom. Since the scientific
mission requires continuous orbit control, the attitude and orbit control
system mutually affect the control performance. The nonsingular fast terminal
sliding mode has the advantage of a fast convergence rate and is able to
improve the control performance. The adaptive smooth controller designed for
the SISO system is expanded and applied to the attitude and orbit control
system. The simulation results verify the efficiency of the adaptive smooth
controller based on the nonsingular fast terminal sliding mode.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04720" title="Abstract">arXiv:2402.04720</a> [<a href="/pdf/2402.04720" title="Download PDF">pdf</a>, <a href="/format/2402.04720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Driving Interactions: A Robust Multi-Agent Simulation  Framework for Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaufeld%2C+M">Marc Kaufeld</a>, 
<a href="/search/cs?searchtype=author&query=Trauth%2C+R">Rainer Trauth</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages. Submitted to IEEE IV 2024 Korea Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Current validation methods often rely on recorded data and basic functional
checks, which may not be sufficient to encompass the scenarios an autonomous
vehicle might encounter. In addition, there is a growing need for complex
scenarios with changing vehicle interactions for comprehensive validation. This
work introduces a novel synchronous multi-agent simulation framework for
autonomous vehicles in interactive scenarios. Our approach creates an
interactive scenario and incorporates publicly available edge-case scenarios
wherein simulated vehicles are replaced by agents navigating to predefined
destinations. We provide a platform that enables the integration of different
autonomous driving planning methodologies and includes a set of evaluation
metrics to assess autonomous driving behavior. Our study explores different
planning setups and adjusts simulation complexity to test the framework's
adaptability and performance. Results highlight the critical role of simulating
vehicle interactions to enhance autonomous driving systems. Our setup offers
unique insights for developing advanced algorithms for complex driving tasks to
accelerate future investigations and developments in this field. The
multi-agent simulation framework is available as open-source software:
https://github.com/TUM-AVS/Frenetix-Motion-Planner
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04722" title="Abstract">arXiv:2402.04722</a> [<a href="/pdf/2402.04722" title="Download PDF">pdf</a>, <a href="/ps/2402.04722" title="Download PostScript">ps</a>, <a href="/format/2402.04722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ten simple rules for teaching sustainable software engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallagher%2C+K">Kit Gallagher</a>, 
<a href="/search/cs?searchtype=author&query=Creswell%2C+R">Richard Creswell</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+B">Ben Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+M">Martin Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+C+L">Chon Lok Lei</a>, 
<a href="/search/cs?searchtype=author&query=Mirams%2C+G+R">Gary R. Mirams</a>, 
<a href="/search/cs?searchtype=author&query=Gavaghan%2C+D+J">David J. Gavaghan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Prepared for submission to PLOS Computational Biology's 10 Simple Rules collection
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Computational methods and associated software implementations are central to
every field of scientific investigation. Modern biological research,
particularly within systems biology, has relied heavily on the development of
software tools to process and organize increasingly large datasets, simulate
complex mechanistic models, provide tools for the analysis and management of
data, and visualize and organize outputs. However, developing high-quality
research software requires scientists to develop a host of software development
skills, and teaching these skills to students is challenging. There has been a
growing importance placed on ensuring reproducibility and good development
practices in computational research. However, less attention has been devoted
to informing the specific teaching strategies which are effective at nurturing
in researchers the complex skillset required to produce high-quality software
that, increasingly, is required to underpin both academic and industrial
biomedical research. Recent articles in the Ten Simple Rules collection have
discussed the teaching of foundational computer science and coding techniques
to biology students. We advance this discussion by describing the specific
steps for effectively teaching the necessary skills scientists need to develop
sustainable software packages which are fit for (re-)use in academic research
or more widely. Although our advice is likely to be applicable to all students
and researchers hoping to improve their software development skills, our
guidelines are directed towards an audience of students that have some
programming literacy but little formal training in software development or
engineering, typical of early doctoral students. These practices are also
applicable outside of doctoral training environments, and we believe they
should form a key part of postgraduate training schemes more generally in the
life sciences.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04728" title="Abstract">arXiv:2402.04728</a> [<a href="/pdf/2402.04728" title="Download PDF">pdf</a>, <a href="/format/2402.04728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for  Transmission with Higher-Order Constellations in the Terahertz Band
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forsch%2C+C">Christian Forsch</a>, 
<a href="/search/cs?searchtype=author&query=Zillmann%2C+P">Peter Zillmann</a>, 
<a href="/search/cs?searchtype=author&query=Alrabadi%2C+O">Osama Alrabadi</a>, 
<a href="/search/cs?searchtype=author&query=Brueck%2C+S">Stefan Brueck</a>, 
<a href="/search/cs?searchtype=author&query=Gerstacker%2C+W">Wolfgang Gerstacker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 19 figures, submitted for possible journal publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, we consider Terahertz (THz) communications with low-resolution
uniform quantization and spatial oversampling at the receiver side. We compare
different analog-to-digital converter (ADC) parametrizations in a fair manner
by keeping the ADC power consumption constant. Here, 1-, 2-, and 3-bit
quantization is investigated with different oversampling factors. We
analytically compute the statistics of the detection variable, and we propose
the optimal as well as several suboptimal detection schemes for arbitrary
quantization resolutions. Then, we evaluate the symbol error rate (SER) of the
different detectors for a 16- and a 64-ary quadrature amplitude modulation
(QAM) constellation. The results indicate that there is a noticeable
performance degradation of the suboptimal detection schemes compared to the
optimal scheme when the constellation size is larger than the number of
quantization levels. Furthermore, at low signal-to-noise ratios (SNRs), 1-bit
quantization outperforms 2- and 3-bit quantization, respectively, even when
employing higher-order constellations. We confirm our analytical results by
Monte Carlo simulations. Both a pure line-of-sight (LoS) and a more
realistically modeled indoor THz channel are considered. Then, we optimize the
input signal constellation with respect to SER for 1-bit quantization. The
results show that the minimum SER can be lowered significantly for 16-QAM by
increasing the distance between the inner and outer points of the input
constellation. For larger constellations, however, the achievable reduction of
the minimum SER is much smaller compared to 16-QAM.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04730" title="Abstract">arXiv:2402.04730</a> [<a href="/pdf/2402.04730" title="Download PDF">pdf</a>, <a href="/format/2402.04730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Trajectory Optimization With Dynamically Changing  Waypoints for Serial Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+F">Florian Beck</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+M+N">Minh Nhat Vu</a>, 
<a href="/search/cs?searchtype=author&query=Hartl-Nesic%2C+C">Christian Hartl-Nesic</a>, 
<a href="/search/cs?searchtype=author&query=Kugi%2C+A">Andreas Kugi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Systematically including dynamically changing waypoints as desired discrete
actions, for instance, resulting from superordinate task planning, has been
challenging for online model predictive trajectory optimization with short
planning horizons. This paper presents a novel waypoint model predictive
control (wMPC) concept for online replanning tasks. The main idea is to split
the planning horizon at the waypoint when it becomes reachable within the
current planning horizon and reduce the horizon length towards the waypoints
and goal points. This approach keeps the computational load low and provides
flexibility in adapting to changing conditions in real time. The presented
approach achieves competitive path lengths and trajectory durations compared to
(global) offline RRT-type planners in a multi-waypoint scenario. Moreover, the
ability of wMPC to dynamically replan tasks online is experimentally
demonstrated on a KUKA LBR iiwa 14 R820 robot in a dynamic pick-and-place
scenario.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04732" title="Abstract">arXiv:2402.04732</a> [<a href="/pdf/2402.04732" title="Download PDF">pdf</a>, <a href="/format/2402.04732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Cuts with Arbitrary Size Constraints Through Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fettal%2C+C">Chakib Fettal</a>, 
<a href="/search/cs?searchtype=author&query=Labiod%2C+L">Lazhar Labiod</a>, 
<a href="/search/cs?searchtype=author&query=Nadif%2C+M">Mohamed Nadif</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A common way of partitioning graphs is through minimum cuts. One drawback of
classical minimum cut methods is that they tend to produce small groups, which
is why more balanced variants such as normalized and ratio cuts have seen more
success. However, we believe that with these variants, the balance constraints
can be too restrictive for some applications like for clustering of imbalanced
datasets, while not being restrictive enough for when searching for perfectly
balanced partitions. Here, we propose a new graph cut algorithm for
partitioning graphs under arbitrary size constraints. We formulate the graph
cut problem as a regularized Gromov-Wasserstein problem. We then propose to
solve it using accelerated proximal GD algorithm which has global convergence
guarantees, results in sparse solutions and only incurs an additional ratio of
$\mathcal{O}(\log(n))$ compared to the classical spectral clustering algorithm
but was seen to be more efficient.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04735" title="Abstract">arXiv:2402.04735</a> [<a href="/pdf/2402.04735" title="Download PDF">pdf</a>, <a href="/ps/2402.04735" title="Download PostScript">ps</a>, <a href="/format/2402.04735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of Cetacean&#x27;s click detection algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gracic%2C+M">Mak Gracic</a>, 
<a href="/search/cs?searchtype=author&query=Gubnisky%2C+G">Guy Gubnisky</a>, 
<a href="/search/cs?searchtype=author&query=Diamant%2C+R">Roee Diamant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The detection of echolocation clicks is key in understanding the intricate
behaviors of cetaceans and monitoring their populations. Cetacean species
relying on clicks for navigation, foraging and even communications are sperm
whales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation
clicks are wideband signals of short duration that are often emitted in
sequences of varying inter-click-intervals. While datasets and models for
clicks exist, the detection and classification of clicks present a significant
challenge, mostly due to the diversity of clicks' structures, overlapping
signals from simultaneously emitting animals, and the abundance of noise
transients from, for example, snapping shrimps and shipping cavitation noise.
This paper provides a survey of the many detection and classification
methodologies of clicks, ranging from 2002 to 2023. We divide the surveyed
techniques into categories by their methodology. Specifically, feature analysis
(e.g., phase, ICI and duration), frequency content, energy based detection,
supervised and unsupervised machine learning, template matching and adaptive
detection approaches. Also surveyed are open access platforms for click
detections, and databases openly available for testing. Details of the method
applied for each paper are given along with advantages and limitations, and for
each category we analyze the remaining challenges. The paper also includes a
performance comparison for several schemes over a shared database. Finally, we
provide tables summarizing the existing detection schemes in terms of
challenges address, methods, detection and classification tools applied,
features used and applications.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04744" title="Abstract">arXiv:2402.04744</a> [<a href="/pdf/2402.04744" title="Download PDF">pdf</a>, <a href="/format/2402.04744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Gradient Flow for Robust N:M Sparsity Training in  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bambhaniya%2C+A+R">Abhimanyu Rajeshkumar Bambhaniya</a>, 
<a href="/search/cs?searchtype=author&query=Yazdanbakhsh%2C+A">Amir Yazdanbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Suvinay Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+S">Sheng-Chun Kao</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shivani Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Evci%2C+U">Utku Evci</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+T">Tushar Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 17 tables. Code is available at <a href="https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">N:M Structured sparsity has garnered significant interest as a result of
relatively modest overhead and improved efficiency. Additionally, this form of
sparsity holds considerable appeal for reducing the memory footprint owing to
their modest representation overhead. There have been efforts to develop
training recipes for N:M structured sparsity, they primarily focus on
low-sparsity regions ($\sim$50\%). Nonetheless, performance of models trained
using these approaches tends to decline when confronted with high-sparsity
regions ($&gt;$80\%). In this work, we study the effectiveness of existing sparse
training recipes at \textit{high-sparsity regions} and argue that these methods
fail to sustain the model quality on par with low-sparsity regions. We
demonstrate that the significant factor contributing to this disparity is the
presence of elevated levels of induced noise in the gradient magnitudes. To
mitigate this undesirable effect, we employ decay mechanisms to progressively
restrict the flow of gradients towards pruned elements. Our approach improves
the model quality by up to 2$\%$ and 5$\%$ in vision and language models at
high sparsity regime, respectively. We also evaluate the trade-off between
model accuracy and training compute cost in terms of FLOPs. At iso-training
FLOPs, our method yields better performance compared to conventional sparse
training recipes, exhibiting an accuracy improvement of up to 2$\%$. The source
code is available at
https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04746" title="Abstract">arXiv:2402.04746</a> [<a href="/pdf/2402.04746" title="Download PDF">pdf</a>, <a href="/format/2402.04746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black Hole Search in Dynamic Tori
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Adri Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Italiano%2C+G+F">Giuseppe F. Italiano</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+P+S">Partha Sarathi Mandal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We investigate the black hole search problem by a set of mobile agents in a
dynamic torus. Black hole is defined to be a dangerous stationary node which
has the capability to destroy any number of incoming agents without leaving any
trace of its existence. A torus of size $n\times m$ ($3\leq n \leq m$) is a
collection of $n$ row rings and $m$ column rings, and the dynamicity is such
that each ring is considered to be 1-interval connected, i.e., in other words
at most one edge can be missing from each ring at any round. The parameters
which define the efficiency of any black hole search algorithm are: the number
of agents and the number of rounds (or \textit{time}) for termination. We
consider two initial configurations of mobile agents: first, the agents are
co-located and second, the agents are scattered. In each case, we establish
lower and upper bounds on the number of agents and on the amount of time
required to solve the black hole search problem.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04750" title="Abstract">arXiv:2402.04750</a> [<a href="/pdf/2402.04750" title="Download PDF">pdf</a>, <a href="/format/2402.04750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AINS: Affordable Indoor Navigation Solution via Line Color  Identification Using Mono-Camera for Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitlo%2C+N">Nizamuddin Maitlo</a>, 
<a href="/search/cs?searchtype=author&query=Noonari%2C+N">Nooruddin Noonari</a>, 
<a href="/search/cs?searchtype=author&query=Arshid%2C+K">Kaleem Arshid</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N">Naveed Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Duraisamy%2C+S">Sathishkumar Duraisamy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, researchers have been exploring various ways to improve the
effectiveness and efficiency of autonomous vehicles by researching new methods,
especially for indoor scenarios. Autonomous Vehicles in indoor navigation
systems possess many challenges especially the limited accuracy of GPS in
indoor scenarios. Several, robust methods have been explored for autonomous
vehicles in indoor scenarios to solve this problem, but the ineffectiveness of
the proposed methods is the high deployment cost. To address the
above-mentioned problems we have presented A low-cost indoor navigation method
for autonomous vehicles called Affordable Indoor Navigation Solution (AINS)
which is based on based on Monocular Camera. Our proposed solution is mainly
based on a mono camera without relying on various huge or power-inefficient
sensors to find the path, such as range finders and other navigation sensors.
Our proposed method shows that we can deploy autonomous vehicles indoor
navigation systems while taking into consideration the cost. We can observe
that the results shown by our solution are better than existing solutions and
we can reduce the estimated error and time consumption.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04754" title="Abstract">arXiv:2402.04754</a> [<a href="/pdf/2402.04754" title="Download PDF">pdf</a>, <a href="/format/2402.04754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Aligned Layout Generation via Diffusion Model with Aesthetic  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yufan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Controllable layout generation refers to the process of creating a plausible
visual arrangement of elements within a graphic design (e.g., document and web
designs) with constraints representing design intentions. Although recent
diffusion-based models have achieved state-of-the-art FID scores, they tend to
exhibit more pronounced misalignment compared to earlier transformer-based
models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint
diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of
layout generation tasks, such as arranging elements with specified attributes
and refining or completing a coarse layout design. The model is based on
continuous diffusion models. Compared with existing methods that use discrete
diffusion models, continuous state-space design can enable the incorporation of
differentiable aesthetic constraint functions in training. For conditional
generation, we introduce conditions via masked input. Extensive experiment
results show that LACE produces high-quality layouts and outperforms existing
state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04756" title="Abstract">arXiv:2402.04756</a> [<a href="/pdf/2402.04756" title="Download PDF">pdf</a>, <a href="/format/2402.04756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ye Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+H">Hao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Linghan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hengrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongbing Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised segmentation methods have demonstrated promising results in
natural scenarios, providing a solution to reduce dependency on manual
annotation. However, these methods face significant challenges when directly
applied to pathological images due to the subtle color differences between
nuclei and tissues, as well as the significant morphological variations among
nuclei. Consequently, the generated pseudo-labels often contain much noise,
especially at the nuclei boundaries. To address the above problem, this paper
proposes a boundary-aware contrastive learning network to denoise the boundary
noise in a semi-supervised nuclei segmentation task. The model has two key
designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive
learning (CRC) module. The LRD improves the smoothness of the nuclei boundary
by pseudo-labels denoising, and the CRC enhances the discrimination between
foreground and background by boundary feature contrastive learning. We conduct
extensive experiments to demonstrate the superiority of our proposed method
over existing semi-supervised instance segmentation methods.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04762" title="Abstract">arXiv:2402.04762</a> [<a href="/pdf/2402.04762" title="Download PDF">pdf</a>, <a href="/format/2402.04762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Color Recognition in Challenging Lighting Environments: CNN Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitlo%2C+N">Nizamuddin Maitlo</a>, 
<a href="/search/cs?searchtype=author&query=Noonari%2C+N">Nooruddin Noonari</a>, 
<a href="/search/cs?searchtype=author&query=Ghanghro%2C+S+A">Sajid Ahmed Ghanghro</a>, 
<a href="/search/cs?searchtype=author&query=Duraisamy%2C+S">Sathishkumar Duraisamy</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Fayaz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Light plays a vital role in vision either human or machine vision, the
perceived color is always based on the lighting conditions of the surroundings.
Researchers are working to enhance the color detection techniques for the
application of computer vision. They have implemented proposed several methods
using different color detection approaches but still, there is a gap that can
be filled. To address this issue, a color detection method, which is based on a
Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is
performed using the edge detection segmentation technique to specify the object
and then the segmented object is fed to the Convolutional Neural Network
trained to detect the color of an object in different lighting conditions. It
is experimentally verified that our method can substantially enhance the
robustness of color detection in different lighting conditions, and our method
performed better results than existing methods.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04763" title="Abstract">arXiv:2402.04763</a> [<a href="/pdf/2402.04763" title="Download PDF">pdf</a>, <a href="/format/2402.04763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of specialized Collective Behaviors in Evolving Heterogeneous  Swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Diggelen%2C+F">Fuda van Diggelen</a>, 
<a href="/search/cs?searchtype=author&query=De+Carlo%2C+M">Matteo De Carlo</a>, 
<a href="/search/cs?searchtype=author&query=Cambier%2C+N">Nicolas Cambier</a>, 
<a href="/search/cs?searchtype=author&query=Ferrante%2C+E">Eliseo Ferrante</a>, 
<a href="/search/cs?searchtype=author&query=Eiben%2C+A+E">A.E. Eiben</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Natural groups of animals, such as swarms of social insects, exhibit
astonishing degrees of task specialization, useful to address complex tasks and
to survive. This is supported by phenotypic plasticity: individuals sharing the
same genotype that is expressed differently for different classes of
individuals, each specializing in one task. In this work, we evolve a swarm of
simulated robots with phenotypic plasticity to study the emergence of
specialized collective behavior during an emergent perception task. Phenotypic
plasticity is realized in the form of heterogeneity of behavior by dividing the
genotype into two components, with one different neural network controller
associated to each component. The whole genotype, expressing the behavior of
the whole group through the two components, is subject to evolution with a
single fitness function. We analyse the obtained behaviors and use the insights
provided by these results to design an online regulatory mechanism. Our
experiments show three main findings: 1) The sub-groups evolve distinct
emergent behaviors. 2) The effectiveness of the whole swarm depends on the
interaction between the two sub-groups, leading to a more robust performance
than with singular sub-group behavior. 3) The online regulatory mechanism
enhances overall performance and scalability.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04764" title="Abstract">arXiv:2402.04764</a> [<a href="/pdf/2402.04764" title="Download PDF">pdf</a>, <a href="/format/2402.04764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code as Reward: Empowering Reinforcement Learning with VLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venuto%2C+D">David Venuto</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+S+N">Sami Nur Islam</a>, 
<a href="/search/cs?searchtype=author&query=Klissarov%2C+M">Martin Klissarov</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sherry Yang</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Ankit Anand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pre-trained Vision-Language Models (VLMs) are able to understand visual
concepts, describe and decompose complex tasks into sub-tasks, and provide
feedback on task completion. In this paper, we aim to leverage these
capabilities to support the training of reinforcement learning (RL) agents. In
principle, VLMs are well suited for this purpose, as they can naturally analyze
image-based observations and provide feedback (reward) on learning progress.
However, inference in VLMs is computationally expensive, so querying them
frequently to compute rewards would significantly slowdown the training of an
RL agent. To address this challenge, we propose a framework named Code as
Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through
code generation, thereby significantly reducing the computational burden of
querying the VLM directly. We show that the dense rewards generated through our
approach are very accurate across a diverse set of discrete and continuous
environments, and can be more effective in training RL policies than the
original sparse environment rewards.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04768" title="Abstract">arXiv:2402.04768</a> [<a href="/pdf/2402.04768" title="Download PDF">pdf</a>, <a href="/format/2402.04768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Interaction Behavior Generation based on Social Motion Forecasting  for Human-Robot Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mascaro%2C+E+V">Esteve Valls Mascaro</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yashuai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongheui Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Integrating robots into populated environments is a complex challenge that
requires an understanding of human social dynamics. In this work, we propose to
model social motion forecasting in a shared human-robot representation space,
which facilitates us to synthesize robot motions that interact with humans in
social scenarios despite not observing any robot in the motion training. We
develop a transformer-based architecture called ECHO, which operates in the
aforementioned shared space to predict the future motions of the agents
encountered in social scenarios. Contrary to prior works, we reformulate the
social motion problem as the refinement of the predicted individual motions
based on the surrounding agents, which facilitates the training while allowing
for single-motion forecasting when only one human is in the scene. We evaluate
our model in multi-person and human-robot motion forecasting tasks and obtain
state-of-the-art performance by a large margin while being efficient and
performing in real-time. Additionally, our qualitative results showcase the
effectiveness of our approach in generating human-robot interaction behaviors
that can be controlled via text commands.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04769" title="Abstract">arXiv:2402.04769</a> [<a href="/pdf/2402.04769" title="Download PDF">pdf</a>, <a href="/format/2402.04769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Motion Planning and Offline Robust Model Predictive Control  for Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+D">Hung Duy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+M+N">Minh Nhat Vu</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+N+N">Nguyen Ngoc Nam</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyoungseok Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 illustrations, Accepted for publication in American Control Conference (ACC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Driving vehicles in complex scenarios under harsh conditions is the biggest
challenge for autonomous vehicles (AVs). To address this issue, we propose
hierarchical motion planning and robust control strategy using the front-active
steering system in complex scenarios with various slippery road adhesion
coefficients while considering vehicle uncertain parameters. Behaviors of human
vehicles (HVs) are considered and modeled in the form of a car-following model
via the Intelligent Driver Model (IDM). Then, in the upper layer, the motion
planner first generates an optimal trajectory by using the artificial potential
field (APF) algorithm to formulate any surrounding objects, e.g., road marks,
boundaries, and static/dynamic obstacles. To track the generated optimal
trajectory, in the lower layer, an offline-constrained output feedback robust
model predictive control (RMPC) is employed for the linear parameter varying
(LPV) system by applying linear matrix inequality (LMI) optimization method
that ensures the robustness against the model parameter uncertainties.
Furthermore, by augmenting the system model, our proposed approach, called
offline RMPC, achieves outstanding efficiency compared to three existing RMPC
approaches, e.g., offset-offline RMPC, online RMPC, and offline RMPC without an
augmented model (offline RMPC w/o AM), in both improving computing time and
reducing input vibrations.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04779" title="Abstract">arXiv:2402.04779</a> [<a href="/pdf/2402.04779" title="Download PDF">pdf</a>, <a href="/format/2402.04779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StableMask: Refining Causal Masking in Decoder-only Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Q">Qingyu Yin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuzheng He</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+X">Xiang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jianhua Yao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The decoder-only Transformer architecture with causal masking and relative
position encoding (RPE) has become the de facto choice in language modeling.
Despite its exceptional performance across various tasks, we have identified
two limitations: First, it requires all attention scores to be non-zero and sum
up to 1, even if the current embedding has sufficient self-contained
information. This compels the model to assign disproportional excessive
attention to specific tokens. Second, RPE-based Transformers are not universal
approximators due to their limited capacity at encoding absolute positional
information, which limits their application in position-critical tasks. In this
work, we propose StableMask: a parameter-free method to address both
limitations by refining the causal mask. It introduces pseudo-attention values
to balance attention distributions and encodes absolute positional information
via a progressively decreasing mask ratio. StableMask's effectiveness is
validated both theoretically and empirically, showing significant enhancements
in language models with parameter sizes ranging from 71M to 1.4B across diverse
datasets and encoding methods. We further show that it naturally supports (1)
efficient extrapolation without special tricks such as StreamingLLM and (2)
easy integration with existing attention optimization techniques.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04783" title="Abstract">arXiv:2402.04783</a> [<a href="/pdf/2402.04783" title="Download PDF">pdf</a>, <a href="/format/2402.04783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saratchandran%2C+H">Hemanth Saratchandran</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+S">Shin-Fang Chng</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.02711">arXiv:2402.02711</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, neural networks utilizing periodic activation functions have been
proven to demonstrate superior performance in vision tasks compared to
traditional ReLU-activated networks. However, there is still a limited
understanding of the underlying reasons for this improved performance. In this
paper, we aim to address this gap by providing a theoretical understanding of
periodically activated networks through an analysis of their Neural Tangent
Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the
finite width setting, using a fairly general network architecture which
requires only one wide layer that grows at least linearly with the number of
data samples. Our findings indicate that periodically activated networks are
\textit{notably more well-behaved}, from the NTK perspective, than ReLU
activated networks. Additionally, we give an application to the memorization
capacity of such networks and verify our theoretical predictions empirically.
Our study offers a deeper understanding of the properties of periodically
activated neural networks and their potential in the field of deep learning.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04786" title="Abstract">arXiv:2402.04786</a> [<a href="/pdf/2402.04786" title="Download PDF">pdf</a>, <a href="/ps/2402.04786" title="Download PostScript">ps</a>, <a href="/format/2402.04786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple bipolar fuzzy measures: an application to community detection  problems for networks with additional information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+I">Inmaculada Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+D">Daniel G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+J">Javier Castro</a>, 
<a href="/search/cs?searchtype=author&query=Esp%C3%ADnola%2C+R">Rosa Esp&#xed;nola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computational Intelligence Systems, 2020,
  13(1), pp. 1636-1649
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Statistics Theory (math.ST); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In this paper we introduce the concept of multiple bipolar fuzzy measures as
a generalization of a bipolar fuzzy measure. We also propose a new definition
of a group, which is based on the multidimensional bipolar fuzzy relations of
its elements. Taking into account this information, we provide a novel
procedure (based on the well-known Louvain algorithm) to deal with community
detection problems. This new method considers the multidimensional bipolar
information provided by multiple bipolar fuzzy measures, as well as the
information provided by a graph. We also give some detailed computational
tests, obtained from the application of this algorithm in several benchmark
models.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04787" title="Abstract">arXiv:2402.04787</a> [<a href="/pdf/2402.04787" title="Download PDF">pdf</a>, <a href="/format/2402.04787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hypothesis-Driven Framework for the Analysis of Self-Rationalising  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braun%2C+M">Marc Braun</a>, 
<a href="/search/cs?searchtype=author&query=Kunz%2C+J">Jenny Kunz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The self-rationalising capabilities of LLMs are appealing because the
generated explanations can give insights into the plausibility of the
predictions. However, how faithful the explanations are to the predictions is
questionable, raising the need to explore the patterns behind them further. To
this end, we propose a hypothesis-driven statistical framework. We use a
Bayesian network to implement a hypothesis about how a task (in our example,
natural language inference) is solved, and its internal states are translated
into natural language with templates. Those explanations are then compared to
LLM-generated free-text explanations using automatic and human evaluations.
This allows us to judge how similar the LLM's and the Bayesian network's
decision processes are. We demonstrate the usage of our framework with an
example hypothesis and two realisations in Bayesian networks. The resulting
models do not exhibit a strong similarity to GPT-3.5. We discuss the
implications of this as well as the framework's potential to approximate LLM
decisions better in future work.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04788" title="Abstract">arXiv:2402.04788</a> [<a href="/pdf/2402.04788" title="Download PDF">pdf</a>, <a href="/format/2402.04788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with  Vision-Language Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongping Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huichi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multimodal Large Language Models (MLLMs) have gained significant attention
recently, showing remarkable potential in artificial general intelligence.
However, assessing the utility of MLLMs presents considerable challenges,
primarily due to the absence multimodal benchmarks that align with human
preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel
benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting
judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and
Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable
human-like discernment in Pair Comparisons, there is a significant divergence
from human preferences in Scoring Evaluation and Batch Ranking tasks.
Furthermore, MLLMs still face challenges in judgment, including diverse biases,
hallucinatory responses, and inconsistencies, even for advanced models such as
GPT-4V. These findings emphasize the pressing need for enhancements and further
research efforts regarding MLLMs as fully reliable evaluators. Code and dataset
are available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04789" title="Abstract">arXiv:2402.04789</a> [<a href="/pdf/2402.04789" title="Download PDF">pdf</a>, <a href="/format/2402.04789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Multicurves Cross Minimally on Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubois%2C+L">Lo&#xef;c Dubois</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">On an orientable surface $S$, consider a collection $\Gamma$ of closed
curves. The (geometric) intersection number $i_S(\Gamma)$ is the minimum number
of self-intersections that a collection $\Gamma'$ can have, where $\Gamma'$
results from a continuous deformation (homotopy) of $\Gamma$. We provide
algorithms that compute $i_S(\Gamma)$ and such a $\Gamma'$, assuming that
$\Gamma$ is given by a collection of closed walks of length $n$ in a graph $M$
cellularly embedded on $S$, in $O(n \log n)$ time when $M$ and $S$ are fixed.
<br />The state of the art is a paper of Despr\'e and Lazarus [SoCG 2017, J. ACM
2019], who compute $i_S(\Gamma)$ in $O(n^2)$ time, and $\Gamma'$ in $O(n^4)$
time if $\Gamma$ is a single closed curve. Our result is more general since we
can put an arbitrary number of closed curves in minimal position. Also, our
algorithms are quasi-linear in $n$ instead of quadratic and quartic, and our
proofs are simpler and shorter.
<br />We use techniques from two-dimensional topology and from the theory of
hyperbolic surfaces. Most notably, we prove a new property of the reducing
triangulations introduced by Colin de Verdi\`ere, Despr\'e, and Dubois [SODA
2024], reducing our problem to the case of surfaces with boundary. As a key
subroutine, we rely on an algorithm of Fulek and T\'oth [JCO 2020].
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04792" title="Abstract">arXiv:2402.04792</a> [<a href="/pdf/2402.04792" title="Download PDF">pdf</a>, <a href="/format/2402.04792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Language Model Alignment from Online AI Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangmin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianlin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Khalman%2C+M">Misha Khalman</a>, 
<a href="/search/cs?searchtype=author&query=Llinares%2C+F">Felipe Llinares</a>, 
<a href="/search/cs?searchtype=author&query=Rame%2C+A">Alexandre Rame</a>, 
<a href="/search/cs?searchtype=author&query=Mesnard%2C+T">Thomas Mesnard</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Piot%2C+B">Bilal Piot</a>, 
<a href="/search/cs?searchtype=author&query=Ferret%2C+J">Johan Ferret</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+M">Mathieu Blondel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Direct alignment from preferences (DAP) methods, such as DPO, have recently
emerged as efficient alternatives to reinforcement learning from human feedback
(RLHF), that do not require a separate reward model. However, the preference
datasets used in DAP methods are usually collected ahead of training and never
updated, thus the feedback is purely offline. Moreover, responses in these
datasets are often sampled from a language model distinct from the one being
aligned, and since the model evolves over training, the alignment phase is
inevitably off-policy. In this study, we posit that online feedback is key and
improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as
annotator: on each training iteration, we sample two responses from the current
model and prompt the LLM annotator to choose which one is preferred, thus
providing online feedback. Despite its simplicity, we demonstrate via human
evaluation in several tasks that OAIF outperforms both offline DAP and RLHF
methods. We further show that the feedback leveraged in OAIF is easily
controllable, via instruction prompts to the LLM annotator.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04794" title="Abstract">arXiv:2402.04794</a> [<a href="/pdf/2402.04794" title="Download PDF">pdf</a>, <a href="/format/2402.04794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Multi-view Clustering via Explicit Kernel Features Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fettal%2C+C">Chakib Fettal</a>, 
<a href="/search/cs?searchtype=author&query=Labiod%2C+L">Lazhar Labiod</a>, 
<a href="/search/cs?searchtype=author&query=Nadif%2C+M">Mohamed Nadif</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A growing awareness of multi-view learning as an important component in data
science and machine learning is a consequence of the increasing prevalence of
multiple views in real-world applications, especially in the context of
networks. In this paper we introduce a new scalability framework for multi-view
subspace clustering. An efficient optimization strategy is proposed, leveraging
kernel feature maps to reduce the computational burden while maintaining good
clustering performance. The scalability of the algorithm means that it can be
applied to large-scale datasets, including those with millions of data points,
using a standard machine, in a few minutes. We conduct extensive experiments on
real-world benchmark networks of various sizes in order to evaluate the
performance of our algorithm against state-of-the-art multi-view subspace
clustering methods and attributed-network multi-view approaches.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04796" title="Abstract">arXiv:2402.04796</a> [<a href="/pdf/2402.04796" title="Download PDF">pdf</a>, <a href="/format/2402.04796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mesh-based Gaussian Splatting for Real-time Large-scale Deformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Lin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo-Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jia-Mu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yu-Jie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongbo Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yu-Kun Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural implicit representations, including Neural Distance Fields and Neural
Radiance Fields, have demonstrated significant capabilities for reconstructing
surfaces with complicated geometry and topology, and generating novel views of
a scene. Nevertheless, it is challenging for users to directly deform or
manipulate these implicit representations with large deformations in the
real-time fashion. Gaussian Splatting(GS) has recently become a promising
method with explicit geometry for representing static scenes and facilitating
high-quality and real-time synthesis of novel views. However,it cannot be
easily deformed due to the use of discrete Gaussians and lack of explicit
topology. To address this, we develop a novel GS-based method that enables
interactive deformation. Our key idea is to design an innovative mesh-based GS
representation, which is integrated into Gaussian learning and manipulation. 3D
Gaussians are defined over an explicit mesh, and they are bound with each
other: the rendering of 3D Gaussians guides the mesh face split for adaptive
refinement, and the mesh face split directs the splitting of 3D Gaussians.
Moreover, the explicit mesh constraints help regularize the Gaussian
distribution, suppressing poor-quality Gaussians(e.g. misaligned
Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and
avoiding artifacts during deformation. Based on this representation, we further
introduce a large-scale Gaussian deformation technique to enable deformable GS,
which alters the parameters of 3D Gaussians according to the manipulation of
the associated mesh. Our method benefits from existing mesh deformation
datasets for more realistic data-driven Gaussian deformation. Extensive
experiments show that our approach achieves high-quality reconstruction and
effective deformation, while maintaining the promising rendering results at a
high frame rate(65 FPS on average).
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04797" title="Abstract">arXiv:2402.04797</a> [<a href="/pdf/2402.04797" title="Download PDF">pdf</a>, <a href="/format/2402.04797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Deep Model Predictive Control (MPC) for Visual Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouzid%2C+T">Taha Bouzid</a>, 
<a href="/search/cs?searchtype=author&query=Alj%2C+Y">Youssef Alj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ROBOVIS 2024 : 4th International Conference on Robotics, Computer Vision and Intelligent Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we propose a new visual navigation method based on a single
RGB perspective camera. Using the Visual Teach &amp; Repeat (VT&amp;R) methodology, the
robot acquires a visual trajectory consisting of multiple subgoal images in the
teaching step. In the repeat step, we propose two network architectures, namely
ViewNet and VelocityNet. The combination of the two networks allows the robot
to follow the visual trajectory. ViewNet is trained to generate a future image
based on the current view and the velocity command. The generated future image
is combined with the subgoal image for training VelocityNet. We develop an
offline Model Predictive Control (MPC) policy within VelocityNet with the dual
goals of (1) reducing the difference between current and subgoal images and (2)
ensuring smooth trajectories by mitigating velocity discontinuities. Offline
training conserves computational resources, making it a more suitable option
for scenarios with limited computational capabilities, such as embedded
systems. We validate our experiments in a simulation environment, demonstrating
that our model can effectively minimize the metric error between real and
played trajectories.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04798" title="Abstract">arXiv:2402.04798</a> [<a href="/pdf/2402.04798" title="Download PDF">pdf</a>, <a href="/format/2402.04798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with  Parallel Spike-driven Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingxaun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiankai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jiahao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kegang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuntao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Mingxuan Liu and Jiankai Tang are co-first authors of the article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Artificial neural networks (ANNs) can help camera-based remote
photoplethysmography (rPPG) in measuring cardiac activity and physiological
signals from facial videos, such as pulse wave, heart rate and respiration rate
with better accuracy. However, most existing ANN-based methods require
substantial computing resources, which poses challenges for effective
deployment on mobile devices. Spiking neural networks (SNNs), on the other
hand, hold immense potential for energy-efficient deep learning owing to their
binary and event-driven architecture. To the best of our knowledge, we are the
first to introduce SNNs into the realm of rPPG, proposing a hybrid neural
network (HNN) model, the Spiking-PhysFormer, aimed at reducing power
consumption. Specifically, the proposed Spiking-PhyFormer consists of an
ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based
predictor head. First, to simplify the transformer block while preserving its
capacity to aggregate local and global spatio-temporal features, we design a
parallel spike transformer block to replace sequential sub-blocks.
Additionally, we propose a simplified spiking self-attention mechanism that
omits the value parameter without compromising the model's performance.
Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD
demonstrate that the proposed model achieves a 12.4\% reduction in power
consumption compared to PhysFormer. Additionally, the power consumption of the
transformer block is reduced by a factor of 12.2, while maintaining decent
performance as PhysFormer and other ANN-based models.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04799" title="Abstract">arXiv:2402.04799</a> [<a href="/pdf/2402.04799" title="Download PDF">pdf</a>, <a href="/ps/2402.04799" title="Download PostScript">ps</a>, <a href="/format/2402.04799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strongly Polynomial Frame Scaling to High Precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadush%2C+D">Daniel Dadush</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+A">Akshay Ramachandran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The frame scaling problem is: given vectors $U := \{u_{1}, ..., u_{n} \}
\subseteq \mathbb{R}^{d}$, marginals $c \in \mathbb{R}^{n}_{++}$, and precision
$\varepsilon &gt; 0$, find left and right scalings $L \in \mathbb{R}^{d \times d},
r \in \mathbb{R}^n$ such that $(v_1,\dots,v_n) := (Lu_1 r_1,\dots,Lu_nr_n)$
simultaneously satisfies $\sum_{i=1}^n v_i v_i^{\mathsf{T}} = I_d$ and
$\|v_{j}\|_{2}^{2} = c_{j}, \forall j \in [n]$, up to error $\varepsilon$. This
problem has appeared in a variety of fields throughout linear algebra and
computer science. In this work, we give a strongly polynomial algorithm for
frame scaling with $\log(1/\varepsilon)$ convergence. This answers a question
of Diakonikolas, Tzamos and Kane (STOC 2023), who gave the first strongly
polynomial randomized algorithm with poly$(1/\varepsilon)$ convergence for the
special case $c = \frac{d}{n} 1_{n}$. Our algorithm is deterministic, applies
for general $c \in \mathbb{R}^{n}_{++}$, and requires $O(n^{3}
\log(n/\varepsilon))$ iterations as compared to $O(n^{5}
d^{11}/\varepsilon^{5})$ iterations of DTK. By lifting the framework of Linial,
Samorodnitsky and Wigderson (Combinatorica 2000) for matrix scaling to frames,
we are able to simplify both the algorithm and analysis. Our main technical
contribution is to generalize the potential analysis of LSW to the frame
setting and compute an update step in strongly polynomial time that achieves
geometric progress in each iteration. In fact, we can adapt our results to give
an improved analysis of strongly polynomial matrix scaling, reducing the
$O(n^{5} \log(n/\varepsilon))$ iteration bound of LSW to $O(n^{3}
\log(n/\varepsilon))$. Additionally, we prove a novel bound on the size of
approximate frame scaling solutions, involving the condition measure
$\bar{\chi}$ studied in the linear programming literature, which may be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04811" title="Abstract">arXiv:2402.04811</a> [<a href="/pdf/2402.04811" title="Download PDF">pdf</a>, <a href="/format/2402.04811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate Coverage Metrics for Compiler-Generated Debugging Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stinnett%2C+J+R">J. Ryan Stinnett</a>, 
<a href="/search/cs?searchtype=author&query=Kell%2C+S">Stephen Kell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Many debugging tools rely on compiler-produced metadata to present a
source-language view of program states, such as variable values and source line
numbers. While this tends to work for unoptimised programs, current compilers
often generate only partial debugging information in optimised programs.
Current approaches for measuring the extent of coverage of local variables are
based on crude assumptions (for example, assuming variables could cover their
whole parent scope) and are not comparable from one compilation to another. In
this work, we propose some new metrics, computable by our tools, which could
serve as motivation for language implementations to improve debugging quality.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04812" title="Abstract">arXiv:2402.04812</a> [<a href="/pdf/2402.04812" title="Download PDF">pdf</a>, <a href="/format/2402.04812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rink%2C+L">Lois Rink</a>, 
<a href="/search/cs?searchtype=author&query=Meijdam%2C+J">Job Meijdam</a>, 
<a href="/search/cs?searchtype=author&query=Graus%2C+D">David Graus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NLP4HR Workshop at EACL2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding preferences, opinions, and sentiment of the workforce is
paramount for effective employee lifecycle management. Open-ended survey
responses serve as a valuable source of information. This paper proposes a
machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch
open-ended responses in employee satisfaction surveys. Our approach aims to
overcome the inherent noise and variability in these responses, enabling a
comprehensive analysis of sentiments that can support employee lifecycle
management. Through response clustering we identify six key aspects (salary,
schedule, contact, communication, personal attention, agreements), which we
validate by domain experts. We compile a dataset of 1,458 Dutch survey
responses, revealing label imbalance in aspects and sentiments. We propose
few-shot approaches for ABSA based on Dutch BERT models, and compare them
against bag-of-words and zero-shot baselines. Our work significantly
contributes to the field of ABSA by demonstrating the first successful
application of Dutch pre-trained language models to aspect-based sentiment
analysis in the domain of human resources (HR).
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04814" title="Abstract">arXiv:2402.04814</a> [<a href="/pdf/2402.04814" title="Download PDF">pdf</a>, <a href="/format/2402.04814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BOWLL: A Deceptively Simple Open World Lifelong Learner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamath%2C+R">Roshni Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+R">Rupert Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Subarnaduti Paul</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>, 
<a href="/search/cs?searchtype=author&query=Mundt%2C+M">Martin Mundt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The quest to improve scalar performance numbers on predetermined benchmarks
seems to be deeply engraved in deep learning. However, the real world is seldom
carefully curated and applications are seldom limited to excelling on test
sets. A practical system is generally required to recognize novel concepts,
refrain from actively including uninformative data, and retain previously
acquired knowledge throughout its lifetime. Despite these key elements being
rigorously researched individually, the study of their conjunction, open world
lifelong learning, is only a recent trend. To accelerate this multifaceted
field's exploration, we introduce its first monolithic and much-needed
baseline. Leveraging the ubiquitous use of batch normalization across deep
neural networks, we propose a deceptively simple yet highly effective way to
repurpose standard models for open world lifelong learning. Through extensive
empirical evaluation, we highlight why our approach should serve as a future
standard for models that are able to effectively maintain their knowledge,
selectively focus on informative data, and accelerate future learning.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04818" title="Abstract">arXiv:2402.04818</a> [<a href="/pdf/2402.04818" title="Download PDF">pdf</a>, <a href="/format/2402.04818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An advanced scheme for queue management inTCP/IP networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boudi%2C+A">Abderrahmane Boudi</a>, 
<a href="/search/cs?searchtype=author&query=Loudini%2C+M">Malik Loudini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Active Queue Management (AQM) is a key congestion control scheme that aims to
find a balance between keeping high link utilization, minimizing queuing
delays, and ensuring a fair share of the bandwidth between the competing flows.
Traditional AQM mechanisms use only information that is present at the
intermediate nodes (routers). They do not take into account the particularities
of the flows composing the traffic. In this paper, we make use of a mechanism,
called Explicit RTT Notification (ERN), that shares with routers information
about the Round Trip Times (RTTs) of the flows. We propose a new fuzzy logic
based AQM controller that relies on the RTTs of the flows to improve fairness
between them. The performances of the new proposed method, FuzzyRTT, is
examined and compared to existing schemes via simulation experiments.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04820" title="Abstract">arXiv:2402.04820</a> [<a href="/pdf/2402.04820" title="Download PDF">pdf</a>, <a href="/format/2402.04820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinematic Motion Retargeting for Contact-Rich Anthropomorphic  Manipulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakshmipathy%2C+A+S">Arjun S. Lakshmipathy</a>, 
<a href="/search/cs?searchtype=author&query=Hodgins%2C+J+K">Jessica K. Hodgins</a>, 
<a href="/search/cs?searchtype=author&query=Pollard%2C+N+S">Nancy S. Pollard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Hand motion capture data is now relatively easy to obtain, even for
complicated grasps; however this data is of limited use without the ability to
retarget it onto the hands of a specific character or robot. The target hand
may differ dramatically in geometry, number of degrees of freedom (DOFs), or
number of fingers. We present a simple, but effective framework capable of
kinematically retargeting multiple human hand-object manipulations from a
publicly available dataset to a wide assortment of kinematically and
morphologically diverse target hands through the exploitation of contact areas.
We do so by formulating the retarget operation as a non-isometric shape
matching problem and use a combination of both surface contact and marker data
to progressively estimate, refine, and fit the final target hand trajectory
using inverse kinematics (IK). Foundational to our framework is the
introduction of a novel shape matching process, which we show enables
predictable and robust transfer of contact data over full manipulations while
providing an intuitive means for artists to specify correspondences with
relatively few inputs. We validate our framework through thirty demonstrations
across five different hand shapes and six motions of different objects. We
additionally compare our method against existing hand retargeting approaches.
Finally, we demonstrate our method enabling novel capabilities such as object
substitution and the ability to visualize the impact of design choices over
full trajectories.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04821" title="Abstract">arXiv:2402.04821</a> [<a href="/pdf/2402.04821" title="Download PDF">pdf</a>, <a href="/format/2402.04821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E(3)-Equivariant Mesh Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trang%2C+T">Thuan Trang</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+N+K">Nhat Khang Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+D">Daniel Levy</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+T+N">Thieu N. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Triangular meshes are widely used to represent three-dimensional objects. As
a result, many recent works have address the need for geometric deep learning
on 3D mesh. However, we observe that the complexities in many of these
architectures does not translate to practical performance, and simple deep
models for geometric graphs are competitive in practice. Motivated by this
observation, we minimally extend the update equations of E(n)-Equivariant Graph
Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face
information, and further improve it to account for long-range interactions
through hierarchy. The resulting architecture, Equivariant Mesh Neural Network
(EMNN), outperforms other, more complicated equivariant methods on mesh tasks,
with a fast run-time and no expensive pre-processing.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04823" title="Abstract">arXiv:2402.04823</a> [<a href="/pdf/2402.04823" title="Download PDF">pdf</a>, <a href="/format/2402.04823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Realistic Is Your Synthetic Data? Constraining Deep Generative  Models for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoian%2C+M+C">Mihaela C&#x103;t&#x103;lina Stoian</a>, 
<a href="/search/cs?searchtype=author&query=Dyrmishi%2C+S">Salijona Dyrmishi</a>, 
<a href="/search/cs?searchtype=author&query=Cordy%2C+M">Maxime Cordy</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Giunchiglia%2C+E">Eleonora Giunchiglia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep Generative Models (DGMs) have been shown to be powerful tools for
generating tabular data, as they have been increasingly able to capture the
complex distributions that characterize them. However, to generate realistic
synthetic data, it is often not enough to have a good approximation of their
distribution, as it also requires compliance with constraints that encode
essential background knowledge on the problem at hand. In this paper, we
address this limitation and show how DGMs for tabular data can be transformed
into Constrained Deep Generative Models (C-DGMs), whose generated samples are
guaranteed to be compliant with the given constraints. This is achieved by
automatically parsing the constraints and transforming them into a Constraint
Layer (CL) seamlessly integrated with the DGM. Our extensive experimental
analysis with various DGMs and tasks reveals that standard DGMs often violate
constraints, some exceeding $95\%$ non-compliance, while their corresponding
C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at
training time, C-DGMs are able to exploit the background knowledge expressed by
the constraints to outperform their standard counterparts with up to $6.5\%$
improvement in utility and detection. Further, we show how our CL does not
necessarily need to be integrated at training time, as it can be also used as a
guardrail at inference time, still producing some improvements in the overall
performance of the models. Finally, we show that our CL does not hinder the
sample generation time of the models.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04824" title="Abstract">arXiv:2402.04824</a> [<a href="/pdf/2402.04824" title="Download PDF">pdf</a>, <a href="/format/2402.04824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Communication Policies for Different Follower Behaviors in a  Collaborative Reference Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadler%2C+P">Philipp Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Hakimov%2C+S">Sherzod Hakimov</a>, 
<a href="/search/cs?searchtype=author&query=Schlangen%2C+D">David Schlangen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work presented at the "Cooperative Multi-Agent Systems Decision-making and Learning" workshop (AAAI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Albrecht and Stone (2018) state that modeling of changing behaviors remains
an open problem "due to the essentially unconstrained nature of what other
agents may do". In this work we evaluate the adaptability of neural artificial
agents towards assumed partner behaviors in a collaborative reference game. In
this game success is achieved when a knowledgeable Guide can verbally lead a
Follower to the selection of a specific puzzle piece among several distractors.
We frame this language grounding and coordination task as a reinforcement
learning problem and measure to which extent a common reinforcement training
algorithm (PPO) is able to produce neural agents (the Guides) that perform well
with various heuristic Follower behaviors that vary along the dimensions of
confidence and autonomy. We experiment with a learning signal that in addition
to the goal condition also respects an assumed communicative effort. Our
results indicate that this novel ingredient leads to communicative strategies
that are less verbose (staying silent in some of the steps) and that with
respect to that the Guide's strategies indeed adapt to the partner's level of
confidence and autonomy.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04825" title="Abstract">arXiv:2402.04825</a> [<a href="/pdf/2402.04825" title="Download PDF">pdf</a>, <a href="/format/2402.04825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Timing-Conditioned Latent Audio Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Evans%2C+Z">Zach Evans</a>, 
<a href="/search/cs?searchtype=author&query=Carr%2C+C">CJ Carr</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">Josiah Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Hawley%2C+S+H">Scott H. Hawley</a>, 
<a href="/search/cs?searchtype=author&query=Pons%2C+J">Jordi Pons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/Stability-AI/stable-audio-tools.">this https URL</a> Metrics: <a href="https://github.com/Stability-AI/stable-audio-metrics.">this https URL</a> Demo: <a href="https://stability-ai.github.io/stable-audio-demo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Generating long-form 44.1kHz stereo audio from text prompts can be
computationally demanding. Further, most previous works do not tackle that
music and sound effects naturally vary in their duration. Our research focuses
on the efficient generation of long-form, variable-length stereo music and
sounds at 44.1kHz using text prompts with a generative model. Stable Audio is
based on latent diffusion, with its latent defined by a fully-convolutional
variational autoencoder. It is conditioned on text prompts as well as timing
embeddings, allowing for fine control over both the content and length of the
generated music and sounds. Stable Audio is capable of rendering stereo signals
of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute
efficiency and fast inference, it is one of the best in two public
text-to-music and -audio benchmarks and, differently from state-of-the-art
models, can generate music with structure and stereo sounds.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04829" title="Abstract">arXiv:2402.04829</a> [<a href="/pdf/2402.04829" title="Download PDF">pdf</a>, <a href="/format/2402.04829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+J">Jingwang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Ruihan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Feng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chun Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page and video: <a href="https://nerfemitterpbir.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Physics-based inverse rendering aims to jointly optimize shape, materials,
and lighting from captured 2D images. Here lighting is an important part of
achieving faithful light transport simulation. While the environment map is
commonly used as the lighting model in inverse rendering, we show that its
distant lighting assumption leads to spatial invariant lighting, which can be
an inaccurate approximation in real-world inverse rendering. We propose to use
NeRF as a spatially varying environment lighting model and build an inverse
rendering pipeline using NeRF as the non-distant environment emitter. By
comparing our method with the environment map on real and synthetic datasets,
we show that our NeRF-based emitter models the scene lighting more accurately
and leads to more accurate inverse rendering. Project page and video:
https://nerfemitterpbir.github.io/.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04830" title="Abstract">arXiv:2402.04830</a> [<a href="/pdf/2402.04830" title="Download PDF">pdf</a>, <a href="/format/2402.04830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the Gap Between SGP4 and High-Precision Propagation via  Differentiable Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acciarini%2C+G">Giacomo Acciarini</a>, 
<a href="/search/cs?searchtype=author&query=Baydin%2C+A+G">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, 
<a href="/search/cs?searchtype=author&query=Izzo%2C+D">Dario Izzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Earth and Planetary Astrophysics (astro-ph.EP)

</div>
<p class="mathjax">The Simplified General Perturbations 4 (SGP4) orbital propagation method is
widely used for predicting the positions and velocities of Earth-orbiting
objects rapidly and reliably. Despite continuous refinement, SGP models still
lack the precision of numerical propagators, which offer significantly smaller
errors. This study presents dSGP4, a novel differentiable version of SGP4
implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates
various space-related applications, including spacecraft orbit determination,
state conversion, covariance transformation, state transition matrix
computation, and covariance propagation. Additionally, dSGP4's PyTorch
implementation allows for embarrassingly parallel orbital propagation across
batches of Two-Line Element Sets (TLEs), leveraging the computational power of
CPUs, GPUs, and advanced hardware for distributed prediction of satellite
positions at future times. Furthermore, dSGP4's differentiability enables
integration with modern machine learning techniques. Thus, we propose a novel
orbital propagation paradigm, ML-dSGP4, where neural networks are integrated
into the orbital propagator. Through stochastic gradient descent, this combined
model's inputs, outputs, and parameters can be iteratively refined, surpassing
SGP4's precision. Neural networks act as identity operators by default,
adhering to SGP4's behavior. However, dSGP4's differentiability allows
fine-tuning with ephemeris data, enhancing precision while maintaining
computational speed. This empowers satellite operators and researchers to train
the model using specific ephemeris or high-precision numerical propagation
data, significantly advancing orbital prediction capabilities.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04831" title="Abstract">arXiv:2402.04831</a> [<a href="/pdf/2402.04831" title="Download PDF">pdf</a>, <a href="/format/2402.04831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel Phase Detector Measurement Procedure Using Quasi-Synchronized RF  Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pulido%2C+V+A">V. A. Pulido</a>, 
<a href="/search/eess?searchtype=author&query=Cabrera-Almeida%2C+F">F. Cabrera-Almeida</a>, 
<a href="/search/eess?searchtype=author&query=Quintana-Morales%2C+P">P. Quintana-Morales</a>, 
<a href="/search/eess?searchtype=author&query=Mendieta-Otero%2C+E">E. Mendieta-Otero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> copyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Instrumentation and Measurement, vol. 72, pp.
  1-9, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a new procedure for phase detector measurements that
allows the use of generators that share a 10 MHz reference oscillator but do
not synchronize in phase, in other words, quasi-synchronized RF generators. The
objectives are taking advantage of the benefits of using two generators but
recovering lower-cost generators that have worse synchronization performance
and opening the door to the possibility of using a very simple control element
based in Arduino Uno and cheaper instruments. The new procedure is
characterized by continuously alternating calibration and measurement sequences
to make up for the phase drift of quasisynchronized generators and guarantee a
maximum phase error specification (+-1 grade in this paper). Data acquisition
has been divided in two stages: measurement of detector curves without phase
reference (in-phase and phase-shifted) and measurement of reference data. All
the data is later combined to obtain correctly referenced in-phase detector
curves. The technique can be reproduced with other equivalent instrumentation.
The novel procedure that allows compensation for errors (amplitude, phase
shift, mismatching, etc.) is detailed, and its relation to the required
measurement accuracy is amply discussed. The proposed technique is applied to
characterize a phase detector based on in-phase and phase-shifted
multiplication from 3 to 8 GHz with 1 GHz step. Measurements have a final
maximum error of +-2 grade for both frequency and calibrated input power,
according to the accuracy specifications of the VNA used to calibrate the
signal distribution network, added to the +-1 grade specified in this new
procedure.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04832" title="Abstract">arXiv:2402.04832</a> [<a href="/pdf/2402.04832" title="Download PDF">pdf</a>, <a href="/ps/2402.04832" title="Download PostScript">ps</a>, <a href="/format/2402.04832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured d-DNNF Is Not Closed Under Negation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vinall-Smeeth%2C+H">Harry Vinall-Smeeth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Both structured d-DNNF and SDD can be exponentially more succinct than OBDD.
Moreover, SDD is essentially as tractable as OBDD. But this has left two
important open questions. Firstly, does OBDD support more tractable
transformations than structured d-DNNF? And secondly, is structured d-DNNF more
succinct than SDD? In this paper, we answer both questions in the affirmative.
For the first question we show that, unlike OBDD, structured d-DNNF does not
support polytime negation, disjunction, or existential quantification
operations. As a corollary, we deduce that there are functions with an
equivalent polynomial-sized structured d-DNNF but with no such representation
as an SDD, thus answering the second question. We also lift this second result
to arithmetic circuits (AC) to show a succinctness gap between PSDD and the
monotone AC analogue to structured d-DNNF.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04833" title="Abstract">arXiv:2402.04833</a> [<a href="/pdf/2402.04833" title="Download PDF">pdf</a>, <a href="/format/2402.04833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for  Instruction Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Andriushchenko%2C+M">Maksym Andriushchenko</a>, 
<a href="/search/cs?searchtype=author&query=Croce%2C+F">Francesco Croce</a>, 
<a href="/search/cs?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 25 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">There is a consensus that instruction fine-tuning of LLMs requires
high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR
2024) are state-of-the-art methods for selecting such high-quality examples,
either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show
that the extremely simple baseline of selecting the 1,000 instructions with
longest responses from standard datasets can consistently outperform these
sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining
competitive on the OpenLLM benchmarks that test factual knowledge. We
demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B,
and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a
lightweight refinement of such long instructions can further improve the
abilities of the fine-tuned LLMs, and allows us to obtain the 2nd
highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only
1,000 examples and no extra preference data. We also conduct a thorough
analysis of our models to ensure that their enhanced performance is not simply
due to GPT-4's preference for longer responses, thus ruling out any artificial
improvement. In conclusion, our findings suggest that fine-tuning on the
longest instructions should be the default baseline for any research on
instruction fine-tuning.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04835" title="Abstract">arXiv:2402.04835</a> [<a href="/pdf/2402.04835" title="Download PDF">pdf</a>, <a href="/format/2402.04835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SARI: Simplistic Average and Robust Identification based Noisy Partial  Label Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+D">Darshana Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Manwani%2C+N">Naresh Manwani</a>, 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+V">Vineet Gandhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Partial label learning (PLL) is a weakly-supervised learning paradigm where
each training instance is paired with a set of candidate labels (partial
label), one of which is the true label. Noisy PLL (NPLL) relaxes this
constraint by allowing some partial labels to not contain the true label,
enhancing the practicality of the problem. Our work centers on NPLL and
presents a minimalistic framework called SARI that initially assigns
pseudo-labels to images by exploiting the noisy partial labels through a
weighted nearest neighbour algorithm. These pseudo-label and image pairs are
then used to train a deep neural network classifier with label smoothing and
standard regularization techniques. The classifier's features and predictions
are subsequently employed to refine and enhance the accuracy of pseudo-labels.
SARI combines the strengths of Average Based Strategies (in pseudo labelling)
and Identification Based Strategies (in classifier training) from the
literature. We perform thorough experiments on seven datasets and compare SARI
against nine NPLL and PLL methods from the prior art. SARI achieves
state-of-the-art results in almost all studied settings, obtaining substantial
gains in fine-grained classification and extreme noise settings.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04836" title="Abstract">arXiv:2402.04836</a> [<a href="/pdf/2402.04836" title="Download PDF">pdf</a>, <a href="/format/2402.04836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Completeness of Invariant Geometric Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Shijia Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Invariant models, one important class of geometric deep learning models, are
capable of generating meaningful geometric representations by leveraging
informative geometric features. These models are characterized by their
simplicity, good experimental results and computational efficiency. However,
their theoretical expressive power still remains unclear, restricting a deeper
understanding of the potential of such models. In this work, we concentrate on
characterizing the theoretical expressiveness of invariant models. We first
rigorously bound the expressiveness of the most classical invariant model,
Vanilla DisGNN (message passing neural networks incorporating distance),
restricting its unidentifiable cases to be only those highly symmetric
geometric graphs. To break these corner cases' symmetry, we introduce a simple
yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN.
Leveraging GeoNGNN as a theoretical tool, we for the first time prove the
E(3)-completeness of three well-established geometric models: DimeNet, GemNet
and SphereNet. Our results fill the gap in the theoretical power of invariant
models, contributing to a rigorous and comprehensive understanding of their
capabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing
local environments, and achieves competitive results w.r.t. complicated models
relying on high-order invariant/equivariant representations while exhibiting
significantly faster computational speed.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04838" title="Abstract">arXiv:2402.04838</a> [<a href="/pdf/2402.04838" title="Download PDF">pdf</a>, <a href="/format/2402.04838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuejing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Can Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we aim to reduce generation latency for Named Entity
Recognition (NER) with Large Language Models (LLMs). The main cause of high
latency in LLMs is the sequential decoding process, which autoregressively
generates all labels and mentions for NER, significantly increase the sequence
length. To this end, we introduce Parallel Decoding in LLM for NE}
(PaDeLLM-NER), a approach that integrates seamlessly into existing generative
model frameworks without necessitating additional modules or architectural
modifications. PaDeLLM-NER allows for the simultaneous decoding of all
mentions, thereby reducing generation latency. Experiments reveal that
PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times
faster than the autoregressive approach for both English and Chinese.
Simultaneously it maintains the quality of predictions as evidenced by the
performance that is on par with the state-of-the-art across various datasets.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04841" title="Abstract">arXiv:2402.04841</a> [<a href="/pdf/2402.04841" title="Download PDF">pdf</a>, <a href="/format/2402.04841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-efficient Large Vision Models through Sequential Autoregression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhiwei Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Training general-purpose vision models on purely sequential visual data,
eschewing linguistic inputs, has heralded a new frontier in visual
understanding. These models are intended to not only comprehend but also
seamlessly transit to out-of-domain tasks. However, current endeavors are
hamstrung by an over-reliance on colossal models, exemplified by models with
upwards of 3B parameters, and the necessity for an extensive corpus of visual
data, often comprising a staggering 400B tokens. In this paper, we delve into
the development of an efficient, autoregression-based vision model,
innovatively architected to operate on a limited dataset. We meticulously
demonstrate how this model achieves proficiency in a spectrum of visual tasks
spanning both high-level and low-level semantic understanding during the
testing phase. Our empirical evaluations underscore the model's agility in
adapting to various tasks, heralding a significant reduction in the parameter
footprint, and a marked decrease in training data requirements, thereby paving
the way for more sustainable and accessible advancements in the field of
generalist vision models. The code is available at
https://github.com/ggjy/DeLVM.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04844" title="Abstract">arXiv:2402.04844</a> [<a href="/pdf/2402.04844" title="Download PDF">pdf</a>, <a href="/format/2402.04844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface for Industrial Automation: mmWave  Propagation Measurement, Simulation, and Control Algorithm Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Radpour%2C+H">Hamed Radpour</a>, 
<a href="/search/eess?searchtype=author&query=Hofer%2C+M">Markus Hofer</a>, 
<a href="/search/eess?searchtype=author&query=Loschenbrand%2C+D">David Loschenbrand</a>, 
<a href="/search/eess?searchtype=author&query=Mayer%2C+L+W">Lukas Walter Mayer</a>, 
<a href="/search/eess?searchtype=author&query=Hofmann%2C+A">Andreas Hofmann</a>, 
<a href="/search/eess?searchtype=author&query=Schiefer%2C+M">Martin Schiefer</a>, 
<a href="/search/eess?searchtype=author&query=Zemen%2C+T">Thomas Zemen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), Valencia, Spain, September 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Reconfigurable intelligent surfaces (RISs) enable reliable low-latency
millimeter wave (mmWave) communication links in cases of a blocked
line-of-sight (LoS) between the base station (BS) and the user equipment (UE),
i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio
communication link. We present an active RIS with 127 patch antenna elements
arranged in a hexagonal grid for a center frequency of 23.8 GHz. Each RIS
element uses an orthogonal polarization transformation to enable amplification
using a field-effect transistor (FET). The source and drain voltages of each
FET is controlled using two bits. We assume that the coordinates of the UE in
an industrial control scenario are known to the RIS. We measure the received
power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and
active mode. The results show that the RIS can successfully focus the radio
signal at the desired target points. The half-power beam width is characterized
in axial and radial directions with respect to the RIS position, obtaining a
practical RIS configuration update criterion for a mobile UE. These results
clearly show that RISs are prominent solutions for enabling reliable wireless
communication in indoor industrial scenarios.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04848" title="Abstract">arXiv:2402.04848</a> [<a href="/pdf/2402.04848" title="Download PDF">pdf</a>, <a href="/format/2402.04848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear behavior of area dependent interface type resistive switching  devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yarragolla%2C+S">Sahitya Yarragolla</a>, 
<a href="/search/cs?searchtype=author&query=Hemke%2C+T">Torben Hemke</a>, 
<a href="/search/cs?searchtype=author&query=Jalled%2C+F">Fares Jalled</a>, 
<a href="/search/cs?searchtype=author&query=Gergs%2C+T">Tobias Gergs</a>, 
<a href="/search/cs?searchtype=author&query=Trieschmann%2C+J">Jan Trieschmann</a>, 
<a href="/search/cs?searchtype=author&query=Arul%2C+T">Tolga Arul</a>, 
<a href="/search/cs?searchtype=author&query=Mussenbrock%2C+T">Thomas Mussenbrock</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall)

</div>
<p class="mathjax">Nonlinearity is a crucial characteristic for implementing hardware security
primitives or neuromorphic computing systems. The main feature of all
memristive devices is this nonlinear behavior observed in their current-voltage
characteristics. To comprehend the nonlinear behavior, we have to understand
the coexistence of resistive, capacitive, and inertia (virtual inductive)
effects in these devices. These effects originate from physical and chemical
processes in memristive devices. The physics-inspired compact model is employed
to model and simulate interface-type RRAMs such as Au/BiFeO$_{3}$/Pt/Ti,
Au/Nb$_{x}$O$_{y}$/Al$_{2}$O$_{3}$/Nb, while accounting for the modeling of
capacitive and inertia effects. The proposed model's current-voltage
characteristics align well with experimental data and accurately capture the
non-zero crossing hysteresis generated by capacitive and inductive effects. The
study examines the response of both devices to various frequencies, showing a
shift in their nonlinear behavior as evidenced by a reduction in their
hysteresis range. Fourier series analysis utilizing a sinusoidal input voltage
of varying amplitudes and frequencies indicates harmonics or frequency
components that considerably influence the functioning of RRAMs. Moreover, We
propose and demonstrate using the frequency spectrum as a fingerprint for
memristive devices.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04852" title="Abstract">arXiv:2402.04852</a> [<a href="/pdf/2402.04852" title="Download PDF">pdf</a>, <a href="/format/2402.04852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Patch Prediction: Adapting LLMs for Time Series Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yuxuan Bian</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+X">Xuan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangtong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhijian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dawei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this study, we present aLLM4TS, an innovative framework that adapts Large
Language Models (LLMs) for time-series representation learning. Central to our
approach is that we reconceive time-series forecasting as a self-supervised,
multi-patch prediction task, which, compared to traditional
mask-and-reconstruction methods, captures temporal dynamics in patch
representations more effectively. Our strategy encompasses two-stage training:
(i). a causal continual pre-training phase on various time-series datasets,
anchored on next patch prediction, effectively syncing LLM capabilities with
the intricacies of time-series data; (ii). fine-tuning for multi-patch
prediction in the targeted time-series context. A distinctive element of our
framework is the patch-wise decoding layer, which departs from previous methods
reliant on sequence-level decoding. Such a design directly transposes
individual patches into temporal sequences, thereby significantly bolstering
the model's proficiency in mastering temporal patch-based representations.
aLLM4TS demonstrates superior performance in several downstream tasks, proving
its effectiveness in deriving temporal representations with enhanced
transferability and marking a pivotal advancement in the adaptation of LLMs for
time-series analysis.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04853" title="Abstract">arXiv:2402.04853</a> [<a href="/pdf/2402.04853" title="Download PDF">pdf</a>, <a href="/format/2402.04853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging LLMs for Unsupervised Dense Retriever Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khramtsova%2C+E">Ekaterina Khramtsova</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Baktashmotlagh%2C+M">Mahsa Baktashmotlagh</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">This paper introduces a novel unsupervised technique that utilizes large
language models (LLMs) to determine the most suitable dense retriever for a
specific test(target) corpus. Selecting the appropriate dense retriever is
vital for numerous IR applications that employ these retrievers, trained on
public datasets, to encode or conduct searches within a new private target
corpus. The effectiveness of a dense retriever can significantly diminish when
applied to a target corpus that diverges in domain or task from the original
training set. The problem becomes more pronounced in cases where the target
corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation
of the model's effectiveness on the target corpus unattainable. Therefore, the
unsupervised selection of an optimally pre-trained dense retriever, especially
under conditions of domain shift, emerges as a critical challenge. Existing
methodologies for ranking dense retrievers fall short in addressing these
domain shift scenarios.
<br />To tackle this, our method capitalizes on LLMs to create pseudo-relevant
queries, labels, and reference lists by analyzing a subset of documents from
the target corpus. This allows for the ranking of dense retrievers based on
their performance with these pseudo-relevant signals. Significantly, this
strategy is the first to depend exclusively on the target corpus data, removing
the necessity for training data and test labels. We assessed the effectiveness
of our approach by compiling a comprehensive pool of cutting-edge dense
retrievers and comparing our method against traditional dense retriever
selection benchmarks. The findings reveal that our proposed solution surpasses
the existing benchmarks in both the selection and ranking of dense retrievers.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04854" title="Abstract">arXiv:2402.04854</a> [<a href="/pdf/2402.04854" title="Download PDF">pdf</a>, <a href="/format/2402.04854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinghong Li</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Huy Phan</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+W">Wen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ota%2C+K">Koichi Ota</a>, 
<a href="/search/cs?searchtype=author&query=Hasegawa%2C+S">Shinobu Hasegawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper will submit to '27th International Symposium on Methodologies for Intelligent Systems'(ISMIS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Research surveys have always posed a challenge for beginner researchers who
lack of research training. These researchers struggle to understand the
directions within their research topic, and the discovery of new research
findings within a short time. One way to provide intuitive assistance to
beginner researchers is by offering relevant knowledge graphs(KG) and
recommending related academic papers. However, existing navigation knowledge
graphs primarily rely on keywords in the research field and often fail to
present the logical hierarchy among multiple related papers clearly. Moreover,
most recommendation systems for academic papers simply rely on high text
similarity, which can leave researchers confused as to why a particular article
is being recommended. They may lack of grasp important information about the
insight connection between "Issue resolved" and "Issue finding" that they hope
to obtain. To address these issues, this study aims to support research insight
surveys for beginner researchers by establishing a hierarchical tree-structured
knowledge graph that reflects the inheritance insight of research topics and
the relevance insight among the academic papers.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04855" title="Abstract">arXiv:2402.04855</a> [<a href="/pdf/2402.04855" title="Download PDF">pdf</a>, <a href="/format/2402.04855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Path Coupled Image Deraining Network via Spatial-Frequency  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuhong He</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+A">Aiwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lingfang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Transformers have recently emerged as a significant force in the field of
image deraining. Existing image deraining methods utilize extensive research on
self-attention. Though showcasing impressive results, they tend to neglect
critical frequency information, as self-attention is generally less adept at
capturing high-frequency details. To overcome this shortcoming, we have
developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that
integrates information from both spatial and frequency domains through Spatial
Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block
(FFEBlock). We have further introduced an effective Adaptive Fusion Module
(AFM) for the dual-path feature aggregation. Extensive experiments on six
public deraining benchmarks and downstream vision tasks have demonstrated that
our proposed method not only outperforms the existing state-of-the-art
deraining method but also achieves visually pleasuring results with excellent
robustness on downstream vision tasks.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04856" title="Abstract">arXiv:2402.04856</a> [<a href="/pdf/2402.04856" title="Download PDF">pdf</a>, <a href="/format/2402.04856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Learned Reward Functions with Counterfactual Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wehner%2C+J">Jan Wehner</a>, 
<a href="/search/cs?searchtype=author&query=Oliehoek%2C+F">Frans Oliehoek</a>, 
<a href="/search/cs?searchtype=author&query=Siebert%2C+L+C">Luciano Cavalcante Siebert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning rewards from human behaviour or feedback is a promising approach to
aligning AI systems with human values but fails to consistently extract correct
reward functions. Interpretability tools could enable users to understand and
evaluate possible flaws in learned reward functions. We propose Counterfactual
Trajectory Explanations (CTEs) to interpret reward functions in reinforcement
learning by contrasting an original with a counterfactual partial trajectory
and the rewards they each receive. We derive six quality criteria for CTEs and
propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises
these quality criteria. Finally, we measure how informative the generated
explanations are to a proxy-human model by training it on CTEs. CTEs are
demonstrably informative for the proxy-human model, increasing the similarity
between its predictions and the reward function on unseen trajectories.
Further, it learns to accurately judge differences in rewards between
trajectories and generalises to out-of-distribution examples. Although CTEs do
not lead to a perfect understanding of the reward, our method, and more
generally the adaptation of XAI methods, are presented as a fruitful approach
for interpreting learned reward functions.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04857" title="Abstract">arXiv:2402.04857</a> [<a href="/pdf/2402.04857" title="Download PDF">pdf</a>, <a href="/format/2402.04857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Anomaly Detection: An Adaptation Model and a New Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liyun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+A">Arjun Raj</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Industry surveillance is widely applicable in sectors like retail,
manufacturing, education, and smart cities, each presenting unique anomalies
requiring specialized detection. However, adapting anomaly detection models to
novel viewpoints within the same scenario poses challenges. Extending these
models to entirely new scenarios necessitates retraining or fine-tuning, a
process that can be time consuming. To address these challenges, we propose the
Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot
learning framework for faster adaptation of pre-trained models to new concepts.
Despite this approach, a significant challenge emerges from the absence of a
comprehensive dataset with diverse scenarios and camera views. In response, we
introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14
distinct scenarios captured from various camera views. This real-world dataset
is the first high-resolution anomaly detection dataset, offering a solid
foundation for training superior models. MSAD includes diverse normal motion
patterns, incorporating challenging variations like different lighting and
weather conditions. Through experimentation, we validate the efficacy of SA2D,
particularly when trained on the MSAD dataset. Our results show that SA2D not
only excels under novel viewpoints within the same scenario but also
demonstrates competitive performance when faced with entirely new scenarios.
This highlights our method's potential in addressing challenges in detecting
anomalies across diverse and evolving surveillance scenarios.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04858" title="Abstract">arXiv:2402.04858</a> [<a href="/pdf/2402.04858" title="Download PDF">pdf</a>, <a href="/format/2402.04858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Butt%2C+N">Natasha Butt</a>, 
<a href="/search/cs?searchtype=author&query=Manczak%2C+B">Blazej Manczak</a>, 
<a href="/search/cs?searchtype=author&query=Wiggers%2C+A">Auke Wiggers</a>, 
<a href="/search/cs?searchtype=author&query=Rainone%2C+C">Corrado Rainone</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">David Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Defferrard%2C+M">Micha&#xeb;l Defferrard</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Taco Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models are increasingly solving tasks that are commonly
believed to require human-level reasoning ability. However, these models still
perform very poorly on benchmarks of general intelligence such as the
Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a
programming-by-examples problem, and introduce a novel and scalable method for
language model self-improvement called Code Iteration (CodeIt). Our method
iterates between 1) program sampling and hindsight relabeling, and 2) learning
from prioritized experience replay. By relabeling the goal of an episode (i.e.,
the target program output given input) to the realized output produced by the
sampled program, our method effectively deals with the extreme sparsity of
rewards in program synthesis. Applying CodeIt to the ARC dataset, we
demonstrate that prioritized hindsight replay, along with pre-training and
data-augmentation, leads to successful inter-task generalization. CodeIt is the
first neuro-symbolic approach that scales to the full ARC evaluation dataset.
Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art
performance and outperforming existing neural and symbolic baselines.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04862" title="Abstract">arXiv:2402.04862</a> [<a href="/pdf/2402.04862" title="Download PDF">pdf</a>, <a href="/format/2402.04862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tactile Ergodic Control Using Diffusion and Geometric Algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilaloglu%2C+C">Cem Bilaloglu</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B6w%2C+T">Tobias L&#xf6;w</a>, 
<a href="/search/cs?searchtype=author&query=Calinon%2C+S">Sylvain Calinon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the special issue for IEEE Transactions on Robotics (T-RO) on Tactile Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Continuous physical interaction between robots and their environment is a
requirement in many industrial and household tasks, such as sanding and
cleaning. Due to the complex tactile information, these tasks are notoriously
difficult to model and to sense. In this article, we introduce a closed-loop
control method that is constrained to surfaces. The applications that we target
have in common that they can be represented by probability distributions on the
surface that correlate to the time the robot should spend in a region. These
surfaces can easily be captured jointly with the target distributions using
coloured point clouds. We present the extension of an ergodic control approach
that can be used with point clouds, based on heat equation-driven area coverage
(HEDAC). Our method enables closed-loop exploration by measuring the actual
coverage using vision. Unlike existing approaches, we approximate the potential
field from non-stationary diffusion using spectral acceleration, which does not
require complex preprocessing steps and achieves real-time closed-loop control
frequencies. We exploit geometric algebra to stay in contact with the target
surface by tracking a line while simultaneously exerting a desired force along
that line. Our approach is suitable for fully autonomous and human-robot
interaction settings where the robot can either directly measure the coverage
of the target with its sensors or by being guided online by markings or
annotations of a human expert. We tested the performance of the approach in
kinematic simulation using point clouds, ranging from the Stanford bunny to a
variety of kitchen utensils. Our real-world experiments demonstrate that the
proposed approach can successfully be used to wash kitchenware with curved
surfaces, by cleaning the dirt detected by vision in an online manner. Website:
https://geometric-algebra.tobiloew.ch/tactile_ergodic_control
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04863" title="Abstract">arXiv:2402.04863</a> [<a href="/pdf/2402.04863" title="Download PDF">pdf</a>, <a href="/format/2402.04863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Smart Contract Summarization via LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yingjie Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenkai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Automatic code Summarization generation technology is widely used in the
development and maintenance of smart contracts. In recent years, with the
advent of Large Language Models (LLMs), Gemini has received a lot of attention
as the first Large Multimodal models (LMMs) to support multimodal input.
However, it is unclear how LMMs can generate contract code summarization from
multimodal inputs. In this paper, we focus on evaluating Gemini on real-world
smart contracts, comparing it to the MMTrans, and exploring how to combine
multimodal prompts to generate a contract code summarization. We used several
widely used metrics (BLEU, METEOR, and ROUGE-L) to measure the quality of the
generated summarization. Our experiments show that METEOR and ROUGE-L metrics,
Gemini-Pro-Vision achieves 21.17% and 21.05% scores for code comments generated
by three-shot prompts. These scores are better than those generated by one-shot
and five-shot prompts.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04867" title="Abstract">arXiv:2402.04867</a> [<a href="/pdf/2402.04867" title="Download PDF">pdf</a>, <a href="/format/2402.04867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from  Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+B">Bingzheng Gan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wei Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In the rapidly evolving landscape of information retrieval, search engines
strive to provide more personalized and relevant results to users. Query
suggestion systems play a crucial role in achieving this goal by assisting
users in formulating effective queries. However, existing query suggestion
systems mainly rely on textual inputs, potentially limiting user search
experiences for querying images. In this paper, we introduce a novel Multimodal
Query Suggestion (MMQS) task, which aims to generate query suggestions based on
user query images to improve the intentionality and diversity of search
results. We present the RL4Sugg framework, leveraging the power of Large
Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human
Feedback to optimize the generation process. Through comprehensive experiments,
we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement
compared to the best existing approach. Moreover, the MMQS has been transferred
into real-world search engine products, which yield enhanced user engagement.
Our research advances query suggestion systems and provides a new perspective
on multimodal information retrieval.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04869" title="Abstract">arXiv:2402.04869</a> [<a href="/pdf/2402.04869" title="Download PDF">pdf</a>, <a href="/format/2402.04869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning by Doing: An Online Causal Reinforcement Learning Framework  with Causal-Aware Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+J">Jie Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Keli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As a key component to intuitive cognition and reasoning solutions in human
intelligence, causal knowledge provides great potential for reinforcement
learning (RL) agents' interpretability towards decision-making by helping
reduce the searching space. However, there is still a considerable gap in
discovering and incorporating causality into RL, which hinders the rapid
development of causal RL. In this paper, we consider explicitly modeling the
generation process of states with the causal graphical model, based on which we
augment the policy. We formulate the causal structure updating into the RL
interaction process with active intervention learning of the environment. To
optimize the derived objective, we propose a framework with theoretical
performance guarantees that alternates between two steps: using interventions
for causal structure learning during exploration and using the learned causal
structure for policy guidance during exploitation. Due to the lack of public
benchmarks that allow direct intervention in the state space, we design the
root cause localization task in our simulated fault alarm environment and then
empirically show the effectiveness and robustness of the proposed method
against state-of-the-art baselines. Theoretical analysis shows that our
performance improvement attributes to the virtuous cycle of causal-guided
policy learning and causal structure learning, which aligns with our
experimental results.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04870" title="Abstract">arXiv:2402.04870</a> [<a href="/pdf/2402.04870" title="Download PDF">pdf</a>, <a href="/format/2402.04870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Knowledge Graphs in Degenerate Clifford Algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamdem%2C+L+M">Louis Mozart Kamdem</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+C">Caglar Demir</a>, 
<a href="/search/cs?searchtype=author&query=Ngonga%2C+A">Axel-Cyrille Ngonga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Clifford algebras are a natural generalization of the real numbers, the
complex numbers, and the quaternions. So far, solely Clifford algebras of the
form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been
studied in the context of knowledge graph embeddings. We propose to consider
nilpotent base vectors with a nilpotency index of two. In these spaces, denoted
$Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which
cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from
the absence of higher-order interactions between real and complex parts of
entity embeddings. We design two new models for the discovery of the parameters
$p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$,
and $r$. The second predicts $(p, q,r)$ based on an embedding of the input
knowledge graph computed using neural networks. The results of our evaluation
on seven benchmark datasets suggest that nilpotent vectors can help capture
embeddings better. Our comparison against the state of the art suggests that
our approach generalizes better than other approaches on all datasets w.r.t.
the MRR it achieves on validation data. We also show that a greedy search
suffices to discover values of $p$, $q$ and $r$ that are close to optimal.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04874" title="Abstract">arXiv:2402.04874</a> [<a href="/pdf/2402.04874" title="Download PDF">pdf</a>, <a href="/format/2402.04874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choosing a Classical Planner with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatter%2C+J">Jana Vatter</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>, 
<a href="/search/cs?searchtype=author&query=Samulowitz%2C+H">Horst Samulowitz</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+M">Michael Katz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Online planner selection is the task of choosing a solver out of a predefined
set for a given planning problem. As planning is computationally hard, the
performance of solvers varies greatly on planning problems. Thus, the ability
to predict their performance on a given problem is of great importance. While a
variety of learning methods have been employed, for classical cost-optimal
planning the prevailing approach uses Graph Neural Networks (GNNs). In this
work, we continue the line of work on using GNNs for online planner selection.
We perform a thorough investigation of the impact of the chosen GNN model,
graph representation and node features, as well as prediction task. Going
further, we propose using the graph representation obtained by a GNN as an
input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more
resource-efficient yet accurate approach. We show the effectiveness of a
variety of GNN-based online planner selection methods, opening up new exciting
avenues for research on online planner selection.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04875" title="Abstract">arXiv:2402.04875</a> [<a href="/pdf/2402.04875" title="Download PDF">pdf</a>, <a href="/format/2402.04875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Provable Length and Compositional Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Length generalization -- the ability to generalize to longer sequences than
ones seen during training, and compositional generalization -- the ability to
generalize to token combinations not seen during training, are crucial forms of
out-of-distribution generalization in sequence-to-sequence models. In this
work, we take the first steps towards provable length and compositional
generalization for a range of architectures, including deep sets, transformers,
state space models, and simple recurrent neural nets. Depending on the
architecture, we prove different degrees of representation identification,
e.g., a linear or a permutation relation with ground truth representation, is
necessary for length and compositional generalization.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04878" title="Abstract">arXiv:2402.04878</a> [<a href="/pdf/2402.04878" title="Download PDF">pdf</a>, <a href="/format/2402.04878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAR: Shape-focused Texture Agnostic Representations for Improved Object  Detection and 6D Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B6nig%2C+P">Peter H&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Thalhammer%2C+S">Stefan Thalhammer</a>, 
<a href="/search/cs?searchtype=author&query=Weibel%2C+J">Jean-Baptiste Weibel</a>, 
<a href="/search/cs?searchtype=author&query=Hirschmanner%2C+M">Matthias Hirschmanner</a>, 
<a href="/search/cs?searchtype=author&query=Vincze%2C+M">Markus Vincze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in machine learning have greatly benefited object detection
and 6D pose estimation for robotic grasping. However, textureless and metallic
objects still pose a significant challenge due to fewer visual cues and the
texture bias of CNNs. To address this issue, we propose a texture-agnostic
approach that focuses on learning from CAD models and emphasizes object shape
features. To achieve a focus on learning shape features, the textures are
randomized during the rendering of the training data. By treating the texture
as noise, the need for real-world object instances or their final appearance
during training data generation is eliminated. The TLESS and ITODD datasets,
specifically created for industrial settings in robotics and featuring
textureless and metallic objects, were used for evaluation. Texture agnosticity
also increases the robustness against image perturbations such as imaging
noise, motion blur, and brightness changes, which are common in robotics
applications. Code and datasets are publicly available at
github.com/hoenigpeter/randomized_texturing.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04879" title="Abstract">arXiv:2402.04879</a> [<a href="/pdf/2402.04879" title="Download PDF">pdf</a>, <a href="/format/2402.04879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Methods for Creating a National Random Sample of Twitter Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Meysam Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Zare%2C+D">Darya Zare</a>, 
<a href="/search/cs?searchtype=author&query=Samei%2C+Z">Zeynab Samei</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammadamin Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kubli%2C+M">Mael Kubli</a>, 
<a href="/search/cs?searchtype=author&query=Aliahmadi%2C+M">Mohammadhadi Aliahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+S">Sarvenaz Ebrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Gilardi%2C+F">Fabrizio Gilardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Twitter data has been widely used by researchers across various social and
computer science disciplines. A common aim when working with Twitter data is
the construction of a random sample of users from a given country. However,
while several methods have been proposed in the literature, their comparative
performance is mostly unexplored. In this paper, we implement four methods to
collect a random sample of Twitter users in the US: 1% Stream, Bounding Box,
Location Query, and Language Query. Then, we compare the methods according to
their tweet- and user-level metrics as well as their accuracy in estimating US
population with and without using inclusion probabilities of various
demographics. Our results show that the 1% Stream method performs differently
than others and best for the construction of a population representative
sample, though its statistical significance is questionable due to large
confidence intervals. We discuss the conditions under which the 1% Stream
method may not be suitable and suggest the Bounding Box method as the
second-best method to use.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04880" title="Abstract">arXiv:2402.04880</a> [<a href="/pdf/2402.04880" title="Download PDF">pdf</a>, <a href="/format/2402.04880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Cloud and Mobile Computing for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruiqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=work%2C+T+Z+c+e+t+t">Tianchi Zhang contributed equally to this work</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although the computing power of mobile devices is increasing, machine
learning models are also growing in size. This trend creates problems for
mobile devices due to limitations like their memory capacity and battery life.
While many services, like ChatGPT and Midjourney, run all the inferences in the
cloud, we believe a flexible and fine-grained task distribution is more
desirable. In this work, we consider model segmentation as a solution to
improving the user experience, dividing the computation between mobile devices
and the cloud in a way that offloads the compute-heavy portion of the model
while minimizing the data transfer required. We show that the division not only
reduces the wait time for users but can also be fine-tuned to optimize the
workloads of the cloud. To achieve that, we design a scheduler that collects
information about network quality, client device capability, and job
requirements, making decisions to achieve consistent performance across a range
of devices while reducing the work the cloud needs to perform.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04881" title="Abstract">arXiv:2402.04881</a> [<a href="/pdf/2402.04881" title="Download PDF">pdf</a>, <a href="/format/2402.04881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistral Network: Revolutionizing Media Curation and Consumption through  Decentralization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+D+S+S">Dipankar Sarkar.Shubham Upadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Blockchain technology has revolutionized media consumption and distribution
in the digital age, allowing creators, consumers, and regulators to participate
in a decentralized, fair, and engaging media environment. Epistral, an
innovative media network that leverages blockchain technology, aims to be the
world's first anti-mimetic media curation and consumption network, addressing
the core challenges facing today's digital media landscape: unfair treatment of
creators and manipulative consumer algorithms, and the complex task of
effective regulation. This paper delves into the conceptualization, design, and
potential impact of epistral and explores how it embodies McLuhan's and
Girard's theories within the realm of blockchain technology and draws from
Hayden's critique of democratic representation. The paper analyzes the
challenges and opportunities presented by this new network, providing a broader
discourse on the future of media consumption, distribution, and regulation.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04882" title="Abstract">arXiv:2402.04882</a> [<a href="/pdf/2402.04882" title="Download PDF">pdf</a>, <a href="/format/2402.04882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre  Memory Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zeyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+G">Gourav Datta</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anni Li</a>, 
<a href="/search/cs?searchtype=author&query=Beerel%2C+P+A">Peter Anthony Beerel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Transformer models have demonstrated high accuracy in numerous applications
but have high complexity and lack sequential processing capability making them
ill-suited for many streaming applications at the edge where devices are
heavily resource-constrained. Thus motivated, many researchers have proposed
reformulating the transformer models as RNN modules which modify the
self-attention computation with explicit states. However, these approaches
often incur significant performance degradation. The ultimate goal is to
develop a model that has the following properties: parallel training, streaming
and low-cost inference, and SOTA performance. In this paper, we propose a new
direction to achieve this goal. We show how architectural modifications to a
recurrent model can help push its performance toward Transformer models while
retaining its sequential processing capability. Specifically, inspired by the
recent success of Legendre Memory Units (LMU) in sequence learning tasks, we
propose LMUFormer, which augments the LMU with convolutional patch embedding
and convolutional channel mixer. Moreover, we present a spiking version of this
architecture, which introduces the benefit of states within the patch embedding
and channel mixer modules while simultaneously reducing the computing
complexity. We evaluated our architectures on multiple sequence datasets. In
comparison to SOTA transformer-based models within the ANN domain on the SCv2
dataset, our LMUFormer demonstrates comparable performance while necessitating
a remarkable 53 times reduction in parameters and a substantial 65 times
decrement in FLOPs. Additionally, owing to our model's proficiency in real-time
data processing, we can achieve a 32.03% reduction in sequence length, all
while incurring an inconsequential decline in performance. Our code is publicly
available at https://github.com/zeyuliu1037/LMUFormer.git.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04883" title="Abstract">arXiv:2402.04883</a> [<a href="/pdf/2402.04883" title="Download PDF">pdf</a>, <a href="/format/2402.04883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Accurate Camera-based 3D Object Detection via Cascade Depth  Estimation and Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoqun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yiran Qin</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zijian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Ningning Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent camera-based 3D object detection is limited by the precision of
transforming from image to 3D feature spaces, as well as the accuracy of object
localization within the 3D space. This paper aims to address such a fundamental
problem of camera-based 3D object detection: How to effectively learn depth
information for accurate feature lifting and object localization. Different
from previous methods which directly predict depth distributions by using a
supervised estimation model, we propose a cascade framework consisting of two
depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages
relative depth information to realize the effective feature lifting from 2D to
3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth
reconstruction to further adjust the 3D object localization perturbation along
the depth axis. In practice, the DE is explicitly realized by using both the
absolute and relative depth optimization loss to promote the precision of depth
prediction, while the capability of DC is implicitly embedded into the
detection Transformer through a depth denoising mechanism in the training
phase. The entire model training is accomplished through an end-to-end manner.
We propose a baseline detector and evaluate the effectiveness of our proposal
with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a
comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct
extensive experiments to demonstrate its generality based on various detectors
with about +2% NDS improvements.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04884" title="Abstract">arXiv:2402.04884</a> [<a href="/pdf/2402.04884" title="Download PDF">pdf</a>, <a href="/format/2402.04884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological relations in water quality monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Figueiredo%2C+B+C">Bruno Chaves Figueiredo</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+M+A">Maria Alexandra Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J+N">Jo&#xe3;o Nuno Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">The Alqueva Multi-Purpose Project (EFMA) is a massive abduction and storage
infrastructure system in the Alentejo, which has a water quality monitoring
network with almost thousands of water quality stations distributed across
three subsystems: Alqueva, Pedrog\~ao, and Ardila. Identification of pollution
sources in complex infrastructure systems, such as the EFMA, requires
recognition of water flow direction and delimitation of areas being drained to
specific sampling points. The transfer channels in the EFMA infrastructure
artificially connect several water bodies that do not share drainage basins,
which further complicates the interpretation of water quality data because the
water does not flow exclusively downstream and is not restricted to specific
basins.
<br />The existing user-friendly GIS tools do not facilitate the exploration and
visualisation of water quality data in spatial-temporal dimensions, such as
defining temporal relationships between monitoring campaigns, nor do they allow
the establishment of topological and hydrological relationships between
different sampling points.
<br />This thesis work proposes a framework capable of aggregating many types of
information in a GIS environment, visualising large water quality-related
datasets and, a graph data model to integrate and relate water quality between
monitoring stations and land use. The graph model allows to exploit the
relationship between water quality in a watercourse and reservoirs associated
with infrastructures.
<br />The graph data model and the developed framework demonstrated encouraging
results and has proven to be preferred when compared to relational databases.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04888" title="Abstract">arXiv:2402.04888</a> [<a href="/pdf/2402.04888" title="Download PDF">pdf</a>, <a href="/format/2402.04888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barahimi%2C+B">Borna Barahimi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Hakam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Tabassum%2C+H">Hina Tabassum</a>, 
<a href="/search/cs?searchtype=author&query=Waqar%2C+O">Omer Waqar</a>, 
<a href="/search/cs?searchtype=author&query=Omer%2C+M">Mohammad Omer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by IEEE International Conference on Communications (ICC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere
communication devices to sensing instruments, leveraging Channel State
Information (CSI) extraction capabilities. Nevertheless, resource-constrained
IoT devices and the intricacies of deep neural networks necessitate
transmitting CSI to cloud servers for sensing. Although feasible, this leads to
considerable communication overhead. In this context, this paper develops a
novel Real-time Sensing and Compression Network (RSCNet) which enables sensing
with compressed CSI; thereby reducing the communication overheads. RSCNet
facilitates optimization across CSI windows composed of a few CSI frames. Once
transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to
harness data from prior windows, thus bolstering both the sensing accuracy and
CSI reconstruction. RSCNet adeptly balances the trade-off between CSI
compression and sensing precision, thus streamlining real-time cloud-based WiFi
sensing with reduced communication costs. Numerical findings demonstrate the
gains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensing
accuracy of 97.4% with minimal CSI reconstruction error. Numerical results also
show a computational analysis of the proposed RSCNet as a function of the
number of CSI frames.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04889" title="Abstract">arXiv:2402.04889</a> [<a href="/pdf/2402.04889" title="Download PDF">pdf</a>, <a href="/format/2402.04889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Generated Native Ads in Conversational Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+S">Sebastian Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Zelch%2C+I">Ines Zelch</a>, 
<a href="/search/cs?searchtype=author&query=Bevendorff%2C+J">Janek Bevendorff</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+B">Benno Stein</a>, 
<a href="/search/cs?searchtype=author&query=Hagen%2C+M">Matthias Hagen</a>, 
<a href="/search/cs?searchtype=author&query=Potthast%2C+M">Martin Potthast</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to WWW'24 Short Papers Track; 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Conversational search engines such as YouChat and Microsoft Copilot use large
language models (LLMs) to generate answers to queries. It is only a small step
to also use this technology to generate and integrate advertising within these
answers - instead of placing ads separately from the organic search results.
This type of advertising is reminiscent of native advertising and product
placement, both of which are very effective forms of subtle and manipulative
advertising. It is likely that information seekers will be confronted with such
use of LLM technology in the near future, especially when considering the high
computational costs associated with LLMs, for which providers need to develop
sustainable business models. This paper investigates whether LLMs can also be
used as a countermeasure against generated native ads, i.e., to block them. For
this purpose we compile a large dataset of ad-prone queries and of generated
answers with automatically integrated ads to experiment with fine-tuned
sentence transformers and state-of-the-art LLMs on the task of recognizing the
ads. In our experiments sentence transformers achieve detection precision and
recall values above 0.9, while the investigated LLMs struggle with the task.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04890" title="Abstract">arXiv:2402.04890</a> [<a href="/pdf/2402.04890" title="Download PDF">pdf</a>, <a href="/ps/2402.04890" title="Download PostScript">ps</a>, <a href="/format/2402.04890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marker+Codeword+Marker: A Coding Structure for Segmented  single-indel/single-edit Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuan He</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaohu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, submitted to 2024 IEEE International Symposium on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">An indel refers to a deletion or an insertion, and an edit refers to an indel
or a substitution. In this paper, we consider the segmented single-indel (resp.
single-edit) channel, where the channel's input bit stream is partitioned into
segments of length $n$ and each segment can suffer from at most a single indel
(resp. edit) error. The value of $n$ is known to the receiver but the
boundaries of segments are not. We propose to encode each segment following a
marker+codeword+marker structure, where the two markers are carefully selected
and the codewords are chosen from Varshamov-Tenegolts (VT) codes. In this way,
we are able to construct a new class of binary codes that can correct segmented
single-indel errors. Our codes have the lowest redundancy of $\log_2(n-6)+7$
bits and are the first one which has linear time encoder/decoder in the
literature. Moreover, by enhancing the VT codes and one of the markers, we are
able to construct the first class of binary codes that can correct segmented
single-edit errors. This class of codes has redundancy $\log_2(n-9)+10$ bits
and has linear time encoder/decoder.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04891" title="Abstract">arXiv:2402.04891</a> [<a href="/pdf/2402.04891" title="Download PDF">pdf</a>, <a href="/format/2402.04891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource  management in multi-user video transcoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costero%2C+L">Luis Costero</a>, 
<a href="/search/cs?searchtype=author&query=Igual%2C+F+D">Francisco D. Igual</a>, 
<a href="/search/cs?searchtype=author&query=Olcoz%2C+K">Katzalin Olcoz</a>, 
<a href="/search/cs?searchtype=author&query=Tirado%2C+F">Francisco Tirado</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Supercomputing 76, pp. 9388 to 9403 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The coexistence of parallel applications in shared computing nodes, each one
featuring different Quality of Service (QoS) requirements, carries out new
challenges to improve resource occupation while keeping acceptable rates in
terms of QoS. As more application-specific and system-wide metrics are included
as QoS dimensions, or under situations in which resource-usage limits are
strict, building and serving the most appropriate set of actions (application
control knobs and system resource assignment) to concurrent applications in an
automatic and optimal fashion becomes mandatory. In this paper, we propose
strategies to build and serve this type of knowledge to concurrent applications
by leveraging Reinforcement Learning techniques. Taking multi-user video
transcoding as a driving example, our experimental results reveal an excellent
adaptation of resource and knob management to heterogeneous QoS requests, and
increases in the amount of concurrently served users up to 1.24x compared with
alternative approaches considering homogeneous QoS requests.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04892" title="Abstract">arXiv:2402.04892</a> [<a href="/pdf/2402.04892" title="Download PDF">pdf</a>, <a href="/format/2402.04892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Probabilistic Verification of AI Systems via  Weighted Model Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morettin%2C+P">Paolo Morettin</a>, 
<a href="/search/cs?searchtype=author&query=Passerini%2C+A">Andrea Passerini</a>, 
<a href="/search/cs?searchtype=author&query=Sebastiani%2C+R">Roberto Sebastiani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The probabilistic formal verification (PFV) of AI systems is in its infancy.
So far, approaches have been limited to ad-hoc algorithms for specific classes
of models and/or properties.
<br />We propose a unifying framework for the PFV of AI systems based onWeighted
Model Integration (WMI), which allows to frame the problem in very general
terms.
<br />Crucially, this reduction enables the verification of many properties of
interest, like fairness, robustness or monotonicity, over a wide range of
machine learning models, without making strong distributional assumptions.
<br />We support the generality of the approach by solving multiple verification
tasks with a single, off-the-shelf WMI solver, then discuss the scalability
challenges and research directions related to this promising framework.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04893" title="Abstract">arXiv:2402.04893</a> [<a href="/pdf/2402.04893" title="Download PDF">pdf</a>, <a href="/format/2402.04893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Category of Iterative Sets in Homotopy Type Theory and Univalent  Foundations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gratzer%2C+D">Daniel Gratzer</a>, 
<a href="/search/cs?searchtype=author&query=Gylterud%2C+H">H&#xe5;kon Gylterud</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6rtberg%2C+A">Anders M&#xf6;rtberg</a>, 
<a href="/search/cs?searchtype=author&query=Stenholm%2C+E">Elisabeth Stenholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">When working in Homotopy Type Theory and Univalent Foundations, the
traditional role of the category of sets, Set, is replaced by the category hSet
of homotopy sets (h-sets); types with h-propositional identity types. Many of
the properties of Set hold for hSet ((co)completeness, exactness, local
cartesian closure, etc.). Notably, however, the univalence axiom implies that
Ob(hSet) is not itself an h-set, but an h-groupoid. This is expected in
univalent foundations, but it is sometimes useful to also have a stricter
universe of sets, for example when constructing internal models of type theory.
In this work, we equip the type of iterative sets V0, due to Gylterud (2018) as
a refinement of the pioneering work of Aczel (1978) on universes of sets in
type theory, with the structure of a Tarski universe and show that it satisfies
many of the good properties of h-sets. In particular, we organize V0 into a
(non-univalent strict) category and prove that it is locally cartesian closed.
This enables us to organize it into a category with families with the structure
necessary to model extensional type theory internally in HoTT/UF. We do this in
a rather minimal univalent type theory with W-types, in particular we do not
rely on any HITs, or other complex extensions of type theory. Furthermore, the
construction of V0 and the model is fully constructive and predicative, while
still being very convenient to work with as the decoding from V0 into h-sets
commutes definitionally for all type constructors. Almost all of the paper has
been formalized in Agda using the agda-unimath library of univalent
mathematics.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04894" title="Abstract">arXiv:2402.04894</a> [<a href="/pdf/2402.04894" title="Download PDF">pdf</a>, <a href="/format/2402.04894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative  Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vashisth%2C+A">Apoorva Vashisth</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCckin%2C+J">Julius R&#xfc;ckin</a>, 
<a href="/search/cs?searchtype=author&query=Magistri%2C+F">Federico Magistri</a>, 
<a href="/search/cs?searchtype=author&query=Stachniss%2C+C">Cyrill Stachniss</a>, 
<a href="/search/cs?searchtype=author&query=Popovi%C4%87%2C+M">Marija Popovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Autonomous robots are often employed for data collection due to their
efficiency and low labour costs. A key task in robotic data acquisition is
planning paths through an initially unknown environment to collect observations
given platform-specific resource constraints, such as limited battery life.
Adaptive online path planning in 3D environments is challenging due to the
large set of valid actions and the presence of unknown occlusions. To address
these issues, we propose a novel deep reinforcement learning approach for
adaptively replanning robot paths to map targets of interest in unknown 3D
environments. A key aspect of our approach is a dynamically constructed graph
that restricts planning actions local to the robot, allowing us to quickly
react to newly discovered obstacles and targets of interest. For replanning, we
propose a new reward function that balances between exploring the unknown
environment and exploiting online-collected data about the targets of interest.
Our experiments show that our method enables more efficient target detection
compared to state-of-the-art learning and non-learning baselines. We also show
the applicability of our approach for orchard monitoring using an unmanned
aerial vehicle in a photorealistic simulator.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04896" title="Abstract">arXiv:2402.04896</a> [<a href="/pdf/2402.04896" title="Download PDF">pdf</a>, <a href="/format/2402.04896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from the Best: Active Learning for Wireless Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soltani%2C+N">Nasim Soltani</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+B">Batool Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Debashri Roy</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+R">Robert Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+K">Kaushik Chowdhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Collecting an over-the-air wireless communications training dataset for deep
learning-based communication tasks is relatively simple. However, labeling the
dataset requires expert involvement and domain knowledge, may involve private
intellectual properties, and is often computationally and financially
expensive. Active learning is an emerging area of research in machine learning
that aims to reduce the labeling overhead without accuracy degradation. Active
learning algorithms identify the most critical and informative samples in an
unlabeled dataset and label only those samples, instead of the complete set. In
this paper, we introduce active learning for deep learning applications in
wireless communications, and present its different categories. We present a
case study of deep learning-based mmWave beam selection, where labeling is
performed by a compute-intensive algorithm based on exhaustive search. We
evaluate the performance of different active learning algorithms on a publicly
available multi-modal dataset with different modalities including image and
LiDAR. Our results show that using an active learning algorithm for
class-imbalanced datasets can reduce labeling overhead by up to 50% for this
dataset while maintaining the same accuracy as classical training.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04897" title="Abstract">arXiv:2402.04897</a> [<a href="/pdf/2402.04897" title="Download PDF">pdf</a>, <a href="/format/2402.04897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benefits and Limitations of Web3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Connors%2C+C">Collin Connors</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dilip Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Web3 provides users and service providers several benefits not found in Web2.
However, despite the benefits provided, Web3 faces several obstacles that
prevent the paradigm from gaining widespread adoption. Developers should
understand the benefits and limitations of the technology in order to create
more accessible Web3 smart applications.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04898" title="Abstract">arXiv:2402.04898</a> [<a href="/pdf/2402.04898" title="Download PDF">pdf</a>, <a href="/ps/2402.04898" title="Download PostScript">ps</a>, <a href="/format/2402.04898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Strain of Success: A Predictive Model for Injury Risk Mitigation and  Team Success in Soccer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Everett%2C+G">Gregory Everett</a>, 
<a href="/search/cs?searchtype=author&query=Beal%2C+R">Ryan Beal</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+T">Tim Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+T+J">Timothy J. Norman</a>, 
<a href="/search/cs?searchtype=author&query=Ramchurn%2C+S+D">Sarvapali D. Ramchurn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages (16 main, 2 references, 1 appendix), 10 figures (9 main, 1 appendix). Accepted at the MIT Sloan Sports Analytics Conference 2024 Research Paper Competition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present a novel sequential team selection model in soccer.
Specifically, we model the stochastic process of player injury and
unavailability using player-specific information learned from real-world soccer
data. Monte-Carlo Tree Search is used to select teams for games that optimise
long-term team performance across a soccer season by reasoning over player
injury probability. We validate our approach compared to benchmark solutions
for the 2018/19 English Premier League season. Our model achieves similar
season expected points to the benchmark whilst reducing first-team injuries by
~13% and the money inefficiently spent on injured players by ~11% -
demonstrating the potential to reduce costs and improve player welfare in
real-world soccer teams.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04901" title="Abstract">arXiv:2402.04901</a> [<a href="/pdf/2402.04901" title="Download PDF">pdf</a>, <a href="/format/2402.04901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research on Mobile Network High-precision Absolute Time Synchronization  based on TAP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xiangming Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Longdan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhaoming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengying Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">With the development of mobile communication and industrial internet
technologies, the demand for robust absolute time synchronization based on
network for diverse scenarios is significantly growing. TAP is a novel network
timing method that aims to achieve sub-microsecond synchronization over air
interface. This paper investigates the improvement and end-to-end realization
of TAP. This paper first analyzes the effectiveness and deficiencies of TAP by
establishing an equivalent clock model which evaluates TAP from timing error
composition and allan variance. Second, this paper proposes a detailed base
station and terminal design and the corresponding improvement of TAP. Both
hardware compensation and protocol software design are taken into account so as
to minimize timing error and system cost while maximizing compatibility with
3GPP. Finally, this paper presents a TAP end-to-end 5G prototype system
developed based on software defined radio base station and COTS baseband
module. The field test results show that the proposed scheme effectively solves
the problems of TAP in application and robustly achieves 200ns level timing
accuracy in various situations. The average accuracy with long observations can
reach 1 nanosecond. It is 2$\sim$3 orders of magnitude better than common
network timing methods, including NTP, PTP and the original TAP.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04902" title="Abstract">arXiv:2402.04902</a> [<a href="/pdf/2402.04902" title="Download PDF">pdf</a>, <a href="/format/2402.04902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L4Q: Parameter Efficient Quantization-Aware Training on Large Language  Models via LoRA-wise LSQ
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyesung Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yulhwa Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jae-joon Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Post-training quantization (PTQ) and quantization-aware training (QAT)
methods are gaining popularity in mitigating the high memory and computational
costs associated with Large Language Models (LLMs). In resource-constrained
scenarios, PTQ, with its reduced training overhead, is often preferred over
QAT, despite the latter's potential for higher accuracy. Meanwhile,
parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA)
have been introduced, and recent efforts have explored quantization-aware PEFT
techniques. However, these approaches may lack generality due to their reliance
on the pre-quantized model's configuration. Their effectiveness may be
compromised by non-linearly quantized or mixed-precision weights, and the
retraining of specific quantization parameters might impede optimal
performance. To address these challenges, we propose L4Q, an algorithm for
parameter-efficient quantization-aware training. L4Q leverages LoRA-wise
learned quantization step size for LLMs, aiming to enhance generality. The
simultaneous quantization-and-fine-tuning process of L4Q is applicable to
high-precision models, yielding linearly quantized weights with superior
accuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families
using an instructional dataset, showcase L4Q's capabilities in language
comprehension and few-shot in-context learning, achieving sub-4-bit precision
while maintaining comparable training times to applying PEFT on a quantized
model.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04906" title="Abstract">arXiv:2402.04906</a> [<a href="/pdf/2402.04906" title="Download PDF">pdf</a>, <a href="/format/2402.04906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Monte Carlo Meta-learners for Predictive Inference of  Individual Treatment Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jonkers%2C+J">Jef Jonkers</a>, 
<a href="/search/cs?searchtype=author&query=Verhaeghe%2C+J">Jarne Verhaeghe</a>, 
<a href="/search/cs?searchtype=author&query=Van+Wallendael%2C+G">Glenn Van Wallendael</a>, 
<a href="/search/cs?searchtype=author&query=Duchateau%2C+L">Luc Duchateau</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hoecke%2C+S">Sofie Van Hoecke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Knowledge of the effect of interventions, called the treatment effect, is
paramount for decision-making. Approaches to estimating this treatment effect,
e.g. by using Conditional Average Treatment Effect (CATE) estimators, often
only provide a point estimate of this treatment effect, while additional
uncertainty quantification is frequently desired instead. Therefore, we present
a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging
conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to
instead produce a predictive distribution usable in individualized
decision-making. Furthermore, we show how specific assumptions on the noise
distribution of the outcome heavily affect these uncertainty predictions.
Nonetheless, the CMC framework shows strong experimental coverage while
retaining small interval widths to provide estimates of the true individual
treatment effect.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04909" title="Abstract">arXiv:2402.04909</a> [<a href="/pdf/2402.04909" title="Download PDF">pdf</a>, <a href="/format/2402.04909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entanglement Definitions for Tethered Robots: Exploration and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Battocletti%2C+G">Gianpietro Battocletti</a>, 
<a href="/search/cs?searchtype=author&query=Boskos%2C+D">Dimitris Boskos</a>, 
<a href="/search/cs?searchtype=author&query=Toli%C4%87%2C+D">Domagoj Toli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Palunko%2C+I">Ivana Palunko</a>, 
<a href="/search/cs?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this article we consider the problem of tether entanglement for tethered
robots. In many applications, such as maintenance of underwater structures,
aerial inspection, and underground exploration, tethered robots are often used
in place of standalone (i.e., untethered) ones. However, the presence of a
tether also introduces the risk for it to get entangled with obstacles present
in the environment or with itself. To avoid these situations, a
non-entanglement constraint can be considered in the motion planning problem
for tethered robots. This constraint can be expressed either as a set of
specific tether configurations that must be avoided, or as a quantitative
measure of a `level of entanglement' that can be minimized. However, the
literature lacks a generally accepted definition of entanglement, with existing
definitions being limited and partial. Namely, the existing entanglement
definitions either require a taut tether to come into contact with an obstacle
or with another tether, or they require for the tether to do a full loop around
an obstacle. In practice, this means that the existing definitions do not
effectively cover all instances of tether entanglement. Our goal in this
article is to bridge this gap and provide new definitions of entanglement,
which, together with the existing ones, can be effectively used to qualify the
entanglement state of a tethered robot in diverse situations. The new
definitions find application mainly in motion planning for tethered robot
systems, where they can be used to obtain more safe and robust
entanglement-free trajectories. The present article focuses exclusively on the
presentation and analysis of the entanglement definitions. The application of
the definitions to the motion planning problem is left for future work.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04910" title="Abstract">arXiv:2402.04910</a> [<a href="/pdf/2402.04910" title="Download PDF">pdf</a>, <a href="/ps/2402.04910" title="Download PostScript">ps</a>, <a href="/format/2402.04910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring responsible applications of Synthetic Data to advance Online  Safety Research and Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johansson%2C+P">Pica Johansson</a>, 
<a href="/search/cs?searchtype=author&query=Bright%2C+J">Jonathan Bright</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+S">Shyam Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+C">Claudia Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Leslie%2C+D">David Leslie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The use of synthetic data provides an opportunity to accelerate online safety
research and development efforts while showing potential for bias mitigation,
facilitating data storage and sharing, preserving privacy and reducing exposure
to harmful content. However, the responsible use of synthetic data requires
caution regarding anticipated risks and challenges. This short report explores
the potential applications of synthetic data to the domain of online safety,
and addresses the ethical challenges that effective use of the technology may
present.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04911" title="Abstract">arXiv:2402.04911</a> [<a href="/pdf/2402.04911" title="Download PDF">pdf</a>, <a href="/ps/2402.04911" title="Download PostScript">ps</a>, <a href="/format/2402.04911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Values Do ImageNet-trained Classifiers Enact?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Penman%2C+W">Will Penman</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+J">Joshua Babu</a>, 
<a href="/search/cs?searchtype=author&query=Raghunathan%2C+A">Abhinaya Raghunathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to FAT [FAccT] 2020, 12 pages, 4 figures, 3 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">We identify "values" as actions that classifiers take that speak to open
questions of significant social concern. Investigating a classifier's values
builds on studies of social bias that uncover how classifiers participate in
social processes beyond their creators' forethought. In our case, this
participation involves what counts as nutritious, what it means to be modest,
and more. Unlike AI social bias, however, a classifier's values are not
necessarily morally loathsome. Attending to image classifiers' values can
facilitate public debate and introspection about the future of society. To
substantiate these claims, we report on an extensive examination of both
ImageNet training/validation data and ImageNet-trained classifiers with custom
testing data. We identify perceptual decision boundaries in 118 categories that
address open questions in society, and through quantitative testing of rival
datasets we find that ImageNet-trained classifiers enact at least 7 values
through their perceptual decisions. To contextualize these results, we develop
a conceptual framework that integrates values, social bias, and accuracy, and
we describe a rhetorical method for identifying how context affects the values
that a classifier enacts. We also discover that classifier performance does not
straightforwardly reflect the proportions of subgroups in a training set. Our
findings bring a rich sense of the social world to ML researchers that can be
applied to other domains beyond computer vision.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04912" title="Abstract">arXiv:2402.04912</a> [<a href="/pdf/2402.04912" title="Download PDF">pdf</a>, <a href="/format/2402.04912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Biologically Plausible and Private Gene Expression Data  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dingfan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Oestreich%2C+M">Marie Oestreich</a>, 
<a href="/search/cs?searchtype=author&query=Afonja%2C+T">Tejumade Afonja</a>, 
<a href="/search/cs?searchtype=author&query=Kerkouche%2C+R">Raouf Kerkouche</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+M">Matthias Becker</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings on Privacy Enhancing Technologies (PoPETs 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative models trained with Differential Privacy (DP) are becoming
increasingly prominent in the creation of synthetic data for downstream
applications. Existing literature, however, primarily focuses on basic
benchmarking datasets and tends to report promising results only for elementary
metrics and relatively simple data distributions. In this paper, we initiate a
systematic analysis of how DP generative models perform in their natural
application scenarios, specifically focusing on real-world gene expression
data. We conduct a comprehensive analysis of five representative DP generation
methods, examining them from various angles, such as downstream utility,
statistical properties, and biological plausibility. Our extensive evaluation
illuminates the unique characteristics of each DP generation method, offering
critical insights into the strengths and weaknesses of each approach, and
uncovering intriguing possibilities for future developments. Perhaps
surprisingly, our analysis reveals that most methods are capable of achieving
seemingly reasonable downstream utility, according to the standard evaluation
metrics considered in existing literature. Nevertheless, we find that none of
the DP methods are able to accurately capture the biological characteristics of
the real dataset. This observation suggests a potential over-optimistic
assessment of current methodologies in this field and underscores a pressing
need for future enhancements in model design.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04913" title="Abstract">arXiv:2402.04913</a> [<a href="/pdf/2402.04913" title="Download PDF">pdf</a>, <a href="/format/2402.04913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Beam Training for Near-Field Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hammadi%2C+A+A">Ahmed Al Hammadi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In millimeter-wave communications, large-scale antenna arrays are commonly
employed to mitigate obstacle occlusion and path loss. However, these
large-scale arrays generate pencil-shaped beams, which necessitate a higher
number of training beams to cover the desired space. This results in the heavy
beam training overhead. Furthermore, as the antenna aperture increases, users
are more likely to be situated in the near-field region of the base station
(BS) antenna array. This motivates our investigation into the beam training
problem in the near-field region to achieve efficient beam alignment. To
address the high complexity and low identification accuracy of existing beam
training techniques, we propose an efficient hashing multi-arm beam (HMB)
training scheme for the near-field scenario. Specifically, we first design a
set of sparse bases based on the polar domain sparsity of the near-field
channel and construct a near-field single-beam training codebook. Then, the
hash functions are chosen to construct the near-field multi-arm beam training
codebook. Each multi-arm beam training codeword is used in a time slot until
the predefined codebook is traversed. Finally, the soft decision and voting
methods are applied to distinguish the signal from different BS and obtain the
correctly aligned beams. In addition, we provide the logically rigorous proof
of computational complexity. Simulation results show that our proposed
near-field HMB training method can achieve 96.4% identification accuracy of the
exhaustive beam training method and greatly reduce the training overhead to the
logarithmic level. Furthermore, we verify its applicability under the far-field
scenario as well.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04914" title="Abstract">arXiv:2402.04914</a> [<a href="/pdf/2402.04914" title="Download PDF">pdf</a>, <a href="/format/2402.04914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Text Generation with Fine-Grained Linguistic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhafni%2C+B">Bashar Alhafni</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+V">Vivek Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As the text generation capabilities of large language models become
increasingly prominent, recent studies have focused on controlling particular
aspects of the generated text to make it more personalized. However, most
research on controllable text generation focuses on controlling the content or
modeling specific high-level/coarse-grained attributes that reflect authors'
writing styles, such as formality, domain, or sentiment. In this paper, we
focus on controlling fine-grained attributes spanning multiple linguistic
dimensions, such as lexical and syntactic attributes. We introduce a novel
benchmark to train generative models and evaluate their ability to generate
personalized text based on multiple fine-grained linguistic attributes. We
systematically investigate the performance of various large language models on
our benchmark and draw insights from the factors that impact their performance.
We make our code, data, and pretrained models publicly available.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04915" title="Abstract">arXiv:2402.04915</a> [<a href="/pdf/2402.04915" title="Download PDF">pdf</a>, <a href="/format/2402.04915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moco: A Learnable Meta Optimizer for Combinatorial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dernedde%2C+T">Tim Dernedde</a>, 
<a href="/search/cs?searchtype=author&query=Thyssens%2C+D">Daniela Thyssens</a>, 
<a href="/search/cs?searchtype=author&query=Dittrich%2C+S">S&#xf6;ren Dittrich</a>, 
<a href="/search/cs?searchtype=author&query=Stubbemann%2C+M">Maximilan Stubbemann</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt-Thieme%2C+L">Lars Schmidt-Thieme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Relevant combinatorial optimization problems (COPs) are often NP-hard. While
they have been tackled mainly via handcrafted heuristics in the past, advances
in neural networks have motivated the development of general methods to learn
heuristics from data. Many approaches utilize a neural network to directly
construct a solution, but are limited in further improving based on already
constructed solutions at inference time. Our approach, Moco, learns a graph
neural network that updates the solution construction procedure based on
features extracted from the current search state. This meta training procedure
targets the overall best solution found during the search procedure given
information such as the search budget. This allows Moco to adapt to varying
circumstances such as different computational budgets. Moco is a fully
learnable meta optimizer that does not utilize any problem specific local
search or decomposition. We test Moco on the Traveling Salesman Problem (TSP)
and Maximum Independent Set (MIS) and show that it outperforms other approaches
on MIS and is overall competitive on the TSP, especially outperforming related
approaches, partially even if they use additional local search.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04916" title="Abstract">arXiv:2402.04916</a> [<a href="/pdf/2402.04916" title="Download PDF">pdf</a>, <a href="/format/2402.04916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple inexpensive vertex and edge invariants distinguishing dataset  strongly regular graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duda%2C+J">Jarek Duda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">While standard Weisfeiler-Leman vertex labels are not able to distinguish
even vertices of regular graphs, there is proposed and tested family of
inexpensive polynomial time vertex and edge invariants, distinguishing much
more difficult SRGs (strongly regular graphs), also often their vertices. Among
43717 SRGs from dataset by Edward Spence, proposed vertex invariants alone were
able to distinguish all but 4 pairs of graphs, which were easily distinguished
by further application of proposed edge invariants. Specifically, proposed
vertex invariants are traces or sorted diagonals of $(A|_{N_a})^p$ adjacency
matrix $A$ restricted to $N_a$ neighborhood of vertex $a$, already for $p=3$
distinguishing all SRGs from 6 out of 13 sets in this dataset, 8 if adding
$p=4$. Proposed edge invariants are analogously traces or diagonals of powers
of $\bar{A}_{ab,cd}=A_{ab} A_{ac} A_{bd}$, nonzero for $(a,b)$ being edges. As
SRGs are considered the most difficult cases for graph isomorphism problem,
such algebraic-combinatorial invariants bring hope that this problem is
polynomial time.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04918" title="Abstract">arXiv:2402.04918</a> [<a href="/pdf/2402.04918" title="Download PDF">pdf</a>, <a href="/format/2402.04918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Implicit Discourse Relation Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yung%2C+F">Frances Yung</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+M">Mansoor Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Scholman%2C+M">Merel Scholman</a>, 
<a href="/search/cs?searchtype=author&query=Demberg%2C+V">Vera Demberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the Linguistic Annotation Workshop 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained large language models, such as ChatGPT, archive outstanding
performance in various reasoning tasks without supervised training and were
found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's
performance in the task of implicit discourse relation classification, prompted
by a standard multiple-choice question, is still far from satisfactory and
considerably inferior to state-of-the-art supervised approaches. This work
investigates several proven prompting techniques to improve ChatGPT's
recognition of discourse relations. In particular, we experimented with
breaking down the classification task that involves numerous abstract labels
into smaller subtasks. Nonetheless, experiment results show that the inference
accuracy hardly changes even with sophisticated prompt engineering, suggesting
that implicit discourse relation classification is not yet resolvable under
zero-shot or few-shot settings.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04924" title="Abstract">arXiv:2402.04924</a> [<a href="/pdf/2402.04924" title="Download PDF">pdf</a>, <a href="/format/2402.04924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Beining Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An effective method for graph condensation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Training on large-scale graphs has achieved remarkable results in graph
representation learning, but its cost and storage have raised growing concerns.
As one of the most promising directions, graph condensation methods address
these issues by employing gradient matching, aiming to condense the full graph
into a more concise yet information-rich synthetic set. Though encouraging,
these strategies primarily emphasize matching directions of the gradients,
which leads to deviations in the training trajectories. Such deviations are
further magnified by the differences between the condensation and evaluation
phases, culminating in accumulated errors, which detrimentally affect the
performance of the condensed graphs. In light of this, we propose a novel graph
condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L}
trajectory (\textbf{CTRL}), which offers an optimized starting point closer to
the original dataset's feature distribution and a more refined strategy for
gradient matching. Theoretically, CTRL can effectively neutralize the impact of
accumulated errors on the performance of condensed graphs. We provide extensive
experiments on various graph datasets and downstream tasks to support the
effectiveness of CTRL. Code is released at
https://github.com/NUS-HPC-AI-Lab/CTRL.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04925" title="Abstract">arXiv:2402.04925</a> [<a href="/pdf/2402.04925" title="Download PDF">pdf</a>, <a href="/format/2402.04925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TP-Aware Dequantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoque%2C+A">Adnan Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Srivatsa%2C+M">Mudhakar Srivatsa</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chih-Chieh Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ganti%2C+R">Raghu Ganti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present a novel method that reduces model inference latency
during distributed deployment of Large Language Models (LLMs). Our contribution
is an optimized inference deployment scheme that address the current
limitations of state-of-the-art quantization kernels when used in conjunction
with Tensor Parallel (TP). Our method preserves data locality in GPU memory
access patterns and exploits a priori knowledge of TP to reduce global
communication. We demonstrate an up to 1.81x speedup over existing methods for
Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer
problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04929" title="Abstract">arXiv:2402.04929</a> [<a href="/pdf/2402.04929" title="Download PDF">pdf</a>, <a href="/format/2402.04929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source-Free Domain Adaptation with Diffusion-Guided Source Data  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Aynaou%2C+H">Houda Aynaou</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.01701">arXiv:2310.01701</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel approach to leverage the generalizability
capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our
proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image
diffusion model to generate source domain images using features from the target
images to guide the diffusion process. Specifically, the pre-trained diffusion
model is fine-tuned to generate source samples that minimize entropy and
maximize confidence for the pre-trained source model. We then apply established
unsupervised domain adaptation techniques to align the generated source images
with target domain data. We validate our approach through comprehensive
experiments across a range of datasets, including Office-31, Office-Home, and
VisDA. The results highlight significant improvements in SFDA performance,
showcasing the potential of diffusion models in generating contextually
relevant, domain-specific images.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04930" title="Abstract">arXiv:2402.04930</a> [<a href="/pdf/2402.04930" title="Download PDF">pdf</a>, <a href="/format/2402.04930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blue noise for diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingchang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sala%C3%BCn%2C+C">Corentin Sala&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Vasconcelos%2C+C">Cristina Vasconcelos</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96ztireli%2C+C">Cengiz &#xd6;ztireli</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gurprit Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Most of the existing diffusion models use Gaussian noise for training and
sampling across all time steps, which may not optimally account for the
frequency contents reconstructed by the denoising network. Despite the diverse
applications of correlated noise in computer graphics, its potential for
improving the training process has been underexplored. In this paper, we
introduce a novel and general class of diffusion models taking correlated noise
within and across images into account. More specifically, we propose a
time-varying noise model to incorporate correlated noise into the training
process, as well as a method for fast generation of correlated noise mask. Our
model is built upon deterministic diffusion models and utilizes blue noise to
help improve the generation quality compared to using Gaussian white (random)
noise only. Further, our framework allows introducing correlation across images
within a single mini-batch to improve gradient flow. We perform both
qualitative and quantitative evaluations on a variety of datasets using our
method, achieving improvements on different tasks over existing deterministic
diffusion models in terms of FID metric.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04931" title="Abstract">arXiv:2402.04931</a> [<a href="/pdf/2402.04931" title="Download PDF">pdf</a>, <a href="/format/2402.04931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of the (Connected) Cluster Vertex Deletion problem on  $H$-free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hoang-Oanh Le</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V+B">Van Bang Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of a MFCS 2022 paper. To appear in Theory of Computing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">The well-known Cluster Vertex Deletion problem (CVD) asks for a given graph
$G$ and an integer $k$ whether it is possible to delete a set $S$ of at most
$k$ vertices of $G$ such that the resulting graph $G-S$ is a cluster graph (a
disjoint union of cliques). We give a complete characterization of graphs $H$
for which CVD on $H$-free graphs is polynomially solvable and for which it is
NP-complete. Moreover, in the NP-completeness cases, CVD cannot be solved in
sub-exponential time in the vertex number of the $H$-free input graphs unless
the Exponential-Time Hypothesis fails. We also consider the connected variant
of CVD, the Connected Cluster Vertex Deletion problem (CCVD), in which the set
$S$ has to induce a connected subgraph of $G$. It turns out that CCVD admits
the same complexity dichotomy for $H$-free graphs. Our results enlarge a list
of rare dichotomy theorems for well-studied problems on $H$-free graphs.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04933" title="Abstract">arXiv:2402.04933</a> [<a href="/pdf/2402.04933" title="Download PDF">pdf</a>, <a href="/format/2402.04933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian Approach to Online Learning for Contextual Restless Bandits  with Applications to Public Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Biyonka Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lily Xu</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aparna Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>, 
<a href="/search/cs?searchtype=author&query=Janson%2C+L">Lucas Janson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Restless multi-armed bandits (RMABs) are used to model sequential resource
allocation in public health intervention programs. In these settings, the
underlying transition dynamics are often unknown a priori, requiring online
reinforcement learning (RL). However, existing methods in online RL for RMABs
cannot incorporate properties often present in real-world public health
applications, such as contextual information and non-stationarity. We present
Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs
that novelly combines techniques in Bayesian modeling with Thompson sampling to
flexibly model a wide range of complex RMAB settings, such as contextual and
non-stationary RMABs. A key contribution of our approach is its ability to
leverage shared information within and between arms to learn unknown RMAB
transition dynamics quickly in budget-constrained settings with relatively
short time horizons. Empirically, we show that BCoR achieves substantially
higher finite-sample performance than existing approaches over a range of
experimental settings, including one constructed from a real-world public
health campaign in India.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04935" title="Abstract">arXiv:2402.04935</a> [<a href="/pdf/2402.04935" title="Download PDF">pdf</a>, <a href="/format/2402.04935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Approximate and Packet Routing Equilibria to Nash Flows  Over Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olver%2C+N">Neil Olver</a>, 
<a href="/search/cs?searchtype=author&query=Sering%2C+L">Leon Sering</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+L+V">Laura Vargas Koch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended abstract of this work appeared at FOCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider a dynamic model of traffic that has received a lot of attention
in the past few years. Infinitesimally small agents aim to travel from a source
to a destination as quickly as possible. Flow patterns vary over time, and
congestion effects are modeled via queues, which form based on the
deterministic queueing model whenever the inflow into a link exceeds its
capacity. Are equilibria in this model meaningful as a prediction of traffic
behavior? For this to be the case, a certain notion of stability under ongoing
perturbations is needed. Real traffic consists of discrete, atomic ''packets'',
rather than being a continuous flow of non-atomic agents. Users may not choose
an absolutely quickest route available, if there are multiple routes with very
similar travel times. We would hope that in both these situations -- a discrete
packet model, with packet size going to 0, and $\epsilon$-equilibria, with
$\epsilon$ going to 0 -- equilibria converge to dynamic equilibria in the flow
over time model. No such convergence results were known. We show that such a
convergence result does hold in single-commodity instances for both of these
settings, in a unified way. More precisely, we introduce a notion of ''strict''
$\epsilon$-equilibria, and show that these must converge to the exact dynamic
equilibrium in the limit as $\epsilon \to 0$. We then show that results for the
two settings mentioned can be deduced from this with only moderate further
technical effort.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04937" title="Abstract">arXiv:2402.04937</a> [<a href="/pdf/2402.04937" title="Download PDF">pdf</a>, <a href="/format/2402.04937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Charting the COVID Long Haul Experience -- A Longitudinal Exploration of  Symptoms, Activity, and Clinical Adherence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pater%2C+J">Jessica Pater</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shaan Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Zaccour%2C+J">Juliette Zaccour</a>, 
<a href="/search/cs?searchtype=author&query=Carroll%2C+J">Jeanne Carroll</a>, 
<a href="/search/cs?searchtype=author&query=Nova%2C+F+F">Fayika Farhat Nova</a>, 
<a href="/search/cs?searchtype=author&query=Toscos%2C+T">Tammy Toscos</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+S">Shion Guha</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+F+L">Fen Lei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures, 7 tables, ACM Conference CHI Conference on Human Factors in Computing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">COVID Long Haul (CLH) is an emerging chronic illness with varied patient
experiences. Our understanding of CLH is often limited to data from electronic
health records (EHRs), such as diagnoses or problem lists, which do not capture
the volatility and severity of symptoms or their impact. To better understand
the unique presentation of CLH, we conducted a 3-month long cohort study with
14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective
(weekly surveys, interviews) data. Our findings reveal a complex presentation
of symptoms, associated uncertainty, and the ensuing impact CLH has on
patients' personal and professional lives. We identify patient needs,
practices, and challenges around adhering to clinical recommendations, engaging
with health data, and establishing "new normals" post COVID. We reflect on the
potential found at the intersection of these various data streams and the
persuasive heuristics possible when designing for this new population and their
specific needs.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04938" title="Abstract">arXiv:2402.04938</a> [<a href="/pdf/2402.04938" title="Download PDF">pdf</a>, <a href="/ps/2402.04938" title="Download PostScript">ps</a>, <a href="/format/2402.04938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An approach to automated videogame beta testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-B%C3%A9cares%2C+J">Jennifer Hern&#xe1;ndez-B&#xe9;cares</a>, 
<a href="/search/cs?searchtype=author&query=Costero%2C+L">Luis Costero</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Mart%C3%ADn%2C+P+P">Pedro Pablo G&#xf3;mez-Mart&#xed;n</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Entertainment Computing, Elsevier. 18. pp 79 to 92. (2017)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Videogames developed in the 1970s and 1980s were modest programs created in a
couple of months by a single person, who played the roles of designer, artist
and programmer. Since then, videogames have evolved to become a multi-million
dollar industry. Today, AAA game development involves hundreds of people
working together over several years. Management and engineering requirements
have changed at the same pace. Although many of the processes have been adapted
over time, this is not quite true for quality assurance tasks, which are still
done mainly manually by human beta testers due to the specific peculiarities of
videogames. This paper presents an approach to automate this beta testing.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04942" title="Abstract">arXiv:2402.04942</a> [<a href="/pdf/2402.04942" title="Download PDF">pdf</a>, <a href="/ps/2402.04942" title="Download PostScript">ps</a>, <a href="/format/2402.04942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Gaussian Vector Broadcast Channel Capacity with Scalar  Lattices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%9Eener%2C+M+Y">M. Yusuf &#x15e;ener</a>, 
<a href="/search/cs?searchtype=author&query=Kramer%2C+G">Gerhard Kramer</a>, 
<a href="/search/cs?searchtype=author&query=Shamai%2C+S">Shlomo Shamai</a> (Shitz), 
<a href="/search/cs?searchtype=author&query=B%C3%B6hnke%2C+R">Ronald B&#xf6;hnke</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wen Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A coding scheme with scalar lattices is applied to K-receiver, Gaussian,
vector broadcast channels with K independent messages, one for each receiver.
The method decomposes each receiver channel into parallel scalar channels with
known interference and applies dirty paper coding with a modulo interval,
amplitude shift keying (ASK), and probabilistic shaping to each scalar channel.
The achievable rate tuples include all points inside the capacity region by
choosing truncated Gaussian shaping, large ASK alphabets, and large modulo
intervals.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04943" title="Abstract">arXiv:2402.04943</a> [<a href="/pdf/2402.04943" title="Download PDF">pdf</a>, <a href="/ps/2402.04943" title="Download PostScript">ps</a>, <a href="/format/2402.04943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cayley hashing with cookies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shpilrain%2C+V">Vladimir Shpilrain</a>, 
<a href="/search/cs?searchtype=author&query=Sosnovski%2C+B">Bianca Sosnovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Group Theory (math.GR)

</div>
<p class="mathjax">Cayley hash functions are based on a simple idea of using a pair of semigroup
elements, A and B, to hash the 0 and 1 bit, respectively, and then to hash an
arbitrary bit string in the natural way, by using multiplication of elements in
the semigroup. The main advantage of Cayley hash functions compared to, say,
hash functions in the SHA family is that when an already hashed document is
amended, one does not have to hash the whole amended document all over again,
but rather hash just the amended part and then multiply the result by the hash
of the original document. Some authors argued that this may be a security
hazard, specifically that this property may facilitate finding a second
preimage by splitting a long bit string into shorter pieces. In this paper, we
offer a way to get rid of this alleged disadvantage and keep the advantages at
the same time. We call this method ``Cayley hashing with cookies" using
terminology borrowed from the theory of random walks in a random environment.
For the platform semigroup, we use 2x2 matrices over F_p.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04953" title="Abstract">arXiv:2402.04953</a> [<a href="/pdf/2402.04953" title="Download PDF">pdf</a>, <a href="/ps/2402.04953" title="Download PostScript">ps</a>, <a href="/format/2402.04953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4-Dimensional deformation part model for pose estimation using Kalman  filter constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez-Berti%2C+E">Enrique Martinez-Berti</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Salmeron%2C+A">Antonio-Jose Sanchez-Salmeron</a>, 
<a href="/search/cs?searchtype=author&query=Ricolfe-Viala%2C+C">Carlos Ricolfe-Viala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The main goal of this article is to analyze the effect on pose estimation
accuracy when using a Kalman filter added to 4-dimensional deformation part
model partial solutions. The experiments run with two data sets showing that
this method improves pose estimation accuracy compared with state-of-the-art
methods and that a Kalman filter helps to increase this accuracy.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04955" title="Abstract">arXiv:2402.04955</a> [<a href="/pdf/2402.04955" title="Download PDF">pdf</a>, <a href="/format/2402.04955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freire%2C+S+K">Samuel Kernan Freire</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaofan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Niforatos%2C+E">Evangelos Niforatos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, under review at an ACM venue
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Cognitive assistants (CA) are chatbots that provide context-aware support to
human workers in knowledge-intensive tasks. Traditionally, cognitive assistants
respond in specific ways to predefined user intents and conversation patterns.
However, this rigidness does not handle the diversity of natural language well.
Recent advances in natural language processing (NLP), powering large language
models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in
a more flexible, human-like manner. However, the additional degrees of freedom
may have unforeseen consequences, especially in knowledge-intensive contexts
where accuracy is crucial. As a preliminary step to assessing the potential of
using LLMs in these contexts, we conducted a user study comparing an LLM-based
CA to an intent-based system regarding interaction efficiency, user experience,
workload, and usability. This revealed that LLM-based CAs exhibited better user
experience, task completion rate, usability, and perceived performance than
intent-based systems, suggesting that switching NLP techniques should be
investigated further.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04957" title="Abstract">arXiv:2402.04957</a> [<a href="/pdf/2402.04957" title="Download PDF">pdf</a>, <a href="/format/2402.04957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfidencing LLMs from the Grouping Loss Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Lebel%2C+A">Alexandre Perez-Lebel</a>, 
<a href="/search/cs?searchtype=author&query=Suchanek%2C+F+M">Fabian M. Suchanek</a>, 
<a href="/search/cs?searchtype=author&query=Varoquaux%2C+G">Ga&#xeb;l Varoquaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to
generating hallucinated answers in a confident tone. While efforts to elicit
and calibrate confidence scores have proven useful, recent findings show that
controlling uncertainty must go beyond calibration: predicted scores may
deviate significantly from the actual posterior probabilities due to the impact
of grouping loss. In this work, we construct a new evaluation dataset derived
from a knowledge base to assess confidence scores given to answers of Mistral
and LLaMA. Experiments show that they tend to be overconfident. Further, we
show that they are more overconfident on some answers than others, \emph{eg}
depending on the nationality of the person in the query. In
uncertainty-quantification theory, this is grouping loss. To address this, we
propose a solution to reconfidence LLMs, canceling not only calibration but
also grouping loss. The LLMs, after the reconfidencing process, indicate
improved confidence alignment with the accuracy of their responses.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04958" title="Abstract">arXiv:2402.04958</a> [<a href="/pdf/2402.04958" title="Download PDF">pdf</a>, <a href="/format/2402.04958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel-Selective Normalization for Label-Shift Robust Test-Time  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vianna%2C+P">Pedro Vianna</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+M">Muawiz Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Mehrbod%2C+P">Paria Mehrbod</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">An Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cloutier%2C+G">Guy Cloutier</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/cs?searchtype=author&query=Belilovsky%2C+E">Eugene Belilovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages including references, 7 figures, 2 tables, Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks have useful applications in many different tasks,
however their performance can be severely affected by changes in the data
distribution. For example, in the biomedical field, their performance can be
affected by changes in the data (different machines, populations) between
training and test datasets. To ensure robustness and generalization to
real-world scenarios, test-time adaptation has been recently studied as an
approach to adjust models to a new data distribution during inference.
Test-time batch normalization is a simple and popular method that achieved
compelling performance on domain shift benchmarks. It is implemented by
recalculating batch normalization statistics on test batches. Prior work has
focused on analysis with test data that has the same label distribution as the
training data. However, in many practical applications this technique is
vulnerable to label distribution shifts, sometimes producing catastrophic
failure. This presents a risk in applying test time adaptation methods in
deployment. We propose to tackle this challenge by only selectively adapting
channels in a deep network, minimizing drastic adaptation that is sensitive to
label shifts. Our selection scheme is based on two principles that we
empirically motivate: (1) later layers of networks are more sensitive to label
shift (2) individual features can be sensitive to specific classes. We apply
the proposed technique to three classification tasks, including CIFAR10-C,
Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and
label distribution shifts. We find that our method allows to bring the benefits
of TTA while significantly reducing the risk of failure common in other
methods, while being robust to choice in hyperparameters.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04959" title="Abstract">arXiv:2402.04959</a> [<a href="/pdf/2402.04959" title="Download PDF">pdf</a>, <a href="/format/2402.04959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Margin Propagation based XOR-SAT Solvers for Decoding of LDPC Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nandi%2C+A">Ankita Nandi</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabartty%2C+S">Shantanu Chakrabartty</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+C+S">Chetan Singh Thakur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures, Paper submitted to IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Decoding of Low-Density Parity Check (LDPC) codes can be viewed as a special
case of XOR-SAT problems, for which low-computational complexity bit-flipping
algorithms have been proposed in the literature. However, a performance gap
exists between the bit-flipping LDPC decoding algorithms and the benchmark LDPC
decoding algorithms, such as the Sum-Product Algorithm (SPA). In this paper, we
propose an XOR-SAT solver using log-sum-exponential functions and demonstrate
its advantages for LDPC decoding. This is then approximated using the Margin
Propagation formulation to attain a low-complexity LDPC decoder. The proposed
algorithm uses soft information to decide the bit-flips that maximize the
number of parity check constraints satisfied over an optimization function. The
proposed solver can achieve results that are within $0.1$dB of the Sum-Product
Algorithm for the same number of code iterations. It is also at least 10x
lesser than other Gradient-Descent Bit Flipping decoding algorithms, which are
also bit-flipping algorithms based on optimization functions. The approximation
using the Margin Propagation formulation does not require any multipliers,
resulting in significantly lower computational complexity than other
soft-decision Bit-Flipping LDPC decoders.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04964" title="Abstract">arXiv:2402.04964</a> [<a href="/pdf/2402.04964" title="Download PDF">pdf</a>, <a href="/format/2402.04964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvLoRA and AdaBN based Domain Adaptation via Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aleem%2C+S">Sidra Aleem</a>, 
<a href="/search/cs?searchtype=author&query=Dietlmeier%2C+J">Julia Dietlmeier</a>, 
<a href="/search/cs?searchtype=author&query=Arazo%2C+E">Eric Arazo</a>, 
<a href="/search/cs?searchtype=author&query=Little%2C+S">Suzanne Little</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing domain adaptation (DA) methods often involve pre-training on the
source domain and fine-tuning on the target domain. For multi-target domain
adaptation, having a dedicated/separate fine-tuned network for each target
domain, that retain all the pre-trained model parameters, is prohibitively
expensive. To address this limitation, we propose Convolutional Low-Rank
Adaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds
trainable low-rank decomposition matrices to convolutional layers, and
backpropagates the gradient through these matrices thus greatly reducing the
number of trainable parameters. To further boost adaptation, we utilize
Adaptive Batch Normalization (AdaBN) which computes target-specific running
statistics and use it along with ConvLoRA. Our method has fewer trainable
parameters and performs better or on-par with large independent fine-tuned
networks (with less than 0.9% trainable parameters of the total base model)
when tested on the segmentation of Calgary-Campinas dataset containing brain
MRI images. Our approach is simple, yet effective and can be applied to any
deep learning-based architecture which uses convolutional and batch
normalization layers. Code is available at:
https://github.com/aleemsidra/ConvLoRA.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04967" title="Abstract">arXiv:2402.04967</a> [<a href="/pdf/2402.04967" title="Download PDF">pdf</a>, <a href="/format/2402.04967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text or Image? What is More Important in Cross-Domain Generalization  Capabilities of Hate Meme Detection Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Piush Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Mehrabanian%2C+J">Jawar Mehrabanian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weigang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Alacam%2C+%C3%96">&#xd6;zge Alacam</a>, 
<a href="/search/cs?searchtype=author&query=Zesch%2C+T">Torsten Zesch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL'2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper delves into the formidable challenge of cross-domain
generalization in multimodal hate meme detection, presenting compelling
findings. We provide enough pieces of evidence supporting the hypothesis that
only the textual component of hateful memes enables the existing multimodal
classifier to generalize across different domains, while the image component
proves highly sensitive to a specific training dataset. The evidence includes
demonstrations showing that hate-text classifiers perform similarly to
hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction
of captions generated from images of memes to the hate-meme classifier worsens
performance by an average F1 of 0.02. Through blackbox explanations, we
identify a substantial contribution of the text modality (average of 83%),
which diminishes with the introduction of meme's image captions (52%).
Additionally, our evaluation on a newly created confounder dataset reveals
higher performance on text confounders as compared to image confounders with an
average $\Delta$F1 of 0.18.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04971" title="Abstract">arXiv:2402.04971</a> [<a href="/pdf/2402.04971" title="Download PDF">pdf</a>, <a href="/format/2402.04971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Sender Persuasion -- A Computational Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tonghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We consider multiple senders with informational advantage signaling to
convince a single self-interested actor towards certain actions. Generalizing
the seminal Bayesian Persuasion framework, such settings are ubiquitous in
computational economics, multi-agent learning, and machine learning with
multiple objectives. The core solution concept here is the Nash equilibrium of
senders' signaling policies. Theoretically, we prove that finding an
equilibrium in general is PPAD-Hard; in fact, even computing a sender's best
response is NP-Hard. Given these intrinsic difficulties, we turn to finding
local Nash equilibria. We propose a novel differentiable neural network to
approximate this game's non-linear and discontinuous utilities. Complementing
this with the extra-gradient algorithm, we discover local equilibria that
Pareto dominates full-revelation equilibria and those found by existing neural
networks. Broadly, our theoretical and empirical contributions are of interest
to a large class of economic problems.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04972" title="Abstract">arXiv:2402.04972</a> [<a href="/pdf/2402.04972" title="Download PDF">pdf</a>, <a href="/format/2402.04972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Fair Assignment and Rebalancing for Mobility-on-Demand  Systems via an Auction-based Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaier Liang</a>, 
<a href="/search/cs?searchtype=author&query=Vasile%2C+C">Cristian-Ioan Vasile</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In this paper, we consider fair assignment of complex requests for
Mobility-On-Demand systems. We model the transportation requests as temporal
logic formulas that must be satisfied by a fleet of vehicles. We require that
the assignment of requests to vehicles is performed in a distributed manner
based only on communication between vehicles while ensuring fair allocation.
Our approach to the vehicle-request assignment problem is based on a
distributed auction scheme with no centralized bidding that leverages utility
history correction of bids to improve fairness. Complementarily, we propose a
rebalancing scheme that employs rerouting vehicles to more rewarding areas to
increase the potential future utility and ensure a fairer utility distribution.
We adopt the max-min and deviation of utility as the two criteria for fairness.
We demonstrate the methods in the mid-Manhattan map with a large number of
requests generated in different probability settings. We show that we increase
the fairness between vehicles based on the fairness criteria without
degenerating the servicing quality.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04975" title="Abstract">arXiv:2402.04975</a> [<a href="/pdf/2402.04975" title="Download PDF">pdf</a>, <a href="/format/2402.04975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming  Learning for Children Aged 6-12
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liuqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shuhong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunnong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yaxuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingyun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 7 figures, accepted by CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">As Computational Thinking (CT) continues to permeate younger age groups in
K-12 education, established CT platforms such as Scratch face challenges in
catering to these younger learners, particularly those in the elementary school
(ages 6-12). Through formative investigation with Scratch experts, we uncover
three key obstacles to children's autonomous Scratch learning: artist's block
in project planning, bounded creativity in asset creation, and inadequate
coding guidance during implementation. To address these barriers, we introduce
ChatScratch, an AI-augmented system to facilitate autonomous programming
learning for young children. ChatScratch employs structured interactive
storyboards and visual cues to overcome artist's block, integrates digital
drawing and advanced image generation technologies to elevate creativity, and
leverages Scratch-specialized Large Language Models (LLMs) for professional
coding guidance. Our study shows that, compared to Scratch, ChatScratch
efficiently fosters autonomous programming learning, and contributes to the
creation of high-quality, personally meaningful Scratch projects for children.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04978" title="Abstract">arXiv:2402.04978</a> [<a href="/pdf/2402.04978" title="Download PDF">pdf</a>, <a href="/ps/2402.04978" title="Download PostScript">ps</a>, <a href="/format/2402.04978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge  Graph-Integrated Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Large Language Models (LLMs) demonstrate exceptional performance in a
multitude of Natural Language Processing (NLP) tasks, they encounter challenges
in practical applications, including issues with hallucinations, inadequate
knowledge updating, and limited transparency in the reasoning process. To
overcome these limitations, this study innovatively proposes a collaborative
training-free reasoning scheme involving tight cooperation between Knowledge
Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively
explore KG, selectively retrieving a task-relevant knowledge subgraph to
support reasoning. The LLMs are then guided to further combine inherent
implicit knowledge to reason on the subgraph while explicitly elucidating the
reasoning process. Through such a cooperative approach, our scheme achieves
more reliable knowledge-based reasoning and facilitates the tracing of the
reasoning results. Experimental results show that our scheme significantly
progressed across multiple datasets, notably achieving over a 10% improvement
on the QALD10 dataset compared to the best baseline and the fine-tuned
state-of-the-art (SOTA) work. Building on this success, this study hopes to
offer a valuable reference for future research in the fusion of KG and LLMs,
thereby enhancing LLMs' proficiency in solving complex issues.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04979" title="Abstract">arXiv:2402.04979</a> [<a href="/pdf/2402.04979" title="Download PDF">pdf</a>, <a href="/format/2402.04979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection and Pose Estimation of flat, Texture-less Industry Objects on  HoloLens using synthetic Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%B6llabauer%2C+T">Thomas P&#xf6;llabauer</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCcker%2C+F">Fabian R&#xfc;cker</a>, 
<a href="/search/cs?searchtype=author&query=Franek%2C+A">Andreas Franek</a>, 
<a href="/search/cs?searchtype=author&query=Gorschl%C3%BCter%2C+F">Felix Gorschl&#xfc;ter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Scandinavian Conference on Image Analysis 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Scandinavian Conference on Image Analysis 2023 (pp. 569-585).
  Cham: Springer Nature Switzerland
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current state-of-the-art 6d pose estimation is too compute intensive to be
deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both
used for an increasing number of augmented reality applications. The quality of
AR is greatly dependent on its capabilities to detect and overlay geometry
within the scene. We propose a synthetically trained client-server-based
augmented reality application, demonstrating state-of-the-art object pose
estimation of metallic and texture-less industry objects on edge devices.
Synthetic data enables training without real photographs, i.e. for
yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted
sorting task, and quantitative evaluation on both renderings, as well as
real-world data recorded on HoloLens 2, sheds light on its real-world
applicability.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04982" title="Abstract">arXiv:2402.04982</a> [<a href="/pdf/2402.04982" title="Download PDF">pdf</a>, <a href="/format/2402.04982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for  Energy Consumption Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clement%2C+T">Tobias Clement</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+T+T">Hung Truong Thanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kemmerzell%2C+N">Nils Kemmerzell</a>, 
<a href="/search/cs?searchtype=author&query=Abdelaal%2C+M">Mohamed Abdelaal</a>, 
<a href="/search/cs?searchtype=author&query=Stjelja%2C+D">Davor Stjelja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A short version of this paper was published at the Australasian Joint Conference on Artificial Intelligence in 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB)

</div>
<p class="mathjax">This paper presents an approach integrating explainable artificial
intelligence (XAI) techniques with adaptive learning to enhance energy
consumption prediction models, with a focus on handling data distribution
shifts. Leveraging SHAP clustering, our method provides interpretable
explanations for model predictions and uses these insights to adaptively refine
the model, balancing model complexity with predictive performance. We introduce
a three-stage process: (1) obtaining SHAP values to explain model predictions,
(2) clustering SHAP values to identify distinct patterns and outliers, and (3)
refining the model based on the derived SHAP clustering characteristics. Our
approach mitigates overfitting and ensures robustness in handling data
distribution shifts. We evaluate our method on a comprehensive dataset
comprising energy consumption records of buildings, as well as two additional
datasets to assess the transferability of our approach to other domains,
regression, and classification problems. Our experiments demonstrate the
effectiveness of our approach in both task types, resulting in improved
predictive performance and interpretable model explanations.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04987" title="Abstract">arXiv:2402.04987</a> [<a href="/pdf/2402.04987" title="Download PDF">pdf</a>, <a href="/format/2402.04987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javanmard%2C+A">Adel Javanmard</a>, 
<a href="/search/cs?searchtype=author&query=Fahrbach%2C+M">Matthew Fahrbach</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">This work studies algorithms for learning from aggregate responses. We focus
on the construction of aggregation sets (called bags in the literature) for
event-level loss functions. We prove for linear regression and generalized
linear models (GLMs) that the optimal bagging problem reduces to
one-dimensional size-constrained $k$-means clustering. Further, we
theoretically quantify the advantage of using curated bags over random bags. We
then propose the PriorBoost algorithm, which adaptively forms bags of samples
that are increasingly homogeneous with respect to (unobserved) individual
responses to improve model quality. We study label differential privacy for
aggregate learning, and we also provide extensive experiments showing that
PriorBoost regularly achieves optimal model quality for event-level
predictions, in stark contrast to non-adaptive algorithms.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04991" title="Abstract">arXiv:2402.04991</a> [<a href="/pdf/2402.04991" title="Download PDF">pdf</a>, <a href="/format/2402.04991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Opportunity of Augmented Reality (AR) in Supporting Older  Adults Explore and Learn Smartphone Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaofu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Wai Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaoying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+E">Emily Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+X">Xiaoyu Mo</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingming Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The global aging trend compels older adults to navigate the evolving digital
landscape, presenting a substantial challenge in mastering smartphone
applications. While Augmented Reality (AR) holds promise for enhancing learning
and user experience, its role in aiding older adults' smartphone app
exploration remains insufficiently explored. Therefore, we conducted a
two-phase study: (1) a workshop with 18 older adults to identify app
exploration challenges and potential AR interventions, and (2) tech-probe
participatory design sessions with 15 participants to co-create AR support
tools. Our research highlights AR's effectiveness in reducing physical and
cognitive strain among older adults during app exploration, especially during
multi-app usage and the trial-and-error learning process. We also examined
their interactional experiences with AR, yielding design considerations on
tailoring AR tools for smartphone app exploration. Ultimately, our study
unveils the prospective landscape of AR in supporting the older demographic,
both presently and in future scenarios.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04999" title="Abstract">arXiv:2402.04999</a> [<a href="/pdf/2402.04999" title="Download PDF">pdf</a>, <a href="/format/2402.04999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Longitudinal Study of Italian and French Reddit Conversations Around  the Russian Invasion of Ukraine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+F">Francesco Corso</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+G">Giuseppe Russo</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, Accepted at ACM WEBSCI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Global events like wars and pandemics can intensify online discussions,
fostering information sharing and connection among individuals. However, the
divisive nature of such events may lead to polarization within online
communities, shaping the dynamics of online interactions. Our study delves into
the conversations within the largest Italian and French Reddit communities,
specifically examining how the Russian invasion of Ukraine affected online
interactions. We use a dataset with over 3 million posts (i.e., comments and
submissions) to (1) describe the patterns of moderation activity and (2)
characterize war-related discussions in the subreddits. We found changes in
moderators' behavior, who became more active during the first month of the war.
Moreover, we identified a connection between the daily sentiment of comments
and the prevalence of war-related discussions. These discussions were not only
more negative and toxic compared to non-war-related ones but also did not
involve a specific demographic group. Our research reveals that there is no
tendency for users with similar characteristics to interact more. Overall, our
study reveals how the war in Ukraine had a negative influence on daily
conversations in the analyzed communities. This sheds light on how users
responded to this significant event, providing insights into the dynamics of
online discussions during events of global relevance.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05000" title="Abstract">arXiv:2402.05000</a> [<a href="/pdf/2402.05000" title="Download PDF">pdf</a>, <a href="/format/2402.05000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pedagogical Alignment of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonkar%2C+S">Shashank Sonkar</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+K">Kangqi Ni</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+S">Sapana Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we introduce the novel concept of pedagogically aligned Large
Language Models (LLMs) that signifies a transformative shift in the application
of LLMs within educational contexts. Rather than providing direct responses to
user queries, pedagogically-aligned LLMs function as scaffolding tools,
breaking complex problems into manageable subproblems and guiding students
towards the final answer through constructive feedback and hints. The objective
is to equip learners with problem-solving strategies that deepen their
understanding and internalization of the subject matter. Previous research in
this field has primarily applied the supervised finetuning approach without
framing the objective as an alignment problem, hence not employing
reinforcement learning through human feedback (RLHF) methods. This study
reinterprets the narrative by viewing the task through the lens of alignment
and demonstrates how RLHF methods emerge naturally as a superior alternative
for aligning LLM behaviour. Building on this perspective, we propose a novel
approach for constructing a reward dataset specifically designed for the
pedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms
and find that they outperform SFT significantly. Our qualitative analyses
across model differences and hyperparameter sensitivity further validate the
superiority of RLHF over SFT. Also, our study sheds light on the potential of
online feedback for enhancing the performance of pedagogically-aligned LLMs,
thus providing valuable insights for the advancement of these models in
educational settings.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05002" title="Abstract">arXiv:2402.05002</a> [<a href="/pdf/2402.05002" title="Download PDF">pdf</a>, <a href="/format/2402.05002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Confidence Bounds for Stochastic Partial Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heuillet%2C+M">Maxime Heuillet</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+O">Ola Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Durand%2C+A">Audrey Durand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The partial monitoring (PM) framework provides a theoretical formulation of
sequential learning problems with incomplete feedback. On each round, a
learning agent plays an action while the environment simultaneously chooses an
outcome. The agent then observes a feedback signal that is only partially
informative about the (unobserved) outcome. The agent leverages the received
feedback signals to select actions that minimize the (unobserved) cumulative
loss. In contextual PM, the outcomes depend on some side information that is
observable by the agent before selecting the action on each round. In this
paper, we consider the contextual and non-contextual PM settings with
stochastic outcomes. We introduce a new class of strategies based on the
randomization of deterministic confidence bounds, that extend regret guarantees
to settings where existing stochastic strategies are not applicable. Our
experiments show that the proposed RandCBP and RandCBPside* strategies improve
state-of-the-art baselines in PM games. To encourage the adoption of the PM
framework, we design a use case on the real-world problem of monitoring the
error rate of any deployed classification system.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05003" title="Abstract">arXiv:2402.05003</a> [<a href="/pdf/2402.05003" title="Download PDF">pdf</a>, <a href="/format/2402.05003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Invariant Kalman Filter for Inertial-based Odometry with  Large-sample Environmental Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoying Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+G">Guangyang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingcheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiaoqiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junfeng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A filter for inertial-based odometry is a recursive method used to estimate
the pose from measurements of ego-motion and relative pose. Currently, there is
no known filter that guarantees the computation of a globally optimal solution
for the non-linear measurement model. In this paper, we demonstrate that an
innovative filter, with the state being $SE_2(3)$ and the
$\sqrt{n}$-\textit{consistent} pose as the initialization, efficiently achieves
\textit{asymptotic optimality} in terms of minimum mean square error. This
approach is tailored for real-time SLAM and inertial-based odometry
applications.
<br />Our first contribution is that we propose an iterative filtering method based
on the Gauss-Newton method on Lie groups which is numerically to solve the
estimation of states from a priori and non-linear measurements. The filtering
stands out due to its iterative mechanism and adaptive initialization. Second,
when dealing with environmental measurements of the surroundings, we utilize a
$\sqrt{n}$-consistent pose as the initial value for the update step in a single
iteration. The solution is closed in form and has computational complexity
$O(n)$. Third, we theoretically show that the approach can achieve asymptotic
optimality in the sense of minimum mean square error from the a priori and
virtual relative pose measurements (see Problem~\ref{prob:new update problem}).
Finally, to validate our method, we carry out extensive numerical and
experimental evaluations. Our results consistently demonstrate that our
approach outperforms other state-of-the-art filter-based methods, including the
iterated extended Kalman filter and the invariant extended Kalman filter, in
terms of accuracy and running time.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05004" title="Abstract">arXiv:2402.05004</a> [<a href="/pdf/2402.05004" title="Download PDF">pdf</a>, <a href="/ps/2402.05004" title="Download PostScript">ps</a>, <a href="/format/2402.05004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Generalized Decoding of Polar-like Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+P">Peihong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Duffy%2C+K+R">Ken R. Duffy</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9dard%2C+M">Muriel M&#xe9;dard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this work, we present a framework that explores the tradeoff between the
undetected error rate (UER) and block error rate (BLER) of polar-like codes. It
relies on a novel approximation for what we call codebook probability, which
assumes an auxiliary distribution mimicking the dynamics of decoding algorithms
with successive cancellation (SC) decoding schedule. Simulation results
demonstrates that, in the case of SC list decoding, the proposed framework
outperforms the state-of-art approximations of Forney's generalized decoding
rule for polar-like codes with dynamic frozen bits. In addition, the proposed
generalized decoding outperforms the CRC-concatenated polar codes significantly
in both BLER and UER. Finally, we briefly discuss two potential applications of
the approximated codebook probability: coded pilot-free channel estimation and
bitwise soft-output decoding.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05006" title="Abstract">arXiv:2402.05006</a> [<a href="/pdf/2402.05006" title="Download PDF">pdf</a>, <a href="/format/2402.05006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in  Signed Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingbang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mang%2C+Q">Qiuyang Mang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hangrui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Richard Peng</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenhao Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Signed networks, characterized by edges labeled as either positive or
negative, offer nuanced insights into interaction dynamics beyond the
capabilities of unsigned graphs. Central to this is the task of identifying the
maximum balanced subgraph, crucial for applications like polarized community
detection in social networks and portfolio analysis in finance. Traditional
models, however, are limited by an assumption of perfect partitioning, which
fails to mirror the complexities of real-world data. Addressing this gap, we
introduce an innovative generalized balanced subgraph model that incorporates
tolerance for irregularities. Our proposed region-based heuristic algorithm,
tailored for this NP-hard problem, strikes a balance between low time
complexity and high-quality outcomes. Comparative experiments validate its
superior performance against leading solutions, delivering enhanced
effectiveness (notably larger subgraph sizes) and efficiency (achieving up to
100x speedup) in both traditional and generalized contexts.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05007" title="Abstract">arXiv:2402.05007</a> [<a href="/pdf/2402.05007" title="Download PDF">pdf</a>, <a href="/format/2402.05007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Example-based Explanations for Random Forests using Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Surve%2C+T">Tanmay Surve</a>, 
<a href="/search/cs?searchtype=author&query=Pradhan%2C+R">Romila Pradhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Tree-based machine learning models, such as decision trees and random
forests, have been hugely successful in classification tasks primarily because
of their predictive power in supervised learning tasks and ease of
interpretation. Despite their popularity and power, these models have been
found to produce unexpected or discriminatory outcomes. Given their
overwhelming success for most tasks, it is of interest to identify sources of
their unexpected and discriminatory behavior. However, there has not been much
work on understanding and debugging tree-based classifiers in the context of
fairness.
<br />We introduce FairDebugger, a system that utilizes recent advances in machine
unlearning research to identify training data subsets responsible for instances
of fairness violations in the outcomes of a random forest classifier.
FairDebugger generates top-$k$ explanations (in the form of coherent training
data subsets) for model unfairness. Toward this goal, FairDebugger first
utilizes machine unlearning to estimate the change in the tree structures of
the random forest when parts of the underlying training data are removed, and
then leverages the Apriori algorithm from frequent itemset mining to reduce the
subset search space. We empirically evaluate our approach on three real-world
datasets, and demonstrate that the explanations generated by FairDebugger are
consistent with insights from prior studies on these datasets.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05008" title="Abstract">arXiv:2402.05008</a> [<a href="/pdf/2402.05008" title="Download PDF">pdf</a>, <a href="/format/2402.05008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientViT-SAM: Accelerated Segment Anything Model Without Performance  Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Han Cai</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present EfficientViT-SAM, a new family of accelerated segment anything
models. We retain SAM's lightweight prompt encoder and mask decoder while
replacing the heavy image encoder with EfficientViT. For the training, we begin
with the knowledge distillation from the SAM-ViT-H image encoder to
EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B
dataset. Benefiting from EfficientViT's efficiency and capacity,
EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over
SAM-ViT-H without sacrificing performance. Our code and pre-trained models are
released at https://github.com/mit-han-lab/efficientvit.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05010" title="Abstract">arXiv:2402.05010</a> [<a href="/pdf/2402.05010" title="Download PDF">pdf</a>, <a href="/format/2402.05010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exhaust Gas Optimization of Modern Scooters by Velocity Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kre%C3%9F%2C+J">Jannis Kre&#xdf;</a>, 
<a href="/search/eess?searchtype=author&query=Rau%2C+J">Jens Rau</a>, 
<a href="/search/eess?searchtype=author&query=Behr%2C+I">Ingo Behr</a>, 
<a href="/search/eess?searchtype=author&query=Mohn%2C+B">Bernd Mohn</a>, 
<a href="/search/eess?searchtype=author&query=Hebert%2C+H">Hektor Hebert</a>, 
<a href="/search/eess?searchtype=author&query=Morgado-Est%C3%A9vez%2C+A">Arturo Morgado-Est&#xe9;vez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper investigates the optimization of the exhaust gas composition by
applying a velocity-controlled Throttle-by-Wire-System on modern 50 cc scooters
(Euro 5). Nowadays combustion-powered scooters are still inefficiently
restricted, resulting in an unreasonably high fuel consumption and unfavorable
exhaust emissions. The velocity control prevents restriction by negatively
shifting the ignition timing and regulates the throttle valve opening instead.
Injection quantity, engine speed, ignition timing, cylinder wall temperature,
exhaust gas temperature, oxygen sensor data, crankshaft position and
in-cylinder pressure were acquired to measure engine parameters. At the same
time, vehicle data on the CAN bus, such as throttle opening angle, the rider's
acceleration command and vehicle velocity were recorded. For determination of
the exhaust gas composition, five probes were sensing CO, CO2, NOx, O2 and HC
in addition to the temperature and mass flow. A Peugeot Kisbee 50 4T (Euro 5)
serves as test vehicle. The original and the optimized restriction were
subjected to various gradients on a roller dynamometer at top speed. Thus, a
statement can be made about all operating points of restriction. The resistance
parameters required, were previously determined in a coast down test. When
driving on level ground, a difference of 50% in the throttle opening leads to a
17% improvement in fuel economy. By measuring the engine parameters, optimum
ignition timing could be proven with increasing internal cylinder pressure.
Further, 17% reduction in exhaust gas flow was demonstrated. CO emissions
decreased by a factor of 8.4, CO2 by 1.17 and HC by 2.1 while NOx increased by
a factor of 3.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05011" title="Abstract">arXiv:2402.05011</a> [<a href="/pdf/2402.05011" title="Download PDF">pdf</a>, <a href="/format/2402.05011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Complexity: Toward Lossless Graph Condensation via Expanding  Window Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bresson%2C+X">Xavier Bresson</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Lossless graph condensation method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph condensation aims to reduce the size of a large-scale graph dataset by
synthesizing a compact counterpart without sacrificing the performance of Graph
Neural Networks (GNNs) trained on it, which has shed light on reducing the
computational cost for training GNNs. Nevertheless, existing methods often fall
short of accurately replicating the original graph for certain datasets,
thereby failing to achieve the objective of lossless condensation. To
understand this phenomenon, we investigate the potential reasons and reveal
that the previous state-of-the-art trajectory matching method provides biased
and restricted supervision signals from the original graph when optimizing the
condensed one. This significantly limits both the scale and efficacy of the
condensed graph. In this paper, we make the first attempt toward
\textit{lossless graph condensation} by bridging the previously neglected
supervision signals. Specifically, we employ a curriculum learning strategy to
train expert trajectories with more diverse supervision signals from the
original graph, and then effectively transfer the information into the
condensed graph with expanding window matching. Moreover, we design a loss
function to further extract knowledge from the expert trajectories. Theoretical
analysis justifies the design of our method and extensive experiments verify
its superiority across different datasets. Code is released at
https://github.com/NUS-HPC-AI-Lab/GEOM.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05012" title="Abstract">arXiv:2402.05012</a> [<a href="/pdf/2402.05012" title="Download PDF">pdf</a>, <a href="/format/2402.05012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Theoretically Secure Encryption Key Generation over Wireless  Networks by Exploiting Packet Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandani%2C+A+K">Amir K. Khandani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This article presents a novel method for establishing an information
theoretically secure encryption key over wireless channels. It exploits the
fact that data transmission over wireless links is accompanied by packet error,
while noise terms, and thereby the error events observed by two separate
receivers are independent of each other. A number of data packets, with random
data, are transmitted from a first legitimate node, say Alice, to a second
legitimate node, say Bob. Bob identifies all packets that are received
error-free in the first transmission attempt and sends their indices to Alice
over a public channel. Then, both Alice and Bob mix the contents of identified
packets, e.g., using a hash function, and thereby derive an identical
encryption key. Since error events from Alice to Bob is independent of error
events from Alice to Eve, the chances that Eve has successfully received all
packets used in key generation error-free diminishes as the number of packet
increases. In many wireless standards, the first stage in error detection and
Automatic Repeat Request (ARQ) is deployed at the PHY/MAC (Physical
Layer/Medium Access Control) layer. In such setups, the first re-transmission
is manged by the PHY/MAC layer without informing higher layers. This makes it
impossible to directly access the information related to packet errors through
high-level programming interfaces available to an end-user. A method is
presented for determining packets received error-free in first transmission
attempts through high-level programming. Examples are presented in conjunction
with an LTE cellular network.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05013" title="Abstract">arXiv:2402.05013</a> [<a href="/pdf/2402.05013" title="Download PDF">pdf</a>, <a href="/format/2402.05013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression of Structured Data with Autoencoders: Provable Benefit of  Nonlinearities and Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%B6gler%2C+K">Kevin K&#xf6;gler</a>, 
<a href="/search/cs?searchtype=author&query=Shevchenko%2C+A">Alexander Shevchenko</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Autoencoders are a prominent model in many empirical branches of machine
learning and lossy data compression. However, basic theoretical questions
remain unanswered even in a shallow two-layer setting. In particular, to what
degree does a shallow autoencoder capture the structure of the underlying data
distribution? For the prototypical case of the 1-bit compression of sparse
Gaussian data, we prove that gradient descent converges to a solution that
completely disregards the sparse structure of the input. Namely, the
performance of the algorithm is the same as if it was compressing a Gaussian
source - with no sparsity. For general data distributions, we give evidence of
a phase transition phenomenon in the shape of the gradient descent minimizer,
as a function of the data sparsity: below the critical sparsity level, the
minimizer is a rotation taken uniformly at random (just like in the compression
of non-sparse data); above the critical sparsity, the minimizer is the identity
(up to a permutation). Finally, by exploiting a connection with approximate
message passing algorithms, we show how to improve upon Gaussian performance
for the compression of sparse data: adding a denoising function to a shallow
architecture already reduces the loss provably, and a suitable multi-layer
decoder leads to a further improvement. We validate our findings on image
datasets, such as CIFAR-10 and MNIST.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05014" title="Abstract">arXiv:2402.05014</a> [<a href="/pdf/2402.05014" title="Download PDF">pdf</a>, <a href="/format/2402.05014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When the Body Became Data: Historical Data Cultures and Anatomical  Illustration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Correll%2C+M">Michael Correll</a>, 
<a href="/search/cs?searchtype=author&query=Garrison%2C+L+A">Laura A. Garrison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">With changing attitudes around knowledge, medicine, art, and technology, the
human body has become a source of information and, ultimately, shareable and
analyzable data. Centuries of illustrations and visualizations of the body
occur within particular historical, social, and political contexts. These
contexts are enmeshed in different so-called data cultures: ways that data,
knowledge, and information are conceptualized and collected, structured and
shared. In this work, we explore how information about the body was collected
as well as the circulation, impact, and persuasive force of the resulting
images. We show how mindfulness of data cultural influences remain crucial for
today's designers, researchers, and consumers of visualizations. We conclude
with a call for the field to reflect on how visualizations are not timeless and
contextless mirrors on objective data, but as much a product of our time and
place as the visualizations of the past.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05015" title="Abstract">arXiv:2402.05015</a> [<a href="/pdf/2402.05015" title="Download PDF">pdf</a>, <a href="/format/2402.05015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sober Look at LLMs for Material Discovery: Are They Actually Good for  Bayesian Optimization Over Molecules?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kristiadi%2C+A">Agustinus Kristiadi</a>, 
<a href="/search/cs?searchtype=author&query=Strieth-Kalthoff%2C+F">Felix Strieth-Kalthoff</a>, 
<a href="/search/cs?searchtype=author&query=Skreta%2C+M">Marta Skreta</a>, 
<a href="/search/cs?searchtype=author&query=Poupart%2C+P">Pascal Poupart</a>, 
<a href="/search/cs?searchtype=author&query=Aspuru-Guzik%2C+A">Al&#xe1;n Aspuru-Guzik</a>, 
<a href="/search/cs?searchtype=author&query=Pleiss%2C+G">Geoff Pleiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Automation is one of the cornerstones of contemporary material discovery.
Bayesian optimization (BO) is an essential part of such workflows, enabling
scientists to leverage prior domain knowledge into efficient exploration of a
large molecular space. While such prior knowledge can take many forms, there
has been significant fanfare around the ancillary scientific knowledge
encapsulated in large language models (LLMs). However, existing work thus far
has only explored LLMs for heuristic materials searches. Indeed, recent work
obtains the uncertainty estimate -- an integral part of BO -- from
point-estimated, non-Bayesian LLMs. In this work, we study the question of
whether LLMs are actually useful to accelerate principled Bayesian optimization
in the molecular space. We take a sober, dispassionate stance in answering this
question. This is done by carefully (i) viewing LLMs as fixed feature
extractors for standard but principled BO surrogate models and by (ii)
leveraging parameter-efficient finetuning methods and Bayesian neural networks
to obtain the posterior of the LLM surrogate. Our extensive experiments with
real-world chemistry problems show that LLMs can be useful for BO over
molecules, but only if they have been pretrained or finetuned with
domain-specific data.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05024" title="Abstract">arXiv:2402.05024</a> [<a href="/pdf/2402.05024" title="Download PDF">pdf</a>, <a href="/format/2402.05024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does the Use of Unusual Combinations of Datasets Contribute to Greater  Scientific Impact?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yulin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+D+M">Daniel M. Romero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI); General Economics (econ.GN)

</div>
<p class="mathjax">Scientific datasets play a crucial role in contemporary data-driven research,
as they allow for the progress of science by facilitating the discovery of new
patterns and phenomena. This mounting demand for empirical research raises
important questions on how strategic data utilization in research projects can
stimulate scientific advancement. In this study, we examine the hypothesis
inspired by the recombination theory, which suggests that innovative
combinations of existing knowledge, including the use of unusual combinations
of datasets, can lead to high-impact discoveries. We investigate the scientific
outcomes of such atypical data combinations in more than 30,000 publications
that leverage over 6,000 datasets curated within one of the largest social
science databases, ICPSR. This study offers four important insights. First,
combining datasets, particularly those infrequently paired, significantly
contributes to both scientific and broader impacts (e.g., dissemination to the
general public). Second, the combination of datasets with atypically combined
topics has the opposite effect -- the use of such data is associated with fewer
citations. Third, younger and less experienced research teams tend to use
atypical combinations of datasets in research at a higher frequency than their
older and more experienced counterparts. Lastly, despite the benefits of data
combination, papers that amalgamate data remain infrequent. This finding
suggests that the unconventional combination of datasets is an under-utilized
but powerful strategy correlated with the scientific and broader impact of
scientific discoveries.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05025" title="Abstract">arXiv:2402.05025</a> [<a href="/pdf/2402.05025" title="Download PDF">pdf</a>, <a href="/format/2402.05025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong convexity-guided hyper-parameter optimization for flatter losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yedida%2C+R">Rahul Yedida</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Snehanshu Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We propose a novel white-box approach to hyper-parameter optimization.
Motivated by recent work establishing a relationship between flat minima and
generalization, we first establish a relationship between the strong convexity
of the loss and its flatness. Based on this, we seek to find hyper-parameter
configurations that improve flatness by minimizing the strong convexity of the
loss. By using the structure of the underlying neural network, we derive
closed-form equations to approximate the strong convexity parameter, and
attempt to find hyper-parameters that minimize it in a randomized fashion.
Through experiments on 14 classification datasets, we show that our method
achieves strong performance at a fraction of the runtime.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05027" title="Abstract">arXiv:2402.05027</a> [<a href="/pdf/2402.05027" title="Download PDF">pdf</a>, <a href="/format/2402.05027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs  with Recurrent Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weil%2C+J">Jannis Weil</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Z">Zhenghua Bao</a>, 
<a href="/search/cs?searchtype=author&query=Abboud%2C+O">Osama Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Meuser%2C+T">Tobias Meuser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAMAS 2024, version with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph-based environments pose unique challenges to multi-agent reinforcement
learning. In decentralized approaches, agents operate within a given graph and
make decisions based on partial or outdated observations. The size of the
observed neighborhood limits the generalizability to different graphs and
affects the reactivity of agents, the quality of the selected actions, and the
communication overhead. This work focuses on generalizability and resolves the
trade-off in observed neighborhood size with a continuous information flow in
the whole graph. We propose a recurrent message-passing model that iterates
with the environment's steps and allows nodes to create a global representation
of the graph by exchanging messages with their neighbors. Agents receive the
resulting learned graph observations based on their location in the graph. Our
approach can be used in a decentralized manner at runtime and in combination
with a reinforcement learning algorithm of choice. We evaluate our method
across 1000 diverse graphs in the context of routing in communication networks
and find that it enables agents to generalize and adapt to changes in the
graph.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05028" title="Abstract">arXiv:2402.05028</a> [<a href="/pdf/2402.05028" title="Download PDF">pdf</a>, <a href="/format/2402.05028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community detection problem based on polarization measures:an  application to Twitter: the COVID-19 case in Spain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+I">Inmaculada Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+J+A">Juan Antonio Guevara</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+D">Daniel G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+J">Javier Castro</a>, 
<a href="/search/cs?searchtype=author&query=Esp%C3%ADnola%2C+R">Rosa Esp&#xed;nola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics, 2021, 9, 443
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Statistics Theory (math.ST); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In this paper, we address one of the most important topics in the field of
Social Networks Analysis: the community detection problem with additional
information. That additional information is modeled by a fuzzy measure that
represents the risk of polarization. Particularly, we are interested in dealing
with the problem of taking into account the polarization of nodes in the
community detection problem. Adding this type of information to the community
detection problem makes it more realistic, as a community is more likely to be
defined if the corresponding elements are willing to maintain a peaceful
dialogue. The polarization capacity is modeled by a fuzzy measure based on the
JDJpol measure of polarization related to two poles. We also present an
efficient algorithm for finding groups whose elements are no polarized.
Hereafter, we work in a real case. It is a network obtained from Twitter,
concerning the political position against the Spanish government taken by
several influential users. We analyze how the partitions obtained change when
some additional information related to how polarized that society is, is added
to the problem.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05029" title="Abstract">arXiv:2402.05029</a> [<a href="/pdf/2402.05029" title="Download PDF">pdf</a>, <a href="/format/2402.05029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Population Exposure to Long-term PM10: A City-wide  Agent-based Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Hyesop Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This study evaluates the health effects of long-term exposure to PM10 in
Seoul. Building on the preliminary model Shin and Bithell (2019), an in-silico
agent-based model (ABM) is used to simulate the travel patterns of individuals
according to their origins and destinations. During the simulation, each
person, with their inherent socio-economic attributes and allocated origin and
destination location, is assumed to commute to and from the same places for 10
consecutive years. A nominal measure of their health is set to decrease
whenever the concentration of PM10 exceeds the national standard. Sensitivity
analysis on calibrated parameters reveals increased vulnerability among certain
demographic groups, particularly those aged over 65 and under 15, with a
significant health decline associated with road proximity. The study reveals a
substantial health disparity after 7,000 simulation ticks (equivalent to 10
years), especially under scenarios of a 3% annual increase in pollution levels.
Long-term exposure to PM10 has a significant impact on health vulnerabilities,
despite initial resilience being minimal. The study emphasises the importance
of future research that takes into account different pollution thresholds as
well as more detailed models of population dynamics and pollution generation in
order to better understand and mitigate the health effects of air pollution on
diverse urban populations.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05033" title="Abstract">arXiv:2402.05033</a> [<a href="/pdf/2402.05033" title="Download PDF">pdf</a>, <a href="/format/2402.05033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulated Overparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzawi%2C+H">Hanna Mazzawi</a>, 
<a href="/search/cs?searchtype=author&query=Awasthi%2C+P">Pranjal Awasthi</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalvo%2C+X">Xavi Gonzalvo</a>, 
<a href="/search/cs?searchtype=author&query=Ramalingam%2C+S">Srikumar Ramalingam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we introduce a novel paradigm called Simulated
Overparametrization (SOP). SOP merges the computational efficiency of compact
models with the advanced learning proficiencies of overparameterized models.
SOP proposes a unique approach to model training and inference, where a model
with a significantly larger number of parameters is trained in such a way that
a smaller, efficient subset of these parameters is used for the actual
computation during inference. Building upon this framework, we present a novel,
architecture agnostic algorithm called "majority kernels", which seamlessly
integrates with predominant architectures, including Transformer models.
Majority kernels enables the simulated training of overparameterized models,
resulting in performance gains across architectures and tasks. Furthermore, our
approach adds minimal overhead to the cost incurred (wall clock time) at
training time. The proposed approach shows strong performance on a wide variety
of datasets and models, even outperforming strong baselines such as
combinatorial optimization methods based on submodular optimization.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05034" title="Abstract">arXiv:2402.05034</a> [<a href="/pdf/2402.05034" title="Download PDF">pdf</a>, <a href="/format/2402.05034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How BERT Speaks Shakespearean English? Evaluating Historical Bias in  Contextual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cuscito%2C+M">Miriam Cuscito</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+A">Alfio Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Ruskov%2C+M">Martin Ruskov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper, we explore the idea of analysing the historical bias of
contextual language models based on BERT by measuring their adequacy with
respect to Early Modern (EME) and Modern (ME) English. In our preliminary
experiments, we perform fill-in-the-blank tests with 60 masked sentences (20
EME-specific, 20 ME-specific and 20 generic) and three different models (i.e.,
BERT Base, MacBERTh, English HLM). We then rate the model predictions according
to a 5-point bipolar scale between the two language varieties and derive a
weighted score to measure the adequacy of each model to EME and ME varieties of
English.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05035" title="Abstract">arXiv:2402.05035</a> [<a href="/pdf/2402.05035" title="Download PDF">pdf</a>, <a href="/format/2402.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Domain Generalization for Medical Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Ziwei Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Shuyi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shiao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lanfen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCAI 2024, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Medical Image Analysis (MedIA) has emerged as a crucial tool in
computer-aided diagnosis systems, particularly with the advancement of deep
learning (DL) in recent years. However, well-trained deep models often
experience significant performance degradation when deployed in different
medical sites, modalities, and sequences, known as a domain shift issue. In
light of this, Domain Generalization (DG) for MedIA aims to address the domain
shift challenge by generalizing effectively and performing robustly across
unknown data distributions. This paper presents the a comprehensive review of
substantial developments in this area. First, we provide a formal definition of
domain shift and domain generalization in medical field, and discuss several
related settings. Subsequently, we summarize the recent methods from three
viewpoints: data manipulation level, feature representation level, and model
training level, and present some algorithms in detail for each viewpoints.
Furthermore, we introduce the commonly used datasets. Finally, we summarize
existing literature and present some potential research topics for the future.
For this survey, we also created a GitHub project by collecting the supporting
resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05037" title="Abstract">arXiv:2402.05037</a> [<a href="/pdf/2402.05037" title="Download PDF">pdf</a>, <a href="/format/2402.05037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth real-time motion planning based on a cascade dual-quaternion  screw-geometry MPC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teimoorzadeh%2C+A">Ainoor Teimoorzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+F+F+A">Frederico Fernandes Afonso Silva</a>, 
<a href="/search/cs?searchtype=author&query=Figueredo%2C+L+F+C">Luis F.C. Figueredo</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper investigates the tracking problem of a smooth coordinate-invariant
trajectory using dual quaternion algebra. The proposed architecture consists of
a cascade structure in which the outer-loop MPC performs real-time smoothing of
the manipulator's end-effector twist while an inner-loop kinematic controller
ensures tracking of the instantaneous desired end-effector pose. Experiments on
a $7$-DoF Franka Emika Panda robotic manipulator validate the proposed method
demonstrating its application to constraint the robot twists, accelerations and
jerks within prescribed bounds.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05039" title="Abstract">arXiv:2402.05039</a> [<a href="/pdf/2402.05039" title="Download PDF">pdf</a>, <a href="/format/2402.05039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC Learnability under Explanation-Preserving Graph Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shirani%2C+F">Farhad Shirani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianchun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shouwei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wenqian Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graphical models capture relations between entities in a wide range of
applications including social networks, biology, and natural language
processing, among others. Graph neural networks (GNN) are neural models that
operate over graphs, enabling the model to leverage the complex relationships
and dependencies in graph-structured data. A graph explanation is a subgraph
which is an `almost sufficient' statistic of the input graph with respect to
its classification label. Consequently, the classification label is invariant,
with high probability, to perturbations of graph edges not belonging to its
explanation subgraph. This work considers two methods for leveraging such
perturbation invariances in the design and training of GNNs. First,
explanation-assisted learning rules are considered. It is shown that the sample
complexity of explanation-assisted learning can be arbitrarily smaller than
explanation-agnostic learning. Next, explanation-assisted data augmentation is
considered, where the training set is enlarged by artificially producing new
training samples via perturbation of the non-explanation edges in the original
training set. It is shown that such data augmentation methods may improve
performance if the augmented data is in-distribution, however, it may also lead
to worse sample complexity compared to explanation-agnostic learning rules if
the augmented data is out-of-distribution. Extensive empirical evaluations are
provided to verify the theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05042" title="Abstract">arXiv:2402.05042</a> [<a href="/pdf/2402.05042" title="Download PDF">pdf</a>, <a href="/format/2402.05042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sticky Fingers: Resilience of Satellite Fingerprinting against Jamming  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smailes%2C+J">Joshua Smailes</a>, 
<a href="/search/cs?searchtype=author&query=Salkield%2C+E">Edd Salkield</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6hler%2C+S">Sebastian K&#xf6;hler</a>, 
<a href="/search/cs?searchtype=author&query=Birnbach%2C+S">Simon Birnbach</a>, 
<a href="/search/cs?searchtype=author&query=Strohmeier%2C+M">Martin Strohmeier</a>, 
<a href="/search/cs?searchtype=author&query=Martinovic%2C+I">Ivan Martinovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the wake of increasing numbers of attacks on radio communication systems,
a range of techniques are being deployed to increase the security of these
systems. One such technique is radio fingerprinting, in which the transmitter
can be identified and authenticated by observing small hardware differences
expressed in the signal. Fingerprinting has been explored in particular in the
defense of satellite systems, many of which are insecure and cannot be
retrofitted with cryptographic security.
<br />In this paper, we evaluate the effectiveness of radio fingerprinting
techniques under interference and jamming attacks, usually intended to deny
service. By taking a pre-trained fingerprinting model and gathering a new
dataset in which different levels of Gaussian noise and tone jamming have been
added to the legitimate signal, we assess the attacker power required in order
to disrupt the transmitter fingerprint such that it can no longer be
recognized. We compare this to Gaussian jamming on the data portion of the
signal, obtaining the remarkable result that transmitter fingerprints are still
recognizable even in the presence of moderate levels of noise. Through deeper
analysis of the results, we conclude that it takes a similar amount of jamming
power in order to disrupt the fingerprint as it does to jam the message
contents itself, so it is safe to include a fingerprinting system to
authenticate satellite communication without opening up the system to easier
denial-of-service attacks.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05044" title="Abstract">arXiv:2402.05044</a> [<a href="/pdf/2402.05044" title="Download PDF">pdf</a>, <a href="/format/2402.05044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bowen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuhao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jing Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the rapidly evolving landscape of Large Language Models (LLMs), ensuring
robust safety measures is paramount. To meet this crucial need, we propose
\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating
LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench
transcends conventional benchmarks through its large scale, rich diversity,
intricate taxonomy spanning three levels, and versatile
functionalities.SALAD-Bench is crafted with a meticulous array of questions,
from standard queries to complex ones enriched with attack, defense
modifications and multiple-choice. To effectively manage the inherent
complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for
QA pairs with a particular focus on attack-enhanced queries, ensuring a
seamless, and reliable evaluation. Above components extend SALAD-Bench from
standard LLM safety evaluation to both LLM attack and defense methods
evaluation, ensuring the joint-purpose utility. Our extensive experiments shed
light on the resilience of LLMs against emerging threats and the efficacy of
contemporary defense tactics. Data and evaluator are released under
\url{https://github.com/OpenSafetyLab/SALAD-BENCH}. Warning: this paper
includes examples that may be offensive or harmful.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05045" title="Abstract">arXiv:2402.05045</a> [<a href="/pdf/2402.05045" title="Download PDF">pdf</a>, <a href="/format/2402.05045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Multi-Resolution Fusion for Remote Sensing Data with Label  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vakharia%2C+H">Hersh Vakharia</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaoxiao Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, 2 tables; Accepted to International Geoscience and Remote Sensing Symposium (IGARSS) 2023; Code available at <a href="https://github.com/hvak/MIMRF-BFM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modal sensor data fusion takes advantage of complementary or
reinforcing information from each sensor and can boost overall performance in
applications such as scene classification and target detection. This paper
presents a new method for fusing multi-modal and multi-resolution remote sensor
data without requiring pixel-level training labels, which can be difficult to
obtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion
(MIMRF) framework that addresses label uncertainty for fusion, but it can be
slow to train due to the large search space for the fuzzy measures used to
integrate sensor data sources. We propose a new method based on binary fuzzy
measures, which reduces the search space and significantly improves the
efficiency of the MIMRF framework. We present experimental results on synthetic
data and a real-world remote sensing detection task and show that the proposed
MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution
fusion given remote sensing data with uncertainty.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05048" title="Abstract">arXiv:2402.05048</a> [<a href="/pdf/2402.05048" title="Download PDF">pdf</a>, <a href="/format/2402.05048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How VADER is your AI? Towards a definition of artificial intelligence  systems appropriate for regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bezerra%2C+L+C+T">Leonardo C. T. Bezerra</a>, 
<a href="/search/cs?searchtype=author&query=Brownlee%2C+A+E+I">Alexander E. I. Brownlee</a>, 
<a href="/search/cs?searchtype=author&query=Alvarenga%2C+L+F">Luana Ferraz Alvarenga</a>, 
<a href="/search/cs?searchtype=author&query=Moioli%2C+R+C">Renan Cipriano Moioli</a>, 
<a href="/search/cs?searchtype=author&query=Batista%2C+T+V">Thais Vasconcelos Batista</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Artificial intelligence (AI) has driven many information and communication
technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has
expanded far beyond AI since the Turing test proposal. Critically, recent AI
regulation proposals adopt AI definitions affecting ICT techniques, approaches,
and systems that are not AI. In some cases, even works from mathematics,
statistics, and engineering would be affected. Worryingly, AI misdefinitions
are observed from Western societies to the Global South. In this paper, we
propose a framework to score how \textit{validated as appropriately-defined for
regulation} (VADER) an AI definition is. Our online, publicly-available VADER
framework scores the coverage of premises that should underlie AI definitions
for regulation, which aim to (i) reproduce principles observed in other
successful technology regulations, and (ii) include all AI techniques and
approaches while excluding non-AI works. Regarding the latter, our score is
based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We
demonstrate our contribution by reviewing the AI regulation proposals of key
players, namely the United States, United Kingdom, European Union, and Brazil.
Importantly, none of the proposals assessed achieve the appropriateness score,
ranging from a revision need to a concrete risk to ICT systems and works from
other fields.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05050" title="Abstract">arXiv:2402.05050</a> [<a href="/pdf/2402.05050" title="Download PDF">pdf</a>, <a href="/format/2402.05050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning Can Find Friends That Are Beneficial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tupitsa%2C+N">Nazarii Tupitsa</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1th%2C+S">Samuel Horv&#xe1;th</a>, 
<a href="/search/cs?searchtype=author&query=Tak%C3%A1%C4%8D%2C+M">Martin Tak&#xe1;&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Gorbunov%2C+E">Eduard Gorbunov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In Federated Learning (FL), the distributed nature and heterogeneity of
client data present both opportunities and challenges. While collaboration
among clients can significantly enhance the learning process, not all
collaborations are beneficial; some may even be detrimental. In this study, we
introduce a novel algorithm that assigns adaptive aggregation weights to
clients participating in FL training, identifying those with data distributions
most conducive to a specific learning objective. We demonstrate that our
aggregation method converges no worse than the method that aggregates only the
updates received from clients with the same data distribution. Furthermore,
empirical evaluations consistently reveal that collaborations guided by our
algorithm outperform traditional FL approaches. This underscores the critical
role of judicious client selection and lays the foundation for more streamlined
and effective FL implementations in the coming years.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05052" title="Abstract">arXiv:2402.05052</a> [<a href="/pdf/2402.05052" title="Download PDF">pdf</a>, <a href="/format/2402.05052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Representation Learning from Multiple Distributions: A General  Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shaoan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+I">Ignavier Ng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yujia Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In many problems, the measured variables (e.g., image pixels) are just
mathematical functions of the hidden causal variables (e.g., the underlying
concepts or objects). For the purpose of making predictions in changing
environments or making proper changes to the system, it is helpful to recover
the hidden causal variables $Z_i$ and their causal relations represented by
graph $\mathcal{G}_Z$. This problem has recently been known as causal
representation learning. This paper is concerned with a general, completely
nonparametric setting of causal representation learning from multiple
distributions (arising from heterogeneous data or nonstationary time series),
without assuming hard interventions behind distribution changes. We aim to
develop general solutions in this fundamental case; as a by product, this helps
see the unique benefit offered by other assumptions such as parametric causal
models or hard interventions. We show that under the sparsity constraint on the
recovered graph over the latent variables and suitable sufficient change
conditions on the causal influences, interestingly, one can recover the
moralized graph of the underlying directed acyclic graph, and the recovered
latent variables and their relations are related to the underlying causal model
in a specific, nontrivial way. In some cases, each latent variable can even be
recovered up to component-wise transformations. Experimental results verify our
theoretical claims.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05054" title="Abstract">arXiv:2402.05054</a> [<a href="/pdf/2402.05054" title="Download PDF">pdf</a>, <a href="/format/2402.05054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content  Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiaxiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaokang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+G">Gang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://me.kiui.moe/lgm/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D content creation has achieved significant progress in terms of both
quality and speed. Although current feed-forward models can produce 3D objects
in seconds, their resolution is constrained by the intensive computation
required during training. In this paper, we introduce Large Multi-View Gaussian
Model (LGM), a novel framework designed to generate high-resolution 3D models
from text prompts or single-view images. Our key insights are two-fold: 1) 3D
Representation: We propose multi-view Gaussian features as an efficient yet
powerful representation, which can then be fused together for differentiable
rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput
backbone operating on multi-view images, which can be produced from text or
single-view image input by leveraging multi-view diffusion models. Extensive
experiments demonstrate the high fidelity and efficiency of our approach.
Notably, we maintain the fast speed to generate 3D objects within 5 seconds
while boosting the training resolution to 512, thereby achieving
high-resolution 3D content generation.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05057" title="Abstract">arXiv:2402.05057</a> [<a href="/pdf/2402.05057" title="Download PDF">pdf</a>, <a href="/format/2402.05057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Integrity Constraints in Incomplete Databases With Limited  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-atar%2C+M">Munqath Al-atar</a>, 
<a href="/search/cs?searchtype=author&query=Sali%2C+A">Attila Sali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">In case of incomplete database tables, a possible world is obtained by
replacing any missing value by a value from the corresponding attribute's
domain that can be infinite. A possible key or possible functional dependency
constraint is satisfied by an incomplete table if we can obtain a possible
world that satisfies the given key or functional dependency. On the other hand,
a certain key or certain functional dependency holds if all possible worlds
satisfy the constraint, A strongly possible constraint is an intermediate
concept between possible and certain constraints, based on the strongly
possible world approach (a strongly possible world is obtained by replacing
\nul's by a value from the ones appearing in the corresponding attribute of the
table). A strongly possible key or functional dependency holds in an incomplete
table if there exists a strongly possible world that satisfies the given
constraint. In the present paper, we introduce strongly possible versions of
multivalued dependencies and cross joins, and we analyse the complexity of
checking the validity of a given strongly possible cross joins. We also study
approximation measures of strongly possible keys (spKeys), functional
dependencies (spFDs), multivalued dependencies (spMVDs) and cross joins
(spCJs). We also treat complexity questions of determination of the
approximation values.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05064" title="Abstract">arXiv:2402.05064</a> [<a href="/pdf/2402.05064" title="Download PDF">pdf</a>, <a href="/format/2402.05064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning the feedback controller gains is a simple way to improve  autonomous driving performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liang%2C+W">Wenyu Liang</a>, 
<a href="/search/eess?searchtype=author&query=Baldivieso%2C+P+R">Pablo R. Baldivieso</a>, 
<a href="/search/eess?searchtype=author&query=Drummond%2C+R">Ross Drummond</a>, 
<a href="/search/eess?searchtype=author&query=Shin%2C+D">Donghwan Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Typical autonomous driving systems are a combination of machine learning
algorithms (often involving neural networks) and classical feedback
controllers. Whilst significant progress has been made in recent years on the
neural network side of these systems, only limited progress has been made on
the feedback controller side. Often, the feedback control gains are simply
passed from paper to paper with little re-tuning taking place, even though the
changes to the neural networks can alter the vehicle's closed loop dynamics.
The aim of this paper is to highlight the limitations of this approach; it is
shown that re-tuning the feedback controller can be a simple way to improve
autonomous driving performance. To demonstrate this, the PID gains of the
longitudinal controller in the TCP autonomous vehicle algorithm are tuned. This
causes the driving score in CARLA to increase from 73.21 to 77.38, with the
results averaged over 16 driving scenarios. Moreover, it was observed that the
performance benefits were most apparent during challenging driving scenarios,
such as during rain or night time, as the tuned controller led to a more
assertive driving style. These results demonstrate the value of developing both
the neural network and feedback control policies of autonomous driving systems
simultaneously, as this can be a simple and methodical way to improve
autonomous driving system performance and robustness.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05066" title="Abstract">arXiv:2402.05066</a> [<a href="/pdf/2402.05066" title="Download PDF">pdf</a>, <a href="/format/2402.05066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration Without Maps via Zero-Shot Out-of-Distribution Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivashangaran%2C+S">Shathushan Sivashangaran</a>, 
<a href="/search/cs?searchtype=author&query=Khairnar%2C+A">Apoorva Khairnar</a>, 
<a href="/search/cs?searchtype=author&query=Eskandarian%2C+A">Azim Eskandarian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Operation of Autonomous Mobile Robots (AMRs) of all forms that include
wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS
denied environments without a-priori maps, exclusively using onboard sensors,
is an unsolved problem that has potential to transform the economy, and vastly
improve humanity's capabilities with improvements to agriculture,
manufacturing, disaster response, military and space exploration. Conventional
AMR automation approaches are modularized into perception, motion planning and
control which is computationally inefficient, and requires explicit feature
extraction and engineering, that inhibits generalization, and deployment at
scale. Few works have focused on real-world end-to-end approaches that directly
map sensor inputs to control outputs due to the large amount of well curated
training data required for supervised Deep Learning (DL) which is time
consuming and labor intensive to collect and label, and sample inefficiency and
challenges to bridging the simulation to reality gap using Deep Reinforcement
Learning (DRL). This paper presents a novel method to efficiently train DRL for
robust end-to-end AMR exploration, in a constrained environment at physical
limits in simulation, transferred zero-shot to the real-world. The
representation learned in a compact parameter space with 2 fully connected
layers with 64 nodes each is demonstrated to exhibit emergent behavior for
out-of-distribution generalization to navigation in new environments that
include unstructured terrain without maps, and dynamic obstacle avoidance. The
learned policy outperforms conventional navigation algorithms while consuming a
fraction of the computation resources, enabling execution on a range of AMR
forms with varying embedded computer payloads.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05070" title="Abstract">arXiv:2402.05070</a> [<a href="/pdf/2402.05070" title="Download PDF">pdf</a>, <a href="/format/2402.05070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Roadmap to Pluralistic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sorensen%2C+T">Taylor Sorensen</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+J">Jared Moore</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+J">Jillian Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+M">Mitchell Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Rytting%2C+C+M">Christopher Michael Rytting</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+A">Andre Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Althoff%2C+T">Tim Althoff</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">With increased power and prevalence of AI systems, it is ever more critical
that AI systems are designed to serve all, i.e., people with diverse values and
perspectives. However, aligning models to serve pluralistic human values
remains an open research question. In this piece, we propose a roadmap to
pluralistic alignment, specifically using language models as a test bed. We
identify and formalize three possible ways to define and operationalize
pluralism in AI systems: 1) Overton pluralistic models that present a spectrum
of reasonable responses; 2) Steerably pluralistic models that can steer to
reflect certain perspectives; and 3) Distributionally pluralistic models that
are well-calibrated to a given population in distribution. We also propose and
formalize three possible classes of pluralistic benchmarks: 1) Multi-objective
benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to
steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which
explicitly model diverse human ratings. We use this framework to argue that
current alignment techniques may be fundamentally limited for pluralistic AI;
indeed, we highlight empirical evidence, both from our own experiments and from
other work, that standard alignment procedures might reduce distributional
pluralism in models, motivating the need for further research on pluralistic
alignment.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05073" title="Abstract">arXiv:2402.05073</a> [<a href="/pdf/2402.05073" title="Download PDF">pdf</a>, <a href="/format/2402.05073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NITO: Neural Implicit Fields for Resolution-free Topology Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nobari%2C+A+H">Amin Heyrani Nobari</a>, 
<a href="/search/cs?searchtype=author&query=Giannone%2C+G">Giorgio Giannone</a>, 
<a href="/search/cs?searchtype=author&query=Regenwetter%2C+L">Lyle Regenwetter</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Topology optimization is a critical task in engineering design, where the
goal is to optimally distribute material in a given space for maximum
performance. We introduce Neural Implicit Topology Optimization (NITO), a novel
approach to accelerate topology optimization problems using deep learning. NITO
stands out as one of the first frameworks to offer a resolution-free and
domain-agnostic solution in deep learning-based topology optimization. NITO
synthesizes structures with up to seven times better structural efficiency
compared to SOTA diffusion models and does so in a tenth of the time. In the
NITO framework, we introduce a novel method, the Boundary Point Order-Invariant
MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic
manner, moving away from expensive simulation-based approaches. Crucially, NITO
circumvents the domain and resolution limitations that restrict Convolutional
Neural Network (CNN) models to a structured domain of fixed size -- limitations
that hinder the widespread adoption of CNNs in engineering applications. This
generalizability allows a single NITO model to train and generate solutions in
countless domains, eliminating the need for numerous domain-specific CNNs and
their extensive datasets. Despite its generalizability, NITO outperforms SOTA
models even in specialized tasks, is an order of magnitude smaller, and is
practically trainable at high resolutions that would be restrictive for CNNs.
This combination of versatility, efficiency, and performance underlines NITO's
potential to transform the landscape of engineering design optimization
problems through implicit fields.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05075" title="Abstract">arXiv:2402.05075</a> [<a href="/pdf/2402.05075" title="Download PDF">pdf</a>, <a href="/format/2402.05075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARCollab: Towards Multi-User Interactive Cardiovascular Surgical  Planning in Mobile Augmented Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehta%2C+P">Pratham Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Karanth%2C+H">Harsha Karanth</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haoyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Slesnick%2C+T">Timothy Slesnick</a>, 
<a href="/search/cs?searchtype=author&query=Shaw%2C+F">Fawwaz Shaw</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Surgical planning for congenital heart diseases requires a collaborative
approach, traditionally involving the 3D-printing of physical heart models for
inspection by surgeons and cardiologists. Recent advancements in mobile
augmented reality (AR) technologies have offered a promising alternative, noted
for their ease-of-use and portability. Despite this progress, there remains a
gap in research exploring the use of multi-user mobile AR environments for
facilitating collaborative cardiovascular surgical planning. We are developing
ARCollab, an iOS AR application designed to allow multiple surgeons and
cardiologists to interact with patient-specific 3D heart models in a shared
environment. ARCollab allows surgeons and cardiologists to import heart models,
perform gestures to manipulate the heart, and collaborate with other users
without having to produce a physical heart model. We are excited by the
potential for ARCollab to make long-term real-world impact, thanks to the
ubiquity of iOS devices that will allow for ARCollab's easy distribution,
deployment and adoption.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05076" title="Abstract">arXiv:2402.05076</a> [<a href="/pdf/2402.05076" title="Download PDF">pdf</a>, <a href="/ps/2402.05076" title="Download PostScript">ps</a>, <a href="/format/2402.05076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markovian Analysis of Information Cascades with Fake Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuming Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">People often learn from other's actions when they make decisions while doing
online shopping. This kind of observational learning may lead to information
cascades, which means agents might ignore their own signals and follow the
'trend' created collectively by the actions of their predecessors. It is
well-known that with rational agents, such a cascade model can result in either
correct or incorrect cascades. In this paper, we additionally consider the
presence of fake agents who always take fixed actions and we investigate their
influence on the outcome of these cascades. We propose an infinite Markov Chain
sequence structure and a tree structure to analyze how the fraction and the
type of such fake agents impacts behavior of the upcoming agents. We show that
an increase in the fraction of fake agents may reduce the chances of their
preferred outcome, and also there is a certain lower bound for the probability
of a wrong cascade. In particular, we discuss the probability of an agent being
fake tends to 1 and the effect of a constant portion of fake agents.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05090" title="Abstract">arXiv:2402.05090</a> [<a href="/pdf/2402.05090" title="Download PDF">pdf</a>, <a href="/format/2402.05090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Based Augmentation to Address Shortcut Learning in Object Goal  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoftijzer%2C+D">Dennis Hoftijzer</a>, 
<a href="/search/cs?searchtype=author&query=Burghouts%2C+G">Gertjan Burghouts</a>, 
<a href="/search/cs?searchtype=author&query=Spreeuwers%2C+L">Luuk Spreeuwers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, to be published in IEEE IRC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep Reinforcement Learning (DRL) has shown great potential in enabling
robots to find certain objects (e.g., `find a fridge') in environments like
homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL
methods are predominantly trained and evaluated using environment simulators.
Although DRL has shown impressive results, the simulators may be biased or
limited. This creates a risk of shortcut learning, i.e., learning a policy
tailored to specific visual details of training environments. We aim to deepen
our understanding of shortcut learning in ObjectNav, its implications and
propose a solution. We design an experiment for inserting a shortcut bias in
the appearance of training environments. As a proof-of-concept, we associate
room types to specific wall colors (e.g., bedrooms with green walls), and
observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to
environments where this is not the case (e.g., bedrooms with blue walls). We
find that shortcut learning is the root cause: the agent learns to navigate to
target objects, by simply searching for the associated wall color of the target
object's room. To solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature space of a
Vision-Language Model (VLM) to augment visual representations directly at the
feature-level, requiring no changes to the simulator, and only an addition of
one layer to the model. Where the SOTA ObjectNav method's success rate drops
69%, our proposal has only a drop of 23%.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05098" title="Abstract">arXiv:2402.05098</a> [<a href="/pdf/2402.05098" title="Download PDF">pdf</a>, <a href="/format/2402.05098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On diffusion models for amortized inference: Benchmarking and improving  stochastic control and sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sendera%2C+M">Marcin Sendera</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sarthak Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/cs?searchtype=author&query=Scimeca%2C+L">Luca Scimeca</a>, 
<a href="/search/cs?searchtype=author&query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+A">Alexandre Adam</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages; code: <a href="https://github.com/GFNOrg/gfn-diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of training diffusion models to sample from a
distribution with a given unnormalized density or energy function. We benchmark
several diffusion-structured inference methods, including simulation-based
variational approaches and off-policy methods (continuous generative flow
networks). Our results shed light on the relative advantages of existing
algorithms while bringing into question some claims from past work. We also
propose a novel exploration strategy for off-policy methods, based on local
search in the target space with the use of a replay buffer, and show that it
improves the quality of samples on a variety of target distributions. Our code
for the sampling methods and benchmarks studied is made public at
https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion
models for amortized inference.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05099" title="Abstract">arXiv:2402.05099</a> [<a href="/pdf/2402.05099" title="Download PDF">pdf</a>, <a href="/format/2402.05099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hydragen: High-Throughput LLM Inference with Shared Prefixes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juravsky%2C+J">Jordan Juravsky</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+B">Bradley Brown</a>, 
<a href="/search/cs?searchtype=author&query=Ehrlich%2C+R">Ryan Ehrlich</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Mirhoseini%2C+A">Azalia Mirhoseini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformer-based large language models (LLMs) are now deployed to hundreds
of millions of users. LLM inference is commonly performed on batches of
sequences that share a prefix, such as few-shot examples or a chatbot system
prompt. Decoding in this large-batch setting can be bottlenecked by the
attention operation, which reads large key-value (KV) caches from memory and
computes inefficient matrix-vector products for every sequence in the batch. In
this work, we introduce Hydragen, a hardware-aware exact implementation of
attention with shared prefixes. Hydragen computes attention over the shared
prefix and unique suffixes separately. This decomposition enables efficient
prefix attention by batching queries together across sequences, reducing
redundant memory reads and enabling the use of hardware-friendly matrix
multiplications. Our method can improve end-to-end LLM throughput by up to 32x
against competitive baselines, with speedup growing with the batch size and
shared prefix length. Hydragen also enables the use of very long shared
contexts: with a high batch size, increasing the prefix length from 1K to 16K
tokens decreases Hydragen throughput by less than 15%, while the throughput of
baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix
decomposition and can be applied to tree-based prompt sharing patterns,
allowing us to further reduce inference time on competitive programming
problems by 55%.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05102" title="Abstract">arXiv:2402.05102</a> [<a href="/pdf/2402.05102" title="Download PDF">pdf</a>, <a href="/format/2402.05102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Can REST Now: Automated Specification Inference and Black-Box  Testing of RESTful APIs with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decrop%2C+A">Alix Decrop</a>, 
<a href="/search/cs?searchtype=author&query=Perrouin%2C+G">Gilles Perrouin</a>, 
<a href="/search/cs?searchtype=author&query=Papadakis%2C+M">Mike Papadakis</a>, 
<a href="/search/cs?searchtype=author&query=Devroey%2C+X">Xavier Devroey</a>, 
<a href="/search/cs?searchtype=author&query=Schobbens%2C+P">Pierre-Yves Schobbens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">RESTful APIs are popular web services, requiring documentation to ease their
comprehension, reusability and testing practices. The OpenAPI Specification
(OAS) is a widely adopted and machine-readable format used to document such
APIs. However, manually documenting RESTful APIs is a time-consuming and
error-prone task, resulting in unavailable, incomplete, or imprecise
documentation. As RESTful API testing tools require an OpenAPI specification as
input, insufficient or informal documentation hampers testing quality.
<br />Recently, Large Language Models (LLMs) have demonstrated exceptional
abilities to automate tasks based on their colossal training data. Accordingly,
such capabilities could be utilized to assist the documentation and testing
process of RESTful APIs.
<br />In this paper, we present RESTSpecIT, the first automated RESTful API
specification inference and black-box testing approach leveraging LLMs. The
approach requires minimal user input compared to state-of-the-art RESTful API
inference and testing tools; Given an API name and an LLM key, HTTP requests
are generated and mutated with data returned by the LLM. By sending the
requests to the API endpoint, HTTP responses can be analyzed for inference and
testing purposes. RESTSpecIT utilizes an in-context prompt masking strategy,
requiring no model fine-tuning. Our evaluation demonstrates that RESTSpecIT is
capable of: (1) inferring specifications with 85.05% of GET routes and 81.05%
of query parameters found on average, (2) discovering undocumented and valid
routes and parameters, and (3) uncovering server errors in RESTful APIs.
Inferred specifications can also be used as testing tool inputs.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05106" title="Abstract">arXiv:2402.05106</a> [<a href="/pdf/2402.05106" title="Download PDF">pdf</a>, <a href="/format/2402.05106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image captioning for Brazilian Portuguese using GRIT model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Alencar%2C+R+S">Rafael Silva de Alencar</a>, 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1eda%2C+W+A+C">William Alberto Cruz Casta&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Amadeus%2C+M">Marcellus Amadeus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2207.09666">arXiv:2207.09666</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This work presents the early development of a model of image captioning for
the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based
Image captioning Transformer) model to accomplish this work. GRIT is a
Transformer-only neural architecture that effectively utilizes two visual
features to generate better captions. The GRIT method emerged as a proposal to
be a more efficient way to generate image captioning. In this work, we adapt
the GRIT model to be trained in a Brazilian Portuguese dataset to have an image
captioning method for the Brazilian Portuguese Language.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05109" title="Abstract">arXiv:2402.05109</a> [<a href="/pdf/2402.05109" title="Download PDF">pdf</a>, <a href="/format/2402.05109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ankner%2C+Z">Zachary Ankner</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+R">Rishab Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Nrusimha%2C+A">Aniruddha Nrusimha</a>, 
<a href="/search/cs?searchtype=author&query=Rinard%2C+C">Christopher Rinard</a>, 
<a href="/search/cs?searchtype=author&query=Ragan-Kelley%2C+J">Jonathan Ragan-Kelley</a>, 
<a href="/search/cs?searchtype=author&query=Brandon%2C+W">William Brandon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To combat the memory bandwidth-bound nature of autoregressive LLM inference,
previous research has proposed the speculative decoding framework. To perform
speculative decoding, a small draft model proposes candidate continuations of
the input sequence, that are then verified in parallel by the base model. One
way to specify the draft model, as used in the recent Medusa decoding
framework, is as a collection of light-weight heads, called draft heads, that
operate on the base model's hidden states. To date, all existing draft heads
have been sequentially independent, meaning that they speculate tokens in the
candidate continuation independently of any preceding tokens in the candidate
continuation. In this work, we propose Hydra heads, a sequentially dependent,
drop-in replacement for standard draft heads that significantly improves
speculation accuracy. Decoding with Hydra heads improves throughput compared to
Medusa decoding with standard draft heads. We further explore the design space
of Hydra head training objectives and architectures, and propose a
carefully-tuned Hydra head recipe, which we call Hydra++, that improves
decoding throughput by 1.31x and 2.71x compared to Medusa decoding and
autoregressive decoding, respectively. Overall, Hydra heads are a simple
intervention on standard draft heads that significantly improve the end-to-end
speed of draft head based speculative decoding.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05110" title="Abstract">arXiv:2402.05110</a> [<a href="/pdf/2402.05110" title="Download PDF">pdf</a>, <a href="/format/2402.05110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opening the AI black box: program synthesis via mechanistic  interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michaud%2C+E+J">Eric J. Michaud</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+I">Isaac Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lad%2C+V">Vedang Lad</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mudide%2C+A">Anish Mudide</a>, 
<a href="/search/cs?searchtype=author&query=Loughridge%2C+C">Chloe Loughridge</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z+C">Zifan Carl Guo</a>, 
<a href="/search/cs?searchtype=author&query=Kheirkhah%2C+T+R">Tara Rezaei Kheirkhah</a>, 
<a href="/search/cs?searchtype=author&query=Vukeli%C4%87%2C+M">Mateja Vukeli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present MIPS, a novel method for program synthesis based on automated
mechanistic interpretability of neural networks trained to perform the desired
task, auto-distilling the learned algorithm into Python code. We test MIPS on a
benchmark of 62 algorithmic tasks that can be learned by an RNN and find it
highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are
not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to
convert the RNN into a finite state machine, then applies Boolean or integer
symbolic regression to capture the learned algorithm. As opposed to large
language models, this program synthesis technique makes no use of (and is
therefore not limited by) human training data such as algorithms and code from
GitHub. We discuss opportunities and challenges for scaling up this approach to
make machine-learned models more interpretable and trustworthy.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05111" title="Abstract">arXiv:2402.05111</a> [<a href="/pdf/2402.05111" title="Download PDF">pdf</a>, <a href="/format/2402.05111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edu-ConvoKit: An Open-Source Library for Education Conversation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R+E">Rose E. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Demszky%2C+D">Dorottya Demszky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/stanfordnlp/edu-convokit">this https URL</a> <a href="https://edu-convokit.readthedocs.io/en/latest/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce Edu-ConvoKit, an open-source library designed to handle
pre-processing, annotation and analysis of conversation data in education.
Resources for analyzing education conversation data are scarce, making the
research challenging to perform and therefore hard to access. We address these
challenges with Edu-ConvoKit. Edu-ConvoKit is open-source
(https://github.com/stanfordnlp/edu-convokit ), pip-installable
(https://pypi.org/project/edu-convokit/ ), with comprehensive documentation
(https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available
at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional
resources, such as Colab applications of Edu-ConvoKit to three diverse
education datasets and a repository of Edu-ConvoKit related papers, that can be
found in our GitHub repository.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu,  8 Feb 24</h3>
<dl>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04254" title="Abstract">arXiv:2402.04254</a> (cross-list from eess.AS) [<a href="/pdf/2402.04254" title="Download PDF">pdf</a>, <a href="/ps/2402.04254" title="Download PostScript">ps</a>, <a href="/format/2402.04254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Vocabulary Spontaneous Speech Recognition for Tigrigna
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kahsu%2C+A">Ataklti Kahsu</a>, 
<a href="/search/eess?searchtype=author&query=Teferra%2C+S">Solomon Teferra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">This thesis proposes and describes a research attempt at designing and
developing a speaker independent spontaneous automatic speech recognition
system for Tigrigna The acoustic model of the Speech Recognition System is
developed using Carnegie Mellon University Automatic Speech Recognition
development tool (Sphinx) while the SRIM tool is used for the development of
the language model.
<br />Keywords Automatic Speech Recognition Tigrigna language
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04255" title="Abstract">arXiv:2402.04255</a> (cross-list from math.FA) [<a href="/pdf/2402.04255" title="Download PDF">pdf</a>, <a href="/ps/2402.04255" title="Download PostScript">ps</a>, <a href="/format/2402.04255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional Kuppinger-Durisi-B&#xf6;lcskei Uncertainty Principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krishna%2C+K+M">K. Mahesh Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 0 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Let $\mathcal{X}$ be a Banach space. Let $\{\tau_j\}_{j=1}^n,
\{\omega_k\}_{k=1}^m\subseteq \mathcal{X}$ and $\{f_j\}_{j=1}^n$,
$\{g_k\}_{k=1}^m\subseteq \mathcal{X}^*$ satisfy $ |f_j(\tau_j)|\geq 1$ for all
$ 1\leq j \leq n$, $|g_k(\omega_k)|\geq 1 $ for all $1\leq k \leq m$. If $x \in
\mathcal{X}\setminus \{0\}$ is such that $x=\theta_\tau\theta_f
x=\theta_\omega\theta_g x$, then we show that \begin{align}\label{FKDB} (1)
\quad\quad\quad\quad \|\theta_fx\|_0\|\theta_gx\|_0\geq
\frac{\bigg[1-(\|\theta_fx\|_0-1)\max\limits_{1\leq j,r \leq n,j\neq
r}|f_j(\tau_r)|\bigg]^+\bigg[1-(\|\theta_g x\|_0-1)\max\limits_{1\leq k,s \leq
m,k\neq s}|g_k(\omega_s)|\bigg]^+}{\left(\displaystyle\max_{1\leq j \leq n,
1\leq k \leq m}|f_j(\omega_k)|\right)\left(\displaystyle\max_{1\leq j \leq n,
1\leq k \leq m}|g_k(\tau_j)|\right)}. \end{align}
<br />We call Inequality (1) as \textbf{Functional Kuppinger-Durisi-B\"{o}lcskei
Uncertainty Principle}. Inequality (1) improves the uncertainty principle
obtained by Kuppinger, Durisi and B\"{o}lcskei \textit{[IEEE Trans. Inform.
Theory (2012)]} (which improved the Donoho-Stark-Elad-Bruckstein uncertainty
principle \textit{[SIAM J. Appl. Math. (1989), IEEE Trans. Inform. Theory
(2002)]}). We also derive functional form of the uncertainity principle
obtained by Studer, Kuppinger, Pope and B\"{o}lcskei \textit{[EEE Trans.
Inform. Theory (2012)]}.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04258" title="Abstract">arXiv:2402.04258</a> (cross-list from eess.IV) [<a href="/pdf/2402.04258" title="Download PDF">pdf</a>, <a href="/format/2402.04258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPLES-DR: MESSIDOR Anatomical and Pathological Labels for Explainable  Screening of Diabetic Retinopathy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lepetit-Aimon%2C+G">Gabriel Lepetit-Aimon</a>, 
<a href="/search/eess?searchtype=author&query=Playout%2C+C">Cl&#xe9;ment Playout</a>, 
<a href="/search/eess?searchtype=author&query=Boucher%2C+M+C">Marie Carole Boucher</a>, 
<a href="/search/eess?searchtype=author&query=Duval%2C+R">Renaud Duval</a>, 
<a href="/search/eess?searchtype=author&query=Brent%2C+M+H">Michael H Brent</a>, 
<a href="/search/eess?searchtype=author&query=Cheriet%2C+F">Farida Cheriet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Databases (cs.DB); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Reliable automatic diagnosis of Diabetic Retinopathy (DR) and Macular Edema
(ME) is an invaluable asset in improving the rate of monitored patients among
at-risk populations and in enabling earlier treatments before the pathology
progresses and threatens vision. However, the explainability of screening
models is still an open question, and specifically designed datasets are
required to support the research. We present MAPLES-DR (MESSIDOR Anatomical and
Pathological Labels for Explainable Screening of Diabetic Retinopathy), which
contains, for 198 images of the MESSIDOR public fundus dataset, new diagnoses
for DR and ME as well as new pixel-wise segmentation maps for 10 anatomical and
pathological biomarkers related to DR. This paper documents the design choices
and the annotation procedure that produced MAPLES-DR, discusses the
interobserver variability and the overall quality of the annotations, and
provides guidelines on using the dataset in a machine learning context.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04267" title="Abstract">arXiv:2402.04267</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.04267" title="Download PDF">pdf</a>, <a href="/ps/2402.04267" title="Download PostScript">ps</a>, <a href="/format/2402.04267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application analysis of ai technology combined with spiral CT scanning  in early lung cancer screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+S">Shulin Li</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+L">Liqiang Yu</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+Q">Qunwei Lin</a>, 
<a href="/search/physics?searchtype=author&query=Huang%2C+J">Jiaxin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article was accepted by Frontiers in Computing and Intelligent Systems <a href="https://drpress.org/ojs/index.php/fcis/article/view/15781.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/nlin/0508031">arXiv:nlin/0508031</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">At present, the incidence and fatality rate of lung cancer in China rank
first among all malignant tumors. Despite the continuous development and
improvement of China's medical level, the overall 5-year survival rate of lung
cancer patients is still lower than 20% and is staged. A number of studies have
confirmed that early diagnosis and treatment of early stage lung cancer is of
great significance to improve the prognosis of patients. In recent years,
artificial intelligence technology has gradually begun to be applied in
oncology. ai is used in cancer screening, clinical diagnosis, radiation therapy
(image acquisition, at-risk organ segmentation, image calibration and delivery)
and other aspects of rapid development. However, whether medical ai can be
socialized depends on the public's attitude and acceptance to a certain extent.
However, at present, there are few studies on the diagnosis of early lung
cancer by AI technology combined with SCT scanning. In view of this, this study
applied the combined method in early lung cancer screening, aiming to find a
safe and efficient screening mode and provide a reference for clinical
diagnosis and treatment.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04268" title="Abstract">arXiv:2402.04268</a> (cross-list from cond-mat.soft) [<a href="/pdf/2402.04268" title="Download PDF">pdf</a>, <a href="/format/2402.04268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProtAgents: Protein discovery via large language model multi-agent  collaborations combining physics and machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Ghafarollahi%2C+A">A. Ghafarollahi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+M+J">M.J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Designing de novo proteins beyond those found in nature holds significant
promise for advancements in both scientific and engineering applications.
Current methodologies for protein design often rely on AI-based models, such as
surrogate models that address end-to-end problems by linking protein structure
to material properties or vice versa. However, these models frequently focus on
specific material objectives or structural properties, limiting their
flexibility when incorporating out-of-domain knowledge into the design process
or comprehensive data analysis is required. In this study, we introduce
ProtAgents, a platform for de novo protein design based on Large Language
Models (LLMs), where multiple AI agents with distinct capabilities
collaboratively address complex tasks within a dynamic environment. The
versatility in agent development allows for expertise in diverse domains,
including knowledge retrieval, protein structure analysis, physics-based
simulations, and results analysis. The dynamic collaboration between agents,
empowered by LLMs, provides a versatile approach to tackling protein design and
analysis problems, as demonstrated through diverse examples in this study. The
problems of interest encompass designing new proteins, analyzing protein
structures and obtaining new first-principles data -- natural vibrational
frequencies -- via physics simulations. The concerted effort of the system
allows for powerful automated and synergistic design of de novo proteins with
targeted mechanical properties. The flexibility in designing the agents, on one
hand, and their capacity in autonomous collaboration through the dynamic
LLM-based multi-agent environment on the other hand, unleashes great potentials
of LLMs in addressing multi-objective materials problems and opens up new
avenues for autonomous materials discovery and design.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04274" title="Abstract">arXiv:2402.04274</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.04274" title="Download PDF">pdf</a>, <a href="/format/2402.04274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPGA Deployment of LFADS for Real-time Neuroscience Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+X">Xiaohan Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+C">ChiJui Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+Y">YanLun Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+L">LingChi Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Khoda%2C+E+E">Elham E Khoda</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+Y">Yihui Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Hauck%2C+S">Scott Hauck</a>, 
<a href="/search/q-bio?searchtype=author&query=Hsu%2C+S">Shih-Chieh Hsu</a>, 
<a href="/search/q-bio?searchtype=author&query=Lai%2C+B">Bo-Cheng Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Fast Machine Learning for Science, ICCAD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large-scale recordings of neural activity are providing new opportunities to
study neural population dynamics. A powerful method for analyzing such
high-dimensional measurements is to deploy an algorithm to learn the
low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical
Systems) is a deep learning method for inferring latent dynamics from
high-dimensional neural spiking data recorded simultaneously in single trials.
This method has shown a remarkable performance in modeling complex brain
signals with an average inference latency in milliseconds. As our capacity of
simultaneously recording many neurons is increasing exponentially, it is
becoming crucial to build capacity for deploying low-latency inference of the
computing algorithms. To improve the real-time processing ability of LFADS, we
introduce an efficient implementation of the LFADS models onto Field
Programmable Gate Arrays (FPGA). Our implementation shows an inference latency
of 41.97 $\mu$s for processing the data in a single trial on a Xilinx U55C.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04275" title="Abstract">arXiv:2402.04275</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.04275" title="Download PDF">pdf</a>, <a href="/ps/2402.04275" title="Download PostScript">ps</a>, <a href="/format/2402.04275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Mapping Cognition: A Nondecomposable Primary Process in Human  Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xie%2C+Z">Zhenping Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Human intelligence seems so mysterious that we have not successfully
understood its foundation until now. Here, I want to present a basic cognitive
process, motion mapping cognition (MMC), which should be a nondecomposable
primary function in human vision. Wherein, I point out that, MMC process can be
used to explain most of human visual functions in fundamental, but can not be
effectively modelled by traditional visual processing ways including image
segmentation, object recognition, object tracking etc. Furthermore, I state
that MMC may be looked as an extension of Chen's theory of topological
perception on human vision, and seems to be unsolvable using existing
intelligent algorithm skills. Finally, along with the requirements of MMC
problem, an interesting computational model, quantized topological matching
principle can be derived by developing the idea of optimal transport theory.
Above results may give us huge inspiration to develop more robust and
interpretable machine vision models.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04278" title="Abstract">arXiv:2402.04278</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.04278" title="Download PDF">pdf</a>, <a href="/format/2402.04278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Plane-Wave Neural Operator for Electron Density Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kim%2C+S">Seongsu Kim</a>, 
<a href="/search/physics?searchtype=author&query=Ahn%2C+S">Sungsoo Ahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work studies machine learning for electron density prediction, which is
fundamental for understanding chemical systems and density functional theory
(DFT) simulations. To this end, we introduce the Gaussian plane-wave neural
operator (GPWNO), which operates in the infinite-dimensional functional space
using the plane-wave and Gaussian-type orbital bases, widely recognized in the
context of DFT. In particular, both high- and low-frequency components of the
density can be effectively represented due to the complementary nature of the
two bases. Extensive experiments on QM9, MD, and material project datasets
demonstrate GPWNO's superior performance over ten baselines.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04286" title="Abstract">arXiv:2402.04286</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.04286" title="Download PDF">pdf</a>, <a href="/ps/2402.04286" title="Download PostScript">ps</a>, <a href="/format/2402.04286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progress and Opportunities of Foundation Models in Bioinformatics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+Z">Zhihang Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Yixuan Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Fan%2C+Y">Yimin Fan</a>, 
<a href="/search/q-bio?searchtype=author&query=King%2C+I">Irwin King</a>, 
<a href="/search/q-bio?searchtype=author&query=Song%2C+L">Le Song</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Y">Yu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bioinformatics has witnessed a paradigm shift with the increasing integration
of artificial intelligence (AI), particularly through the adoption of
foundation models (FMs). These AI techniques have rapidly advanced, addressing
historical challenges in bioinformatics such as the scarcity of annotated data
and the presence of data noise. FMs are particularly adept at handling
large-scale, unlabeled data, a common scenario in biological contexts due to
the time-consuming and costly nature of experimentally determining labeled
data. This characteristic has allowed FMs to excel and achieve notable results
in various downstream validation tasks, demonstrating their ability to
represent diverse biological entities effectively. Undoubtedly, FMs have
ushered in a new era in computational biology, especially in the realm of deep
learning. The primary goal of this survey is to conduct a systematic
investigation and summary of FMs in bioinformatics, tracing their evolution,
current research status, and the methodologies employed. Central to our focus
is the application of FMs to specific biological problems, aiming to guide the
research community in choosing appropriate FMs for their research needs. We
delve into the specifics of the problem at hand including sequence analysis,
structure prediction, function annotation, and multimodal integration,
comparing the structures and advancements against traditional methods.
Furthermore, the review analyses challenges and limitations faced by FMs in
biology, such as data noise, model explainability, and potential biases.
Finally, we outline potential development paths and strategies for FMs in
future biological research, setting the stage for continued innovation and
application in this rapidly evolving field. This comprehensive review serves
not only as an academic resource but also as a roadmap for future explorations
and applications of FMs in biology.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04287" title="Abstract">arXiv:2402.04287</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.04287" title="Download PDF">pdf</a>, <a href="/ps/2402.04287" title="Download PostScript">ps</a>, <a href="/format/2402.04287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Association between Prefrontal fNIRS signals during Cognitive tasks and  College scholastic ability test (CSAT) scores: Analysis using a quantum  annealing approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+Y">Yeaju Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Choi%2C+J">Junggu Choi</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+B">Bora Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Park%2C+Y">Yongwan Park</a>, 
<a href="/search/q-bio?searchtype=author&query=Cha%2C+J">Jihyun Cha</a>, 
<a href="/search/q-bio?searchtype=author&query=Choi%2C+J">Jongkwan Choi</a>, 
<a href="/search/q-bio?searchtype=author&query=Han%2C+S">Sanghoon Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Academic achievement is a critical measure of intellectual ability, prompting
extensive research into cognitive tasks as potential predictors. Neuroimaging
technologies, such as functional near-infrared spectroscopy (fNIRS), offer
insights into brain hemodynamics, allowing understanding of the link between
cognitive performance and academic achievement. Herein, we explored the
association between cognitive tasks and academic achievement by analyzing
prefrontal fNIRS signals. A novel quantum annealer (QA) feature selection
algorithm was applied to fNIRS data to identify cognitive tasks correlated with
CSAT scores. Twelve features (signal mean, median, variance, peak, number of
peaks, sum of peaks, slope, minimum, kurtosis, skewness, standard deviation,
and root mean square) were extracted from fNIRS signals at two time windows
(10- and 60-second) to compare results from various feature variable
conditions. The feature selection results from the QA-based and XGBoost
regressor algorithms were compared to validate the former's performance. In a
three-step validation process using multiple linear regression models,
correlation coefficients between the feature variables and the CSAT scores,
model fitness (adjusted R2), and model prediction error (RMSE) values were
calculated. The quantum annealer demonstrated comparable performance to
classical machine learning models, and specific cognitive tasks, including
verbal fluency, recognition, and the Corsi block tapping task, were correlated
with academic achievement. Group analyses revealed stronger associations
between Tower of London and N-back tasks with higher CSAT scores. Quantum
annealing algorithms have significant potential in feature selection using
fNIRS data, and represents a novel research approach. Future studies should
explore predictors of academic achievement and cognitive ability.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04301" title="Abstract">arXiv:2402.04301</a> (cross-list from eess.IV) [<a href="/pdf/2402.04301" title="Download PDF">pdf</a>, <a href="/format/2402.04301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep PCCT: Photon Counting Computed Tomography Deep Learning  Applications Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alves%2C+A+C">Ana Carolina Alves</a>, 
<a href="/search/eess?searchtype=author&query=Ferreira%2C+A">Andr&#xe9; Ferreira</a>, 
<a href="/search/eess?searchtype=author&query=Luijten%2C+G">Gijs Luijten</a>, 
<a href="/search/eess?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/eess?searchtype=author&query=Puladi%2C+B">Behrus Puladi</a>, 
<a href="/search/eess?searchtype=author&query=Egger%2C+J">Jan Egger</a>, 
<a href="/search/eess?searchtype=author&query=Alves%2C+V">Victor Alves</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computational Engineering, Finance, and Science (cs.CE); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical imaging faces challenges such as limited spatial resolution,
interference from electronic noise and poor contrast-to-noise ratios. Photon
Counting Computed Tomography (PCCT) has emerged as a solution, addressing these
issues with its innovative technology. This review delves into the recent
developments and applications of PCCT in pre-clinical research, emphasizing its
potential to overcome traditional imaging limitations. For example PCCT has
demonstrated remarkable efficacy in improving the detection of subtle
abnormalities in breast, providing a level of detail previously unattainable.
Examining the current literature on PCCT, it presents a comprehensive analysis
of the technology, highlighting the main features of scanners and their varied
applications. In addition, it explores the integration of deep learning into
PCCT, along with the study of radiomic features, presenting successful
applications in data processing. While acknowledging these advances, it also
discusses the existing challenges in this field, paving the way for future
research and improvements in medical imaging technologies. Despite the limited
number of articles on this subject, due to the recent integration of PCCT at a
clinical level, its potential benefits extend to various diagnostic
applications.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04355" title="Abstract">arXiv:2402.04355</a> (cross-list from stat.ML) [<a href="/pdf/2402.04355" title="Download PDF">pdf</a>, <a href="/format/2402.04355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PQMass: Probabilistic Assessment of the Quality of Generative Models  using Probability Mass Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/stat?searchtype=author&query=Sharief%2C+S">Sammy Sharief</a>, 
<a href="/search/stat?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/stat?searchtype=author&query=Perreault-Levasseur%2C+L">Laurence Perreault-Levasseur</a>, 
<a href="/search/stat?searchtype=author&query=Hezaveh%2C+Y">Yashar Hezaveh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">We propose a comprehensive sample-based method for assessing the quality of
generative models. The proposed approach enables the estimation of the
probability that two sets of samples are drawn from the same distribution,
providing a statistically rigorous method for assessing the performance of a
single generative model or the comparison of multiple competing models trained
on the same dataset. This comparison can be conducted by dividing the space
into non-overlapping regions and comparing the number of data samples in each
region. The method only requires samples from the generative model and the test
data. It is capable of functioning directly on high-dimensional data, obviating
the need for dimensionality reduction. Significantly, the proposed method does
not depend on assumptions regarding the density of the true distribution, and
it does not rely on training or fitting any auxiliary models. Instead, it
focuses on approximating the integral of the density (probability mass) across
various sub-regions within the data space.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04419" title="Abstract">arXiv:2402.04419</a> (cross-list from eess.IV) [<a href="/pdf/2402.04419" title="Download PDF">pdf</a>, <a href="/ps/2402.04419" title="Download PostScript">ps</a>, <a href="/format/2402.04419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What limits performance of weakly supervised deep learning for chest CT  classification?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tushar%2C+F+I">Fakrul Islam Tushar</a>, 
<a href="/search/eess?searchtype=author&query=D%27Anniballe%2C+V+M">Vincent M. D&#x27;Anniballe</a>, 
<a href="/search/eess?searchtype=author&query=Rubin%2C+G+D">Geoffrey D. Rubin</a>, 
<a href="/search/eess?searchtype=author&query=Lo%2C+J+Y">Joseph Y. Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages , 8 figures. arXiv admin note: text overlap with <a href="/abs/2202.11709">arXiv:2202.11709</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Weakly supervised learning with noisy data has drawn attention in the medical
imaging community due to the sparsity of high-quality disease labels. However,
little is known about the limitations of such weakly supervised learning and
the effect of these constraints on disease classification performance. In this
paper, we test the effects of such weak supervision by examining model
tolerance for three conditions. First, we examined model tolerance for noisy
data by incrementally increasing error in the labels within the training data.
Second, we assessed the impact of dataset size by varying the amount of
training data. Third, we compared performance differences between binary and
multi-label classification. Results demonstrated that the model could endure up
to 10% added label error before experiencing a decline in disease
classification performance. Disease classification performance steadily rose as
the amount of training data was increased for all disease classes, before
experiencing a plateau in performance at 75% of training data. Last, the binary
model outperformed the multilabel model in every disease category. However,
such interpretations may be misleading, as the binary model was heavily
influenced by co-occurring diseases and may not have learned the specific
features of the disease in the image. In conclusion, this study may help the
medical imaging community understand the benefits and risks of weak supervision
with noisy labels. Such studies demonstrate the need to build diverse,
large-scale datasets and to develop explainable and responsible AI.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04426" title="Abstract">arXiv:2402.04426</a> (cross-list from eess.IV) [<a href="/pdf/2402.04426" title="Download PDF">pdf</a>, <a href="/format/2402.04426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative Metrics for Benchmarking Medical Image Harmonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Parida%2C+A">Abhijeet Parida</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Zhifan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Packer%2C+R+J">Roger J. Packer</a>, 
<a href="/search/eess?searchtype=author&query=Avery%2C+R+A">Robert A. Avery</a>, 
<a href="/search/eess?searchtype=author&query=Anwar%2C+S+M">Syed M. Anwar</a>, 
<a href="/search/eess?searchtype=author&query=Linguraru%2C+M+G">Marius G. Linguraru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image harmonization is an important preprocessing strategy to address domain
shifts arising from data acquired using different machines and scanning
protocols in medical imaging. However, benchmarking the effectiveness of
harmonization techniques has been a challenge due to the lack of widely
available standardized datasets with ground truths. In this context, we propose
three metrics: two intensity harmonization metrics and one anatomy preservation
metric for medical images during harmonization, where no ground truths are
required. Through extensive studies on a dataset with available harmonization
ground truth, we demonstrate that our metrics are correlated with established
image quality assessment metrics. We show how these novel metrics may be
applied to real-world scenarios where no harmonization ground truth exists.
Additionally, we provide insights into different interpretations of the metric
values, shedding light on their significance in the context of the
harmonization process. As a result of our findings, we advocate for the
adoption of these quantitative harmonization metrics as a standard for
benchmarking the performance of image harmonization techniques.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04436" title="Abstract">arXiv:2402.04436</a> (cross-list from stat.ML) [<a href="/pdf/2402.04436" title="Download PDF">pdf</a>, <a href="/format/2402.04436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Multidimensional Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Trosset%2C+M+W">Michael W. Trosset</a>, 
<a href="/search/stat?searchtype=author&query=Priebe%2C+C+E">Carey E. Priebe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multidimensional scaling (MDS) is the act of embedding proximity information
about a set of $n$ objects in $d$-dimensional Euclidean space. As originally
conceived by the psychometric community, MDS was concerned with embedding a
fixed set of proximities associated with a fixed set of objects. Modern
concerns, e.g., that arise in developing asymptotic theories for statistical
inference on random graphs, more typically involve studying the limiting
behavior of a sequence of proximities associated with an increasing set of
objects. Standard results from the theory of point-to-set maps imply that, if
$n$ is fixed, then the limit of the embedded structures is the embedded
structure of the limiting proximities. But what if $n$ increases? It then
becomes necessary to reformulate MDS so that the entire sequence of embedding
problems can be viewed as a sequence of optimization problems in a fixed space.
We present such a reformulation and derive some consequences.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04446" title="Abstract">arXiv:2402.04446</a> (cross-list from eess.IV) [<a href="/pdf/2402.04446" title="Download PDF">pdf</a>, <a href="/format/2402.04446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the limits of cell segmentation models for imaging mass  cytometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bird%2C+K+M">Kimberley M. Bird</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+X">Xujiong Ye</a>, 
<a href="/search/eess?searchtype=author&query=Race%2C+A+M">Alan M. Race</a>, 
<a href="/search/eess?searchtype=author&query=Brown%2C+J+M">James M. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Symposium on Biomedical Imaging (ISBI) 2024 Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Imaging mass cytometry (IMC) is a relatively new technique for imaging
biological tissue at subcellular resolution. In recent years, learning-based
segmentation methods have enabled precise quantification of cell type and
morphology, but typically rely on large datasets with fully annotated ground
truth (GT) labels. This paper explores the effects of imperfect labels on
learning-based segmentation models and evaluates the generalisability of these
models to different tissue types. Our results show that removing 50% of cell
annotations from GT masks only reduces the dice similarity coefficient (DSC)
score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT
masks). This implies that annotation time can in fact be reduced by at least
half without detrimentally affecting performance. Furthermore, training our
single-tissue model on imperfect labels only decreases DSC by 0.031 on an
unseen tissue type compared to its multi-tissue counterpart, with negligible
qualitative differences in segmentation. Additionally, bootstrapping the
worst-performing model (with 5% of cell annotations) a total of ten times
improves its original DSC score of 0.720 to 0.829. These findings imply that
less time and work can be put into the process of producing comparable
segmentation models; this includes eliminating the need for multiple IMC tissue
types during training, whilst also providing the potential for models with very
few labels to improve on themselves. Source code is available on GitHub:
https://github.com/kimberley/ISBI2024.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04471" title="Abstract">arXiv:2402.04471</a> (cross-list from quant-ph) [<a href="/pdf/2402.04471" title="Download PDF">pdf</a>, <a href="/format/2402.04471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reductive Quantum Phase Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Papadopoulos%2C+N+J+C">Nicholas J.C. Papadopoulos</a>, 
<a href="/search/quant-ph?searchtype=author&query=Reilly%2C+J+T">Jarrod T. Reilly</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wilson%2C+J+D">John Drew Wilson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Holland%2C+M+J">Murray J. Holland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Estimating a quantum phase is a necessary task in a wide range of fields of
quantum science. To accomplish this task, two well-known methods have been
developed in distinct contexts, namely, Ramsey interferometry (RI) in atomic
and molecular physics and quantum phase estimation (QPE) in quantum computing.
We demonstrate that these canonical examples are instances of a larger class of
phase estimation protocols, which we call reductive quantum phase estimation
(RQPE) circuits. Here we present an explicit algorithm that allows one to
create an RQPE circuit. This circuit distinguishes an arbitrary set of phases
with a fewer number of qubits and unitary applications, thereby solving a
general class of quantum hypothesis testing to which RI and QPE belong. We
further demonstrate a trade-off between measurement precision and phase
distinguishability, which allows one to tune the circuit to be optimal for a
specific application.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04480" title="Abstract">arXiv:2402.04480</a> (cross-list from eess.IV) [<a href="/pdf/2402.04480" title="Download PDF">pdf</a>, <a href="/format/2402.04480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIRT: a simultaneous reconstruction and affine motion compensation  technique for four dimensional computed tomography (4DCT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+A">Anh-Tuan Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Renders%2C+J">Jens Renders</a>, 
<a href="/search/eess?searchtype=author&query=Iuso%2C+D">Domenico Iuso</a>, 
<a href="/search/eess?searchtype=author&query=Maris%2C+Y">Yves Maris</a>, 
<a href="/search/eess?searchtype=author&query=Soete%2C+J">Jeroen Soete</a>, 
<a href="/search/eess?searchtype=author&query=Wevers%2C+M">Martine Wevers</a>, 
<a href="/search/eess?searchtype=author&query=Sijbers%2C+J">Jan Sijbers</a>, 
<a href="/search/eess?searchtype=author&query=De+Beenhouwer%2C+J">Jan De Beenhouwer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the SIAM Journal on Imaging Sciences (SIIMS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC)

</div>
<p class="mathjax">In four-dimensional computed tomography (4DCT), 3D images of moving or
deforming samples are reconstructed from a set of 2D projection images. Recent
techniques for iterative motion-compensated reconstruction either necessitate a
reference acquisition or alternate image reconstruction and motion estimation
steps. In these methods, the motion estimation step involves the estimation of
either complete deformation vector fields (DVFs) or a limited set of parameters
corresponding to the affine motion, including rigid motion or scaling. The
majority of these approaches rely on nested iterations, incurring significant
computational expenses. Notably, despite the direct benefits of an analytical
formulation and a substantial reduction in computational complexity, there has
been no exploration into parameterizing DVFs for general affine motion in CT
imaging. In this work, we propose the Motion-compensated Iterative
Reconstruction Technique (MIRT)- an efficient iterative reconstruction scheme
that combines image reconstruction and affine motion estimation in a single
update step, based on the analytical gradients of the motion towards both the
reconstruction and the affine motion parameters. When most of the
state-of-the-art 4DCT methods have not attempted to be tested on real data,
results from simulation and real experiments show that our method outperforms
the state-of-the-art CT reconstruction with affine motion correction methods in
computational feasibility and projection distance. In particular, this allows
accurate reconstruction for a proper microscale diamond in the appearance of
motion from the practically acquired projection radiographs, which leads to a
novel application of 4DCT.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04493" title="Abstract">arXiv:2402.04493</a> (cross-list from stat.ML) [<a href="/pdf/2402.04493" title="Download PDF">pdf</a>, <a href="/ps/2402.04493" title="Download PostScript">ps</a>, <a href="/format/2402.04493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning  with Low-Rank MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hong%2C+K">Kihyuk Hong</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Offline reinforcement learning (RL) aims to learn a policy that maximizes the
expected cumulative reward using a pre-collected dataset. Offline RL with
low-rank MDPs or general function approximation has been widely studied
recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for
finding an $\epsilon$-optimal policy either require a uniform data coverage
assumptions or are computationally inefficient. In this paper, we propose a
primal dual algorithm for offline RL with low-rank MDPs in the discounted
infinite-horizon setting. Our algorithm is the first computationally efficient
algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$
with partial data coverage assumption. This improves upon a recent work that
requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the
previous work to the offline constrained RL setting by supporting constraints
on additional reward signals.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04498" title="Abstract">arXiv:2402.04498</a> (cross-list from stat.ML) [<a href="/pdf/2402.04498" title="Download PDF">pdf</a>, <a href="/format/2402.04498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing  Time-course Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Agrahar%2C+C">Chaitra Agrahar</a>, 
<a href="/search/stat?searchtype=author&query=Poole%2C+W">William Poole</a>, 
<a href="/search/stat?searchtype=author&query=Bianco%2C+S">Simone Bianco</a>, 
<a href="/search/stat?searchtype=author&query=El-Samad%2C+H">Hana El-Samad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures, Submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Kalman Filter (KF) is an optimal linear state prediction algorithm, with
applications in fields as diverse as engineering, economics, robotics, and
space exploration. Here, we develop an extension of the KF, called a Pathspace
Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties
associated with the underlying data and prior knowledge, and b) take as input
an entire trajectory and an underlying mechanistic model, and using a Bayesian
methodology quantify the different sources of uncertainty. An application of
this algorithm is to automatically detect temporal windows where the internal
mechanistic model deviates from the data in a time-dependent manner. First, we
provide theorems characterizing the convergence of the PKF algorithm. Then, we
numerically demonstrate that the PKF outperforms conventional KF methods on a
synthetic dataset lowering the mean-squared-error by several orders of
magnitude. Finally, we apply this method to biological time-course dataset
involving over 1.8 million gene expression measurements.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04506" title="Abstract">arXiv:2402.04506</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.04506" title="Download PDF">pdf</a>, <a href="/ps/2402.04506" title="Download PostScript">ps</a>, <a href="/format/2402.04506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Processing All-Sky Images At Scale On The Amazon Cloud: A HiPS Example
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Berriman%2C+G+B">G. Bruce Berriman</a>, 
<a href="/search/astro-ph?searchtype=author&query=Good%2C+J+C">John C. Good</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure, ADASS 2024 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We report here on a project that has developed a practical approach to
processing all-sky image collections on cloud platforms, using as an exemplar
application the creation of three-color Hierarchical Progressive Survey (HiPS)
maps of the 2MASS data set with the Montage Image Mosaic Engine on Amazon Web
Services. We will emphasize issues that must be considered by scientists
wishing to use cloud platforms to perform such parallel processing, so
providing a guide for scientists wishing to exploit cloud platforms for similar
large-scale processing. A HiPS map is based on the HEALPix sky-tiling scheme.
Progressive zooming of a HiPS map reveals an image sampled at ever smaller or
larger spatial scales that are defined by the HEALPix standard. Briefly, the
approach used by Montage involves creating a base mosaic at the lowest required
HEALPix level, usually chosen to match as closely as possible the spatial
sampling of the input images, then cutting out the HiPS cells in PNG format
from this mosaic. The process is repeated at successive HEALPix levels to
create a nested collection of FITS files, from which PNG files are created that
are shown in HiPS viewers. Stretching FITS files to produce PNGs is based on an
image histogram. For composite regions (up and including the whole sky), the
histograms for each tile can be combined to create a composite histogram for
the region. Using this single histogram for each of the individual FITS files
means all the PNGs are on the same brightness scale and displaying them side by
side in a HiPS viewer produces a continuous uniform map across the entire sky.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04516" title="Abstract">arXiv:2402.04516</a> (cross-list from stat.ML) [<a href="/pdf/2402.04516" title="Download PDF">pdf</a>, <a href="/format/2402.04516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Sobolev Transport for Probability Measures on a Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Le%2C+T">Tam Le</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+T">Truyen Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Fukumizu%2C+K">Kenji Fukumizu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the optimal transport (OT) problem for measures supported on a graph
metric space. Recently, Le et al. (2022) leverage the graph structure and
propose a variant of OT, namely Sobolev transport (ST), which yields a
closed-form expression for a fast computation. However, ST is essentially
coupled with the $L^p$ geometric structure within its definition which makes it
nontrivial to utilize ST for other prior structures. In contrast, the classic
OT has the flexibility to adapt to various geometric structures by modifying
the underlying cost function. An important instance is the Orlicz-Wasserstein
(OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz
geometric structure}. Comparing to the usage of standard $p$-order Wasserstein,
OW remarkably helps to advance certain machine learning approaches.
Nevertheless, OW brings up a new challenge on its computation due to its
two-level optimization formulation. In this work, we leverage a specific class
of convex functions for Orlicz structure to propose the generalized Sobolev
transport (GST). GST encompasses the ST as its special case, and can be
utilized for prior structures beyond the $L^p$ geometry. In connection with the
OW, we show that one only needs to simply solve a univariate optimization
problem to compute the GST, unlike the complex two-level optimization problem
in OW. We empirically illustrate that GST is several-order faster than the OW.
Moreover, we provide preliminary evidences on the advantages of GST for
document classification and for several tasks in topological data analysis.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04550" title="Abstract">arXiv:2402.04550</a> (cross-list from stat.ML) [<a href="/pdf/2402.04550" title="Download PDF">pdf</a>, <a href="/format/2402.04550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemann-Lebesgue Forest for Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Qin%2C+T">Tian Qin</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+W">Wei-Min Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for
regression. The core idea of RLF is to mimic the way how a measurable function
can be approximated by partitioning its range into a few intervals. With this
idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which
has a chance to split the node from response $Y$ or a direction in feature
space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic
performance of RLF under different parameter settings mainly through Hoeffding
decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the
underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF
is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The
competitive performance of RLF against original random forest
\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and
real world datasets.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04557" title="Abstract">arXiv:2402.04557</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.04557" title="Download PDF">pdf</a>, <a href="/ps/2402.04557" title="Download PostScript">ps</a>, <a href="/format/2402.04557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Artificial Intelligence (AI) workflow for catalyst design and  optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lai%2C+N+S">Nung Siong Lai</a>, 
<a href="/search/physics?searchtype=author&query=Tew%2C+Y+S">Yi Shen Tew</a>, 
<a href="/search/physics?searchtype=author&query=Zhong%2C+X">Xialin Zhong</a>, 
<a href="/search/physics?searchtype=author&query=Yin%2C+J">Jun Yin</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+J">Jiali Li</a>, 
<a href="/search/physics?searchtype=author&query=Yan%2C+B">Binhang Yan</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+X">Xiaonan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ind. Eng. Chem. Res. 2023, 62, 43, 17835-17848
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the pursuit of novel catalyst development to address pressing
environmental concerns and energy demand, conventional design and optimization
methods often fall short due to the complexity and vastness of the catalyst
parameter space. The advent of Machine Learning (ML) has ushered in a new era
in the field of catalyst optimization, offering potential solutions to the
shortcomings of traditional techniques. However, existing methods fail to
effectively harness the wealth of information contained within the burgeoning
body of scientific literature on catalyst synthesis. To address this gap, this
study proposes an innovative Artificial Intelligence (AI) workflow that
integrates Large Language Models (LLMs), Bayesian optimization, and an active
learning loop to expedite and enhance catalyst optimization. Our methodology
combines advanced language understanding with robust optimization strategies,
effectively translating knowledge extracted from diverse literature into
actionable parameters for practical experimentation and optimization. In this
article, we demonstrate the application of this AI workflow in the optimization
of catalyst synthesis for ammonia production. The results underscore the
workflow's ability to streamline the catalyst development process, offering a
swift, resource-efficient, and high-precision alternative to conventional
methods.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04566" title="Abstract">arXiv:2402.04566</a> (cross-list from eess.IV) [<a href="/pdf/2402.04566" title="Download PDF">pdf</a>, <a href="/ps/2402.04566" title="Download PostScript">ps</a>, <a href="/format/2402.04566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triplet-constraint Transformer with Multi-scale Refinement for Dose  Prediction in Radiotherapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wen%2C+L">Lu Wen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qihun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Z">Zhenghao Feng</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yuanyuan Xu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jiliu Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by 2024 IEEE ISBI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Radiotherapy is a primary treatment for cancers with the aim of applying
sufficient radiation dose to the planning target volume (PTV) while minimizing
dose hazards to the organs at risk (OARs). Convolutional neural networks (CNNs)
have automated the radiotherapy plan-making by predicting the dose maps.
However, current CNN-based methods ignore the remarkable dose difference in the
dose map, i.e., high dose value in the interior PTV while low value in the
exterior PTV, leading to a suboptimal prediction. In this paper, we propose a
triplet-constraint transformer (TCtrans) with multi-scale refinement to predict
the high-quality dose distribution. Concretely, a novel PTV-guided triplet
constraint is designed to refine dose feature representations in the interior
and exterior PTV by utilizing the explicit geometry of PTV. Furthermore, we
introduce a multi-scale refinement (MSR) module to effectively fulfill the
triplet constraint in different decoding layers with multiple scales. Besides,
a transformer encoder is devised to learn the important global dosimetric
knowledge. Experiments on a clinical cervical cancer dataset demonstrate the
superiority of our method.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04584" title="Abstract">arXiv:2402.04584</a> (cross-list from eess.IV) [<a href="/pdf/2402.04584" title="Download PDF">pdf</a>, <a href="/format/2402.04584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Troublemaker Learning for Low-Light Image Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+Y">Yinghao Song</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+Z">Zhiyuan Cao</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+W">Wanhong Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Long%2C+S">Sifan Long</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+B">Bo Yang</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+H">Hongwei Ge</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+Y">Yanchun Liang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+C">Chunguo Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Low-light image enhancement (LLIE) restores the color and brightness of
underexposed images. Supervised methods suffer from high costs in collecting
low/normal-light image pairs. Unsupervised methods invest substantial effort in
crafting complex loss functions. We address these two challenges through the
proposed TroubleMaker Learning (TML) strategy, which employs normal-light
images as inputs for training. TML is simple: we first dim the input and then
increase its brightness. TML is based on two core components. First, the
troublemaker model (TM) constructs pseudo low-light images from normal images
to relieve the cost of pairwise data. Second, the predicting model (PM)
enhances the brightness of pseudo low-light images. Additionally, we
incorporate an enhancing model (EM) to further improve the visual performance
of PM outputs. Moreover, in LLIE tasks, characterizing global element
correlations is important because more information on the same object can be
captured. CNN cannot achieve this well, and self-attention has high time
complexity. Accordingly, we propose Global Dynamic Convolution (GDC) with O(n)
time complexity, which essentially imitates the partial calculation process of
self-attention to formulate elementwise correlations. Based on the GDC module,
we build the UGDC model. Extensive quantitative and qualitative experiments
demonstrate that UGDC trained with TML can achieve competitive performance
against state-of-the-art approaches on public datasets. The code is available
at https://github.com/Rainbowman0/TML_LLIE.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04602" title="Abstract">arXiv:2402.04602</a> (cross-list from math.ST) [<a href="/pdf/2402.04602" title="Download PDF">pdf</a>, <a href="/format/2402.04602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Quantile Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shen%2C+Y">Yinan Shen</a>, 
<a href="/search/math?searchtype=author&query=Xia%2C+D">Dong Xia</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+W">Wen-Xin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Methodology (stat.ME)

</div>
<p class="mathjax">This paper tackles the challenge of integrating sequentially arriving data
within the quantile regression framework, where the number of covariates is
allowed to grow with the number of observations, the horizon is unknown, and
memory is limited. We employ stochastic sub-gradient descent to minimize the
empirical check loss and study its statistical properties and regret
performance. In our analysis, we unveil the delicate interplay between updating
iterates based on individual observations versus batches of observations,
revealing distinct regularity properties in each scenario. Our method ensures
long-term optimal estimation irrespective of the chosen update strategy.
Importantly, our contributions go beyond prior works by achieving
exponential-type concentration inequalities and attaining optimal regret and
error rates that exhibit only short-term sensitivity to initial errors. A key
insight from our study is the delicate statistical analyses and the revelation
that appropriate stepsize schemes significantly mitigate the impact of initial
errors on subsequent errors and regrets. This underscores the robustness of
stochastic sub-gradient descent in handling initial uncertainties, emphasizing
its efficacy in scenarios where the sequential arrival of data introduces
uncertainties regarding both the horizon and the total number of observations.
Additionally, when the initial error rate is well controlled, there is a
trade-off between short-term error rate and long-term optimality. Due to the
lack of delicate statistical analysis for square loss, we also briefly discuss
its properties and proper schemes. Extensive simulations support our
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04613" title="Abstract">arXiv:2402.04613</a> (cross-list from stat.ML) [<a href="/pdf/2402.04613" title="Download PDF">pdf</a>, <a href="/format/2402.04613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in  Reproducing Kernel Hilbert Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Neumayer%2C+S">Sebastian Neumayer</a>, 
<a href="/search/stat?searchtype=author&query=Stein%2C+V">Viktor Stein</a>, 
<a href="/search/stat?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA); Optimization and Control (math.OC)

</div>
<p class="mathjax">Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler
divergence, are subject to limitations regarding the support of the involved
measures. A remedy consists of regularizing the $f$-divergence by a squared
maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In
this paper, we use the so-called kernel mean embedding to show that the
corresponding regularization can be rewritten as the Moreau envelope of some
function in the reproducing kernel Hilbert space associated with $K$. Then, we
exploit well-known results on Moreau envelopes in Hilbert spaces to prove
properties of the MMD-regularized $f$-divergences and, in particular, their
gradients. Subsequently, we use our findings to analyze Wasserstein gradient
flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein
gradient flows starting from empirical measures and provide
proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04670" title="Abstract">arXiv:2402.04670</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.04670" title="Download PDF">pdf</a>, <a href="/format/2402.04670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comparison of different approaches to compute surface tension  contribution in incompressible two-phase flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Orlando%2C+G">Giuseppe Orlando</a>, 
<a href="/search/physics?searchtype=author&query=Barbante%2C+P+F">Paolo Francesco Barbante</a>, 
<a href="/search/physics?searchtype=author&query=Bonaventura%2C+L">Luca Bonaventura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Mathematical Physics (math-ph); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We perform a quantitative assessment of different strategies to compute the
contribution due to surface tension in incompressible two-phase flows using a
conservative level set (CLS) method. More specifically, we compare classical
approaches, such as the direct computation of the curvature from the level set
or the Laplace-Beltrami operator, with an evolution equation for the mean
curvature recently proposed in literature. We consider the test case of a
static bubble, for which an exact solution for the pressure jump across the
interface is available, and the test case of an oscillating bubble, showing
pros and cons of the different approaches.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04691" title="Abstract">arXiv:2402.04691</a> (cross-list from stat.ML) [<a href="/pdf/2402.04691" title="Download PDF">pdf</a>, <a href="/ps/2402.04691" title="Download PostScript">ps</a>, <a href="/format/2402.04691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Operators with Stochastic Gradient Descent in General Hilbert  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+J">Jia-Qi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA); Statistics Theory (math.ST)

</div>
<p class="mathjax">This study investigates leveraging stochastic gradient descent (SGD) to learn
operators between general Hilbert spaces. We propose weak and strong regularity
conditions for the target operator to depict its intrinsic structure and
complexity. Under these conditions, we establish upper bounds for convergence
rates of the SGD algorithm and conduct a minimax lower bound analysis, further
illustrating that our convergence analysis and regularity conditions
quantitatively characterize the tractability of solving operator learning
problems using the SGD algorithm. It is crucial to highlight that our
convergence analysis is still valid for nonlinear operator learning. We show
that the SGD estimator will converge to the best linear approximation of the
nonlinear target operator. Moreover, applying our analysis to operator learning
problems based on vector-valued and real-valued reproducing kernel Hilbert
spaces yields new convergence results, thereby refining the conclusions of
existing literature.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04692" title="Abstract">arXiv:2402.04692</a> (cross-list from stat.ML) [<a href="/pdf/2402.04692" title="Download PDF">pdf</a>, <a href="/format/2402.04692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From explained variance of correlated components to PCA without  orthogonality constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chavent%2C+M">Marie Chavent</a> (IMB), 
<a href="/search/stat?searchtype=author&query=Chavent%2C+G">Guy Chavent</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Block Principal Component Analysis (Block PCA) of a data matrix A, where
loadings Z are determined by maximization of AZ 2 over unit norm orthogonal
loadings, is difficult to use for the design of sparse PCA by 1 regularization,
due to the difficulty of taking care of both the orthogonality constraint on
loadings and the non differentiable 1 penalty. Our objective in this paper is
to relax the orthogonality constraint on loadings by introducing new objective
functions expvar(Y) which measure the part of the variance of the data matrix A
explained by correlated components Y = AZ. So we propose first a comprehensive
study of mathematical and numerical properties of expvar(Y) for two existing
definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions.
Then we show that only two of these explained variance are fit to use as
objective function in block PCA formulations for A rid of orthogonality
constraints.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04727" title="Abstract">arXiv:2402.04727</a> (cross-list from stat.ME) [<a href="/pdf/2402.04727" title="Download PDF">pdf</a>, <a href="/format/2402.04727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Bayesian estimation of Monod kinetics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Colin%2C+K">K&#xe9;vin Colin</a>, 
<a href="/search/stat?searchtype=author&query=Hjalmarsson%2C+H">H&#xe5;kan Hjalmarsson</a>, 
<a href="/search/stat?searchtype=author&query=Chotteau%2C+V">V&#xe9;ronique Chotteau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we consider the well known problem of non-linear
identification of the rates of the reactions involved in cells with Monod
functions. In bioprocesses, generating data is very expensive and long and so
it is important to incorporate prior knowledge on the Monod kinetic parameters.
Bayesian estimation is an elegant estimation technique which deals with
parameter estimation with prior knowledge modeled as probability density
functions. However, we might not have an accurate knowledge of the kinetic
parameters such as interval bounds, especially for newly developed cell lines.
Hence, we consider the case when there is no accurate prior information on the
kinetic parameters except qualitative knowledge such that their non-negativity.
A log-Gaussian prior distribution is considered for the parameters and the mean
and variances of these distribution are tuned using the Expectation
Maximization algorithm. The algorithm requires to use Metropolis Hastings
within Gibbs sampling which can be computationally expensive. We develop a
novel variant of the Metropolis-Hastings within Gibbs sampling sampling scheme
in order to accelerate and improve on the hyperparameter tuning. We show that
it can give better modeling performances on a relatively large-scale simulation
example compared to available methods in the literature.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04740" title="Abstract">arXiv:2402.04740</a> (cross-list from stat.ML) [<a href="/pdf/2402.04740" title="Download PDF">pdf</a>, <a href="/format/2402.04740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Joseph%2C+S">Sobin Joseph</a>, 
<a href="/search/stat?searchtype=author&query=Jain%2C+S">Shashi Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">An extension of the Hawkes process, the Marked Hawkes process distinguishes
itself by featuring variable jump size across each event, in contrast to the
constant jump size observed in a Hawkes process without marks. While extensive
literature has been dedicated to the non-parametric estimation of both the
linear and non-linear Hawkes process, there remains a significant gap in the
literature regarding the marked Hawkes process. In response to this, we propose
a methodology for estimating the conditional intensity of the marked Hawkes
process. We introduce two distinct models: \textit{Shallow Neural Hawkes with
marks}- for Hawkes processes with excitatory kernels and \textit{Neural Network
for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these
approaches take the past arrival times and their corresponding marks as the
input to obtain the arrival intensity. This approach is entirely
non-parametric, preserving the interpretability associated with the marked
Hawkes process. To validate the efficacy of our method, we subject the method
to synthetic datasets with known ground truth. Additionally, we apply our
method to model cryptocurrency order book data, demonstrating its applicability
to real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04753" title="Abstract">arXiv:2402.04753</a> (cross-list from eess.IV) [<a href="/pdf/2402.04753" title="Download PDF">pdf</a>, <a href="/format/2402.04753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cortical Surface Diffusion Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+Z">Zhenshan Xie</a>, 
<a href="/search/eess?searchtype=author&query=Dahan%2C+S">Simon Dahan</a>, 
<a href="/search/eess?searchtype=author&query=Williams%2C+L+Z+J">Logan Z. J. Williams</a>, 
<a href="/search/eess?searchtype=author&query=Cardoso%2C+M+J">M. Jorge Cardoso</a>, 
<a href="/search/eess?searchtype=author&query=Robinson%2C+E+C">Emma C. Robinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cortical surface analysis has gained increased prominence, given its
potential implications for neurological and developmental disorders.
Traditional vision diffusion models, while effective in generating natural
images, present limitations in capturing intricate development patterns in
neuroimaging due to limited datasets. This is particularly true for generating
cortical surfaces where individual variability in cortical morphology is high,
leading to an urgent need for better methods to model brain development and
diverse variability inherent across different individuals. In this work, we
proposed a novel diffusion model for the generation of cortical surface
metrics, using modified surface vision transformers as the principal
architecture. We validate our method in the developing Human Connectome Project
(dHCP), the results suggest our model demonstrates superior performance in
capturing the intricate details of evolving cortical surfaces. Furthermore, our
model can generate high-quality realistic samples of cortical surfaces
conditioned on postmenstrual age(PMA) at scan.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04770" title="Abstract">arXiv:2402.04770</a> (cross-list from quant-ph) [<a href="/pdf/2402.04770" title="Download PDF">pdf</a>, <a href="/format/2402.04770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous-Variable QKD with key rates far above Devetak-Winter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ray%2C+A+A">Arpan Akash Ray</a>, 
<a href="/search/quant-ph?searchtype=author&query=Skoric%2C+B">Boris Skoric</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Continuous-Variable Quantum Key Distribution (CVQKD) at large distances has
such high noise levels that the employed error-correcting codes must have very
low rate. In this regime it becomes feasible to implement random-codebook error
correction, which is known to perform close to capacity. We propose a
random-codebook reverse reconciliation scheme for CVQKD that is inspired by
spread-spectrum watermarking. Our scheme has a novel way of achieving
statistical decoupling between the publicly sent reconciliation data and the
secret key. We provide a theoretical analysis of the secret key rate and we
present numerical results. The best performance is obtained when the message
size exceeds the mutual information I(X;Y) between Alice and Bob's
measurements. This somewhat counter-intuitive result is understood from a
tradeoff between code rate and frame rejection rate, combined with the fact
that error correction for QKD needs to reconcile only random data. We obtain
secret key lengths that lie far above the Devetak-Winter value I(X;Y)-I(E;Y).
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04772" title="Abstract">arXiv:2402.04772</a> (cross-list from math.FA) [<a href="/pdf/2402.04772" title="Download PDF">pdf</a>, <a href="/ps/2402.04772" title="Download PostScript">ps</a>, <a href="/format/2402.04772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth  Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bajpai%2C+H">Harshit Bajpai</a>, 
<a href="/search/math?searchtype=author&query=Mittal%2C+G">Gaurav Mittal</a>, 
<a href="/search/math?searchtype=author&query=Giri%2C+A+K">Ankik Kumar Giri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Classical Analysis and ODEs (math.CA); Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this study, we present and analyze a novel variant of the stochastic
gradient descent method, referred as Stochastic data-driven Bouligand Landweber
iteration tailored for addressing the system of non-smooth ill-posed inverse
problems. Our method incorporates the utilization of training data, using a
bounded linear operator, which guides the iterative procedure. At each
iteration step, the method randomly chooses one equation from the nonlinear
system with data-driven term. When dealing with the precise or exact data, it
has been established that mean square iteration error converges to zero.
However, when confronted with the noisy data, we employ our approach in
conjunction with a predefined stopping criterion, which we refer to as an
\textit{a-priori} stopping rule. We provide a comprehensive theoretical
foundation, establishing convergence and stability for this scheme within the
realm of infinite-dimensional Hilbert spaces. These theoretical underpinnings
are further bolstered by discussing an example that fulfills assumptions of the
paper.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04777" title="Abstract">arXiv:2402.04777</a> (cross-list from stat.ML) [<a href="/pdf/2402.04777" title="Download PDF">pdf</a>, <a href="/format/2402.04777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast score-based search algorithm for maximal ancestral graphs using  entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hu%2C+Z">Zhongyi Hu</a>, 
<a href="/search/stat?searchtype=author&query=Evans%2C+R">Robin Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">\emph{Maximal ancestral graph} (MAGs) is a class of graphical model that
extend the famous \emph{directed acyclic graph} in the presence of latent
confounders. Most score-based approaches to learn the unknown MAG from
empirical data rely on BIC score which suffers from instability and heavy
computations. We propose to use the framework of imsets
\citep{studeny2006probabilistic} to score MAGs using empirical entropy
estimation and the newly proposed \emph{refined Markov property}
\citep{hu2023towards}. Our graphical search procedure is similar to
\citet{claassen2022greedy} but improved from our theoretical results. We show
that our search algorithm is polynomial in number of nodes by restricting
degree, maximal head size and number of discriminating paths. In simulated
experiment, our algorithm shows superior performance compared to other state of
art MAG learning algorithms.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04782" title="Abstract">arXiv:2402.04782</a> (cross-list from math.ST) [<a href="/pdf/2402.04782" title="Download PDF">pdf</a>, <a href="/format/2402.04782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From fuzzy information to community detection: an approach to social  networks analysis with soft information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guti%C3%A9rrez%2C+I">Inmaculada Guti&#xe9;rrez</a>, 
<a href="/search/math?searchtype=author&query=G%C3%B3mez%2C+D">Daniel G&#xf3;mez</a>, 
<a href="/search/math?searchtype=author&query=Castro%2C+J">Javier Castro</a>, 
<a href="/search/math?searchtype=author&query=Esp%C3%ADnola%2C+R">Rosa Esp&#xed;nola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics 2022, 10, 4348
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">On the basis of network analysis, and within the context of modeling
imprecision or vague information with fuzzy sets, we propose an innovative way
to analyze, aggregate and apply this uncertain knowledge into community
detection of real-life problems. This work is set on the existence of one (or
multiple) soft information sources, independent of the network considered,
assuming this extra knowledge is modeled by a vector of fuzzy sets (or a family
of vectors). This information may represent, for example, how much some people
agree with a specific law, or their position against several politicians. We
emphasize the importance of being able to manage the vagueness which usually
appears in real life because of the common use of linguistic terms. Then, we
propose a constructive method to build fuzzy measures from fuzzy sets. These
measures are the basis of a new representation model which combines the
information of a network with that of fuzzy sets, specifically when it comes to
linguistic terms. We propose a specific application of that model in terms of
finding communities in a network with additional soft information. To do so, we
propose an efficient algorithm and measure its performance by means of a
benchmarking process, obtaining high-quality results.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04785" title="Abstract">arXiv:2402.04785</a> (cross-list from math.OC) [<a href="/pdf/2402.04785" title="Download PDF">pdf</a>, <a href="/format/2402.04785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time  Complexity Under Arbitrary Computation and Communication Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tyurin%2C+A">Alexander Tyurin</a>, 
<a href="/search/math?searchtype=author&query=Pozzi%2C+M">Marta Pozzi</a>, 
<a href="/search/math?searchtype=author&query=Ilin%2C+I">Ivan Ilin</a>, 
<a href="/search/math?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider nonconvex stochastic optimization problems in the asynchronous
centralized distributed setup where the communication times from workers to a
server can not be ignored, and the computation and communication times are
potentially different for all workers. Using an unbiassed compression
technique, we develop a new method-Shadowheart SGD-that provably improves the
time complexities of all previous centralized methods. Moreover, we show that
the time complexity of Shadowheart SGD is optimal in the family of centralized
methods with compressed communication. We also consider the bidirectional
setup, where broadcasting from the server to the workers is non-negligible, and
develop a corresponding method.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04834" title="Abstract">arXiv:2402.04834</a> (cross-list from quant-ph) [<a href="/pdf/2402.04834" title="Download PDF">pdf</a>, <a href="/format/2402.04834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A blockBP decoder for the surface code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kaufmann%2C+A">Aviad Kaufmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Arad%2C+I">Itai Arad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures. Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We present a new decoder for the surface code, which combines the accuracy of
the tensor-network decoders with the efficiency and parallelism of the
belief-propagation algorithm. Our main idea is to replace the expensive
tensor-network contraction step in the tensor-network decoders with the blockBP
algorithm - a recent approximate contraction algorithm, based on belief
propagation. Our decoder is therefore a belief-propagation decoder that works
in the degenerate maximal likelihood decoding framework. Unlike conventional
tensor-network decoders, our algorithm can run efficiently in parallel, and may
therefore be suitable for real-time decoding. We numerically test our decoder
and show that for a large range of lattice sizes and noise levels it delivers a
logical error probability that outperforms the Minimal-Weight-Perfect-Matching
(MWPM) decoder, sometimes by more than an order of magnitude.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04845" title="Abstract">arXiv:2402.04845</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.04845" title="Download PDF">pdf</a>, <a href="/format/2402.04845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlphaFold Meets Flow Matching for Generating Protein Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Jing%2C+B">Bowen Jing</a>, 
<a href="/search/q-bio?searchtype=author&query=Berger%2C+B">Bonnie Berger</a>, 
<a href="/search/q-bio?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The biological functions of proteins often depend on dynamic structural
ensembles. In this work, we develop a flow-based generative modeling approach
for learning and sampling the conformational landscapes of proteins. We
repurpose highly accurate single-state predictors such as AlphaFold and ESMFold
and fine-tune them under a custom flow matching framework to obtain
sequence-conditoned generative models of protein structure called AlphaFlow and
ESMFlow. When trained and evaluated on the PDB, our method provides a superior
combination of precision and diversity compared to AlphaFold with MSA
subsampling. When further trained on ensembles from all-atom MD, our method
accurately captures conformational flexibility, positional distributions, and
higher-order ensemble observables for unseen proteins. Moreover, our method can
diversify a static PDB structure with faster wall-clock convergence to certain
equilibrium properties than replicate MD trajectories, demonstrating its
potential as a proxy for expensive physics-based simulations. Code is available
at https://github.com/bjing2016/alphaflow.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04851" title="Abstract">arXiv:2402.04851</a> (cross-list from math.CO) [<a href="/pdf/2402.04851" title="Download PDF">pdf</a>, <a href="/ps/2402.04851" title="Download PostScript">ps</a>, <a href="/format/2402.04851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grand zigzag knight&#x27;s paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baril%2C+J">Jean-Luc Baril</a>, 
<a href="/search/math?searchtype=author&query=Hassler%2C+N">Nathana&#xeb;l Hassler</a>, 
<a href="/search/math?searchtype=author&query=Kirgizov%2C+S">Sergey Kirgizov</a>, 
<a href="/search/math?searchtype=author&query=Ram%C3%ADrez%2C+J+L">Jos&#xe9; L. Ram&#xed;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We study the enumeration of different classes of grand knight's paths in the
plane. In particular, we focus on the subsets of zigzag knight's paths subject
to constraints. These constraints include ending at ordinate 0, bounded by a
horizontal line, confined within a tube, among other considerations. We present
our results using generating functions or direct closed-form expressions. We
derive asymptotic results, finding approximations for quantities such as the
probability that a zigzag knight's path stays in some area of the plane, or for
the average of the final height of such a path. Additionally, we exhibit some
bijections between grand zigzag knight's paths and some pairs of compositions.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04866" title="Abstract">arXiv:2402.04866</a> (cross-list from eess.AS) [<a href="/pdf/2402.04866" title="Download PDF">pdf</a>, <a href="/format/2402.04866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Room transfer function reconstruction using complex-valued neural  networks and irregularly distributed microphones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ronchini%2C+F">Francesca Ronchini</a>, 
<a href="/search/eess?searchtype=author&query=Comanducci%2C+L">Luca Comanducci</a>, 
<a href="/search/eess?searchtype=author&query=Pezzoli%2C+M">Mirco Pezzoli</a>, 
<a href="/search/eess?searchtype=author&query=Antonacci%2C+F">Fabio Antonacci</a>, 
<a href="/search/eess?searchtype=author&query=Sarti%2C+A">Augusto Sarti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of room transfer functions measured at scattered points in the room. In
this study, we employ complex-valued neural networks to estimate room transfer
functions in the frequency range of the first room resonances, using a few
irregularly distributed microphones. To the best of our knowledge, this is the
first time complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
real-valued neural network method and a state-of-the-art kernel-based signal
processing approach for sound field reconstruction, showing that the proposed
technique exhibits relevant advantages in terms of phase accuracy and overall
quality of the reconstructed sound field.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04885" title="Abstract">arXiv:2402.04885</a> (cross-list from stat.ML) [<a href="/pdf/2402.04885" title="Download PDF">pdf</a>, <a href="/format/2402.04885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Gaussian Process for Branching and Nested Hyperparameter  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+J">Jiazhao Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Hung%2C+Y">Ying Hung</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Choosing appropriate hyperparameters plays a crucial role in the success of
neural networks as hyper-parameters directly control the behavior and
performance of the training algorithms. To obtain efficient tuning, Bayesian
optimization methods based on Gaussian process (GP) models are widely used.
Despite numerous applications of Bayesian optimization in deep learning, the
existing methodologies are developed based on a convenient but restrictive
assumption that the tuning parameters are independent of each other. However,
tuning parameters with conditional dependence are common in practice. In this
paper, we focus on two types of them: branching and nested parameters. Nested
parameters refer to those tuning parameters that exist only within a particular
setting of another tuning parameter, and a parameter within which other
parameters are nested is called a branching parameter. To capture the
conditional dependence between branching and nested parameters, a unified
Bayesian optimization framework is proposed. The sufficient conditions are
rigorously derived to guarantee the validity of the kernel function, and the
asymptotic convergence of the proposed optimization framework is proven under
the continuum-armed-bandit setting. Based on the new GP model, which accounts
for the dependent structure among input variables through a new kernel
function, higher prediction accuracy and better optimization efficiency are
observed in a series of synthetic simulations and real data applications of
neural networks. Sensitivity analysis is also performed to provide insights
into how changes in hyperparameter values affect prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04907" title="Abstract">arXiv:2402.04907</a> (cross-list from math.CO) [<a href="/pdf/2402.04907" title="Download PDF">pdf</a>, <a href="/ps/2402.04907" title="Download PostScript">ps</a>, <a href="/format/2402.04907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a Combinatorial Problem Arising in Machine Teaching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=H%C3%A5vardstun%2C+B">Brigt H&#xe5;vardstun</a>, 
<a href="/search/math?searchtype=author&query=Kratochv%C3%ADl%2C+J">Jan Kratochv&#xed;l</a>, 
<a href="/search/math?searchtype=author&query=Sunde%2C+J">Joakim Sunde</a>, 
<a href="/search/math?searchtype=author&query=Arne%2C+J">Jan Arne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study a model of machine teaching where the teacher mapping is constructed
from a size function on both concepts and examples. The main question in
machine teaching is the minimum number of examples needed for any concept, the
so-called teaching dimension. A recent paper [7] conjectured that the worst
case for this model, as a function of the size of the concept class, occurs
when the consistency matrix contains the binary representations of numbers from
zero and up. In this paper we prove their conjecture. The result can be seen as
a generalization of a theorem resolving the edge isoperimetry problem for
hypercubes [12], and our proof is based on a lemma of [10].
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04921" title="Abstract">arXiv:2402.04921</a> (cross-list from eess.IV) [<a href="/pdf/2402.04921" title="Download PDF">pdf</a>, <a href="/format/2402.04921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Two-shot All You Need? A Label-efficient Approach for Video  Segmentation in Breast Ultrasound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zeng%2C+J">Jiajun Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Ruobing Huang</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+D">Dong Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, 2 tables, accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Breast lesion segmentation from breast ultrasound (BUS) videos could assist
in early diagnosis and treatment. Existing video object segmentation (VOS)
methods usually require dense annotation, which is often inaccessible for
medical datasets. Furthermore, they suffer from accumulative errors and a lack
of explicit space-time awareness. In this work, we propose a novel two-shot
training paradigm for BUS video segmentation. It not only is able to capture
free-range space-time consistency but also utilizes a source-dependent
augmentation scheme. This label-efficient learning framework is validated on a
challenging in-house BUS video dataset. Results showed that it gained
comparable performance to the fully annotated ones given only 1.9% training
labels.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04922" title="Abstract">arXiv:2402.04922</a> (cross-list from stat.ML) [<a href="/pdf/2402.04922" title="Download PDF">pdf</a>, <a href="/format/2402.04922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voronoi Candidates for Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wycoff%2C+N">Nathan Wycoff</a>, 
<a href="/search/stat?searchtype=author&query=Smith%2C+J+W">John W. Smith</a>, 
<a href="/search/stat?searchtype=author&query=Booth%2C+A+S">Annie S. Booth</a>, 
<a href="/search/stat?searchtype=author&query=Gramacy%2C+R+B">Robert B. Gramacy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> comments very welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian optimization (BO) offers an elegant approach for efficiently
optimizing black-box functions. However, acquisition criteria demand their own
challenging inner-optimization, which can induce significant overhead. Many
practical BO methods, particularly in high dimension, eschew a formal,
continuous optimization of the acquisition function and instead search
discretely over a finite set of space-filling candidates. Here, we propose to
use candidates which lie on the boundary of the Voronoi tessellation of the
current design points, so they are equidistant to two or more of them. We
discuss strategies for efficient implementation by directly sampling the
Voronoi boundary without explicitly generating the tessellation, thus
accommodating large designs in high dimension. On a battery of test problems
optimized via Gaussian processes with expected improvement, our proposed
approach significantly improves the execution time of a multi-start continuous
search without a loss in accuracy.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04944" title="Abstract">arXiv:2402.04944</a> (cross-list from math.DG) [<a href="/pdf/2402.04944" title="Download PDF">pdf</a>, <a href="/format/2402.04944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elastic Analysis of Augmented Curves and Constrained Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nava-Yazdani%2C+E">Esfandiar Nava-Yazdani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Differential Geometry (math.DG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The square root velocity transformation is crucial for efficiently employing
the elastic approach in functional and shape data analysis of curves. We study
fundamental geometric properties of curves under this transformation. Moreover,
utilizing natural geometric constructions, we employ the approach for intrinsic
comparison within several classes of surfaces and augmented curves, which arise
in the real world applications such as tubes, ruled surfaces spherical strips,
protein molecules and hurricane tracks.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04980" title="Abstract">arXiv:2402.04980</a> (cross-list from stat.ML) [<a href="/pdf/2402.04980" title="Download PDF">pdf</a>, <a href="/format/2402.04980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotics of feature learning in two-layer networks after one  gradient-step
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cui%2C+H">Hugo Cui</a>, 
<a href="/search/stat?searchtype=author&query=Pesce%2C+L">Luca Pesce</a>, 
<a href="/search/stat?searchtype=author&query=Dandi%2C+Y">Yatin Dandi</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+Y+M">Yue M. Lu</a>, 
<a href="/search/stat?searchtype=author&query=Zdeborov%C3%A1%2C+L">Lenka Zdeborov&#xe1;</a>, 
<a href="/search/stat?searchtype=author&query=Loureiro%2C+B">Bruno Loureiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this manuscript we investigate the problem of how two-layer neural
networks learn features from data, and improve over the kernel regime, after
being trained with a single gradient descent step. Leveraging a connection from
(Ba et al., 2022) with a non-linear spiked matrix model and recent progress on
Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic
description of the generalization error in the high-dimensional limit where the
number of samples $n$, the width $p$ and the input dimension $d$ grow at a
proportional rate. We characterize exactly how adapting to the data is crucial
for the network to efficiently learn non-linear functions in the direction of
the gradient -- where at initialization it can only express linear functions in
this regime. To our knowledge, our results provides the first tight description
of the impact of feature learning in the generalization of two-layer neural
networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
perturbative finite width corrections of the conjugate and neural tangent
kernels.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04997" title="Abstract">arXiv:2402.04997</a> (cross-list from stat.ML) [<a href="/pdf/2402.04997" title="Download PDF">pdf</a>, <a href="/format/2402.04997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows  with Applications to Protein Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Campbell%2C+A">Andrew Campbell</a>, 
<a href="/search/stat?searchtype=author&query=Yim%2C+J">Jason Yim</a>, 
<a href="/search/stat?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/stat?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>, 
<a href="/search/stat?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 11 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Combining discrete and continuous data is an important capability for
generative models. We present Discrete Flow Models (DFMs), a new flow-based
model of discrete data that provides the missing link in enabling flow-based
generative models to be applied to multimodal continuous and discrete data
problems. Our key insight is that the discrete equivalent of continuous space
flow matching can be realized using Continuous Time Markov Chains. DFMs benefit
from a simple derivation that includes discrete diffusion models as a specific
instance while allowing improved performance over existing diffusion-based
approaches. We utilize our DFMs method to build a multimodal flow-based
modeling framework. We apply this capability to the task of protein co-design,
wherein we learn a model for jointly generating protein structure and sequence.
Our approach achieves state-of-the-art co-design performance while allowing the
same multimodal model to be used for flexible generation of the sequence or
structure.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05041" title="Abstract">arXiv:2402.05041</a> (cross-list from math.PR) [<a href="/pdf/2402.05041" title="Download PDF">pdf</a>, <a href="/format/2402.05041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-reversible lifts of reversible diffusion processes and relaxation  times
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eberle%2C+A">Andreas Eberle</a>, 
<a href="/search/math?searchtype=author&query=L%C3%B6rler%2C+F">Francis L&#xf6;rler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We propose a new concept of lifts of reversible diffusion processes and show
that various well-known non-reversible Markov processes arising in applications
are lifts in this sense of simple reversible diffusions. Furthermore, we
introduce a concept of non-asymptotic relaxation times and show that these can
at most be reduced by a square root through lifting, generalising a related
result in discrete time. Finally, we demonstrate how the recently developed
approach to quantitative hypocoercivity based on space-time Poincar\'e
inequalities can be rephrased and simplified in the language of lifts and how
it can be applied to find optimal lifts.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05059" title="Abstract">arXiv:2402.05059</a> (cross-list from math.NT) [<a href="/pdf/2402.05059" title="Download PDF">pdf</a>, <a href="/format/2402.05059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting Kani&#x27;s Lemma and path-finding in the Bruhat-Tits tree to  compute supersingular endomorphism rings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eisentraeger%2C+K">Kirsten Eisentraeger</a>, 
<a href="/search/math?searchtype=author&query=Scullard%2C+G">Gabrielle Scullard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages. 5 figures. Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We give a deterministic polynomial time algorithm to compute the endomorphism
ring of a supersingular elliptic curve in characteristic p, provided that we
are given two noncommuting endomorphisms and the factorization of the
discriminant of the ring $\mathcal{O}_0$ they generate. At each prime $q$ for
which $\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally
by computing a q-maximal order containing it and, when $q \neq p$, recovering a
path to $\text{End}(E) \otimes \mathbb{Z}_q$ in the Bruhat-Tits tree. We use
techniques of higher-dimensional isogenies to navigate towards the local
endomorphism ring. Our algorithm improves on a previous algorithm which
requires a restricted input and runs in subexponential time under certain
heuristics. Page and Wesolowski give a probabilistic polynomial time algorithm
to compute the endomorphism ring on input of a single non-scalar endomorphism.
Beyond using techniques of higher-dimensional isogenies to divide endomorphisms
by a scalar, our methods are completely different.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05061" title="Abstract">arXiv:2402.05061</a> (cross-list from math.OC) [<a href="/pdf/2402.05061" title="Download PDF">pdf</a>, <a href="/format/2402.05061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $H_{\infty}$-Optimal Estimator Synthesis for Coupled Linear 2D PDEs  using Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jagt%2C+D+S">Declan S. Jagt</a>, 
<a href="/search/math?searchtype=author&query=Peet%2C+M+M">Matthew M. Peet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">It has recently been shown that, any suitably well-posed PDE in one or two
spatial dimensions can be equivalently represented as a Partial Integral
Equation (PIE), expressing the dynamics in terms of Partial Integral (PI)
operators. Parameterizing storage functionals by PI operators as well,
$L_2$-gain analysis of the PDE can then be posed as a linear operator
inequality on PI operators, which can be solved using convex optimization. In
this paper, we build on this result to derive a convex-optimization-based test
for constructing an $H_{\infty}$-optimal estimator for 2D PDEs, extending a
similar result for 1D PDEs. In particular, we first show how a well-posed 2D
PDE with infinite-dimensional outputs can be equivalently represented as a PIE,
and we parameterize an associated Luenberger-type estimator by a 2D PI operator
$\mathcal{L}$. Parameterizing a storage functional for the resulting error
dynamics by another PI operator $\mathcal{P}$, we prove that the
$H_{\infty}$-norm of the error dynamics can be minimized by solving a linear
operator inequality on PI operator variables $\mathcal{P}$ and
$\mathcal{W}:=\mathcal{P}\mathcal{L}$. Finally, we derive an explicit
expression for the inverse of the operator $\mathcal{P}$, and propose a
parameterization of variables $\mathcal{P}$ and $\mathcal{W}$ by matrices,
posing the problem of finding an $H_{\infty}$-optimal estimator gain
$\mathcal{L}=\mathcal{P}^{-1}\mathcal{W}$ as a convex optimization problem. We
implement this test in the PIETOOLS software suite, and apply this software to
construct an estimator for a 2D heat equation with state observations along the
boundary.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05067" title="Abstract">arXiv:2402.05067</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.05067" title="Download PDF">pdf</a>, <a href="/format/2402.05067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Modelling with Physics-informed Neural Network: from  Large-scale Dynamics to Small-scale Predictions in Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/physics?searchtype=author&query=Lai%2C+P">Pengyu Lai</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+D">Di Yang</a>, 
<a href="/search/physics?searchtype=author&query=Xu%2C+H">Hui Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Multiscale phenomena manifest across various scientific domains, presenting a
ubiquitous challenge in accurately and effectively predicting multiscale
dynamics in complex systems. In this paper, a novel solving mode is proposed
for characterizing multiscale dynamics through a decoupling method. By
modelling large-scale dynamics independently and treating small-scale dynamics
as a slaved system, a Spectral PINN is developed to approach the small-scale
system in an orthogonal basis functional space. The effectiveness of the method
is demonstrated through extensive numerical experiments, including
one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional
Navier-Stokes (NS) equations, showcasing its versatility in addressing problems
of fluid dynamics. Furthermore, we also delve into the application of the
proposed approach to more complex problems, including non-uniform meshes,
complex geometries, large-scale data with noise, and high-dimensional
small-scale dynamics. The discussions about these scenarios contribute to a
comprehensive understanding of the method's capabilities and limitations. This
novel decoupling approach simplifies the analysis and prediction of
spatiotemporal systems, where large-scale data can be obtained with low
computational demands, followed by Spectral PINNs for capturing small-scale
dynamics with improved efficiency and accuracy.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05071" title="Abstract">arXiv:2402.05071</a> (cross-list from math.OC) [<a href="/pdf/2402.05071" title="Download PDF">pdf</a>, <a href="/format/2402.05071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the Reach of First-Order Algorithms for Nonconvex Min-Max  Problems with Cohypomonotonicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alacaoglu%2C+A">Ahmet Alacaoglu</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+D">Donghwan Kim</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S+J">Stephen J. Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems
either satisfying $\rho$-cohypomonotonicity or admitting a solution to the
$\rho$-weakly Minty Variational Inequality (MVI), where larger values of the
parameter $\rho&gt;0$ correspond to a greater degree of nonconvexity. These
problem classes include examples in two player reinforcement learning,
interaction dominant min-max problems, and certain synthetic test problems on
which classical min-max algorithms fail. It has been conjectured that
first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$,
but existing results in the literature have stagnated at the tighter
requirement $\rho &lt; \frac{1}{2L}$. With a simple argument, we obtain optimal or
best-known complexity guarantees with cohypomonotonicity or weak MVI conditions
for $\rho &lt; \frac{1}{L}$. The algorithms we analyze are inexact variants of
Halpern and Krasnosel'ski\u{\i}-Mann (KM) iterations. We also provide
algorithms and complexity guarantees in the stochastic case with the same range
on $\rho$. Our main insight for the improvements in the convergence analyses is
to harness the recently proposed "conic nonexpansiveness" property of
operators. As byproducts, we provide a refined analysis for inexact Halpern
iteration and propose a stochastic KM iteration with a multilevel Monte Carlo
estimator.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05079" title="Abstract">arXiv:2402.05079</a> (cross-list from eess.IV) [<a href="/pdf/2402.05079" title="Download PDF">pdf</a>, <a href="/format/2402.05079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Ziyang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+J">Jian-Qing Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+G">Ge Cui</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In recent advancements in medical image analysis, Convolutional Neural
Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks.
While the former excels in capturing local features through its convolution
operations, the latter achieves remarkable global context understanding by
leveraging self-attention mechanisms. However, both architectures exhibit
limitations in efficiently modeling long-range dependencies within medical
images, which is a critical aspect for precise segmentation. Inspired by the
Mamba architecture, known for its proficiency in handling long sequences and
global contextual information with enhanced computational efficiency as a State
Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes
the U-Net in medical image segmentation with Mamba's capability. Mamba-UNet
adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused
with skip connections to preserve spatial information across different scales
of the network. This design facilitates a comprehensive feature learning
process, capturing intricate details and broader semantic contexts within
medical images. We introduce a novel integration mechanism within the VMamba
blocks to ensure seamless connectivity and information flow between the encoder
and decoder paths, enhancing the segmentation performance. We conducted
experiments on publicly available MRI cardiac multi-structures segmentation
dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in
medical image segmentation under the same hyper-parameter setting. The source
code and baseline implementations are available.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05080" title="Abstract">arXiv:2402.05080</a> (cross-list from quant-ph) [<a href="/pdf/2402.05080" title="Download PDF">pdf</a>, <a href="/format/2402.05080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing three-way entangled and nonlocal two-way entangled single  particle states via alternate quantum walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Panda%2C+D+K">Dinesh Kumar Panda</a>, 
<a href="/search/quant-ph?searchtype=author&query=Benjamin%2C+C">Colin Benjamin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Systems and Control (eess.SY); Optics (physics.optics)

</div>
<p class="mathjax">Entanglement with single-particle states is advantageous in quantum
technology because of their ability to encode and process information more
securely than their multi-particle analogs. Three-way and nonlocal two-way
entangled single-particle states are desirable in this context. Herein, we
generate three-way entanglement from an initially separable state involving
three degrees of freedom (DoF) of a quantum particle, which evolves via a 2D
alternate quantum walk employing a resource-saving single-qubit coin. We
achieve maximum possible values for the three-way entanglement quantified by
the $\pi$-tangle between the 3 DoF. We also generate optimal two-way nonlocal
entanglement, quantified by the negativity between the nonlocal position and
the DoF of the particle. This prepared architecture using quantum walks can be
experimentally realized with a photon.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05088" title="Abstract">arXiv:2402.05088</a> (cross-list from math.CO) [<a href="/pdf/2402.05088" title="Download PDF">pdf</a>, <a href="/format/2402.05088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domination and packing in graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guti%C3%A9rrez%2C+R+G+J">Renzo G&#xf3;mez Juan Guti&#xe9;rrez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Given a graph~$G$, the domination number, denoted by~$\gamma(G)$, is the
minimum cardinality of a dominating set in~$G$. Dual to the notion of
domination number is the packing number of a graph. A packing of~$G$ is a set
of vertices whose pairwise distance is at least three. The packing
number~$\rho(G)$ of~$G$ is the maximum cardinality of one such set.
Furthermore, the inequality~$\rho(G) \leq \gamma(G)$ is well-known. Henning et
al.\ conjectured that~$\gamma(G) \leq 2\rho(G)+1$ if~$G$ is subcubic. In this
paper we progress towards this conjecture by showing that~${\gamma(G) \leq
\frac{120}{49}\rho(G)}$ if~$G$ is a bipartite cubic graph. We also show that
$\gamma(G) \leq 3\rho(G)$ if~$G$ is a maximal outerplanar graph, and
that~$\gamma(G) \leq 2\rho(G)$ if~$G$ is a biconvex graph. Moreover, in the
last case, we show that this upper bound is tight.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05101" title="Abstract">arXiv:2402.05101</a> (cross-list from stat.ML) [<a href="/pdf/2402.05101" title="Download PDF">pdf</a>, <a href="/ps/2402.05101" title="Download PostScript">ps</a>, <a href="/format/2402.05101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tighter Generalisation Bounds via Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Viallard%2C+P">Paul Viallard</a>, 
<a href="/search/stat?searchtype=author&query=Haddouche%2C+M">Maxime Haddouche</a>, 
<a href="/search/stat?searchtype=author&query=%C5%9Eim%C5%9Fekli%2C+U">Umut &#x15e;im&#x15f;ekli</a>, 
<a href="/search/stat?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper contains a recipe for deriving new PAC-Bayes generalisation bounds
based on the $(f, \Gamma)$-divergence, and, in addition, presents PAC-Bayes
generalisation bounds where we interpolate between a series of probability
divergences (including but not limited to KL, Wasserstein, and total
variation), making the best out of many worlds depending on the posterior
distributions properties. We explore the tightness of these bounds and connect
them to earlier results from statistical learning, which are specific cases. We
also instantiate our bounds as training objectives, yielding non-trivial
guarantees and practical performances.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu,  8 Feb 24</h3>
<dl>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.07488" title="Abstract">arXiv:2001.07488</a> (replaced) [<a href="/pdf/2001.07488" title="Download PDF">pdf</a>, <a href="/format/2001.07488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Profunctor Optics, a Categorical Update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clarke%2C+B">Bryce Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Elkins%2C+D">Derek Elkins</a>, 
<a href="/search/cs?searchtype=author&query=Gibbons%2C+J">Jeremy Gibbons</a>, 
<a href="/search/cs?searchtype=author&query=Loregian%2C+F">Fosco Loregian</a>, 
<a href="/search/cs?searchtype=author&query=Milewski%2C+B">Bartosz Milewski</a>, 
<a href="/search/cs?searchtype=author&query=Pillmore%2C+E">Emily Pillmore</a>, 
<a href="/search/cs?searchtype=author&query=Rom%C3%A1n%2C+M">Mario Rom&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages. Final version with Compositionality metadata, does not change theorem numbering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.00178" title="Abstract">arXiv:2002.00178</a> (replaced) [<a href="/pdf/2002.00178" title="Download PDF">pdf</a>, <a href="/format/2002.00178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Equivalence between Bayesian Priors and Penalties in Variational  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolinski%2C+P">Pierre Wolinski</a>, 
<a href="/search/cs?searchtype=author&query=Charpiat%2C+G">Guillaume Charpiat</a>, 
<a href="/search/cs?searchtype=author&query=Ollivier%2C+Y">Yann Ollivier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.08388" title="Abstract">arXiv:2011.08388</a> (replaced) [<a href="/pdf/2011.08388" title="Download PDF">pdf</a>, <a href="/format/2011.08388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptation based Interpretable Image Emotion Recognition using  Facial Expression Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Puneet Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+B">Balasubramanian Raman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.14600" title="Abstract">arXiv:2012.14600</a> (replaced) [<a href="/pdf/2012.14600" title="Download PDF">pdf</a>, <a href="/format/2012.14600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Guide to CAN IDS Data &amp; Introduction of the ROAD Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+M+E">Miki E. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Bridges%2C+R+A">Robert A. Bridges</a>, 
<a href="/search/cs?searchtype=author&query=Iannacone%2C+M+D">Michael D. Iannacone</a>, 
<a href="/search/cs?searchtype=author&query=Hollifield%2C+S+C">Samuel C. Hollifield</a>, 
<a href="/search/cs?searchtype=author&query=Moriano%2C+P">Pablo Moriano</a>, 
<a href="/search/cs?searchtype=author&query=Hespeler%2C+S+C">Steven C. Hespeler</a>, 
<a href="/search/cs?searchtype=author&query=Kay%2C+B">Bill Kay</a>, 
<a href="/search/cs?searchtype=author&query=Combs%2C+F+L">Frank L. Combs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> title changed and author added from original version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PLoS one 19, no. 1 (2024): e0296879
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.07563" title="Abstract">arXiv:2104.07563</a> (replaced) [<a href="/pdf/2104.07563" title="Download PDF">pdf</a>, <a href="/format/2104.07563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate and discrete Euclidean vector bundles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scoccola%2C+L">Luis Scoccola</a>, 
<a href="/search/math?searchtype=author&query=Perea%2C+J+A">Jose A. Perea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 9 figures; v2: improvements to exposition; v3: improvements to exposition, final version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Forum of Mathematics, Sigma, Volume 11, 2023, e20
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00582" title="Abstract">arXiv:2105.00582</a> (replaced) [<a href="/pdf/2105.00582" title="Download PDF">pdf</a>, <a href="/ps/2105.00582" title="Download PostScript">ps</a>, <a href="/format/2105.00582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised learning for generalizable intracranial hemorrhage  detection and segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+E">Emily Lin</a>, 
<a href="/search/eess?searchtype=author&query=Yuh%2C+E">Esther Yuh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.04197" title="Abstract">arXiv:2111.04197</a> (replaced) [<a href="/pdf/2111.04197" title="Download PDF">pdf</a>, <a href="/ps/2111.04197" title="Download PostScript">ps</a>, <a href="/format/2111.04197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivalences of biprojective almost perfect nonlinear functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=G%C3%B6lo%C4%9Flu%2C+F">Faruk G&#xf6;lo&#x11f;lu</a>, 
<a href="/search/math?searchtype=author&query=K%C3%B6lsch%2C+L">Lukas K&#xf6;lsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08748" title="Abstract">arXiv:2111.08748</a> (replaced) [<a href="/pdf/2111.08748" title="Download PDF">pdf</a>, <a href="/format/2111.08748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-based diffusion approximated Markov decision processes for  autonomous navigation and control on unstructured terrains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junhong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Kai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+J+M">Jason M. Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Stump%2C+E+A">Ethan A. Stump</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lantao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Journal of Robotics Research. arXiv admin note: text overlap with <a href="/abs/2006.02008">arXiv:2006.02008</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.12581" title="Abstract">arXiv:2111.12581</a> (replaced) [<a href="/pdf/2111.12581" title="Download PDF">pdf</a>, <a href="/format/2111.12581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medium Access Control protocol for Collaborative Spectrum Learning in  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boyarski%2C+T">Tomer Boyarski</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leshem%2C+A">Amir Leshem</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Trans. on Signal Processing, 2023, pages: 3149-3163
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.06065" title="Abstract">arXiv:2112.06065</a> (replaced) [<a href="/pdf/2112.06065" title="Download PDF">pdf</a>, <a href="/ps/2112.06065" title="Download PostScript">ps</a>, <a href="/format/2112.06065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric bases for finite element exterior calculus spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berchenko-Kogan%2C+Y">Yakov Berchenko-Kogan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.11901" title="Abstract">arXiv:2112.11901</a> (replaced) [<a href="/pdf/2112.11901" title="Download PDF">pdf</a>, <a href="/format/2112.11901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the stability of multigraded Betti numbers and Hilbert functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Oudot%2C+S">Steve Oudot</a>, 
<a href="/search/math?searchtype=author&query=Scoccola%2C+L">Luis Scoccola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 figures; v2: adds section on efficient computability of lower bounds, section on consequences of main results, no-go result (Prop. 4), generalization of Thm. 1 (Thm. 26), and improves exposition; v3: adds several details and improves exposition; v4: minor clarifications and use numbering as in published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Applied Algebra and Geometry. Vol. 8, Iss. 1
  (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG); Representation Theory (math.RT)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.14839" title="Abstract">arXiv:2205.14839</a> (replaced) [<a href="/pdf/2205.14839" title="Download PDF">pdf</a>, <a href="/ps/2205.14839" title="Download PostScript">ps</a>, <a href="/format/2205.14839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Bandits against Arbitrary Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jung-hun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07862" title="Abstract">arXiv:2207.07862</a> (replaced) [<a href="/pdf/2207.07862" title="Download PDF">pdf</a>, <a href="/format/2207.07862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAC-DO: An Efficient Output-Stationary GEMM Accelerator for CNNs Using  DRAM Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+M">Minki Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wanyeong Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10097" title="Abstract">arXiv:2207.10097</a> (replaced) [<a href="/pdf/2207.10097" title="Download PDF">pdf</a>, <a href="/format/2207.10097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of the Guided Local Hamiltonian Problem: Improved Parameters  and Extension to Excited States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cade%2C+C">Chris Cade</a>, 
<a href="/search/quant-ph?searchtype=author&query=Folkertsma%2C+M">Marten Folkertsma</a>, 
<a href="/search/quant-ph?searchtype=author&query=Weggemans%2C+J">Jordi Weggemans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 3 figures. This article was merged with and is superseded by <a href="/abs/2207.10250">arXiv:2207.10250</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01908" title="Abstract">arXiv:2208.01908</a> (replaced) [<a href="/pdf/2208.01908" title="Download PDF">pdf</a>, <a href="/format/2208.01908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mass Exit Attacks on the Lightning Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sguanci%2C+C">Cosimo Sguanci</a>, 
<a href="/search/cs?searchtype=author&query=Sidiropoulos%2C+A">Anastasios Sidiropoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08991" title="Abstract">arXiv:2208.08991</a> (replaced) [<a href="/pdf/2208.08991" title="Download PDF">pdf</a>, <a href="/format/2208.08991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Equivalent Linearization for Random Vibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04538" title="Abstract">arXiv:2209.04538</a> (replaced) [<a href="/pdf/2209.04538" title="Download PDF">pdf</a>, <a href="/format/2209.04538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase field model for multi-material shape optimization of inextensible  rods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dondl%2C+P">Patrick Dondl</a>, 
<a href="/search/math?searchtype=author&query=Maione%2C+A">Alberto Maione</a>, 
<a href="/search/math?searchtype=author&query=Wolff-Vorbeck%2C+S">Steve Wolff-Vorbeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Mathematical Physics (math-ph); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05448" title="Abstract">arXiv:2209.05448</a> (replaced) [<a href="/pdf/2209.05448" title="Download PDF">pdf</a>, <a href="/format/2209.05448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composing Copyless Streaming String Transducers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alur%2C+R">Rajeev Alur</a>, 
<a href="/search/cs?searchtype=author&query=Dohmen%2C+T">Taylor Dohmen</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Ashutosh Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08996" title="Abstract">arXiv:2209.08996</a> (replaced) [<a href="/pdf/2209.08996" title="Download PDF">pdf</a>, <a href="/format/2209.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDO-Net: Learning Elastic Properties of Deformable Objects from Graph  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Longhini%2C+A">Alberta Longhini</a>, 
<a href="/search/cs?searchtype=author&query=Moletta%2C+M">Marco Moletta</a>, 
<a href="/search/cs?searchtype=author&query=Reichlin%2C+A">Alfredo Reichlin</a>, 
<a href="/search/cs?searchtype=author&query=Welle%2C+M+C">Michael C. Welle</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01426" title="Abstract">arXiv:2210.01426</a> (replaced) [<a href="/pdf/2210.01426" title="Download PDF">pdf</a>, <a href="/format/2210.01426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Monte Carlo Graph Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kujanp%C3%A4%C3%A4%2C+K">Kalle Kujanp&#xe4;&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Babadi%2C+A">Amin Babadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kannala%2C+J">Juho Kannala</a>, 
<a href="/search/cs?searchtype=author&query=Ilin%2C+A">Alexander Ilin</a>, 
<a href="/search/cs?searchtype=author&query=Pajarinen%2C+J">Joni Pajarinen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAMAS 2024 (full paper &amp; oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02939" title="Abstract">arXiv:2210.02939</a> (replaced) [<a href="/pdf/2210.02939" title="Download PDF">pdf</a>, <a href="/format/2210.02939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fault-tolerant Coding for Entanglement-Assisted Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Belzig%2C+P">Paula Belzig</a>, 
<a href="/search/quant-ph?searchtype=author&query=Christandl%2C+M">Matthias Christandl</a>, 
<a href="/search/quant-ph?searchtype=author&query=M%C3%BCller-Hermes%2C+A">Alexander M&#xfc;ller-Hermes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13386" title="Abstract">arXiv:2210.13386</a> (replaced) [<a href="/pdf/2210.13386" title="Download PDF">pdf</a>, <a href="/format/2210.13386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contraction of Locally Differentially Private Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asoodeh%2C+S">Shahab Asoodeh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huanyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04085" title="Abstract">arXiv:2211.04085</a> (replaced) [<a href="/pdf/2211.04085" title="Download PDF">pdf</a>, <a href="/ps/2211.04085" title="Download PostScript">ps</a>, <a href="/format/2211.04085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection and depth estimation for domestic waste in outdoor  environments by sensors fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+L.+P%C3%A1ez-Ubieta%2C+I">Ignacio de L. P&#xe1;ez-Ubieta</a>, 
<a href="/search/cs?searchtype=author&query=Velasco-S%C3%A1nchez%2C+E">Edison Velasco-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Puente%2C+S+T">Santiago T. Puente</a>, 
<a href="/search/cs?searchtype=author&query=Candelas%2C+F+A">Francisco A. Candelas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been published in IFAC WC 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IFAC-PapersOnLine, Volume 56, Issue 2, 2023, Pages 9276-9281
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09869" title="Abstract">arXiv:2211.09869</a> (replaced) [<a href="/pdf/2211.09869" title="Download PDF">pdf</a>, <a href="/format/2211.09869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anciukevicius%2C+T">Titas Anciukevicius</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zexiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+M">Matthew Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Paul Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Bilen%2C+H">Hakan Bilen</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+N+J">Niloy J. Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+P">Paul Guerrero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR 2023. Project page: <a href="https://github.com/Anciukevicius/RenderDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10743" title="Abstract">arXiv:2211.10743</a> (replaced) [<a href="/pdf/2211.10743" title="Download PDF">pdf</a>, <a href="/ps/2211.10743" title="Download PostScript">ps</a>, <a href="/format/2211.10743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitoring the edges of product networks using distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Klasing%2C+R">Ralf Klasing</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yaping Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+B">Bo Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2, 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Networking and Internet Architecture (cs.NI); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10805" title="Abstract">arXiv:2211.10805</a> (replaced) [<a href="/pdf/2211.10805" title="Download PDF">pdf</a>, <a href="/format/2211.10805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Pointwise Behavior of Recursive Partitioning and Its Implications  for Heterogeneous Causal Effect Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cattaneo%2C+M+D">Matias D. Cattaneo</a>, 
<a href="/search/stat?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>, 
<a href="/search/stat?searchtype=author&query=Tian%2C+P+M">Peter M. Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13692" title="Abstract">arXiv:2211.13692</a> (replaced) [<a href="/pdf/2211.13692" title="Download PDF">pdf</a>, <a href="/format/2211.13692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To be or not to be stable, that is the question: understanding neural  networks for inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Evangelista%2C+D">Davide Evangelista</a>, 
<a href="/search/math?searchtype=author&query=Nagy%2C+J">James Nagy</a>, 
<a href="/search/math?searchtype=author&query=Morotti%2C+E">Elena Morotti</a>, 
<a href="/search/math?searchtype=author&query=Piccolomini%2C+E+L">Elena Loli Piccolomini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figure. Paper will be sent for publication on a journal soon. This is a preliminary version, updated versions will be uploaded on ArXiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00720" title="Abstract">arXiv:2212.00720</a> (replaced) [<a href="/pdf/2212.00720" title="Download PDF">pdf</a>, <a href="/format/2212.00720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive  Coding Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvatori%2C+T">Tommaso Salvatori</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuhang Song</a>, 
<a href="/search/cs?searchtype=author&query=Yordanov%2C+Y">Yordan Yordanov</a>, 
<a href="/search/cs?searchtype=author&query=Millidge%2C+B">Beren Millidge</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenghua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Lei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Emde%2C+C">Cornelius Emde</a>, 
<a href="/search/cs?searchtype=author&query=Bogacz%2C+R">Rafal Bogacz</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Change of title and abstract, that now reflect the version accepted for publication. One co-author also added, that performed the additional experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03128" title="Abstract">arXiv:2212.03128</a> (replaced) [<a href="/pdf/2212.03128" title="Download PDF">pdf</a>, <a href="/format/2212.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chromatic Alpha Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=di+Montesano%2C+S+C">Sebastiano Cultrera di Montesano</a>, 
<a href="/search/math?searchtype=author&query=Draganov%2C+O">Ond&#x159;ej Draganov</a>, 
<a href="/search/math?searchtype=author&query=Edelsbrunner%2C+H">Herbert Edelsbrunner</a>, 
<a href="/search/math?searchtype=author&query=Saghafian%2C+M">Morteza Saghafian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 15 figures; v3 only updates the title; v2 brings many changes over v1, most notably adds a proof that the chromatic radius function is generalised discrete Morse
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04466" title="Abstract">arXiv:2212.04466</a> (replaced) [<a href="/pdf/2212.04466" title="Download PDF">pdf</a>, <a href="/format/2212.04466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On inclusion of time-varying source in the acoustic wave equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Javaherian%2C+A">Ashkan Javaherian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04999" title="Abstract">arXiv:2212.04999</a> (replaced) [<a href="/pdf/2212.04999" title="Download PDF">pdf</a>, <a href="/ps/2212.04999" title="Download PostScript">ps</a>, <a href="/format/2212.04999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Implementation of the Extended Tower Number Field Sieve using 4d  Sieving in a Box and a Record Computation in Fp4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Robinson%2C+O">Oisin Robinson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06594" title="Abstract">arXiv:2212.06594</a> (replaced) [<a href="/pdf/2212.06594" title="Download PDF">pdf</a>, <a href="/format/2212.06594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hausdorff-measure boundary element method for acoustic scattering by  fractal screens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caetano%2C+A+M">Ant&#xf3;nio M. Caetano</a>, 
<a href="/search/math?searchtype=author&query=Chandler-Wilde%2C+S+N">Simon N. Chandler-Wilde</a>, 
<a href="/search/math?searchtype=author&query=Gibbs%2C+A">Andrew Gibbs</a>, 
<a href="/search/math?searchtype=author&query=Hewett%2C+D+P">David P. Hewett</a>, 
<a href="/search/math?searchtype=author&query=Moiola%2C+A">Andrea Moiola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03962" title="Abstract">arXiv:2301.03962</a> (replaced) [<a href="/pdf/2301.03962" title="Download PDF">pdf</a>, <a href="/format/2301.03962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Theory of Diversity in Ensemble Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wood%2C+D">Danny Wood</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+T">Tingting Mu</a>, 
<a href="/search/cs?searchtype=author&query=Webb%2C+A">Andrew Webb</a>, 
<a href="/search/cs?searchtype=author&query=Reeve%2C+H">Henry Reeve</a>, 
<a href="/search/cs?searchtype=author&query=Luj%C3%A1n%2C+M">Mikel Luj&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+G">Gavin Brown</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Machine Learning Research, 24(359), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08918" title="Abstract">arXiv:2301.08918</a> (replaced) [<a href="/pdf/2301.08918" title="Download PDF">pdf</a>, <a href="/format/2301.08918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Signed Propagation for Multi-Class Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yoonhyuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jiho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+T">Taewook Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chong-Kwon Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02639" title="Abstract">arXiv:2302.02639</a> (replaced) [<a href="/pdf/2302.02639" title="Download PDF">pdf</a>, <a href="/ps/2302.02639" title="Download PostScript">ps</a>, <a href="/format/2302.02639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New lower bounds for the integration of periodic functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krieg%2C+D">David Krieg</a>, 
<a href="/search/math?searchtype=author&query=Vybiral%2C+J">Jan Vybiral</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J Fourier Anal Appl 29, 41 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03693" title="Abstract">arXiv:2302.03693</a> (replaced) [<a href="/pdf/2302.03693" title="Download PDF">pdf</a>, <a href="/format/2302.03693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept Algebra for (Score-Based) Text-Controlled Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Negrea%2C+J">Jeffrey Negrea</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02949" title="Abstract">arXiv:2303.02949</a> (replaced) [<a href="/pdf/2303.02949" title="Download PDF">pdf</a>, <a href="/format/2303.02949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Angle-Constrained Formation Control under Directed Non-Triangulated  Sensing Graphs (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Z">Zhixi Shen</a>, 
<a href="/search/eess?searchtype=author&query=Jing%2C+G">Gangshan Jing</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Y">Yongduan Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the extended version of our paper published in Automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10019" title="Abstract">arXiv:2303.10019</a> (replaced) [<a href="/pdf/2303.10019" title="Download PDF">pdf</a>, <a href="/format/2303.10019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Probabilistic CRPS Learning with an Application to  Day-Ahead Electricity Prices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Berrisch%2C+J">Jonathan Berrisch</a>, 
<a href="/search/stat?searchtype=author&query=Ziel%2C+F">Florian Ziel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Computational Finance (q-fin.CP); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14086" title="Abstract">arXiv:2303.14086</a> (replaced) [<a href="/pdf/2303.14086" title="Download PDF">pdf</a>, <a href="/format/2303.14086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Field Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qi-yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiang-xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shu Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14438" title="Abstract">arXiv:2303.14438</a> (replaced) [<a href="/pdf/2303.14438" title="Download PDF">pdf</a>, <a href="/format/2303.14438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Highly Available Blockchain Nodes With N-Version Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ron%2C+J">Javier Ron</a>, 
<a href="/search/cs?searchtype=author&query=Soto-Valero%2C+C">C&#xe9;sar Soto-Valero</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Long Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Baudry%2C+B">Benoit Baudry</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Dependable and Secure Computing, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14733" title="Abstract">arXiv:2303.14733</a> (replaced) [<a href="/pdf/2303.14733" title="Download PDF">pdf</a>, <a href="/format/2303.14733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Matrix Weighted Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Le-Phan%2C+N">Nhat-Minh Le-Phan</a>, 
<a href="/search/eess?searchtype=author&query=Trinh%2C+M+H">Minh Hoang Trinh</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+P+D">Phuoc Doan Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15919" title="Abstract">arXiv:2303.15919</a> (replaced) [<a href="/pdf/2303.15919" title="Download PDF">pdf</a>, <a href="/format/2303.15919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Hyperbolic Convolutional Neural Networks for Computer Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bdeir%2C+A">Ahmad Bdeir</a>, 
<a href="/search/cs?searchtype=author&query=Schwethelm%2C+K">Kristian Schwethelm</a>, 
<a href="/search/cs?searchtype=author&query=Landwehr%2C+N">Niels Landwehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17152" title="Abstract">arXiv:2303.17152</a> (replaced) [<a href="/pdf/2303.17152" title="Download PDF">pdf</a>, <a href="/format/2303.17152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Autoencoder for Self-supervised Visual Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17235" title="Abstract">arXiv:2303.17235</a> (replaced) [<a href="/pdf/2303.17235" title="Download PDF">pdf</a>, <a href="/format/2303.17235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kaizen: Practical Self-supervised Continual Learning with Continual  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C+I">Chi Ian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qendro%2C+L">Lorena Qendro</a>, 
<a href="/search/cs?searchtype=author&query=Spathis%2C+D">Dimitris Spathis</a>, 
<a href="/search/cs?searchtype=author&query=Kawsar%2C+F">Fahim Kawsar</a>, 
<a href="/search/cs?searchtype=author&query=Mascolo%2C+C">Cecilia Mascolo</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Akhil Mathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024. The code for this work is available at <a href="https://github.com/dr-bell/kaizen">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2024, pp. 2841-2850
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01300" title="Abstract">arXiv:2304.01300</a> (replaced) [<a href="/pdf/2304.01300" title="Download PDF">pdf</a>, <a href="/ps/2304.01300" title="Download PostScript">ps</a>, <a href="/format/2304.01300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Mitigating the Utility-Loss in Differentially Private Learning: A new  Perspective by a Geometrically Inspired Kernel Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Mohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+A">Bernhard A. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+L">Lukas Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02500" title="Abstract">arXiv:2304.02500</a> (replaced) [<a href="/pdf/2304.02500" title="Download PDF">pdf</a>, <a href="/format/2304.02500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wardrop Equilibrium Can Be Boundedly Rational: A New Behavioral Theory  of Route Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Li%2C+J">Jiayang Li</a>, 
<a href="/search/econ?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/econ?searchtype=author&query=Nie%2C+Y+M">Yu Marco Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06523" title="Abstract">arXiv:2304.06523</a> (replaced) [<a href="/pdf/2304.06523" title="Download PDF">pdf</a>, <a href="/format/2304.06523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The 2-Attractor Problem is NP-Complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+J">Janosch Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Whittington%2C+P">Philip Whittington</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06894" title="Abstract">arXiv:2304.06894</a> (replaced) [<a href="/pdf/2304.06894" title="Download PDF">pdf</a>, <a href="/format/2304.06894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Noise Resilience of Successor Features and Predecessor  Features Algorithms in One and Two-Dimensional Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunsu Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 11 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07235" title="Abstract">arXiv:2304.07235</a> (replaced) [<a href="/pdf/2304.07235" title="Download PDF">pdf</a>, <a href="/format/2304.07235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What does self-attention learn from Masked Language Modelling?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Rende%2C+R">Riccardo Rende</a>, 
<a href="/search/cond-mat?searchtype=author&query=Gerace%2C+F">Federica Gerace</a>, 
<a href="/search/cond-mat?searchtype=author&query=Laio%2C+A">Alessandro Laio</a>, 
<a href="/search/cond-mat?searchtype=author&query=Goldt%2C+S">Sebastian Goldt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12234" title="Abstract">arXiv:2304.12234</a> (replaced) [<a href="/e-print/2304.12234" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peer-to-Peer Network: Kantian Cooperation Discourage Free Riding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mane%2C+P+C">Pramod C. Mane</a>, 
<a href="/search/cs?searchtype=author&query=Ratnaparkhi%2C+S">Snehal Ratnaparkhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Critical errors in the contents
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03462" title="Abstract">arXiv:2305.03462</a> (replaced) [<a href="/pdf/2305.03462" title="Download PDF">pdf</a>, <a href="/format/2305.03462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Neural Gauge Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kortylewski%2C+A">Adam Kortylewski</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06746" title="Abstract">arXiv:2305.06746</a> (replaced) [<a href="/pdf/2305.06746" title="Download PDF">pdf</a>, <a href="/ps/2305.06746" title="Download PostScript">ps</a>, <a href="/format/2305.06746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A maturity model for catalogues of semantic artefacts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corcho%2C+O">Oscar Corcho</a>, 
<a href="/search/cs?searchtype=author&query=Ekaputra%2C+F+J">Fajar J. Ekaputra</a>, 
<a href="/search/cs?searchtype=author&query=Heibi%2C+I">Ivan Heibi</a>, 
<a href="/search/cs?searchtype=author&query=Jonquet%2C+C">Clement Jonquet</a>, 
<a href="/search/cs?searchtype=author&query=Micsik%2C+A">Andras Micsik</a>, 
<a href="/search/cs?searchtype=author&query=Peroni%2C+S">Silvio Peroni</a>, 
<a href="/search/cs?searchtype=author&query=Storti%2C+E">Emanuele Storti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08809" title="Abstract">arXiv:2305.08809</a> (replaced) [<a href="/pdf/2305.08809" title="Download PDF">pdf</a>, <a href="/format/2305.08809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretability at Scale: Identifying Causal Mechanisms in Alpaca
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Atticus Geiger</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+T">Thomas Icard</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N+D">Noah D. Goodman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 with Author Corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12138" title="Abstract">arXiv:2305.12138</a> (replaced) [<a href="/pdf/2305.12138" title="Download PDF">pdf</a>, <a href="/format/2305.12138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMs: Understanding Code Syntax and Semantics for Code Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shangqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhihao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liming Nie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13876" title="Abstract">arXiv:2305.13876</a> (replaced) [<a href="/pdf/2305.13876" title="Download PDF">pdf</a>, <a href="/format/2305.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyanishi%2C+T">Taiki Miyanishi</a>, 
<a href="/search/cs?searchtype=author&query=Azuma%2C+D">Daichi Azuma</a>, 
<a href="/search/cs?searchtype=author&query=Kurita%2C+S">Shuhei Kurita</a>, 
<a href="/search/cs?searchtype=author&query=Kawanabe%2C+M">Motoki Kawanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14459" title="Abstract">arXiv:2305.14459</a> (replaced) [<a href="/pdf/2305.14459" title="Download PDF">pdf</a>, <a href="/format/2305.14459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Precise Outline-Conditioned Text Generation with Task Duality  and Explicit Outline Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Weixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+H">Hari Sundaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15371" title="Abstract">arXiv:2305.15371</a> (replaced) [<a href="/pdf/2305.15371" title="Download PDF">pdf</a>, <a href="/format/2305.15371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Unrolled Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadou%2C+S">Samar Hadou</a>, 
<a href="/search/cs?searchtype=author&query=NaderiAlizadeh%2C+N">Navid NaderiAlizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15611" title="Abstract">arXiv:2305.15611</a> (replaced) [<a href="/pdf/2305.15611" title="Download PDF">pdf</a>, <a href="/format/2305.15611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Size Generalization of Graph Neural Networks on Biological Data:  Insights and Practices from the Spectral Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gaotang Li</a>, 
<a href="/search/cs?searchtype=author&query=Koutra%2C+D">Danai Koutra</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yujun Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, including appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15613" title="Abstract">arXiv:2305.15613</a> (replaced) [<a href="/pdf/2305.15613" title="Download PDF">pdf</a>, <a href="/format/2305.15613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> O$n$ Learning Deep O($n$)-Equivariant Hyperspheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melnyk%2C+P">Pavlo Melnyk</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>, 
<a href="/search/cs?searchtype=author&query=Wadenb%C3%A4ck%2C+M">M&#xe5;rten Wadenb&#xe4;ck</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+A">Andreas Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+C">Cuong Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15896" title="Abstract">arXiv:2305.15896</a> (replaced) [<a href="/pdf/2305.15896" title="Download PDF">pdf</a>, <a href="/format/2305.15896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MixFormerV2: Efficient Fully Transformer Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yutao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tianhui Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Gangshan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16978" title="Abstract">arXiv:2305.16978</a> (replaced) [<a href="/pdf/2305.16978" title="Download PDF">pdf</a>, <a href="/format/2305.16978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meandering microstrip leaky-wave antenna with dual-band linear-circular  polarization and suppressed open stopband
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Vadher%2C+P">Pratik Vadher</a>, 
<a href="/search/physics?searchtype=author&query=Sacco%2C+G">Giulia Sacco</a>, 
<a href="/search/physics?searchtype=author&query=Nikolayev%2C+D">Denys Nikolayev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19235" title="Abstract">arXiv:2305.19235</a> (replaced) [<a href="/pdf/2305.19235" title="Download PDF">pdf</a>, <a href="/format/2305.19235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Input State Stability of Gated Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marino%2C+A">Antonio Marino</a> (RAINBOW), 
<a href="/search/cs?searchtype=author&query=Pacchierotti%2C+C">Claudio Pacchierotti</a> (RAINBOW), 
<a href="/search/cs?searchtype=author&query=Giordano%2C+P+R">Paolo Robuffo Giordano</a> (RAINBOW)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Control of Network Systems, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19510" title="Abstract">arXiv:2305.19510</a> (replaced) [<a href="/pdf/2305.19510" title="Download PDF">pdf</a>, <a href="/format/2305.19510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karhadkar%2C+K">Kedar Karhadkar</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+M">Michael Murray</a>, 
<a href="/search/cs?searchtype=author&query=Tseran%2C+H">Hanna Tseran</a>, 
<a href="/search/cs?searchtype=author&query=Mont%C3%BAfar%2C+G">Guido Mont&#xfa;far</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Combinatorics (math.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00107" title="Abstract">arXiv:2306.00107</a> (replaced) [<a href="/pdf/2306.00107" title="Download PDF">pdf</a>, <a href="/format/2306.00107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MERT: Acoustic Music Understanding Model with Large-Scale  Self-supervised Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yizhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruibin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yinghao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hanzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenghao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ragni%2C+A">Anton Ragni</a>, 
<a href="/search/cs?searchtype=author&query=Benetos%2C+E">Emmanouil Benetos</a>, 
<a href="/search/cs?searchtype=author&query=Gyenge%2C+N">Norbert Gyenge</a>, 
<a href="/search/cs?searchtype=author&query=Dannenberg%2C+R">Roger Dannenberg</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yemin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01237" title="Abstract">arXiv:2306.01237</a> (replaced) [<a href="/pdf/2306.01237" title="Download PDF">pdf</a>, <a href="/format/2306.01237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Regret Minimization in Offline Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Petrik%2C+M">Marek Petrik</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+G">Guy Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01731" title="Abstract">arXiv:2306.01731</a> (replaced) [<a href="/pdf/2306.01731" title="Download PDF">pdf</a>, <a href="/format/2306.01731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAGAR: Taming Reward Misalignment in Inverse Reinforcement  Learning-Based Imitation Learning with Protagonist Antagonist Guided  Adversarial Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weichao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenchao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03801" title="Abstract">arXiv:2306.03801</a> (replaced) [<a href="/pdf/2306.03801" title="Download PDF">pdf</a>, <a href="/ps/2306.03801" title="Download PostScript">ps</a>, <a href="/format/2306.03801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Vectorization of Multiparameter Persistent Homology using Signed  Barcodes as Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loiseaux%2C+D">David Loiseaux</a>, 
<a href="/search/cs?searchtype=author&query=Scoccola%2C+L">Luis Scoccola</a>, 
<a href="/search/cs?searchtype=author&query=Carri%C3%A8re%2C+M">Mathieu Carri&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Botnan%2C+M+B">Magnus Bakke Botnan</a>, 
<a href="/search/cs?searchtype=author&query=Oudot%2C+S">Steve Oudot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures, 9 tables; v2: final version in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG); Algebraic Topology (math.AT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03933" title="Abstract">arXiv:2306.03933</a> (replaced) [<a href="/pdf/2306.03933" title="Download PDF">pdf</a>, <a href="/format/2306.03933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional and Permutation Invariant Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Mikuni%2C+V">Vinicius Mikuni</a>, 
<a href="/search/hep-ph?searchtype=author&query=Nachman%2C+B">Benjamin Nachman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08191" title="Abstract">arXiv:2306.08191</a> (replaced) [<a href="/pdf/2306.08191" title="Download PDF">pdf</a>, <a href="/format/2306.08191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Large-scale Spatial Problems with Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Owerko%2C+D">Damian Owerko</a>, 
<a href="/search/cs?searchtype=author&query=Kanatsoulis%2C+C+I">Charilaos I. Kanatsoulis</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, submitted to Asilomar Conference on Signals, Systems, and Computers 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08309" title="Abstract">arXiv:2306.08309</a> (replaced) [<a href="/pdf/2306.08309" title="Download PDF">pdf</a>, <a href="/format/2306.08309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Reversible Halftoning via Predictive Luminance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+C">Cheuk-Kit Lau</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+T">Tien-Tsin Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08754" title="Abstract">arXiv:2306.08754</a> (replaced) [<a href="/pdf/2306.08754" title="Download PDF">pdf</a>, <a href="/format/2306.08754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClimSim: A large multi-scale dataset for hybrid physics-ML climate  emulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sungduk Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hannah%2C+W">Walter Hannah</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liran Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jerry Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bhouri%2C+M+A">Mohamed Aziz Bhouri</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Ritwik Gupta</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BCtjens%2C+B">Bj&#xf6;rn L&#xfc;tjens</a>, 
<a href="/search/cs?searchtype=author&query=Will%2C+J+C">Justus Christopher Will</a>, 
<a href="/search/cs?searchtype=author&query=Behrens%2C+G">Gunnar Behrens</a>, 
<a href="/search/cs?searchtype=author&query=Busecke%2C+J">Julius Busecke</a>, 
<a href="/search/cs?searchtype=author&query=Loose%2C+N">Nora Loose</a>, 
<a href="/search/cs?searchtype=author&query=Stern%2C+C+I">Charles I Stern</a>, 
<a href="/search/cs?searchtype=author&query=Beucler%2C+T">Tom Beucler</a>, 
<a href="/search/cs?searchtype=author&query=Harrop%2C+B">Bryce Harrop</a>, 
<a href="/search/cs?searchtype=author&query=Hillman%2C+B+R">Benjamin R Hillman</a>, 
<a href="/search/cs?searchtype=author&query=Jenney%2C+A">Andrea Jenney</a>, 
<a href="/search/cs?searchtype=author&query=Ferretti%2C+S">Savannah Ferretti</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nana Liu</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Brenowitz%2C+N+D">Noah D Brenowitz</a>, 
<a href="/search/cs?searchtype=author&query=Eyring%2C+V">Veronika Eyring</a>, 
<a href="/search/cs?searchtype=author&query=Geneva%2C+N">Nicholas Geneva</a>, 
<a href="/search/cs?searchtype=author&query=Gentine%2C+P">Pierre Gentine</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+J">Jaideep Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Subramaniam%2C+A">Akshay Subramaniam</a>, 
<a href="/search/cs?searchtype=author&query=Vondrick%2C+C">Carl Vondrick</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zanna%2C+L">Laure Zanna</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Abernathey%2C+R">Ryan Abernathey</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Fiaz Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+D+C">David C Bader</a>, 
<a href="/search/cs?searchtype=author&query=Baldi%2C+P">Pierre Baldi</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+E">Elizabeth Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Bretherton%2C+C">Christopher Bretherton</a>, 
<a href="/search/cs?searchtype=author&query=Caldwell%2C+P">Peter Caldwell</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+W">Wayne Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yilun Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Iglesias-Suarez%2C+F">Fernando Iglesias-Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Jantre%2C+S">Sanket Jantre</a>, 
<a href="/search/cs?searchtype=author&query=Kashinath%2C+K">Karthik Kashinath</a>, 
<a href="/search/cs?searchtype=author&query=Khairoutdinov%2C+M">Marat Khairoutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Kurth%2C+T">Thorsten Kurth</a>, 
<a href="/search/cs?searchtype=author&query=Lutsko%2C+N">Nicholas Lutsko</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Po-Lun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mooers%2C+G">Griffin Mooers</a>, 
<a href="/search/cs?searchtype=author&query=Neelin%2C+J+D">J. David Neelin</a>, 
<a href="/search/cs?searchtype=author&query=Randall%2C+D">David Randall</a>, 
<a href="/search/cs?searchtype=author&query=Shamekh%2C+S">Sara Shamekh</a>,  et al. (5 additional authors not shown)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Outstanding Datasets and Benchmarks Track Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10947" title="Abstract">arXiv:2306.10947</a> (replaced) [<a href="/pdf/2306.10947" title="Download PDF">pdf</a>, <a href="/format/2306.10947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Chernoff Bounds: Understanding Generalization in the Interpolation  Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masegosa%2C+A+R">Andr&#xe9;s R. Masegosa</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+L+A">Luis A. Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 10 figures, Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11305" title="Abstract">arXiv:2306.11305</a> (replaced) [<a href="/pdf/2306.11305" title="Download PDF">pdf</a>, <a href="/format/2306.11305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Fourier Neural Representation for Sequential Video  Compilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Haeyong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">DaHyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+C+D">Chang D Yoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11903" title="Abstract">arXiv:2306.11903</a> (replaced) [<a href="/pdf/2306.11903" title="Download PDF">pdf</a>, <a href="/format/2306.11903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Fusion: Efficient Network Training via Pre-trained Initializations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzawi%2C+H">Hanna Mazzawi</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalvo%2C+X">Xavi Gonzalvo</a>, 
<a href="/search/cs?searchtype=author&query=Wunder%2C+M">Michael Wunder</a>, 
<a href="/search/cs?searchtype=author&query=Jerome%2C+S">Sammy Jerome</a>, 
<a href="/search/cs?searchtype=author&query=Dherin%2C+B">Benoit Dherin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12498" title="Abstract">arXiv:2306.12498</a> (replaced) [<a href="/pdf/2306.12498" title="Download PDF">pdf</a>, <a href="/format/2306.12498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective  and Improved Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+X">Xufeng Cai</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+C+Y">Cheuk Yin Lin</a>, 
<a href="/search/math?searchtype=author&query=Diakonikolas%2C+J">Jelena Diakonikolas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01037" title="Abstract">arXiv:2307.01037</a> (replaced) [<a href="/pdf/2307.01037" title="Download PDF">pdf</a>, <a href="/format/2307.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector Quantile Regression on Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pegoraro%2C+M">Marco Pegoraro</a>, 
<a href="/search/stat?searchtype=author&query=Vedula%2C+S">Sanketh Vedula</a>, 
<a href="/search/stat?searchtype=author&query=Rosenberg%2C+A+A">Aviv A. Rosenberg</a>, 
<a href="/search/stat?searchtype=author&query=Tallini%2C+I">Irene Tallini</a>, 
<a href="/search/stat?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>, 
<a href="/search/stat?searchtype=author&query=Bronstein%2C+A+M">Alex M. Bronstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01946" title="Abstract">arXiv:2307.01946</a> (replaced) [<a href="/pdf/2307.01946" title="Download PDF">pdf</a>, <a href="/format/2307.01946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECG-Image-Kit: A Synthetic Image Generation Toolbox to Facilitate Deep  Learning-Based Electrocardiogram Digitization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shivashankara%2C+K+K">Kshama Kodthalu Shivashankara</a>, 
<a href="/search/cs?searchtype=author&query=Deepanshi">Deepanshi</a>, 
<a href="/search/cs?searchtype=author&query=Shervedani%2C+A+M">Afagh Mehri Shervedani</a>, 
<a href="/search/cs?searchtype=author&query=Clifford%2C+G+D">Gari D. Clifford</a>, 
<a href="/search/cs?searchtype=author&query=Reyna%2C+M+A">Matthew A. Reyna</a>, 
<a href="/search/cs?searchtype=author&query=Sameni%2C+R">Reza Sameni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04081" title="Abstract">arXiv:2307.04081</a> (replaced) [<a href="/pdf/2307.04081" title="Download PDF">pdf</a>, <a href="/format/2307.04081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Conditional Generation with Fewer Labeled Data by  Self-calibrating Classifier Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+P+K">Paul Kuo-Ming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si-An Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsuan-Tien Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06985" title="Abstract">arXiv:2307.06985</a> (replaced) [<a href="/pdf/2307.06985" title="Download PDF">pdf</a>, <a href="/ps/2307.06985" title="Download PostScript">ps</a>, <a href="/format/2307.06985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering Design Knowledge Graphs from Patented Artefact Descriptions  for Retrieval-Augmented Generation in the Design Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+L">L Siddharth</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jianxi Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13492" title="Abstract">arXiv:2307.13492</a> (replaced) [<a href="/pdf/2307.13492" title="Download PDF">pdf</a>, <a href="/format/2307.13492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NormAUG: Normalization-guided Augmentation for Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongpeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yinghuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Image Processing (TIP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13795" title="Abstract">arXiv:2307.13795</a> (replaced) [<a href="/pdf/2307.13795" title="Download PDF">pdf</a>, <a href="/format/2307.13795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-Order Asynchronous Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahman%2C+D">Danel Ahman</a>, 
<a href="/search/cs?searchtype=author&query=Pretnar%2C+M">Matija Pretnar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of POPL 2021 paper "Asynchronous Effects", <a href="/abs/2003.02110">arXiv:2003.02110</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16120" title="Abstract">arXiv:2307.16120</a> (replaced) [<a href="/pdf/2307.16120" title="Download PDF">pdf</a>, <a href="/format/2307.16120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Unrolling Networks with Recurrent Momentum Acceleration for  Nonlinear Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qingping Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+J">Jiayu Qian</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Junqi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinglai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16517" title="Abstract">arXiv:2307.16517</a> (replaced) [<a href="/pdf/2307.16517" title="Download PDF">pdf</a>, <a href="/format/2307.16517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Select2Col: Leveraging Spatial-Temporal Importance of Semantic  Information for Efficient Collaborative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuntao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rongpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianfu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhifeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yongdong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honggang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16545" title="Abstract">arXiv:2307.16545</a> (replaced) [<a href="/pdf/2307.16545" title="Download PDF">pdf</a>, <a href="/format/2307.16545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards General Visual-Linguistic Face Forgery Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+T">Taiping Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haozhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shouhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01070" title="Abstract">arXiv:2308.01070</a> (replaced) [<a href="/pdf/2308.01070" title="Download PDF">pdf</a>, <a href="/ps/2308.01070" title="Download PostScript">ps</a>, <a href="/format/2308.01070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Analytic Calculus Cracks AdaBoost Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brossier%2C+J">Jean-Marc Brossier</a>, 
<a href="/search/cs?searchtype=author&query=Lafitte%2C+O">Olivier Lafitte</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9thor%C3%A9%2C+L">Lenny R&#xe9;thor&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01285" title="Abstract">arXiv:2308.01285</a> (replaced) [<a href="/pdf/2308.01285" title="Download PDF">pdf</a>, <a href="/format/2308.01285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flows: Building Blocks of Reasoning and Collaborating AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+L">Lars Klein</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+N">Nicolas Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Saibo Geng</a>, 
<a href="/search/cs?searchtype=author&query=Schnitzler%2C+J+P">Julian Paul Schnitzler</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05810" title="Abstract">arXiv:2308.05810</a> (replaced) [<a href="/pdf/2308.05810" title="Download PDF">pdf</a>, <a href="/format/2308.05810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spintronics for image recognition: performance benchmarking via  ultrafast data-driven simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moureaux%2C+A">Anatole Moureaux</a>, 
<a href="/search/cs?searchtype=author&query=Chopin%2C+C">Chlo&#xe9; Chopin</a>, 
<a href="/search/cs?searchtype=author&query=de+Wergifosse%2C+S">Simon de Wergifosse</a>, 
<a href="/search/cs?searchtype=author&query=Jacques%2C+L">Laurent Jacques</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+F+A">Flavio Abreu Araujo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05869" title="Abstract">arXiv:2308.05869</a> (replaced) [<a href="/pdf/2308.05869" title="Download PDF">pdf</a>, <a href="/format/2308.05869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shared Memory-contention-aware Concurrent DNN Execution for Diversely  Heterogeneous System-on-Chips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagli%2C+I">Ismet Dagli</a>, 
<a href="/search/cs?searchtype=author&query=Belviranli%2C+M">Mehmet Belviranli</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 29th ACM SIGPLAN Annual Symposium on Principles and Practice of
  Parallel Programming, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07123" title="Abstract">arXiv:2308.07123</a> (replaced) [<a href="/pdf/2308.07123" title="Download PDF">pdf</a>, <a href="/format/2308.07123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Outlook into the Future of Egocentric Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plizzari%2C+C">Chiara Plizzari</a>, 
<a href="/search/cs?searchtype=author&query=Goletto%2C+G">Gabriele Goletto</a>, 
<a href="/search/cs?searchtype=author&query=Furnari%2C+A">Antonino Furnari</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+S">Siddhant Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Ragusa%2C+F">Francesco Ragusa</a>, 
<a href="/search/cs?searchtype=author&query=Farinella%2C+G+M">Giovanni Maria Farinella</a>, 
<a href="/search/cs?searchtype=author&query=Damen%2C+D">Dima Damen</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+T">Tatiana Tommasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We invite comments, suggestions and corrections here: <a href="https://openreview.net/forum?id=V3974SUk1w">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07545" title="Abstract">arXiv:2308.07545</a> (replaced) [<a href="/pdf/2308.07545" title="Download PDF">pdf</a>, <a href="/format/2308.07545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Dataset Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xindi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byron Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Russakovsky%2C+O">Olga Russakovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08055" title="Abstract">arXiv:2308.08055</a> (replaced) [<a href="/pdf/2308.08055" title="Download PDF">pdf</a>, <a href="/ps/2308.08055" title="Download PostScript">ps</a>, <a href="/format/2308.08055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple online learning with consistent oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kozachinskiy%2C+A">Alexander Kozachinskiy</a>, 
<a href="/search/cs?searchtype=author&query=Steifer%2C+T">Tomasz Steifer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Changes to previous version: added 3^d lower bound
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13838" title="Abstract">arXiv:2308.13838</a> (replaced) [<a href="/pdf/2308.13838" title="Download PDF">pdf</a>, <a href="/format/2308.13838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Price-Discrimination Game for Distributed Resource Management in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Halvin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guopeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01775" title="Abstract">arXiv:2309.01775</a> (replaced) [<a href="/pdf/2309.01775" title="Download PDF">pdf</a>, <a href="/format/2309.01775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gated recurrent neural networks discover attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zucchet%2C+N">Nicolas Zucchet</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+S">Seijin Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Akram%2C+Y">Yassir Akram</a>, 
<a href="/search/cs?searchtype=author&query=von+Oswald%2C+J">Johannes von Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Larcher%2C+M">Maxime Larcher</a>, 
<a href="/search/cs?searchtype=author&query=Steger%2C+A">Angelika Steger</a>, 
<a href="/search/cs?searchtype=author&query=Sacramento%2C+J">Jo&#xe3;o Sacramento</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01945" title="Abstract">arXiv:2309.01945</a> (replaced) [<a href="/pdf/2309.01945" title="Download PDF">pdf</a>, <a href="/format/2309.01945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OHQ: On-chip Hardware-aware Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yangdong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jingzhuo Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03180" title="Abstract">arXiv:2309.03180</a> (replaced) [<a href="/pdf/2309.03180" title="Download PDF">pdf</a>, <a href="/ps/2309.03180" title="Download PostScript">ps</a>, <a href="/format/2309.03180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arithmetical subword complexity of automatic sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Konieczny%2C+J">Jakub Konieczny</a>, 
<a href="/search/math?searchtype=author&query=M%C3%BCllner%2C+C">Clemens M&#xfc;llner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Formal Languages and Automata Theory (cs.FL); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05767" title="Abstract">arXiv:2309.05767</a> (replaced) [<a href="/pdf/2309.05767" title="Download PDF">pdf</a>, <a href="/format/2309.05767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Supervision for General-Purpose Audio Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elizalde%2C+B">Benjamin Elizalde</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaming Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08030" title="Abstract">arXiv:2309.08030</a> (replaced) [<a href="/pdf/2309.08030" title="Download PDF">pdf</a>, <a href="/format/2309.08030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised  Features for Audio-Visual Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chou%2C+J">Ju-Chieh Chou</a>, 
<a href="/search/eess?searchtype=author&query=Chien%2C+C">Chung-Ming Chien</a>, 
<a href="/search/eess?searchtype=author&query=Livescu%2C+K">Karen Livescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09422" title="Abstract">arXiv:2309.09422</a> (replaced) [<a href="/pdf/2309.09422" title="Download PDF">pdf</a>, <a href="/format/2309.09422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Desensitization and Deception in Differential Games with Asymmetric  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Comandur%2C+V">Vinodhini Comandur</a>, 
<a href="/search/eess?searchtype=author&query=Vechalapu%2C+T+R">Tulasi Ram Vechalapu</a>, 
<a href="/search/eess?searchtype=author&query=Makkapati%2C+V+R">Venkata Ramana Makkapati</a>, 
<a href="/search/eess?searchtype=author&query=Tsiotras%2C+P">Panagiotis Tsiotras</a>, 
<a href="/search/eess?searchtype=author&query=Hutchinson%2C+S">Seth Hutchinson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09668" title="Abstract">arXiv:2309.09668</a> (replaced) [<a href="/pdf/2309.09668" title="Download PDF">pdf</a>, <a href="/format/2309.09668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DFormer: Rethinking RGBD Representation Learning for Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bowen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qibin Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09992" title="Abstract">arXiv:2309.09992</a> (replaced) [<a href="/pdf/2309.09992" title="Download PDF">pdf</a>, <a href="/ps/2309.09992" title="Download PostScript">ps</a>, <a href="/format/2309.09992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blair-Stanek%2C+A">Andrew Blair-Stanek</a>, 
<a href="/search/cs?searchtype=author&query=Holzenberger%2C+N">Nils Holzenberger</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 180 TAX NOTES FEDERAL 1101 (AUG. 14, 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10980" title="Abstract">arXiv:2309.10980</a> (replaced) [<a href="/pdf/2309.10980" title="Download PDF">pdf</a>, <a href="/format/2309.10980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Multi-Agent Deep Reinforcement Learning for Timely Healthcare  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hong-Ning Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+J">Jianming Yong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the ELSEVIER for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. arXiv admin note: text overlap with <a href="/abs/2309.10576">arXiv:2309.10576</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14641" title="Abstract">arXiv:2309.14641</a> (replaced) [<a href="/pdf/2309.14641" title="Download PDF">pdf</a>, <a href="/format/2309.14641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Denoising-Enhanced LiDAR Odometry for Degeneration Resilience  in Diverse Terrains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mazeyu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenbo Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yujie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16825" title="Abstract">arXiv:2309.16825</a> (replaced) [<a href="/pdf/2309.16825" title="Download PDF">pdf</a>, <a href="/format/2309.16825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tavakoli%2C+F">Fatemeh Tavakoli</a>, 
<a href="/search/cs?searchtype=author&query=Emerson%2C+D+B">D.B. Emerson</a>, 
<a href="/search/cs?searchtype=author&query=Ayromlou%2C+S">Sana Ayromlou</a>, 
<a href="/search/cs?searchtype=author&query=Jewell%2C+J">John Jewell</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+A">Amrit Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Amol Verma</a>, 
<a href="/search/cs?searchtype=author&query=Razak%2C+F">Fahad Razak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures, 11 tables, 1 algorithm Update includes a significant number of new experiments, a new format, and additional results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00841" title="Abstract">arXiv:2310.00841</a> (replaced) [<a href="/pdf/2310.00841" title="Download PDF">pdf</a>, <a href="/format/2310.00841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drug Discovery with Dynamic Goal-aware Fragments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01655" title="Abstract">arXiv:2310.01655</a> (replaced) [<a href="/pdf/2310.01655" title="Download PDF">pdf</a>, <a href="/format/2310.01655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacham%2C+P">Praneeth Kacham</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Adding learned sketches and results on downstream tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01701" title="Abstract">arXiv:2310.01701</a> (replaced) [<a href="/e-print/2310.01701" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending Domains through Text-to-Image Diffusion: A Source-Free  Approach to Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Aynaou%2C+H">Houda Aynaou</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revamped the whole paper; new version will be re-submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03272" title="Abstract">arXiv:2310.03272</a> (replaced) [<a href="/pdf/2310.03272" title="Download PDF">pdf</a>, <a href="/format/2310.03272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Alignment with Transferable Graph Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiashu He</a>, 
<a href="/search/cs?searchtype=author&query=Kanatsoulis%2C+C+I">Charilaos I. Kanatsoulis</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03311" title="Abstract">arXiv:2310.03311</a> (replaced) [<a href="/pdf/2310.03311" title="Download PDF">pdf</a>, <a href="/format/2310.03311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Variational Multivariate Information Bottleneck -- A Framework for  Variational Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelaleem%2C+E">Eslam Abdelaleem</a>, 
<a href="/search/cs?searchtype=author&query=Nemenman%2C+I">Ilya Nemenman</a>, 
<a href="/search/cs?searchtype=author&query=Martini%2C+K+M">K. Michael Martini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05358" title="Abstract">arXiv:2310.05358</a> (replaced) [<a href="/pdf/2310.05358" title="Download PDF">pdf</a>, <a href="/format/2310.05358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A family of permutationally invariant quantum codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Aydin%2C+A">Arda Aydin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Alekseyev%2C+M+A">Max A. Alekseyev</a>, 
<a href="/search/quant-ph?searchtype=author&query=Barg%2C+A">Alexander Barg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages. Changes in v2: added more detailed proofs concerning the deletion channel; added explanations of the relation of our construction and previously known permutation-invariant codes; added a new section on transversal gates for our codes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05401" title="Abstract">arXiv:2310.05401</a> (replaced) [<a href="/pdf/2310.05401" title="Download PDF">pdf</a>, <a href="/format/2310.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-MCMC: Sampling from Flat Basins with Ease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bolian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07433" title="Abstract">arXiv:2310.07433</a> (replaced) [<a href="/pdf/2310.07433" title="Download PDF">pdf</a>, <a href="/format/2310.07433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitation Learning from Observation with Automatic Discount Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Weijun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yingdong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chuan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhao-Heng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07898" title="Abstract">arXiv:2310.07898</a> (replaced) [<a href="/pdf/2310.07898" title="Download PDF">pdf</a>, <a href="/format/2310.07898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlorDB: Multiversion Hindsight Logging for Continuous Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+R">Rolando Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Dandamudi%2C+A">Anusha Dandamudi</a>, 
<a href="/search/cs?searchtype=author&query=Matute%2C+G">Gabriel Matute</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Lehan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J">Joseph Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Hellerstein%2C+J+M">Joseph M. Hellerstein</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+K">Koushik Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08164" title="Abstract">arXiv:2310.08164</a> (replaced) [<a href="/pdf/2310.08164" title="Download PDF">pdf</a>, <a href="/format/2310.08164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Training Objectives: Interpreting Reward Model Divergence in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marks%2C+L">Luke Marks</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+A">Amir Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Neo%2C+C">Clement Neo</a>, 
<a href="/search/cs?searchtype=author&query=Arike%2C+R">Rauno Arike</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08320" title="Abstract">arXiv:2310.08320</a> (replaced) [<a href="/pdf/2310.08320" title="Download PDF">pdf</a>, <a href="/format/2310.08320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Our Privacy With Backdoors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09943" title="Abstract">arXiv:2310.09943</a> (replaced) [<a href="/pdf/2310.09943" title="Download PDF">pdf</a>, <a href="/format/2310.09943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%2C+C">Chahyon Ku</a>, 
<a href="/search/cs?searchtype=author&query=Winge%2C+C">Carl Winge</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+R">Ryan Diaz</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wentao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Desingh%2C+K">Karthik Desingh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12446" title="Abstract">arXiv:2310.12446</a> (replaced) [<a href="/pdf/2310.12446" title="Download PDF">pdf</a>, <a href="/format/2310.12446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Electromagnetic Information Theory Improve Wireless Systems? A  Channel Estimation Example
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jieao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongzhichao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+T+J">Tie Jun Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Electromagnetic information theory (EIT) is an emerging interdisciplinary subject, aiming at providing a unified analytical framework for wireless systems as well as guiding practical system design. This paper answers the question: "Whether can we improve wireless communication systems via EIT"?
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13007" title="Abstract">arXiv:2310.13007</a> (replaced) [<a href="/pdf/2310.13007" title="Download PDF">pdf</a>, <a href="/format/2310.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Survey on Fairness Benefits of XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deck%2C+L">Luca Deck</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=De-Arteaga%2C+M">Maria De-Arteaga</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13549" title="Abstract">arXiv:2310.13549</a> (replaced) [<a href="/pdf/2310.13549" title="Download PDF">pdf</a>, <a href="/format/2310.13549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Perils &amp; Promises of Fact-checking with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quelle%2C+D">Dorian Quelle</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+A">Alexandre Bovet</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Frontiers in Artificial Intelligence, Volume 7, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15830" title="Abstract">arXiv:2310.15830</a> (replaced) [<a href="/pdf/2310.15830" title="Download PDF">pdf</a>, <a href="/format/2310.15830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localizing Anomalies in Critical Infrastructure using Model-Based Drift  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaquet%2C+V">Valerie Vaquet</a>, 
<a href="/search/cs?searchtype=author&query=Hinder%2C+F">Fabian Hinder</a>, 
<a href="/search/cs?searchtype=author&query=Vaquet%2C+J">Jonas Vaquet</a>, 
<a href="/search/cs?searchtype=author&query=Lammers%2C+K">Kathrin Lammers</a>, 
<a href="/search/cs?searchtype=author&query=Quakernack%2C+L">Lars Quakernack</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+B">Barbara Hammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18030" title="Abstract">arXiv:2310.18030</a> (replaced) [<a href="/pdf/2310.18030" title="Download PDF">pdf</a>, <a href="/format/2310.18030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confucius: Achieving Consistent Low Latency with Practical Queue  Management for Real-Time Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zili Meng</a>, 
<a href="/search/cs?searchtype=author&query=Atre%2C+N">Nirav Atre</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sherry%2C+J">Justine Sherry</a>, 
<a href="/search/cs?searchtype=author&query=Apostolaki%2C+M">Maria Apostolaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18562" title="Abstract">arXiv:2310.18562</a> (replaced) [<a href="/pdf/2310.18562" title="Download PDF">pdf</a>, <a href="/format/2310.18562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization-Free Test-Time Adaptation for Cross-Person Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+H">HuaJun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bob Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at UbiComp 2024; Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19283" title="Abstract">arXiv:2310.19283</a> (replaced) [<a href="/pdf/2310.19283" title="Download PDF">pdf</a>, <a href="/format/2310.19283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature  Extraction for IMU-based Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enokibori%2C+Y">Yu Enokibori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updating abstract length to clear a submission target's requirement. Updating English quality. Updating the best results of OPPORTUNITY (not iSPL version) and PAMAP2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20007" title="Abstract">arXiv:2310.20007</a> (replaced) [<a href="/pdf/2310.20007" title="Download PDF">pdf</a>, <a href="/ps/2310.20007" title="Download PostScript">ps</a>, <a href="/format/2310.20007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Moradipari%2C+A">Ahmadreza Moradipari</a>, 
<a href="/search/stat?searchtype=author&query=Pedramfar%2C+M">Mohammad Pedramfar</a>, 
<a href="/search/stat?searchtype=author&query=Zini%2C+M+S">Modjtaba Shokrian Zini</a>, 
<a href="/search/stat?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20172" title="Abstract">arXiv:2310.20172</a> (replaced) [<a href="/pdf/2310.20172" title="Download PDF">pdf</a>, <a href="/format/2310.20172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compact Binary Systems Waveform Generation with Generative Pre-trained  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Shi%2C+R">Ruijun Shi</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>, 
<a href="/search/gr-qc?searchtype=author&query=Cao%2C+Z">Zhoujian Cao</a>, 
<a href="/search/gr-qc?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20285" title="Abstract">arXiv:2310.20285</a> (replaced) [<a href="/pdf/2310.20285" title="Download PDF">pdf</a>, <a href="/format/2310.20285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Generalized Linear Models by Trading off Computation for  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tatzel%2C+L">Lukas Tatzel</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Frank Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text: 11 pages, 6 figures; Supplements: 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01222" title="Abstract">arXiv:2311.01222</a> (replaced) [<a href="/pdf/2311.01222" title="Download PDF">pdf</a>, <a href="/ps/2311.01222" title="Download PostScript">ps</a>, <a href="/format/2311.01222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Reflection on Process Graphs -- A Novel Approach for the  Completeness of an Axiomatization of 1-Free Regular Expressions Modulo  Bisimilarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinxin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor typos and further polishing from the reviewers' comments of the conference that rejected us
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05658" title="Abstract">arXiv:2311.05658</a> (replaced) [<a href="/pdf/2311.05658" title="Download PDF">pdf</a>, <a href="/ps/2311.05658" title="Download PostScript">ps</a>, <a href="/format/2311.05658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three non-cubical applications of extension types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tesla Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06650" title="Abstract">arXiv:2311.06650</a> (replaced) [<a href="/pdf/2311.06650" title="Download PDF">pdf</a>, <a href="/format/2311.06650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heuristic Optimal Transport in Branching Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Andrecut%2C+M">M. Andrecut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Int. J. Mod. Phys. C, 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06918" title="Abstract">arXiv:2311.06918</a> (replaced) [<a href="/pdf/2311.06918" title="Download PDF">pdf</a>, <a href="/format/2311.06918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource-Aware Hierarchical Federated Learning for Video Caching in  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pervej%2C+M+F">Md Ferdous Pervej</a>, 
<a href="/search/cs?searchtype=author&query=Molisch%2C+A+F">Andreas F Molisch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE ICC 2024. \c{opyright} 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06973" title="Abstract">arXiv:2311.06973</a> (replaced) [<a href="/pdf/2311.06973" title="Download PDF">pdf</a>, <a href="/ps/2311.06973" title="Download PostScript">ps</a>, <a href="/format/2311.06973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Verification of Deep Neural Network Performance for  Time-Synchronized Distribution System State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azimian%2C+B">Behrouz Azimian</a>, 
<a href="/search/cs?searchtype=author&query=Moshtagh%2C+S">Shiva Moshtagh</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Anamitra Pal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shanshan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07134" title="Abstract">arXiv:2311.07134</a> (replaced) [<a href="/pdf/2311.07134" title="Download PDF">pdf</a>, <a href="/format/2311.07134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of Integrated Data and Energy Transfer Assisted by  Fluid Antenna Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Halvin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yizhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07558" title="Abstract">arXiv:2311.07558</a> (replaced) [<a href="/pdf/2311.07558" title="Download PDF">pdf</a>, <a href="/format/2311.07558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Efficient Task Generalization via Probabilistic Model-based Meta  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+A">Arjun Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Rothfuss%2C+J">Jonas Rothfuss</a>, 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=As%2C+Y">Yarden As</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Coros%2C+S">Stelian Coros</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08367" title="Abstract">arXiv:2311.08367</a> (replaced) [<a href="/pdf/2311.08367" title="Download PDF">pdf</a>, <a href="/format/2311.08367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arboricity-Dependent Algorithms for Edge Coloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Mart&#xed;n Costa</a>, 
<a href="/search/cs?searchtype=author&query=Panski%2C+N">Nadav Panski</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+S">Shay Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Started to circulate in September 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09438" title="Abstract">arXiv:2311.09438</a> (replaced) [<a href="/pdf/2311.09438" title="Download PDF">pdf</a>, <a href="/format/2311.09438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeled Interactive Topic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seelman%2C+K">Kyle Seelman</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11123" title="Abstract">arXiv:2311.11123</a> (replaced) [<a href="/pdf/2311.11123" title="Download PDF">pdf</a>, <a href="/format/2311.11123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for  Evolving LLM APIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wanqin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4stner%2C+C">Christian K&#xe4;stner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> conference version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11734" title="Abstract">arXiv:2311.11734</a> (replaced) [<a href="/pdf/2311.11734" title="Download PDF">pdf</a>, <a href="/ps/2311.11734" title="Download PostScript">ps</a>, <a href="/format/2311.11734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private and Secure Post-Quantum Verifiable Random Function with NIZK  Proof and Ring-LWE Encryption in Blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B+G">Bong Gon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D">Dennis Wong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y+S">Yoon Seok Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures, In the 2023 Proceedings of International Conference on Cryptography and Blockchain
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of International Conference on Cryptography and
  Blockchain, 13(21), 47-67 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12568" title="Abstract">arXiv:2311.12568</a> (replaced) [<a href="/pdf/2311.12568" title="Download PDF">pdf</a>, <a href="/format/2311.12568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The $&#x3b2;$ Maps: Strong Clustering and Distribution Results on the  Complex Unit Circle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Piazza%2C+A+S">Alec Schiavoni Piazza</a>, 
<a href="/search/math?searchtype=author&query=Meadon%2C+D">David Meadon</a>, 
<a href="/search/math?searchtype=author&query=Serra-Capizzano%2C+S">Stefano Serra-Capizzano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12610" title="Abstract">arXiv:2311.12610</a> (replaced) [<a href="/pdf/2311.12610" title="Download PDF">pdf</a>, <a href="/format/2311.12610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VALUED -- Vision and Logical Understanding Evaluation Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Soumadeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Saptarshi Saha</a>, 
<a href="/search/cs?searchtype=author&query=Garain%2C+U">Utpal Garain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14220" title="Abstract">arXiv:2311.14220</a> (replaced) [<a href="/pdf/2311.14220" title="Download PDF">pdf</a>, <a href="/format/2311.14220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assumption-lean and Data-adaptive Post-Prediction Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Miao%2C+J">Jiacheng Miao</a>, 
<a href="/search/stat?searchtype=author&query=Miao%2C+X">Xinran Miao</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+Y">Yixuan Wu</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+J">Jiwei Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+Q">Qiongshi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14455" title="Abstract">arXiv:2311.14455</a> (replaced) [<a href="/pdf/2311.14455" title="Download PDF">pdf</a>, <a href="/format/2311.14455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Jailbreak Backdoors from Poisoned Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rando%2C+J">Javier Rando</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as conference paper in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16421" title="Abstract">arXiv:2311.16421</a> (replaced) [<a href="/pdf/2311.16421" title="Download PDF">pdf</a>, <a href="/format/2311.16421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CDEval: A Benchmark for Measuring the Cultural Dimensions of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shuyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xiaoyuan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+J">Jitao Sang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16543" title="Abstract">arXiv:2311.16543</a> (replaced) [<a href="/pdf/2311.16543" title="Download PDF">pdf</a>, <a href="/format/2311.16543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Da Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haoxing Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17245" title="Abstract">arXiv:2311.17245</a> (replaced) [<a href="/pdf/2311.17245" title="Download PDF">pdf</a>, <a href="/format/2311.17245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and  200+ FPS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhiwen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kevin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+K">Kairun Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zehao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dejia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16pages, 8figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17515" title="Abstract">arXiv:2311.17515</a> (replaced) [<a href="/pdf/2311.17515" title="Download PDF">pdf</a>, <a href="/ps/2311.17515" title="Download PostScript">ps</a>, <a href="/format/2311.17515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion of Single and Integral Multispectral Aerial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Youssef%2C+M">Mohamed Youssef</a>, 
<a href="/search/eess?searchtype=author&query=Bimber%2C+O">Oliver Bimber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00639" title="Abstract">arXiv:2312.00639</a> (replaced) [<a href="/pdf/2312.00639" title="Download PDF">pdf</a>, <a href="/format/2312.00639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassab%2C+K">Karim Kassab</a>, 
<a href="/search/cs?searchtype=author&query=Schnepf%2C+A">Antoine Schnepf</a>, 
<a href="/search/cs?searchtype=author&query=Franceschi%2C+J">Jean-Yves Franceschi</a>, 
<a href="/search/cs?searchtype=author&query=Caraffa%2C+L">Laurent Caraffa</a>, 
<a href="/search/cs?searchtype=author&query=Mary%2C+J">Jeremie Mary</a>, 
<a href="/search/cs?searchtype=author&query=Gouet-Brunet%2C+V">Val&#xe9;rie Gouet-Brunet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01538" title="Abstract">arXiv:2312.01538</a> (replaced) [<a href="/pdf/2312.01538" title="Download PDF">pdf</a>, <a href="/format/2312.01538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Distance Filtering for Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuhui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Orvieto%2C+A">Antonio Orvieto</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bobby He</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04828" title="Abstract">arXiv:2312.04828</a> (replaced) [<a href="/pdf/2312.04828" title="Download PDF">pdf</a>, <a href="/format/2312.04828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Readable Fingerprint for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Boyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenghu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04967" title="Abstract">arXiv:2312.04967</a> (replaced) [<a href="/pdf/2312.04967" title="Download PDF">pdf</a>, <a href="/format/2312.04967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control of a pendulum system: From simulation to reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+I+V">Iyer Venkataraman Natarajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05253" title="Abstract">arXiv:2312.05253</a> (replaced) [<a href="/pdf/2312.05253" title="Download PDF">pdf</a>, <a href="/format/2312.05253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiSK: A Diffusion Model for Structured Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kitouni%2C+O">Ouail Kitouni</a>, 
<a href="/search/cs?searchtype=author&query=Nolte%2C+N">Niklas Nolte</a>, 
<a href="/search/cs?searchtype=author&query=Hensman%2C+J">James Hensman</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+B">Bhaskar Mitra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05596" title="Abstract">arXiv:2312.05596</a> (replaced) [<a href="/pdf/2312.05596" title="Download PDF">pdf</a>, <a href="/format/2312.05596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factorized Explainer for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rundong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shirani%2C+F">Farhad Shirani</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05647" title="Abstract">arXiv:2312.05647</a> (replaced) [<a href="/pdf/2312.05647" title="Download PDF">pdf</a>, <a href="/format/2312.05647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding matrix exponentials for large transition rate matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pessoa%2C+P">Pedro Pessoa</a>, 
<a href="/search/physics?searchtype=author&query=Schweiger%2C+M">Max Schweiger</a>, 
<a href="/search/physics?searchtype=author&query=Presse%2C+S">Steve Presse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Numerical Analysis (math.NA); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07266" title="Abstract">arXiv:2312.07266</a> (replaced) [<a href="/pdf/2312.07266" title="Download PDF">pdf</a>, <a href="/format/2312.07266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for  Open-Vocabulary Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Joonhyun Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Geondo Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jayeon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyungsik Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heesu Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07737" title="Abstract">arXiv:2312.07737</a> (replaced) [<a href="/pdf/2312.07737" title="Download PDF">pdf</a>, <a href="/format/2312.07737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability of Ecological Systems: A Theoretical Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Can Chen</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xu-Wen Wang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yang-Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07930" title="Abstract">arXiv:2312.07930</a> (replaced) [<a href="/pdf/2312.07930" title="Download PDF">pdf</a>, <a href="/format/2312.07930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimal Statistical Watermarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baihe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08906" title="Abstract">arXiv:2312.08906</a> (replaced) [<a href="/pdf/2312.08906" title="Download PDF">pdf</a>, <a href="/format/2312.08906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using eye tracking to investigate what native Chinese speakers notice  about linguistic landscape images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zichao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yewei Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09108" title="Abstract">arXiv:2312.09108</a> (replaced) [<a href="/pdf/2312.09108" title="Download PDF">pdf</a>, <a href="/ps/2312.09108" title="Download PostScript">ps</a>, <a href="/format/2312.09108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Greedy Shapley Client Selection for Communication-Efficient Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+P">Pranava Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S+R">Shashi Raj Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Networking Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09305" title="Abstract">arXiv:2312.09305</a> (replaced) [<a href="/pdf/2312.09305" title="Download PDF">pdf</a>, <a href="/format/2312.09305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Score Distillation for High-Quality 3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Boshi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10396" title="Abstract">arXiv:2312.10396</a> (replaced) [<a href="/pdf/2312.10396" title="Download PDF">pdf</a>, <a href="/ps/2312.10396" title="Download PostScript">ps</a>, <a href="/format/2312.10396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can Fairness Constraints Help Recover From Biased Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mohit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Amit Deshpande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11404" title="Abstract">arXiv:2312.11404</a> (replaced) [<a href="/pdf/2312.11404" title="Download PDF">pdf</a>, <a href="/ps/2312.11404" title="Download PostScript">ps</a>, <a href="/format/2312.11404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An information-theoretic proof of the Shannon-Hagelbarger theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anantharam%2C+V">Venkat Anantharam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11549" title="Abstract">arXiv:2312.11549</a> (replaced) [<a href="/pdf/2312.11549" title="Download PDF">pdf</a>, <a href="/format/2312.11549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Free Multivariate Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shibo He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+W">Wenchao Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2208.02108">arXiv:2208.02108</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12275" title="Abstract">arXiv:2312.12275</a> (replaced) [<a href="/pdf/2312.12275" title="Download PDF">pdf</a>, <a href="/format/2312.12275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of In-Context Reinforcement Learning from Noise Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zisman%2C+I">Ilya Zisman</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Sinii%2C+V">Viacheslav Sinii</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, Under Review; code: <a href="https://github.com/corl-team/ad-eps">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12568" title="Abstract">arXiv:2312.12568</a> (replaced) [<a href="/pdf/2312.12568" title="Download PDF">pdf</a>, <a href="/format/2312.12568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Opponent Shaping to High Dimensional Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Akbir Khan</a>, 
<a href="/search/cs?searchtype=author&query=Willi%2C+T">Timon Willi</a>, 
<a href="/search/cs?searchtype=author&query=Kwan%2C+N">Newton Kwan</a>, 
<a href="/search/cs?searchtype=author&query=Tacchetti%2C+A">Andrea Tacchetti</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13768" title="Abstract">arXiv:2312.13768</a> (replaced) [<a href="/pdf/2312.13768" title="Download PDF">pdf</a>, <a href="/format/2312.13768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Interference from Millimeter Wave and Terahertz Bands  Cross-links in Low Earth Orbit Satellite Networks for 6G and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aliaga%2C+S">Sergi Aliaga</a>, 
<a href="/search/cs?searchtype=author&query=Petrov%2C+V">Vitaly Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Jornet%2C+J+M">Josep M. Jornet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 Figures, 2 Tables. The work has been accepted for publication in IEEE Journal on Selected Areas in Communications (JSAC), 2024. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14395" title="Abstract">arXiv:2312.14395</a> (replaced) [<a href="/pdf/2312.14395" title="Download PDF">pdf</a>, <a href="/format/2312.14395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Deep Learning Image Verification Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Solomon%2C+E">Enoch Solomon</a>, 
<a href="/search/cs?searchtype=author&query=Woubie%2C+A">Abraham Woubie</a>, 
<a href="/search/cs?searchtype=author&query=Emiru%2C+E+S">Eyael Solomon Emiru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15692" title="Abstract">arXiv:2312.15692</a> (replaced) [<a href="/pdf/2312.15692" title="Download PDF">pdf</a>, <a href="/format/2312.15692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Fusion: Advancing Prompt Evolution through Hybridization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Weidong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiuding Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+Z">Zhuwei Rao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+D">Di Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17552" title="Abstract">arXiv:2312.17552</a> (replaced) [<a href="/pdf/2312.17552" title="Download PDF">pdf</a>, <a href="/format/2312.17552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Deep Reinforcement Learning for Robust Target Tracking using  Micro Aerial Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dionigi%2C+A">Alberto Dionigi</a>, 
<a href="/search/cs?searchtype=author&query=Leomanni%2C+M">Mirko Leomanni</a>, 
<a href="/search/cs?searchtype=author&query=Saviolo%2C+A">Alessandro Saviolo</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>, 
<a href="/search/cs?searchtype=author&query=Costante%2C+G">Gabriele Costante</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 21st International Conference on Advanced Robotics (ICAR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00681" title="Abstract">arXiv:2401.00681</a> (replaced) [<a href="/pdf/2401.00681" title="Download PDF">pdf</a>, <a href="/ps/2401.00681" title="Download PostScript">ps</a>, <a href="/format/2401.00681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Procrastination in Spatial Crowdsourcing Via Efficient  Scheduling Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Debnath%2C+N">Naren Debnath</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+S">Sajal Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Xhafa%2C+F">Fatos Xhafa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00775" title="Abstract">arXiv:2401.00775</a> (replaced) [<a href="/pdf/2401.00775" title="Download PDF">pdf</a>, <a href="/format/2401.00775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advances in Text Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ke%2C+Z+T">Zheng Tracy Ke</a>, 
<a href="/search/stat?searchtype=author&query=Ji%2C+P">Pengsheng Ji</a>, 
<a href="/search/stat?searchtype=author&query=Jin%2C+J">Jiashun Jin</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+W">Wanshan Li</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Annual Review of Statistics and Its Application 2024 11:1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00813" title="Abstract">arXiv:2401.00813</a> (replaced) [<a href="/pdf/2401.00813" title="Download PDF">pdf</a>, <a href="/format/2401.00813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ultraspherical/Gegenbauer polynomials to unify 2D/3D Ambisonic  directivity designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zotter%2C+F">Franz Zotter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02122" title="Abstract">arXiv:2401.02122</a> (replaced) [<a href="/pdf/2401.02122" title="Download PDF">pdf</a>, <a href="/format/2401.02122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and  Ensemble Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Han Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">How-Shing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+H">Hao-Yung Weng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kuang-Chen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zih-Ching Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond (SASB) workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03506" title="Abstract">arXiv:2401.03506</a> (replaced) [<a href="/pdf/2401.03506" title="Download PDF">pdf</a>, <a href="/format/2401.03506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiarizationLM: Speaker Diarization Post-Processing with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Quan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yiling Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+G">Guanlong Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Clark%2C+E">Evan Clark</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+W">Wei Xia</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+H">Hank Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04122" title="Abstract">arXiv:2401.04122</a> (replaced) [<a href="/pdf/2401.04122" title="Download PDF">pdf</a>, <a href="/format/2401.04122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Prompt Engineering to Prompt Science With Human in the Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04319" title="Abstract">arXiv:2401.04319</a> (replaced) [<a href="/pdf/2401.04319" title="Download PDF">pdf</a>, <a href="/format/2401.04319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Know Your Needs Better: Towards Structured Understanding of Marketer  Demands with Analogical Reasoning Augmented LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Binbin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04472" title="Abstract">arXiv:2401.04472</a> (replaced) [<a href="/pdf/2401.04472" title="Download PDF">pdf</a>, <a href="/format/2401.04472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Efficient Federated Learning Methods for Foundation Model  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woisetschl%C3%A4ger%2C+H">Herbert Woisetschl&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Isenko%2C+A">Alexander Isenko</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05861" title="Abstract">arXiv:2401.05861</a> (replaced) [<a href="/pdf/2401.05861" title="Download PDF">pdf</a>, <a href="/format/2401.05861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Boosting Many-to-Many Multilingual Machine Translation with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Pengzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhongjun He</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haifeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06704" title="Abstract">arXiv:2401.06704</a> (replaced) [<a href="/pdf/2401.06704" title="Download PDF">pdf</a>, <a href="/format/2401.06704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable 3D Panoptic Segmentation As Superpoint Graph Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Robert%2C+D">Damien Robert</a>, 
<a href="/search/cs?searchtype=author&query=Raguet%2C+H">Hugo Raguet</a>, 
<a href="/search/cs?searchtype=author&query=Landrieu%2C+L">Loic Landrieu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 3DV 2024, Oral presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07836" title="Abstract">arXiv:2401.07836</a> (replaced) [<a href="/pdf/2401.07836" title="Download PDF">pdf</a>, <a href="/ps/2401.07836" title="Download PostScript">ps</a>, <a href="/format/2401.07836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Types of AI Existential Risk: Decisive and Accumulative
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasirzadeh%2C+A">Atoosa Kasirzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10034" title="Abstract">arXiv:2401.10034</a> (replaced) [<a href="/pdf/2401.10034" title="Download PDF">pdf</a>, <a href="/format/2401.10034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Computation in the Era of Large Language Model: Survey and  Roadmap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Sheng-hao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Liang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> evolutionary algorithm (EA), large language model (LLM), optimization problem, prompt optimization, architecture search, code generation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10664" title="Abstract">arXiv:2401.10664</a> (replaced) [<a href="/pdf/2401.10664" title="Download PDF">pdf</a>, <a href="/format/2401.10664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PTPsec: Securing the Precision Time Protocol Against Time Delay Attacks  Using Cyclic Path Asymmetry Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkenzeller%2C+A">Andreas Finkenzeller</a>, 
<a href="/search/cs?searchtype=author&query=Butowski%2C+O">Oliver Butowski</a>, 
<a href="/search/cs?searchtype=author&query=Regnath%2C+E">Emanuel Regnath</a>, 
<a href="/search/cs?searchtype=author&query=Hamad%2C+M">Mohammad Hamad</a>, 
<a href="/search/cs?searchtype=author&query=Steinhorst%2C+S">Sebastian Steinhorst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at INFOCOM24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11053" title="Abstract">arXiv:2401.11053</a> (replaced) [<a href="/e-print/2401.11053" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StreamVoice: Streamable Context-Aware Language Modeling for Real-time  Zero-Shot Voice Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yuanzhe Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xinsheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuping Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuxuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There is an error in the submitted version. The author and institution information needs to be modified, and the company requires re-examination before submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13967" title="Abstract">arXiv:2401.13967</a> (replaced) [<a href="/pdf/2401.13967" title="Download PDF">pdf</a>, <a href="/format/2401.13967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual-oriented Learned Image Compression with Dynamic Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+N">Nianxiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huairui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenzhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14168" title="Abstract">arXiv:2401.14168</a> (replaced) [<a href="/pdf/2401.14168" title="Download PDF">pdf</a>, <a href="/format/2401.14168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vivim: a Video Vision Mamba for Medical Video Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yijun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhaohu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14595" title="Abstract">arXiv:2401.14595</a> (replaced) [<a href="/pdf/2401.14595" title="Download PDF">pdf</a>, <a href="/ps/2401.14595" title="Download PostScript">ps</a>, <a href="/format/2401.14595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recency Ranking by Diversification of Result Set
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Styskin%2C+A">Andrey Styskin</a>, 
<a href="/search/cs?searchtype=author&query=Romanenko%2C+F">Fedor Romanenko</a>, 
<a href="/search/cs?searchtype=author&query=Vorobyev%2C+F">Fedor Vorobyev</a>, 
<a href="/search/cs?searchtype=author&query=Serdyukov%2C+P">Pavel Serdyukov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CIKM 2011 Proceedings of the 20th ACM international conference on
  Information and knowledge management
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14694" title="Abstract">arXiv:2401.14694</a> (replaced) [<a href="/pdf/2401.14694" title="Download PDF">pdf</a>, <a href="/ps/2401.14694" title="Download PostScript">ps</a>, <a href="/format/2401.14694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TA-RNN: an Attention-based Time-aware Recurrent Neural Network  Architecture for Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olaimat%2C+M+A">Mohammad Al Olaimat</a>, 
<a href="/search/cs?searchtype=author&query=Bozdag%2C+S">Serdar Bozdag</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15473" title="Abstract">arXiv:2401.15473</a> (replaced) [<a href="/pdf/2401.15473" title="Download PDF">pdf</a>, <a href="/ps/2401.15473" title="Download PostScript">ps</a>, <a href="/format/2401.15473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iDeLog: Iterative Dual Spatial and Kinematic Extraction of  Sigma-Lognormal Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+M+A">Miguel A. Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+M">Moises Diaz</a>, 
<a href="/search/cs?searchtype=author&query=Carmona-Duarte%2C+C">Cristina Carmona-Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Plamondon%2C+R">Rejean Plamondon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted Version published by Transactions on Pattern Analysis and Machine Intelligence
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Pattern Analysis and Machine Intelligence,
  42(1); p.p. 114-125, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15517" title="Abstract">arXiv:2401.15517</a> (replaced) [<a href="/pdf/2401.15517" title="Download PDF">pdf</a>, <a href="/ps/2401.15517" title="Download PostScript">ps</a>, <a href="/format/2401.15517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal Recovery From Product of Two Vandermonde Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapetanovic%2C+D">Dzevdan Kapetanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15604" title="Abstract">arXiv:2401.15604</a> (replaced) [<a href="/pdf/2401.15604" title="Download PDF">pdf</a>, <a href="/ps/2401.15604" title="Download PostScript">ps</a>, <a href="/format/2401.15604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network-Based Score Estimation in Diffusion Models: Optimization  and Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yinbin Han</a>, 
<a href="/search/cs?searchtype=author&query=Razaviyayn%2C+M">Meisam Razaviyayn</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renyuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15753" title="Abstract">arXiv:2401.15753</a> (replaced) [<a href="/pdf/2401.15753" title="Download PDF">pdf</a>, <a href="/format/2401.15753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An objective comparison of methods for augmented reality in laparoscopic  liver resection by preoperative-to-intraoperative image fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Sharib Ali</a>, 
<a href="/search/cs?searchtype=author&query=Espinel%2C+Y">Yamid Espinel</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yueming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCttner%2C+B">Bianca G&#xfc;ttner</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xukun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dowrick%2C+T">Tom Dowrick</a>, 
<a href="/search/cs?searchtype=author&query=Clarkson%2C+M+J">Matthew J. Clarkson</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shiting Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yijun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Dai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lan Li</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+M">Micha Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Farid%2C+S">Shahid Farid</a>, 
<a href="/search/cs?searchtype=author&query=Maier-Hein%2C+L">Lena Maier-Hein</a>, 
<a href="/search/cs?searchtype=author&query=Buc%2C+E">Emmanuel Buc</a>, 
<a href="/search/cs?searchtype=author&query=Bartoli%2C+A">Adrien Bartoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15879" title="Abstract">arXiv:2401.15879</a> (replaced) [<a href="/pdf/2401.15879" title="Download PDF">pdf</a>, <a href="/format/2401.15879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> lil&#x27;HDoC: An Algorithm for Good Arm Identification under Small Threshold  Gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+T">Tzu-Hsien Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Da Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shou-De Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16108" title="Abstract">arXiv:2401.16108</a> (replaced) [<a href="/pdf/2401.16108" title="Download PDF">pdf</a>, <a href="/format/2401.16108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Future Impact Decomposition in Request-level Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaobei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueliang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Q">Qingpeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lantao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guangming Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16123" title="Abstract">arXiv:2401.16123</a> (replaced) [<a href="/pdf/2401.16123" title="Download PDF">pdf</a>, <a href="/format/2401.16123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looking for a better fit? An Incremental Learning Multimodal Object  Referencing Framework adapting to Individual Drivers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomaa%2C+A">Amr Gomaa</a>, 
<a href="/search/cs?searchtype=author&query=Reyes%2C+G">Guillermo Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Feld%2C+M">Michael Feld</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%BCger%2C+A">Antonio Kr&#xfc;ger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the Proceedings of the 29th International Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in Greenville, SC, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16356" title="Abstract">arXiv:2401.16356</a> (replaced) [<a href="/pdf/2401.16356" title="Download PDF">pdf</a>, <a href="/format/2401.16356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and  Glitch Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Dooney%2C+T">Tom Dooney</a>, 
<a href="/search/physics?searchtype=author&query=Curier%2C+L">Lyana Curier</a>, 
<a href="/search/physics?searchtype=author&query=Tan%2C+D">Daniel Tan</a>, 
<a href="/search/physics?searchtype=author&query=Lopez%2C+M">Melissa Lopez</a>, 
<a href="/search/physics?searchtype=author&query=Van+Den+Broeck%2C+C">Chris Van Den Broeck</a>, 
<a href="/search/physics?searchtype=author&query=Bromuri%2C+S">Stefano Bromuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16803" title="Abstract">arXiv:2401.16803</a> (replaced) [<a href="/pdf/2401.16803" title="Download PDF">pdf</a>, <a href="/format/2401.16803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Arhan Jain</a>, 
<a href="/search/cs?searchtype=author&query=Bunn%2C+A">Alec Bunn</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+A">Austin Pham</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+T">TJ Tsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16919" title="Abstract">arXiv:2401.16919</a> (replaced) [<a href="/pdf/2401.16919" title="Download PDF">pdf</a>, <a href="/format/2401.16919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bit-flipping Decoder Failure Rate Estimation for (v,w)-regular Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annechini%2C+A">Alessandro Annechini</a>, 
<a href="/search/cs?searchtype=author&query=Barenghi%2C+A">Alessandro Barenghi</a>, 
<a href="/search/cs?searchtype=author&query=Pelosi%2C+G">Gerardo Pelosi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed typos: derivation of a from a=(x-y+v)/2 to a=(y-x+v)/2; replaced (x-y+v)/2 with (y-x+v)/2 and (x-y+v-1)/2 with (y-x+v-1)/2 in rho(x,y,l); replaced d+ with d- in the def. of delta-(d-); replaced epsilon01-l with l in zeta(tc,l,epsilon01) and epsilon11-l with l in lambda(tc,l,epsilon11) (apart from the def.s); explicited epsilon01 and epsilon11 in zeta and chi_odd
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17049" title="Abstract">arXiv:2401.17049</a> (replaced) [<a href="/pdf/2401.17049" title="Download PDF">pdf</a>, <a href="/ps/2401.17049" title="Download PostScript">ps</a>, <a href="/format/2401.17049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Movable Antenna-Enabled Co-Frequency Co-Time Full-Duplex Wireless  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingze Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zijian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lifeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+B">Bingli Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE Wireless Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17760" title="Abstract">arXiv:2401.17760</a> (replaced) [<a href="/pdf/2401.17760" title="Download PDF">pdf</a>, <a href="/ps/2401.17760" title="Download PostScript">ps</a>, <a href="/format/2401.17760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Linear Discriminant Analysis Using a Nonlinear Covariance  Matrix Estimator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mahadi%2C+M">Maaz Mahadi</a>, 
<a href="/search/stat?searchtype=author&query=Ballal%2C+T">Tarig Ballal</a>, 
<a href="/search/stat?searchtype=author&query=Moinuddin%2C+M">Muhammad Moinuddin</a>, 
<a href="/search/stat?searchtype=author&query=Al-Naffouri%2C+T+Y">Tareq Y. Al-Naffouri</a>, 
<a href="/search/stat?searchtype=author&query=Al-Saggaf%2C+U+M">Ubaid M. Al-Saggaf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.18079" title="Abstract">arXiv:2401.18079</a> (replaced) [<a href="/pdf/2401.18079" title="Download PDF">pdf</a>, <a href="/format/2401.18079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache  Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hooper%2C+C">Coleman Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadzadeh%2C+H">Hiva Mohammadzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y+S">Yakun Sophia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00045" title="Abstract">arXiv:2402.00045</a> (replaced) [<a href="/pdf/2402.00045" title="Download PDF">pdf</a>, <a href="/format/2402.00045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Multimedia Generated by Large AI Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Li Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Neeraj Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hainan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chun-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Feng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Verdoliva%2C+L">Luisa Verdoliva</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00350" title="Abstract">arXiv:2402.00350</a> (replaced) [<a href="/pdf/2402.00350" title="Download PDF">pdf</a>, <a href="/format/2402.00350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Based Fuzzing Techniques: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Linghan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peizhou Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages submission under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00838" title="Abstract">arXiv:2402.00838</a> (replaced) [<a href="/pdf/2402.00838" title="Download PDF">pdf</a>, <a href="/format/2402.00838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OLMo: Accelerating the Science of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Groeneveld%2C+D">Dirk Groeneveld</a>, 
<a href="/search/cs?searchtype=author&query=Beltagy%2C+I">Iz Beltagy</a>, 
<a href="/search/cs?searchtype=author&query=Walsh%2C+P">Pete Walsh</a>, 
<a href="/search/cs?searchtype=author&query=Bhagia%2C+A">Akshita Bhagia</a>, 
<a href="/search/cs?searchtype=author&query=Kinney%2C+R">Rodney Kinney</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+A+H">Ananya Harsh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Ivison%2C+H">Hamish Ivison</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+I">Ian Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Shane Arora</a>, 
<a href="/search/cs?searchtype=author&query=Atkinson%2C+D">David Atkinson</a>, 
<a href="/search/cs?searchtype=author&query=Authur%2C+R">Russell Authur</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K+R">Khyathi Raghavi Chandu</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Dumas%2C+J">Jennifer Dumas</a>, 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuling Gu</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Khot%2C+T">Tushar Khot</a>, 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Morrison%2C+J">Jacob Morrison</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aakanksha Naik</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+C">Crystal Nam</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+M+E">Matthew E. Peters</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Ravichander%2C+A">Abhilasha Ravichander</a>, 
<a href="/search/cs?searchtype=author&query=Schwenk%2C+D">Dustin Schwenk</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Saurabh Shah</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+W">Will Smith</a>, 
<a href="/search/cs?searchtype=author&query=Strubell%2C+E">Emma Strubell</a>, 
<a href="/search/cs?searchtype=author&query=Subramani%2C+N">Nishant Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Wortsman%2C+M">Mitchell Wortsman</a>, 
<a href="/search/cs?searchtype=author&query=Dasigi%2C+P">Pradeep Dasigi</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+K">Kyle Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jesse Dodge</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00850" title="Abstract">arXiv:2402.00850</a> (replaced) [<a href="/pdf/2402.00850" title="Download PDF">pdf</a>, <a href="/format/2402.00850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constant Degree Direct Product Testers with Small Soundness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bafna%2C+M">Mitali Bafna</a>, 
<a href="/search/cs?searchtype=author&query=Lifshitz%2C+N">Noam Lifshitz</a>, 
<a href="/search/cs?searchtype=author&query=Minzer%2C+D">Dor Minzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01000" title="Abstract">arXiv:2402.01000</a> (replaced) [<a href="/pdf/2402.01000" title="Download PDF">pdf</a>, <a href="/format/2402.01000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Probabilistic Time Series Forecasting with Correlated  Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zheng%2C+V+Z">Vincent Zhihao Zheng</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+L">Lijun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper extends the work presented in <a href="/abs/2305.17028">arXiv:2305.17028</a> to a multivariate setting
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01035" title="Abstract">arXiv:2402.01035</a> (replaced) [<a href="/pdf/2402.01035" title="Download PDF">pdf</a>, <a href="/format/2402.01035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting the most out of your tokenizer for pre-training and domain  adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagan%2C+G">Gautier Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Synnaeve%2C+G">Gabriel Synnaeve</a>, 
<a href="/search/cs?searchtype=author&query=Rozi%C3%A8re%2C+B">Baptiste Rozi&#xe8;re</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01263" title="Abstract">arXiv:2402.01263</a> (replaced) [<a href="/pdf/2402.01263" title="Download PDF">pdf</a>, <a href="/format/2402.01263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Differentiable Partially Observable Generalized Linear Model with  Forward-Backward Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yule Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01345" title="Abstract">arXiv:2402.01345</a> (replaced) [<a href="/pdf/2402.01345" title="Download PDF">pdf</a>, <a href="/format/2402.01345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip \n: A Simple Method to Reduce Hallucination in Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zongbo Han</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zechen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01364" title="Abstract">arXiv:2402.01364</a> (replaced) [<a href="/pdf/2402.01364" title="Download PDF">pdf</a>, <a href="/format/2402.01364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongtong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01580" title="Abstract">arXiv:2402.01580</a> (replaced) [<a href="/pdf/2402.01580" title="Download PDF">pdf</a>, <a href="/format/2402.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Education (GAIED): Advances, Opportunities, and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Heffernan%2C+N+T">Neil T. Heffernan</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4ser%2C+T">Tanja K&#xe4;ser</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+S">Steven Moore</a>, 
<a href="/search/cs?searchtype=author&query=Rafferty%2C+A+N">Anna N. Rafferty</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Adish Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01598" title="Abstract">arXiv:2402.01598</a> (replaced) [<a href="/pdf/2402.01598" title="Download PDF">pdf</a>, <a href="/format/2402.01598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Two Decades of Blood Pressure Data: Demography-Specific  Patterns Across 75 Million Patient Encounters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Mousavi%2C+S+S">Seyedeh Somayyeh Mousavi</a>, 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+Y">Yuting Guo</a>, 
<a href="/search/q-bio?searchtype=author&query=Sarker%2C+A">Abeed Sarker</a>, 
<a href="/search/q-bio?searchtype=author&query=Sameni%2C+R">Reza Sameni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01697" title="Abstract">arXiv:2402.01697</a> (replaced) [<a href="/pdf/2402.01697" title="Download PDF">pdf</a>, <a href="/format/2402.01697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhizhuo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+E">Ehsan-Ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lik-Hang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Just accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01726" title="Abstract">arXiv:2402.01726</a> (replaced) [<a href="/pdf/2402.01726" title="Download PDF">pdf</a>, <a href="/format/2402.01726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Does Not Alter Perceptions of Text Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diamond%2C+N">N&#x27;yoma Diamond</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01729" title="Abstract">arXiv:2402.01729</a> (replaced) [<a href="/pdf/2402.01729" title="Download PDF">pdf</a>, <a href="/format/2402.01729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualization Distillation from Large Language Model for Knowledge  Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024 findings v2: revise the citation problem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01734" title="Abstract">arXiv:2402.01734</a> (replaced) [<a href="/pdf/2402.01734" title="Download PDF">pdf</a>, <a href="/format/2402.01734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFTM: Continuous time fractional topic model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakagawa%2C+K">Kei Nakagawa</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Kohei Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+Y">Yugo Fujimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01748" title="Abstract">arXiv:2402.01748</a> (replaced) [<a href="/pdf/2402.01748" title="Download PDF">pdf</a>, <a href="/format/2402.01748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Multi-Modal Models (LMMs) as Universal Foundation Models for  AI-Native Wireless Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shengzhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+C+K">Christo Kurisummoottil Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Hashash%2C+O">Omar Hashash</a>, 
<a href="/search/cs?searchtype=author&query=Muralidhar%2C+N">Nikhil Muralidhar</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+W">Walid Saad</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+N">Naren Ramakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01852" title="Abstract">arXiv:2402.01852</a> (replaced) [<a href="/pdf/2402.01852" title="Download PDF">pdf</a>, <a href="/ps/2402.01852" title="Download PostScript">ps</a>, <a href="/format/2402.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QPP and HPPK: Unifying Non-Commutativity for Quantum-Secure Cryptography  with Galois Permutation Group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+R">Randy Kuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01995" title="Abstract">arXiv:2402.01995</a> (replaced) [<a href="/pdf/2402.01995" title="Download PDF">pdf</a>, <a href="/format/2402.01995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Uniform Risk Times Sampling: First Approximation Algorithms,  Learning Augmentation with Full Confidence Interval Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xueqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+K">Kyra Gan</a>, 
<a href="/search/cs?searchtype=author&query=Keyvanshokooh%2C+E">Esmaeil Keyvanshokooh</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+S">Susan Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02018" title="Abstract">arXiv:2402.02018</a> (replaced) [<a href="/pdf/2402.02018" title="Download PDF">pdf</a>, <a href="/format/2402.02018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Landscape and Challenges of HPC Research and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Le Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Akash Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Arijit Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+Q+I">Quazi Ishtiaque Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Abebe%2C+W">Waqwoya Abebe</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hung Phan</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Aishwarya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+B">Branden Butler</a>, 
<a href="/search/cs?searchtype=author&query=Hasabnis%2C+N">Niranjan Hasabnis</a>, 
<a href="/search/cs?searchtype=author&query=Oren%2C+G">Gal Oren</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+V+A">Vy A. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Munoz%2C+J+P">Juan Pablo Munoz</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T+L">Theodore L. Willke</a>, 
<a href="/search/cs?searchtype=author&query=Mattson%2C+T">Tim Mattson</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02172" title="Abstract">arXiv:2402.02172</a> (replaced) [<a href="/pdf/2402.02172" title="Download PDF">pdf</a>, <a href="/format/2402.02172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Agents for Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Daniel Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kisub Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yewei Song</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Haoye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ezzini%2C+S">Saad Ezzini</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongfeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyande%2C+T+F">Tegawende F. Bissyande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02205" title="Abstract">arXiv:2402.02205</a> (replaced) [<a href="/pdf/2402.02205" title="Download PDF">pdf</a>, <a href="/format/2402.02205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model  on Complex Traffic Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingcheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A+C">Alois C. Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02216" title="Abstract">arXiv:2402.02216</a> (replaced) [<a href="/pdf/2402.02216" title="Download PDF">pdf</a>, <a href="/format/2402.02216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenzhuo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Galkin%2C+M">Mikhail Galkin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02304" title="Abstract">arXiv:2402.02304</a> (replaced) [<a href="/pdf/2402.02304" title="Download PDF">pdf</a>, <a href="/format/2402.02304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep  Learning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaiser%2C+L">Luis Kaiser</a>, 
<a href="/search/math?searchtype=author&query=Tsai%2C+R">Richard Tsai</a>, 
<a href="/search/math?searchtype=author&query=Klingenberg%2C+C">Christian Klingenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02347" title="Abstract">arXiv:2402.02347</a> (replaced) [<a href="/pdf/2402.02347" title="Download PDF">pdf</a>, <a href="/format/2402.02347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fangzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02355" title="Abstract">arXiv:2402.02355</a> (replaced) [<a href="/pdf/2402.02355" title="Download PDF">pdf</a>, <a href="/format/2402.02355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbol: Generating Flexible Black-Box Optimizers through Symbolic  Equation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zeyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongshu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yue-Jiao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02441" title="Abstract">arXiv:2402.02441</a> (replaced) [<a href="/pdf/2402.02441" title="Download PDF">pdf</a>, <a href="/format/2402.02441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Agerberg%2C+J">Jens Agerberg</a>, 
<a href="/search/cs?searchtype=author&query=AlJabea%2C+I">Ibrahem AlJabea</a>, 
<a href="/search/cs?searchtype=author&query=Ballester%2C+R">Ruben Ballester</a>, 
<a href="/search/cs?searchtype=author&query=Battiloro%2C+C">Claudio Battiloro</a>, 
<a href="/search/cs?searchtype=author&query=Bern%C3%A1rdez%2C+G">Guillermo Bern&#xe1;rdez</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Brent%2C+A">Aiden Brent</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Fiorellino%2C+S">Simone Fiorellino</a>, 
<a href="/search/cs?searchtype=author&query=Gardaa%2C+O+H">Odin Hoff Gardaa</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+G">Gurusankar Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+D">Devendra Govil</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+J">Josef Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M+R">Maneel Reddy Karri</a>, 
<a href="/search/cs?searchtype=author&query=Khouja%2C+J">Jude Khouja</a>, 
<a href="/search/cs?searchtype=author&query=Lecha%2C+M">Manuel Lecha</a>, 
<a href="/search/cs?searchtype=author&query=Livesay%2C+N">Neal Livesay</a>, 
<a href="/search/cs?searchtype=author&query=Mei%C3%9Fner%2C+J">Jan Mei&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Soham Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Nikitin%2C+A">Alexander Nikitin</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Pr%C3%ADlepok%2C+J">Jaro Pr&#xed;lepok</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n-S%C3%A1enz%2C+A">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, 
<a href="/search/cs?searchtype=author&query=Salatiello%2C+A">Alessandro Salatiello</a>, 
<a href="/search/cs?searchtype=author&query=Samaga%2C+S+N">Shreyas N. Samaga</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Scofano%2C+L">Luca Scofano</a>, 
<a href="/search/cs?searchtype=author&query=Spinelli%2C+I">Indro Spinelli</a>, 
<a href="/search/cs?searchtype=author&query=Telyatnikov%2C+L">Lev Telyatnikov</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zaghen%2C+O">Olga Zaghen</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+A">Ali Zia</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Mathematical Software (cs.MS); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02452" title="Abstract">arXiv:2402.02452</a> (replaced) [<a href="/pdf/2402.02452" title="Download PDF">pdf</a>, <a href="/format/2402.02452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI-CF -- Examining the Role of Explainable Artificial Intelligence in  Cyber Forensics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shahid Alam</a>, 
<a href="/search/cs?searchtype=author&query=Altiparmak%2C+Z">Zeynep Altiparmak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02544" title="Abstract">arXiv:2402.02544</a> (replaced) [<a href="/pdf/2402.02544" title="Download PDF">pdf</a>, <a href="/format/2402.02544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhtar%2C+D">Dilxat Muhtar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+F">Feng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+P">Pengfeng Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 8 figures. Github <a href="https://github.com/NJU-LHRS/LHRS-Bot">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02619" title="Abstract">arXiv:2402.02619</a> (replaced) [<a href="/pdf/2402.02619" title="Download PDF">pdf</a>, <a href="/format/2402.02619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing Trust in Language Models through the Reuse of Verified  Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quirke%2C+P">Philip Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Neo%2C+C">Clement Neo</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02692" title="Abstract">arXiv:2402.02692</a> (replaced) [<a href="/pdf/2402.02692" title="Download PDF">pdf</a>, <a href="/format/2402.02692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Guarantees for Link Prediction using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+A">Alan Chung</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+A">Amin Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Austern%2C+M">Morgane Austern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02910" title="Abstract">arXiv:2402.02910</a> (replaced) [<a href="/pdf/2402.02910" title="Download PDF">pdf</a>, <a href="/format/2402.02910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage  Temporal Convolutional Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Meng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Dedeyne%2C+L">Lenore Dedeyne</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+J">Jolan Dupont</a>, 
<a href="/search/cs?searchtype=author&query=Vercauteren%2C+L">Laura Vercauteren</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+N">Nadjia Amini</a>, 
<a href="/search/cs?searchtype=author&query=Lapauw%2C+L">Laurence Lapauw</a>, 
<a href="/search/cs?searchtype=author&query=Gielen%2C+E">Evelien Gielen</a>, 
<a href="/search/cs?searchtype=author&query=Verschueren%2C+S">Sabine Verschueren</a>, 
<a href="/search/cs?searchtype=author&query=Varon%2C+C">Carolina Varon</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+W">Walter De Raedt</a>, 
<a href="/search/cs?searchtype=author&query=Vanrumste%2C+B">Bart Vanrumste</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02950" title="Abstract">arXiv:2402.02950</a> (replaced) [<a href="/pdf/2402.02950" title="Download PDF">pdf</a>, <a href="/format/2402.02950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Entropy Can Simultaneously Benefit Transmission Efficiency and  Channel Security of Wireless Semantic Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yankai Rong</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+G">Guoshun Nan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Songtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Nan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shixun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qimei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaofeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q.S. Quek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03019" title="Abstract">arXiv:2402.03019</a> (replaced) [<a href="/pdf/2402.03019" title="Download PDF">pdf</a>, <a href="/format/2402.03019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taylor Videos for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiuyuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03214" title="Abstract">arXiv:2402.03214</a> (replaced) [<a href="/pdf/2402.03214" title="Download PDF">pdf</a>, <a href="/format/2402.03214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Organic or Diffused: Can We Distinguish Human Art from AI-generated  Images?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+A+Y+J">Anna Yoo Jeong Ha</a>, 
<a href="/search/cs?searchtype=author&query=Passananti%2C+J">Josephine Passananti</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+R">Ronik Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shawn Shan</a>, 
<a href="/search/cs?searchtype=author&query=Southen%2C+R">Reid Southen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B+Y">Ben Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03279" title="Abstract">arXiv:2402.03279</a> (replaced) [<a href="/pdf/2402.03279" title="Download PDF">pdf</a>, <a href="/format/2402.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stepping into the Right Shoes: The Effects of User-Matched Avatar  Ethnicity and Gender on Sense of Embodiment in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+T+D">Tiffany D. Do</a>, 
<a href="/search/cs?searchtype=author&query=Protko%2C+C+I">Camille Isabella Protko</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03292" title="Abstract">arXiv:2402.03292</a> (replaced) [<a href="/pdf/2402.03292" title="Download PDF">pdf</a>, <a href="/format/2402.03292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quang-Huy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+P">Jin Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+K">Khanh-Huyen Bui</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+D">Dung D. Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03307" title="Abstract">arXiv:2402.03307</a> (replaced) [<a href="/pdf/2402.03307" title="Download PDF">pdf</a>, <a href="/format/2402.03307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yuanxing Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Qiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuhang He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenzheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03358" title="Abstract">arXiv:2402.03358</a> (replaced) [<a href="/pdf/2402.03358" title="Download PDF">pdf</a>, <a href="/format/2402.03358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,  and Condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Mohammad Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Juntong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03363" title="Abstract">arXiv:2402.03363</a> (replaced) [<a href="/pdf/2402.03363" title="Download PDF">pdf</a>, <a href="/format/2402.03363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Prime Number Classification: Achieving High Recall Rate and  Rapid Convergence with Sparse Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+S">Serin Lee</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+S">S. Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03391" title="Abstract">arXiv:2402.03391</a> (replaced) [<a href="/pdf/2402.03391" title="Download PDF">pdf</a>, <a href="/format/2402.03391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear model predictive control-based guidance law for path following  of unmanned surface vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Manzano%2C+J+M">J. M. Manzano</a>, 
<a href="/search/eess?searchtype=author&query=Salvador%2C+J+R">J. R. Salvador</a>, 
<a href="/search/eess?searchtype=author&query=Limon%2C+D">D. Limon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering (2022), 258, 111764
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03392" title="Abstract">arXiv:2402.03392</a> (replaced) [<a href="/pdf/2402.03392" title="Download PDF">pdf</a>, <a href="/format/2402.03392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal control analysis and Practical NMPC applied to refrigeration  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/math?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>, 
<a href="/search/math?searchtype=author&query=Normey-Rico%2C+J+E">J. E. Normey-Rico</a>, 
<a href="/search/math?searchtype=author&query=Rubio%2C+F+R">F. R Rubio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 14 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISA Transactions (2020), 107, 90-106
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03395" title="Abstract">arXiv:2402.03395</a> (replaced) [<a href="/pdf/2402.03395" title="Download PDF">pdf</a>, <a href="/ps/2402.03395" title="Download PostScript">ps</a>, <a href="/format/2402.03395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel scheme for a PCM-based cold energy storage system. Design,  modelling, and simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Suffo%2C+J+J">J. J. Suffo</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 14 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Thermal Engineering (2018), 132, 256-274
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03465" title="Abstract">arXiv:2402.03465</a> (replaced) [<a href="/pdf/2402.03465" title="Download PDF">pdf</a>, <a href="/format/2402.03465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband  Signal Stitching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uvaydov%2C+D">Daniel Uvaydov</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Milin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C+P">Clifton Paul Robinson</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+S">Salvatore D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Melodia%2C+T">Tommaso Melodia</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03543" title="Abstract">arXiv:2402.03543</a> (replaced) [<a href="/pdf/2402.03543" title="Download PDF">pdf</a>, <a href="/ps/2402.03543" title="Download PostScript">ps</a>, <a href="/format/2402.03543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Lawvere Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacci%2C+G">Giorgio Bacci</a>, 
<a href="/search/cs?searchtype=author&query=Mardare%2C+R">Radu Mardare</a>, 
<a href="/search/cs?searchtype=author&query=Panangaden%2C+P">Prakash Panangaden</a>, 
<a href="/search/cs?searchtype=author&query=Plotkin%2C+G">Gordon Plotkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03561" title="Abstract">arXiv:2402.03561</a> (replaced) [<a href="/pdf/2402.03561" title="Download PDF">pdf</a>, <a href="/format/2402.03561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+A">Aishwarya Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G">Gaurav Sukhatme</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03580" title="Abstract">arXiv:2402.03580</a> (replaced) [<a href="/pdf/2402.03580" title="Download PDF">pdf</a>, <a href="/format/2402.03580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MINLP-based hybrid strategy for operating mode selection of  TES-backed-up refrigeration systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/math?searchtype=author&query=Rodr%C3%ADguez%2C+D">D. Rodr&#xed;guez</a>, 
<a href="/search/math?searchtype=author&query=Lemos%2C+J+M">J. M. Lemos</a>, 
<a href="/search/math?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/math?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures. Postprint of the final published work
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Robust and Nonlinear Control (2020), 30,
  6091-6111
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03583" title="Abstract">arXiv:2402.03583</a> (replaced) [<a href="/pdf/2402.03583" title="Download PDF">pdf</a>, <a href="/format/2402.03583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MQuinE: a cure for &quot;Z-paradox&quot; in knowledge graph embedding models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yunfeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingming Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03659" title="Abstract">arXiv:2402.03659</a> (replaced) [<a href="/pdf/2402.03659" title="Download PDF">pdf</a>, <a href="/format/2402.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Explainable Stock Predictions using Self-Reflective  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koa%2C+K+J+L">Kelvin J.L. Koa</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunshan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+R">Ritchie Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Statistical Finance (q-fin.ST)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03697" title="Abstract">arXiv:2402.03697</a> (replaced) [<a href="/pdf/2402.03697" title="Download PDF">pdf</a>, <a href="/format/2402.03697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHMC-Net: A Mask-guided Feature Fusion Network for Sperm Head Morphology  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+N">Nishchal Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yejia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sirui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peixian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D+Z">Danny Z Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A shorter version is published on ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03760" title="Abstract">arXiv:2402.03760</a> (replaced) [<a href="/pdf/2402.03760" title="Download PDF">pdf</a>, <a href="/format/2402.03760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeMarking: A Defense for Network Flow Watermarking in Real-Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yali Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03780" title="Abstract">arXiv:2402.03780</a> (replaced) [<a href="/pdf/2402.03780" title="Download PDF">pdf</a>, <a href="/format/2402.03780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing propaganda: an analysis of stylistic cues comparing human  annotations and machine classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faye%2C+G">G&#xe9;raud Faye</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+B">Benjamin Icard</a>, 
<a href="/search/cs?searchtype=author&query=Casanova%2C+M">Morgane Casanova</a>, 
<a href="/search/cs?searchtype=author&query=Chanson%2C+J">Julien Chanson</a>, 
<a href="/search/cs?searchtype=author&query=Maine%2C+F">Fran&#xe7;ois Maine</a>, 
<a href="/search/cs?searchtype=author&query=Bancilhon%2C+F">Fran&#xe7;ois Bancilhon</a>, 
<a href="/search/cs?searchtype=author&query=Gadek%2C+G">Guillaume Gadek</a>, 
<a href="/search/cs?searchtype=author&query=Gravier%2C+G">Guillaume Gravier</a>, 
<a href="/search/cs?searchtype=author&query=%C3%89gr%C3%A9%2C+P">Paul &#xc9;gr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper to appear in the EACL 2024 Proceedings of the Third Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03784" title="Abstract">arXiv:2402.03784</a> (replaced) [<a href="/pdf/2402.03784" title="Download PDF">pdf</a>, <a href="/format/2402.03784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hettige%2C+K+H">Kethmi Hirushini Hettige</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiahao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+S">Shili Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Cheng Long</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gao Cong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03962" title="Abstract">arXiv:2402.03962</a> (replaced) [<a href="/pdf/2402.03962" title="Download PDF">pdf</a>, <a href="/format/2402.03962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI  Claims
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altmeyer%2C+P">Patrick Altmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Demetriou%2C+A+M">Andrew M. Demetriou</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+A">Antony Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Liem%2C+C+C+S">Cynthia C. S. Liem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 15 figures. Preliminary work. Under review by the International Conference on Machine Learning (ICML)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03979" title="Abstract">arXiv:2402.03979</a> (replaced) [<a href="/pdf/2402.03979" title="Download PDF">pdf</a>, <a href="/format/2402.03979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Li Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+K">Keith Ross</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Andriopoulos%2C+G">George Andriopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">Shuyang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yufeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zixuan Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04022" title="Abstract">arXiv:2402.04022</a> (replaced) [<a href="/pdf/2402.04022" title="Download PDF">pdf</a>, <a href="/format/2402.04022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory for Kernel Packets: from state space model to compactly  supported basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/stat?searchtype=author&query=Rui%2C+T">Tuo Rui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04140" title="Abstract">arXiv:2402.04140</a> (replaced) [<a href="/pdf/2402.04140" title="Download PDF">pdf</a>, <a href="/ps/2402.04140" title="Download PostScript">ps</a>, <a href="/format/2402.04140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Legal Reasoning: The Integration of AI to Navigate  Complexities and Biases in Global Jurisprudence with Semi-Automated  Arbitration Processes (SAAPs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De%27Shazer%2C+M">Michael De&#x27;Shazer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04193" title="Abstract">arXiv:2402.04193</a> (replaced) [<a href="/pdf/2402.04193" title="Download PDF">pdf</a>, <a href="/format/2402.04193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Coding in Decentralized Learning for Evading Stragglers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04232" title="Abstract">arXiv:2402.04232</a> (replaced) [<a href="/pdf/2402.04232" title="Download PDF">pdf</a>, <a href="/format/2402.04232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Generative Agents Predict Emotion?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Regan%2C+C">Ciaran Regan</a>, 
<a href="/search/cs?searchtype=author&query=Iwahashi%2C+N">Nanami Iwahashi</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+S">Shogo Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Oka%2C+M">Mizuki Oka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04247" title="Abstract">arXiv:2402.04247</a> (replaced) [<a href="/pdf/2402.04247" title="Download PDF">pdf</a>, <a href="/format/2402.04247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kunlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tongxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+M">Meng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item419">Replacements</a></li>
</ul>
<small>[ total of 674 entries:  <b>1-674</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
