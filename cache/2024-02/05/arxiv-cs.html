<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Thu  1 Feb 24  to  Fri  2 Feb 24, announced Mon,  5 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item372">Cross-lists</a></li>
<li><a href="#item422">Replacements</a></li>
</ul>
<small>[ total of 674 entries:  <b>1-674</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Mon,  5 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00870" title="Abstract">arXiv:2402.00870</a> [<a href="/pdf/2402.00870" title="Download PDF">pdf</a>, <a href="/format/2402.00870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritising Interactive Flows in Data Center Networks With Central  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moorthy%2C+M+P+S">Mohana Prasad Sathya Moorthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bachelor's thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Data centers are on the rise and scientists are re-thinking and re-designing
networks for data centers. The concept of central control which was not
effective in the Internet era is now gaining popularity and is used in many
data centers due to lower scale of operation (compared to Internet), structured
topologies and as the entire network resources is under a single entity's
control. With new opportunities, data center networks also pose new problems.
Data centers require: high utilization, low median, tail latencies and
fairness. In the traditional systems, the bulk traffic generally stalls the
interactive flows thereby affecting their flow completion times adversely. In
this thesis, we deal with two problems relating to central controller assisted
prioritization of interactive flow in data center networks.
<br />Fastpass is a centralized "zero-queue" data center network. But the central
arbiter of Fastpass doesn't scale well for more than 256 nodes (or 8 cores). In
our test runs, it supports only about 1.5 Terabits's of network traffic. In
this work, we re-design their timeslot allocator of their central arbiter so
that it scales linearly till 12 cores and supports about 1024 nodes and 7.1
Terabits's of network traffic.
<br />In the second part of the thesis, we deal with the problem of congestion
control in a software defined network. We propose a framework, where the
controller with its global view of the network actively participates in the
congestion control decisions of the end TCP hosts, by setting the ECN bits of
IPV4 packets appropriately. Our framework can be deployed very easily without
any change to the end node TCPs or the SDN switches. We also show 30x
improvement over TCP cubic and 1.7x improvement over RED in flow completion
times of interactive traffic for one implementation of this framework.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00871" title="Abstract">arXiv:2402.00871</a> [<a href="/pdf/2402.00871" title="Download PDF">pdf</a>, <a href="/format/2402.00871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research on Resource Allocation under Unlicensed Spectrum Using  Q-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ial%2C+U">Uyoy Ial</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">In response to the advent of the 5G era, enhancing throughput and increasing
transmission efficiency within limited spectrum resources is an important
research topic. In the LTE system, utilizing unlicensed spectrum to assist
traditional mobile networks, known as License Assisted Access, has emerged as a
viable solution to effectively improve transmission efficiency. However, as the
unlicensed spectrum also accommodates other users, such as Wi-Fi for mobile
communication, there is a need to address the issue of spectrum resource
allocation, aiming to achieve fair transmission among different mobile
communication users. This research project aims to explore and compare two
approaches: traditional communication algorithms and reinforcement learning
method Q-leaning, under the condition of achieving maximum system throughput,
in order to determine the differences between the two methods.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00872" title="Abstract">arXiv:2402.00872</a> [<a href="/pdf/2402.00872" title="Download PDF">pdf</a>, <a href="/ps/2402.00872" title="Download PostScript">ps</a>, <a href="/format/2402.00872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data On the Go: Seamless Data Routing for Intermittently-Powered  Battery-Free Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gaosheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The rising demand for sustainable IoT has promoted the adoption of
battery-free devices intermittently powered by ambient energy for sensing.
However, the intermittency poses significant challenges in sensing data
collection. Despite recent efforts to enable one-to-one communication, routing
data across multiple intermittently-powered battery-free devices, a crucial
requirement for a sensing system, remains a formidable challenge.
<br />This paper fills this gap by introducing Swift, which enables seamless data
routing in intermittently-powered battery-free sensing systems. Swift overcomes
the challenges posed by device intermittency and heterogeneous energy
conditions through three major innovative designs. First, Swift incorporates a
reliable node synchronization protocol backed by number theory, ensuring
successful synchronization regardless of energy conditions. Second, Swift
adopts a low-latency message forwarding protocol, allowing continuous message
forwarding without repeated synchronization. Finally, Swift features a simple
yet effective mechanism for routing path construction, enabling nodes to obtain
the optimal path to the sink node with minimum hops. We implement Swift and
perform large-scale experiments representing diverse realworld scenarios. The
results demonstrate that Swift achieves an order of magnitude reduction in
end-to-end message delivery time compared with the state-of-the-art approaches
for intermittentlypowered battery-free sensing systems.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00874" title="Abstract">arXiv:2402.00874</a> [<a href="/pdf/2402.00874" title="Download PDF">pdf</a>, <a href="/ps/2402.00874" title="Download PostScript">ps</a>, <a href="/format/2402.00874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> dRG-MEC: Decentralized Reinforced Green Offloading for MEC-enabled Cloud  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aftab%2C+A">Asad Aftab</a>, 
<a href="/search/cs?searchtype=author&query=Rehman%2C+S">Semeen Rehman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Multi-access-Mobile Edge Computing (MEC) is a promising solution for
computationally demanding rigorous applications, that can meet 6G network
service requirements. However, edge servers incur high computation costs during
task processing. In this paper, we proposed a technique to minimize the total
computation and communication overhead for optimal resource utilization with
joint computational offloading that enables a green environment. Our
optimization problem is NP-hard; thus, we proposed a decentralized
Reinforcement Learning (dRL) approach where we eliminate the problem of
dimensionality and over-estimation of the value functions. Compared to baseline
schemes our technique achieves a 37.03% reduction in total system costs.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00875" title="Abstract">arXiv:2402.00875</a> [<a href="/pdf/2402.00875" title="Download PDF">pdf</a>, <a href="/format/2402.00875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum-Cost Sensor Channel Selection For Wearable Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sah%2C+R+K">Ramesh Kumar Sah</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Sensor systems are constrained by design and finding top sensor channel(s)
for a given computational task is an important but hard problem. We define an
optimization framework and mathematically formulate the minimum-cost channel
selection problem. We then propose two novel algorithms of varying scope and
complexity to solve the optimization problem. Branch and bound channel
selection finds a globally optimal channel subset and the greedy channel
selection finds the best intermediate subset based on the value of a score
function. Proposed channel selection algorithms are conditioned with
performance as well as the cost of the channel subset. We evaluate both
algorithms on two publicly available time series datasets of human activity
recognition and mental task detection. Branch and bound channel selection
achieved a cost saving of up to 94.8% and the greedy search reduced the cost by
89.6% while maintaining performance thresholds.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00876" title="Abstract">arXiv:2402.00876</a> [<a href="/pdf/2402.00876" title="Download PDF">pdf</a>, <a href="/format/2402.00876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alamouti%2C+S">Siavash Alamouti</a>, 
<a href="/search/cs?searchtype=author&query=Arjomandi%2C+F">Fay Arjomandi</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+M">Michel Burger</a>, 
<a href="/search/cs?searchtype=author&query=Altakouri%2C+D+B">Dr. Bashar Altakouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As we transition from the mobile internet to the 'Cognitive Internet,' a
significant shift occurs in how we engage with technology and intelligence. We
contend that the Cognitive Internet goes beyond the Cognitive Internet of
Things (Cognitive IoT), enabling connected objects to independently acquire
knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the
Cognitive Internet integrates collaborative intelligence throughout the
network, blending the cognitive IoT realm with system-wide collaboration and
human intelligence. This integrated intelligence facilitates interactions
between devices, services, entities, and individuals across diverse domains
while preserving decision-making autonomy and accommodating various identities.
<br />The paper delves into the foundational elements, distinct characteristics,
benefits, and industrial impact of the 'Cognitive Internet' paradigm. It
highlights the importance of adaptable AI infrastructures and hybrid edge cloud
(HEC) platforms in enabling this shift. This evolution brings forth cognitive
services, a Knowledge as a Service (KaaS) economy, enhanced decision-making
autonomy, sustainable digital progress, advancements in data management,
processing techniques, and a stronger emphasis on privacy. In essence, this
paper serves as a crucial resource for understanding and leveraging the
transformative potential of HEC for Cognitive Internet. Supported by case
studies, forward-looking perspectives, and real-world applications, it provides
comprehensive insights into this emerging paradigm.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00877" title="Abstract">arXiv:2402.00877</a> [<a href="/pdf/2402.00877" title="Download PDF">pdf</a>, <a href="/ps/2402.00877" title="Download PostScript">ps</a>, <a href="/format/2402.00877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Recent Energy Harvesting Methods for Increasing Battery  Efficiency in WBANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yektamoghadam%2C+H">Hossein Yektamoghadam</a>, 
<a href="/search/cs?searchtype=author&query=Nikoofard%2C+A">Amirhossein Nikoofard</a>, 
<a href="/search/cs?searchtype=author&query=Doust%2C+F+P">Fatemeh Pourhanifeh Doust</a>, 
<a href="/search/cs?searchtype=author&query=Delrobaei%2C+M">Mehdi Delrobaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Today, technology development has led humans to employ wearable and
implantable devices for biomedical applications. An important research issue in
this field is the wireless body area networks (WBANs), which focus on such
devices. In WBAN, using batteries as the only energy supply is a significant
challenge, especially in medical applications. Charging the batteries is a
problem for patients who use WBAN. Replacing the battery is not very difficult
for wearable devices, but implantable devices have different conditions. The
use of batteries in implantable devices has many problems, including pain and
costs due to surgery, mental stress, and lack of comfort. Batteries' life
depends on their type, operation, the patient's medical condition, and other
factors. This paper reviews recent energy harvesting methods for battery
recharge in WBAN's sensors. Moreover, we provide future research directions on
energy harvesting methods in WBANs. Therefore, active research fields such as
reinforcement learning (RL) and distributed optimization in WBAN applications
were investigated. We strongly believe that these insights will aid in studying
and developing a new generation of rechargeable sensors in WBANs for fellow
researchers.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00878" title="Abstract">arXiv:2402.00878</a> [<a href="/pdf/2402.00878" title="Download PDF">pdf</a>, <a href="/format/2402.00878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radio Map Estimation -- An Open Dataset with Directive Transmitter  Antennas and Initial Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaensch%2C+F">Fabian Jaensch</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+B">Beg&#xfc;m Demir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 121 figures, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Over the last years, several works have explored the application of deep
learning algorithms to determine the large-scale signal fading (also referred
to as ``path loss'') between transmitter and receiver pairs in urban
communication networks. The central idea is to replace costly measurement
campaigns, inaccurate statistical models or computationally expensive
ray-tracing simulations by machine learning models which, once trained, produce
accurate predictions almost instantly. Although the topic has attracted
attention from many researchers, there are few open benchmark datasets and
codebases that would allow everyone to test and compare the developed methods
and algorithms. We take a step towards filling this gap by releasing a publicly
available dataset of simulated path loss radio maps together with realistic
city maps from real-world locations and aerial images from open datasources.
Initial experiments regarding model architectures, input feature design and
estimation of radio maps from aerial images are presented and the code is made
available.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00879" title="Abstract">arXiv:2402.00879</a> [<a href="/pdf/2402.00879" title="Download PDF">pdf</a>, <a href="/format/2402.00879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Representation Learning for Contention and Interference Management  in Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zhouyou Gu</a>, 
<a href="/search/cs?searchtype=author&query=Vucetic%2C+B">Branka Vucetic</a>, 
<a href="/search/cs?searchtype=author&query=Chikkam%2C+K">Kishore Chikkam</a>, 
<a href="/search/cs?searchtype=author&query=Aliberti%2C+P">Pasquale Aliberti</a>, 
<a href="/search/cs?searchtype=author&query=Hardjawana%2C+W">Wibowo Hardjawana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted in the IEEE/ACM Transactions on Networking. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Restricted access window (RAW) in Wi-Fi 802.11ah networks manages contention
and interference by grouping users and allocating periodic time slots for each
group's transmissions. We will find the optimal user grouping decisions in RAW
to maximize the network's worst-case user throughput. We review existing user
grouping approaches and highlight their performance limitations in the above
problem. We propose formulating user grouping as a graph construction problem
where vertices represent users and edge weights indicate the contention and
interference. This formulation leverages the graph's max cut to group users and
optimizes edge weights to construct the optimal graph whose max cut yields the
optimal grouping decisions. To achieve this optimal graph construction, we
design an actor-critic graph representation learning (AC-GRL) algorithm.
Specifically, the actor neural network (NN) is trained to estimate the optimal
graph's edge weights using path losses between users and access points. A graph
cut procedure uses semidefinite programming to solve the max cut efficiently
and return the grouping decisions for the given weights. The critic NN
approximates user throughput achieved by the above-returned decisions and is
used to improve the actor. Additionally, we present an architecture that uses
the online-measured throughput and path losses to fine-tune the decisions in
response to changes in user populations and their locations. Simulations show
that our methods achieve $30\%\sim80\%$ higher worst-case user throughput than
the existing approaches and that the proposed architecture can further improve
the worst-case user throughput by $5\%\sim30\%$ while ensuring timely updates
of grouping decisions.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00881" title="Abstract">arXiv:2402.00881</a> [<a href="/pdf/2402.00881" title="Download PDF">pdf</a>, <a href="/format/2402.00881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Interplay of Artificial Intelligence and Space-Air-Ground  Integrated Networks: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakambekova%2C+A">Adilya Bakambekova</a>, 
<a href="/search/cs?searchtype=author&query=Kouzayha%2C+N">Nour Kouzayha</a>, 
<a href="/search/cs?searchtype=author&query=Al-Naffouri%2C+T">Tareq Al-Naffouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and
aerial networks with terrestrial wireless systems, are vital enablers of the
emerging sixth-generation (6G) wireless networks. Besides bringing significant
benefits to various applications and services, SAGINs are envisioned to extend
high-speed broadband coverage to remote areas, such as small towns or mining
sites, or areas where terrestrial infrastructure cannot reach, such as
airplanes or maritime use cases. However, due to the limited power and storage
resources, as well as other constraints introduced by the design of terrestrial
networks, SAGINs must be intelligently configured and controlled to satisfy the
envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another
critical enabler of 6G. Due to massive amounts of available data, AI has been
leveraged to address pressing challenges of current and future wireless
networks. By adding AI and facilitating the decision-making and prediction
procedures, SAGINs can effectively adapt to their surrounding environment, thus
enhancing the performance of various metrics. In this work, we aim to
investigate the interplay of AI and SAGINs by providing a holistic overview of
state-of-the-art research in AI-enabled SAGINs. Specifically, we present a
comprehensive overview of some potential applications of AI in SAGINs. We also
cover open issues in employing AI and detail the contributions of SAGINs in the
development of AI. Finally, we highlight some limitations of the existing
research works and outline potential future research directions.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00888" title="Abstract">arXiv:2402.00888</a> [<a href="/pdf/2402.00888" title="Download PDF">pdf</a>, <a href="/format/2402.00888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security and Privacy Challenges of Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+B+C">Badhan Chandra Das</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+M+H">M. Hadi Amini</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanzhao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated extraordinary capabilities and
contributed to multiple fields, such as generating and summarizing text,
language translation, and question-answering. Nowadays, LLM is becoming a very
popular tool in computerized language processing tasks, with the capability to
analyze complicated linguistic patterns and provide relevant and appropriate
responses depending on the context. While offering significant advantages,
these models are also vulnerable to security and privacy attacks, such as
jailbreaking attacks, data poisoning attacks, and Personally Identifiable
Information (PII) leakage attacks. This survey provides a thorough review of
the security and privacy challenges of LLMs for both training data and users,
along with the application-based risks in various domains, such as
transportation, education, and healthcare. We assess the extent of LLM
vulnerabilities, investigate emerging security and privacy attacks for LLMs,
and review the potential defense mechanisms. Additionally, the survey outlines
existing research gaps in this domain and highlights future research
directions.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00890" title="Abstract">arXiv:2402.00890</a> [<a href="/pdf/2402.00890" title="Download PDF">pdf</a>, <a href="/format/2402.00890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Large Language Models to Translate RFC Protocol Specifications  to CPSA Definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duclos%2C+M">Martin Duclos</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+I+A">Ivan A. Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+K">Kaneesha Moore</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sudip Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Zieglar%2C+E">Edward Zieglar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Software Engineering (cs.SE)

</div>
<p class="mathjax">This paper proposes the use of Large Language Models (LLMs) for translating
Request for Comments (RFC) protocol specifications into a format compatible
with the Cryptographic Protocol Shapes Analyzer (CPSA). This novel approach
aims to reduce the complexities and efforts involved in protocol analysis, by
offering an automated method for translating protocol specifications into
structured models suitable for CPSA. In this paper we discuss the
implementation of an RFC Protocol Translator, its impact on enhancing the
accessibility of formal methods analysis, and its potential for improving the
security of internet protocols.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00891" title="Abstract">arXiv:2402.00891</a> [<a href="/pdf/2402.00891" title="Download PDF">pdf</a>, <a href="/format/2402.00891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models in Cybersecurity: State-of-the-Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motlagh%2C+F+N">Farzad Nourmohammadzadeh Motlagh</a>, 
<a href="/search/cs?searchtype=author&query=Hajizadeh%2C+M">Mehrdad Hajizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Majd%2C+M">Mehryar Majd</a>, 
<a href="/search/cs?searchtype=author&query=Najafi%2C+P">Pejman Najafi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Feng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Meinel%2C+C">Christoph Meinel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rise of Large Language Models (LLMs) has revolutionized our comprehension
of intelligence bringing us closer to Artificial Intelligence. Since their
introduction, researchers have actively explored the applications of LLMs
across diverse fields, significantly elevating capabilities. Cybersecurity,
traditionally resistant to data-driven solutions and slow to embrace machine
learning, stands out as a domain. This study examines the existing literature,
providing a thorough characterization of both defensive and adversarial
applications of LLMs within the realm of cybersecurity. Our review not only
surveys and categorizes the current landscape but also identifies critical
research gaps. By evaluating both offensive and defensive applications, we aim
to provide a holistic understanding of the potential risks and opportunities
associated with LLM-driven cybersecurity.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00892" title="Abstract">arXiv:2402.00892</a> [<a href="/pdf/2402.00892" title="Download PDF">pdf</a>, <a href="/format/2402.00892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EVA-GAN: Enhanced Various Audio Generation via Scalable Generative  Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+S">Shijia Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+S">Shiyi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zachariah%2C+A+G">Arun George Zachariah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The advent of Large Models marks a new era in machine learning, significantly
outperforming smaller models by leveraging vast datasets to capture and
synthesize complex patterns. Despite these advancements, the exploration into
scaling, especially in the audio generation domain, remains limited, with
previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and
suffering from both spectral discontinuities and blurriness in the
high-frequency domain, alongside a lack of robustness against out-of-domain
data. These limitations restrict the applicability of models to diverse use
cases, including music and singing generation. Our work introduces Enhanced
Various Audio Generation via Scalable Generative Adversarial Networks
(EVA-GAN), yields significant improvements over previous state-of-the-art in
spectral and high-frequency reconstruction and robustness in out-of-domain data
performance, enabling the generation of HiFi audios by employing an extensive
dataset of 36,000 hours of 44.1kHz audio, a context-aware module, a
Human-In-The-Loop artifact measurement toolkit, and expands the model to
approximately 200 million parameters. Demonstrations of our work are available
at https://double-blind-eva-gan.cc.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00893" title="Abstract">arXiv:2402.00893</a> [<a href="/pdf/2402.00893" title="Download PDF">pdf</a>, <a href="/format/2402.00893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoDE: A Mixture-of-Experts Model with Mutual Distillation among the  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhitian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinger Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+C">Chenyi Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qitao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhining Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The application of mixture-of-experts (MoE) is gaining popularity due to its
ability to improve model's performance. In an MoE structure, the gate layer
plays a significant role in distinguishing and routing input features to
different experts. This enables each expert to specialize in processing their
corresponding sub-tasks. However, the gate's routing mechanism also gives rise
to narrow vision: the individual MoE's expert fails to use more samples in
learning the allocated sub-task, which in turn limits the MoE to further
improve its generalization ability. To effectively address this, we propose a
method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual
distillation among experts to enable each expert to pick up more features
learned by other experts and gain more accurate perceptions on their original
allocated sub-tasks. We conduct plenty experiments including tabular, NLP and
CV datasets, which shows MoDE's effectiveness, universality and robustness.
Furthermore, we develop a parallel study through innovatively constructing
"expert probing", to experimentally prove why MoDE works: moderate distilling
knowledge can improve each individual expert's test performances on their
assigned tasks, leading to MoE's overall performance improvement.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00896" title="Abstract">arXiv:2402.00896</a> [<a href="/pdf/2402.00896" title="Download PDF">pdf</a>, <a href="/format/2402.00896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy and Security Implications of Cloud-Based AI Services : A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luqman%2C+A">Alka Luqman</a>, 
<a href="/search/cs?searchtype=author&query=Mahesh%2C+R">Riya Mahesh</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+A">Anupam Chattopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper details the privacy and security landscape in today's cloud
ecosystem and identifies that there is a gap in addressing the risks introduced
by machine learning models. As machine learning algorithms continue to evolve
and find applications across diverse domains, the need to categorize and
quantify privacy and security risks becomes increasingly critical. With the
emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML
models) are deployed on the cloud by model providers and used by model
consumers. We first survey the AIaaS landscape to document the various kinds of
liabilities that ML models, especially Deep Neural Networks pose and then
introduce a taxonomy to bridge this gap by holistically examining the risks
that creators and consumers of ML models are exposed to and their known
defences till date. Such a structured approach will be beneficial for ML model
providers to create robust solutions. Likewise, ML model consumers will find it
valuable to evaluate such solutions and understand the implications of their
engagement with such services. The proposed taxonomies provide a foundational
basis for solutions in private, secure and robust ML, paving the way for more
transparent and resilient AI systems.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00897" title="Abstract">arXiv:2402.00897</a> [<a href="/pdf/2402.00897" title="Download PDF">pdf</a>, <a href="/ps/2402.00897" title="Download PostScript">ps</a>, <a href="/format/2402.00897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Screening method for early dementia using sound objects as voice  biomarkers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pluta%2C+A">Adam Pluta</a>, 
<a href="/search/cs?searchtype=author&query=Pioch%2C+Z">Zbigniew Pioch</a>, 
<a href="/search/cs?searchtype=author&query=Kardach%2C+J">J&#x119;drzej Kardach</a>, 
<a href="/search/cs?searchtype=author&query=Zio%C5%82o%2C+P">Piotr Zio&#x142;o</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C4%99cicki%2C+T">Tomasz Kr&#x119;cicki</a>, 
<a href="/search/cs?searchtype=author&query=Trypka%2C+E">El&#x17c;bieta Trypka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Introduction: We present a screening method for early dementia using features
based on sound objects as voice biomarkers.
<br />Methods: The final dataset used for machine learning models consisted of 266
observations, with a distribution of 186 healthy individuals, 46 diagnosed with
Alzheimer's, and 34 with MCI. This method is based on six-second recordings of
the sustained vowel /a/ spoken by the subject. The main original contribution
of this work is the use of carefully crafted features based on sound objects.
This approach allows one to first represent the sound spectrum in a more
accurate way than the standard spectrum, and then build interpretable features
containing relevant information about subjects' control over their voice.
<br />Results: ROC AUC obtained in this work for distinguishing healthy subjects
from those with MCI was 0.85, while accuracy was 0.76. For distinguishing
between healthy subjects and those with either MCI or Alzheimer's the results
were 0.84, 0.77, respectively.
<br />Conclusion: The use of features based on sound objects enables screening for
early dementia even on very short recordings of language-independent voice
samples.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00898" title="Abstract">arXiv:2402.00898</a> [<a href="/pdf/2402.00898" title="Download PDF">pdf</a>, <a href="/format/2402.00898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Early Categorization of Prompt Injection Attacks on Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rossi%2C+S">Sippo Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+A+M">Alisia Marianne Michel</a>, 
<a href="/search/cs?searchtype=author&query=Mukkamala%2C+R+R">Raghava Rao Mukkamala</a>, 
<a href="/search/cs?searchtype=author&query=Thatcher%2C+J+B">Jason Bennett Thatcher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages double spacing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models and AI chatbots have been at the forefront of
democratizing artificial intelligence. However, the releases of ChatGPT and
other similar tools have been followed by growing concerns regarding the
difficulty of controlling large language models and their outputs. Currently,
we are witnessing a cat-and-mouse game where users attempt to misuse the models
with a novel attack called prompt injections. In contrast, the developers
attempt to discover the vulnerabilities and block the attacks simultaneously.
In this paper, we provide an overview of these emergent threats and present a
categorization of prompt injections, which can guide future research on prompt
injections and act as a checklist of vulnerabilities in the development of LLM
interfaces. Moreover, based on previous literature and our own empirical
research, we discuss the implications of prompt injections to LLM end users,
developers, and researchers.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00899" title="Abstract">arXiv:2402.00899</a> [<a href="/pdf/2402.00899" title="Download PDF">pdf</a>, <a href="/format/2402.00899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Learners for Correction of AI Errors with Provable  Performance Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyukin%2C+I+Y">Ivan Y. Tyukin</a>, 
<a href="/search/cs?searchtype=author&query=Tyukina%2C+T">Tatiana Tyukina</a>, 
<a href="/search/cs?searchtype=author&query=van+Helden%2C+D">Daniel van Helden</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zedong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mirkes%2C+E+M">Evgeny M. Mirkes</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+O+J">Oliver J. Sutton</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qinghua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gorban%2C+A+N">Alexander N. Gorban</a>, 
<a href="/search/cs?searchtype=author&query=Allison%2C+P">Penelope Allison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a new methodology for handling AI errors by introducing weakly
supervised AI error correctors with a priori performance guarantees. These AI
correctors are auxiliary maps whose role is to moderate the decisions of some
previously constructed underlying classifier by either approving or rejecting
its decisions. The rejection of a decision can be used as a signal to suggest
abstaining from making a decision. A key technical focus of the work is in
providing performance guarantees for these new AI correctors through bounds on
the probabilities of incorrect decisions. These bounds are distribution
agnostic and do not rely on assumptions on the data dimension. Our empirical
example illustrates how the framework can be applied to improve the performance
of an image classifier in a challenging real-world task where training data are
scarce.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00901" title="Abstract">arXiv:2402.00901</a> [<a href="/pdf/2402.00901" title="Download PDF">pdf</a>, <a href="/ps/2402.00901" title="Download PostScript">ps</a>, <a href="/format/2402.00901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real Sparks of Artificial Intelligence and the Importance of Inner  Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grzankowski%2C+A">Alex Grzankowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The present paper looks at one of the most thorough articles on the
intelligence of GPT, research conducted by engineers at Microsoft. Although
there is a great deal of value in their work, I will argue that, for familiar
philosophical reasons, their methodology, !Blackbox Interpretability"#is
wrongheaded. But there is a better way. There is an exciting and emerging
discipline of !Inner Interpretability"#(and specifically Mechanistic
Interpretability) that aims to uncover the internal activations and weights of
models in order to understand what they represent and the algorithms they
implement. In my view, a crucial mistake in Black-box Interpretability is the
failure to appreciate that how processes are carried out matters when it comes
to intelligence and understanding. I can#t pretend to have a full story that
provides both necessary and sufficient conditions for being intelligent, but I
do think that Inner Interpretability dovetails nicely with plausible
philosophical views of what intelligence requires. So the conclusion is modest,
but the important point in my view is seeing how to get the research on the
right track. Towards the end of the paper, I will show how some of the
philosophical concepts can be used to further refine how Inner Interpretability
is approached, so the paper helps draw out a profitable, future two-way
exchange between Philosophers and Computer Scientists.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00904" title="Abstract">arXiv:2402.00904</a> [<a href="/pdf/2402.00904" title="Download PDF">pdf</a>, <a href="/ps/2402.00904" title="Download PostScript">ps</a>, <a href="/format/2402.00904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Domain Adaptation: Challenges, Progress and Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boshen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Fangda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bingbing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As graph representation learning often suffers from label scarcity problems
in real-world applications, researchers have proposed graph domain adaptation
(GDA) as an effective knowledge-transfer paradigm across graphs. In particular,
to enhance model performance on target graphs with specific tasks, GDA
introduces a bunch of task-related graphs as source graphs and adapts the
knowledge learnt from source graphs to the target graphs. Since GDA combines
the advantages of graph representation learning and domain adaptation, it has
become a promising direction of transfer learning on graphs and has attracted
an increasing amount of research interest in recent years. In this paper, we
comprehensively overview the studies of GDA and present a detailed survey of
recent advances. Specifically, we outline the research status and challenges,
propose a taxonomy, introduce the details of representative works, and discuss
the prospects. To the best of our knowledge, this paper is the first survey for
graph domain adaptation. A detailed paper list is available at
https://github.com/Skyorca/Awesome-Graph-Domain-Adaptation-Papers.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00905" title="Abstract">arXiv:2402.00905</a> [<a href="/pdf/2402.00905" title="Download PDF">pdf</a>, <a href="/format/2402.00905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-3.5 for Code Review Automation: How Do Few-Shot Learning, Prompt  Design, and Model Fine-Tuning Impact Their Performance?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pornprasit%2C+C">Chanathip Pornprasit</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages. Submit to IST journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Recently, several large language models (LLMs)-the large pre-trained models
based on the transformer architecture-were proposed. Prior studies in the
natural language processing field and software engineering field conducted
experiments focusing on different approaches to leveraging LLMs for downstream
tasks. However, the existing literature still lacks the study of different
approaches to leveraging GPT-3.5 (e.g., prompt engineering, few-shot learning
and model fine-tuning) for the code review automation task (i.e., automatically
generating improved code from submitted code). Thus, little is known about how
GPT-3.5 should be leveraged for this task. To fill this knowledge gap, we set
out to investigate the impact of few-shot learning, prompt design (i.e., using
a persona pattern), and model fine-tuning on GPT-3.5 for the code review
automation task. Through the experimental study of the three code review
automation datasets, we find that (1) when few-shot learning is performed,
GPT-3.5 achieves at least 46.38% higher Exact Match and at least 3.97% higher
CodeBLEU than GPT-3.5 that zero-shot learning is performed, (2) when persona is
included in input prompts to generate improved code, GPT-3.5 achieves at least
1.02% lower Exact Match and 0.15% lower CodeBLEU than when persona is not
included in input prompts, (3) fine-tuned GPT-3.5 achieves at least 9.74%
higher Exact Match and 0.12% higher CodeBLEU than GPT-3.5 that zero-shot and
few-shot learning is performed, and (4) fine-tuned GPT-3.5 achieves at least
11.48% higher Exact Match than the existing code review automation approaches.
Based on our experiment results, we recommend that when using GPT-3.5 for code
review automation (1) few-shot learning should be performed rather than
zero-shot learning, (2) persona should not be included when constructing
prompts, and (3) GPT-3.5 should be fine-tuned by using a small training
dataset.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00906" title="Abstract">arXiv:2402.00906</a> [<a href="/pdf/2402.00906" title="Download PDF">pdf</a>, <a href="/format/2402.00906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic  Architectures against Model Inversion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poursiami%2C+H">Hamed Poursiami</a>, 
<a href="/search/cs?searchtype=author&query=Alouani%2C+I">Ihsen Alouani</a>, 
<a href="/search/cs?searchtype=author&query=Parsa%2C+M">Maryam Parsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">With the mainstream integration of machine learning into security-sensitive
domains such as healthcare and finance, concerns about data privacy have
intensified. Conventional artificial neural networks (ANNs) have been found
vulnerable to several attacks that can leak sensitive data. Particularly, model
inversion (MI) attacks enable the reconstruction of data samples that have been
used to train the model. Neuromorphic architectures have emerged as a paradigm
shift in neural computing, enabling asynchronous and energy-efficient
computation. However, little to no existing work has investigated the privacy
of neuromorphic architectures against model inversion. Our study is motivated
by the intuition that the non-differentiable aspect of spiking neural networks
(SNNs) might result in inherent privacy-preserving properties, especially
against gradient-based attacks. To investigate this hypothesis, we propose a
thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we
develop novel inversion attack strategies that are comprehensively designed to
target SNNs, offering a comparative analysis with their conventional ANN
counterparts. Our experiments, conducted on diverse event-based and static
datasets, demonstrate the effectiveness of the proposed attack strategies and
therefore questions the assumption of inherent privacy-preserving in
neuromorphic architectures.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00907" title="Abstract">arXiv:2402.00907</a> [<a href="/pdf/2402.00907" title="Download PDF">pdf</a>, <a href="/format/2402.00907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlphaRank: An Artificial Intelligence Approach for Ranking and Selection  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L+J">L. Jeff Hong</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yijie Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">We introduce AlphaRank, an artificial intelligence approach to address the
fixed-budget ranking and selection (R&amp;S) problems. We formulate the sequential
sampling decision as a Markov decision process and propose a Monte Carlo
simulation-based rollout policy that utilizes classic R&amp;S procedures as base
policies for efficiently learning the value function of stochastic dynamic
programming. We accelerate online sample-allocation by using deep reinforcement
learning to pre-train a neural network model offline based on a given prior. We
also propose a parallelizable computing framework for large-scale problems,
effectively combining "divide and conquer" and "recursion" for enhanced
scalability and efficiency. Numerical experiments demonstrate that the
performance of AlphaRank is significantly improved over the base policies,
which could be attributed to AlphaRank's superior capability on the trade-off
among mean, variance, and induced correlation overlooked by many existing
policies.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00909" title="Abstract">arXiv:2402.00909</a> [<a href="/pdf/2402.00909" title="Download PDF">pdf</a>, <a href="/format/2402.00909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing GradCAM for Embedding Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bachhawat%2C+M">Mudit Bachhawat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visualizing CNN is an important part in building trust and explaining model's
prediction. Methods like CAM and GradCAM have been really successful in
localizing area of the image responsible for the output but are only limited to
classification models. In this paper, we present a new method EmbeddingCAM,
which generalizes the Grad-CAM for embedding networks. We show that for
classification networks, EmbeddingCAM reduces to GradCAM. We show the
effectiveness of our method on CUB-200-2011 dataset and also present
quantitative and qualitative analysis on the dataset.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00910" title="Abstract">arXiv:2402.00910</a> [<a href="/pdf/2402.00910" title="Download PDF">pdf</a>, <a href="/format/2402.00910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radwan%2C+A">Ahmed Radwan</a>, 
<a href="/search/cs?searchtype=author&query=Zaafarani%2C+L">Layan Zaafarani</a>, 
<a href="/search/cs?searchtype=author&query=Abudawood%2C+J">Jetana Abudawood</a>, 
<a href="/search/cs?searchtype=author&query=AlZahrani%2C+F">Faisal AlZahrani</a>, 
<a href="/search/cs?searchtype=author&query=Fourat%2C+F">Fares Fourat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Addressing biases in AI models is crucial for ensuring fair and accurate
predictions. However, obtaining large, unbiased datasets for training can be
challenging. This paper proposes a comprehensive approach using multiple
methods to remove bias in AI models, with only a small dataset and a
potentially biased pretrained model. We train multiple models with the
counter-bias of the pre-trained model through data splitting, local training,
and regularized fine-tuning, gaining potentially counter-biased models. Then,
we employ ensemble learning for all models to reach unbiased predictions. To
further accelerate the inference time of our ensemble model, we conclude our
solution with knowledge distillation that results in a single unbiased neural
network. We demonstrate the effectiveness of our approach through experiments
on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work
contributes to the ongoing effort to create more unbiased and reliable AI
models, even with limited data availability.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00912" title="Abstract">arXiv:2402.00912</a> [<a href="/pdf/2402.00912" title="Download PDF">pdf</a>, <a href="/format/2402.00912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can we Constrain Concept Bottleneck Models to Learn Semantically  Meaningful Input Features?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furby%2C+J">Jack Furby</a>, 
<a href="/search/cs?searchtype=author&query=Cunnington%2C+D">Daniel Cunnington</a>, 
<a href="/search/cs?searchtype=author&query=Braines%2C+D">Dave Braines</a>, 
<a href="/search/cs?searchtype=author&query=Preece%2C+A">Alun Preece</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper: 7 pages, 8 figures, Appendix: 15 pages, 22 figures. This paper is a preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Concept Bottleneck Models (CBMs) are considered inherently interpretable
because they first predict a set of human-defined concepts before using these
concepts to predict the output of a downstream task. For inherent
interpretability to be fully realised, and ensure trust in a model's output, we
need to guarantee concepts are predicted based on semantically mapped input
features. For example, one might expect the pixels representing a broken bone
in an image to be used for the prediction of a fracture. However, current
literature indicates this is not the case, as concept predictions are often
mapped to irrelevant input features. We hypothesise that this occurs when
concept annotations are inaccurate or how input features should relate to
concepts is unclear. In general, the effect of dataset labelling on concept
representations in CBMs remains an understudied area. Therefore, in this paper,
we examine how CBMs learn concepts from datasets with fine-grained concept
annotations. We demonstrate that CBMs can learn concept representations with
semantic mapping to input features by removing problematic concept
correlations, such as two concepts always appearing together. To support our
evaluation, we introduce a new synthetic image dataset based on a playing cards
domain, which we hope will serve as a benchmark for future CBM research. For
validation, we provide empirical evidence on a real-world dataset of chest
X-rays, to demonstrate semantically meaningful concepts can be learned in
real-world applications.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00913" title="Abstract">arXiv:2402.00913</a> [<a href="/pdf/2402.00913" title="Download PDF">pdf</a>, <a href="/format/2402.00913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Institutional Platform for Secure Self-Service Large Language Model  Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bumgardner%2C+V+K+C">V. K. Cody Bumgardner</a>, 
<a href="/search/cs?searchtype=author&query=Klusty%2C+M+A">Mitchell A. Klusty</a>, 
<a href="/search/cs?searchtype=author&query=Logan%2C+W+V">W. Vaiden Logan</a>, 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+S+E">Samuel E. Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Hickey%2C+C">Caylin Hickey</a>, 
<a href="/search/cs?searchtype=author&query=Talbert%2C+J">Jeff Talbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages 11 figures, 5 listings, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper introduces a user-friendly platform developed by the University of
Kentucky Center for Applied AI, designed to make large, customized language
models (LLMs) more accessible. By capitalizing on recent advancements in
multi-LoRA inference, the system efficiently accommodates custom adapters for a
diverse range of users and projects. The paper outlines the system's
architecture and key features, encompassing dataset curation, model training,
secure inference, and text-based feature extraction.
<br />We illustrate the establishment of a tenant-aware computational network using
agent-based methods, securely utilizing islands of isolated resources as a
unified system. The platform strives to deliver secure LLM services,
emphasizing process and data isolation, end-to-end encryption, and role-based
resource authentication. This contribution aligns with the overarching goal of
enabling simplified access to cutting-edge AI models and technology in support
of scientific discovery.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00918" title="Abstract">arXiv:2402.00918</a> [<a href="/pdf/2402.00918" title="Download PDF">pdf</a>, <a href="/format/2402.00918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUSTAN: Multi-scale Temporal Context as Attention for Robust Video  Foreground Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokala%2C+P+K">Praveen Kumar Pokala</a>, 
<a href="/search/cs?searchtype=author&query=Patibandla%2C+J+S+K">Jaya Sai Kiran Patibandla</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+N+K">Naveen Kumar Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Pailla%2C+B+R">Balakrishna Reddy Pailla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Video foreground segmentation (VFS) is an important computer vision task
wherein one aims to segment the objects under motion from the background. Most
of the current methods are image-based, i.e., rely only on spatial cues while
ignoring motion cues. Therefore, they tend to overfit the training data and
don't generalize well to out-of-domain (OOD) distribution. To solve the above
problem, prior works exploited several cues such as optical flow, background
subtraction mask, etc. However, having a video data with annotations like
optical flow is a challenging task. In this paper, we utilize the temporal
information and the spatial cues from the video data to improve OOD
performance. However, the challenge lies in how we model the temporal
information given the video data in an interpretable way creates a very
noticeable difference. We therefore devise a strategy that integrates the
temporal context of the video in the development of VFS. Our approach give rise
to deep learning architectures, namely MUSTAN1 and MUSTAN2 and they are based
on the idea of multi-scale temporal context as an attention, i.e., aids our
models to learn better representations that are beneficial for VFS. Further, we
introduce a new video dataset, namely Indoor Surveillance Dataset (ISD) for
VFS. It has multiple annotations on a frame level such as foreground binary
mask, depth map, and instance semantic annotations. Therefore, ISD can benefit
other computer vision tasks. We validate the efficacy of our architectures and
compare the performance with baselines. We demonstrate that proposed methods
significantly outperform the benchmark methods on OOD. In addition, the
performance of MUSTAN2 is significantly improved on certain video categories on
OOD data due to ISD.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00920" title="Abstract">arXiv:2402.00920</a> [<a href="/pdf/2402.00920" title="Download PDF">pdf</a>, <a href="/ps/2402.00920" title="Download PostScript">ps</a>, <a href="/format/2402.00920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Approaches for Network Traffic Classification in the  Internet of Things (IoT): A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalwar%2C+J+H">Jawad Hussain Kalwar</a>, 
<a href="/search/cs?searchtype=author&query=Bhatti%2C+S">Sania Bhatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The Internet of Things (IoT) has witnessed unprecedented growth, resulting in
a massive influx of diverse network traffic from interconnected devices.
Effectively classifying this network traffic is crucial for optimizing resource
allocation, enhancing security measures, and ensuring efficient network
management in IoT systems. Deep learning has emerged as a powerful technique
for network traffic classification due to its ability to automatically learn
complex patterns and representations from raw data. This survey paper aims to
provide a comprehensive overview of the existing deep learning approaches
employed in network traffic classification specifically tailored for IoT
environments. By systematically analyzing and categorizing the latest research
contributions in this domain, we explore the strengths and limitations of
various deep learning models in handling the unique challenges posed by IoT
network traffic. Through this survey, we aim to offer researchers and
practitioners valuable insights, identify research gaps, and provide directions
for future research to further enhance the effectiveness and efficiency of deep
learning-based network traffic classification in IoT.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00921" title="Abstract">arXiv:2402.00921</a> [<a href="/pdf/2402.00921" title="Download PDF">pdf</a>, <a href="/ps/2402.00921" title="Download PostScript">ps</a>, <a href="/format/2402.00921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Allocation of Indivisible Items with a Common Preference Graph:  Minimizing Total Dissatisfaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiarelli%2C+N">Nina Chiarelli</a>, 
<a href="/search/cs?searchtype=author&query=Dallard%2C+C">Cl&#xe9;ment Dallard</a>, 
<a href="/search/cs?searchtype=author&query=Darmann%2C+A">Andreas Darmann</a>, 
<a href="/search/cs?searchtype=author&query=Lendl%2C+S">Stefan Lendl</a>, 
<a href="/search/cs?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Mur%C5%A1i%C4%8D%2C+P">Peter Mur&#x161;i&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Pferschy%2C+U">Ulrich Pferschy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Allocating indivisible items among a set of agents is a frequently studied
discrete optimization problem. In the setting considered in this work, the
agents' preferences over the items are assumed to be identical. We consider a
very recent measure for the overall quality of an allocation which does not
rely on numerical valuations of the items. Instead, it captures the agents'
opinion by a directed acyclic preference graph with vertices representing
items. An arc $(a,b)$ in such a graph means that the agents prefer item $a$
over item $b$. For a given allocation of items the dissatisfaction of an agent
is defined as the number of items which the agent does not receive and for
which no more preferred item is given to the agent. Our goal is to find an
efficient allocation of the items to the agents such that the total
dissatisfaction over all agents is minimized.
<br />We explore the dichotomy between NP-hard and polynomially solvable instances,
depending on properties of the underlying preference graph. While the problem
is NP-hard already for three agents even on very restricted graph classes, it
is polynomially solvable for two agents on general preference graphs. For an
arbitrary number of agents, we derive polynomial-time algorithms for relevant
restrictions of the underlying undirected graph. These are trees and, among the
graphs of treewidth two, series-parallel graphs and cactus graphs.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00922" title="Abstract">arXiv:2402.00922</a> [<a href="/pdf/2402.00922" title="Download PDF">pdf</a>, <a href="/ps/2402.00922" title="Download PostScript">ps</a>, <a href="/format/2402.00922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards post-quantum blockchain: A review on blockchain cryptography  resistant to quantum computing attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez-Carames%2C+T+M">Tiago M. Fernandez-Carames</a>, 
<a href="/search/cs?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version of an IEEE Access journal paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> T. M. Fernandez-Carames and P. Fraga-Lamas, "Towards Post-Quantum
  Blockchain: A Review on Blockchain Cryptography Resistant to Quantum
  Computing Attacks," in IEEE Access, vol. 8, pp. 21091-21116, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Blockchain and other Distributed Ledger Technologies (DLTs) have evolved
significantly in the last years and their use has been suggested for numerous
applications due to their ability to provide transparency, redundancy and
accountability. In the case of blockchain, such characteristics are provided
through public-key cryptography and hash functions. However, the fast progress
of quantum computing has opened the possibility of performing attacks based on
Grover's and Shor's algorithms in the near future. Such algorithms threaten
both public-key cryptography and hash functions, forcing to redesign
blockchains to make use of cryptosystems that withstand quantum attacks, thus
creating which are known as post-quantum, quantum-proof, quantum-safe or
quantum-resistant cryptosystems. For such a purpose, this article first studies
current state of the art on post-quantum cryptosystems and how they can be
applied to blockchains and DLTs. Moreover, the most relevant post-quantum
blockchain systems are studied, as well as their main challenges. Furthermore,
extensive comparisons are provided on the characteristics and performance of
the most promising post-quantum public-key encryption and digital signature
schemes for blockchains. Thus, this article seeks to provide a broad view and
useful guidelines on post-quantum blockchain security to future blockchain
researchers and developers.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00924" title="Abstract">arXiv:2402.00924</a> [<a href="/pdf/2402.00924" title="Download PDF">pdf</a>, <a href="/format/2402.00924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fragile Nature of Road Transportation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+L">Linghang Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Axenie%2C+C">Cristian Axenie</a>, 
<a href="/search/eess?searchtype=author&query=Grossi%2C+M">Margherita Grossi</a>, 
<a href="/search/eess?searchtype=author&query=Kouvelas%2C+A">Anastasios Kouvelas</a>, 
<a href="/search/eess?searchtype=author&query=Makridis%2C+M+A">Michail A. Makridis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Major cities worldwide experience problems with the performance of their road
transportation systems. The continuous increase in traffic demand presents a
substantial challenge to the optimal operation of urban road networks and the
efficiency of traffic control strategies. Although robust and resilient
transportation systems have been extensively researched over the past decades,
their performance under an ever-growing traffic demand can still be
questionable. The operation of transportation systems is widely believed to
display fragile property, i.e., the loss in performance increases exponentially
with the linearly increasing magnitude of disruptions, which undermines their
continuous operation. The risk engineering community is now embracing the novel
concept of (anti-)fragility, which enables systems to learn from historical
disruptions and exhibit improved performance as disruption levels reach
unprecedented magnitudes. In this study, we demonstrate the fragile nature of
road transportation systems when faced with either demand or supply
disruptions. First, we conducted a rigorous mathematical analysis to
theoretically establish the fragile nature of the systems. Subsequently, by
taking into account real-world stochasticity, we implemented a numerical
simulation with realistic network data to bridge the gap between the
theoretical proof and the real-world operations, to study the impact of
uncertainty on the fragile property of the systems. This work aims to help
researchers better comprehend the necessity to explicitly consider antifragile
design toward the application of future traffic control strategies, coping with
constantly growing traffic demand and subsequent traffic accidents.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00925" title="Abstract">arXiv:2402.00925</a> [<a href="/pdf/2402.00925" title="Download PDF">pdf</a>, <a href="/ps/2402.00925" title="Download PostScript">ps</a>, <a href="/format/2402.00925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Practical Evaluation of Commercial Industrial Augmented Reality  Systems in an Industry 4.0 Shipyard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanco-Novoa%2C+O">Oscar Blanco-Novoa</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Carames%2C+T+M">Tiago M Fernandez-Carames</a>, 
<a href="/search/cs?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/cs?searchtype=author&query=Vilar-Montesinos%2C+M">Miguel Vilar-Montesinos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version of an IEEE Access journal paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> O. Blanco-Novoa, T. M. Fernandez-Carames, P. Fraga-Lamas and M. A.
  Vilar-Montesinos, "A Practical Evaluation of Commercial Industrial Augmented
  Reality Systems in an Industry 4.0 Shipyard," in IEEE Access, vol. 6, pp.
  8201-8218, 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The principles of the Industry 4.0 are guiding manufacturing companies
towards more automated and computerized factories. Such principles are also
applied in shipbuilding, which usually involves numerous complex processes
whose automation will improve its efficiency and performance. Navantia, a
company that has been building ships for 300 years, is modernizing its
shipyards according to the Industry 4.0 principles with the help of the latest
technologies. Augmented Reality (AR), which when utilized in an industrial
environment is called Industrial AR (IAR), is one of such technologies, since
it can be applied in numerous situations in order to provide useful and
attractive interfaces that allow shipyard operators to obtain information on
their tasks and to interact with certain elements that surround them. This
article first reviews the state of the art on IAR applications for shipbuilding
and smart manufacturing. Then, the most relevant IAR hardware and software
tools are detailed, as well as the main use cases for the application of IAR in
a shipyard. Next, it is described Navantia's IAR system, which is based on a
fog-computing architecture. Such a system is evaluated when making use of three
IAR devices (a smartphone, a tablet and a pair of smart glasses), two AR SDKs
(ARToolKit and Vuforia) and multiple IAR markers, with the objective of
determining their performance in a shipyard workshop and inside a ship under
construction. The results obtained show remarkable performance differences
among the different IAR tools and the impact of factors like lighting, pointing
out the best combinations of markers, hardware and software to be used
depending on the characteristics of the shipyard scenario.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00943" title="Abstract">arXiv:2402.00943</a> [<a href="/pdf/2402.00943" title="Download PDF">pdf</a>, <a href="/format/2402.00943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Nearest Neighbor Search with Window Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engels%2C+J">Joshua Engels</a>, 
<a href="/search/cs?searchtype=author&query=Landrum%2C+B">Benjamin Landrum</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shangdi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Dhulipala%2C+L">Laxman Dhulipala</a>, 
<a href="/search/cs?searchtype=author&query=Shun%2C+J">Julian Shun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available: <a href="https://github.com/JoshEngels/RangeFilteredANN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We define and investigate the problem of $\textit{c-approximate window
search}$: approximate nearest neighbor search where each point in the dataset
has a numeric label, and the goal is to find nearest neighbors to queries
within arbitrary label ranges. Many semantic search problems, such as image and
document search with timestamp filters, or product search with cost filters,
are natural examples of this problem. We propose and theoretically analyze a
modular tree-based framework for transforming an index that solves the
traditional c-approximate nearest neighbor problem into a data structure that
solves window search. On standard nearest neighbor benchmark datasets equipped
with random label values, adversarially constructed embeddings, and image
search embeddings with real timestamps, we obtain up to a $75\times$ speedup
over existing solutions at the same level of recall.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00946" title="Abstract">arXiv:2402.00946</a> [<a href="/pdf/2402.00946" title="Download PDF">pdf</a>, <a href="/format/2402.00946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High order recovery of geometric interfaces from cell-average data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cohen%2C+A">Albert Cohen</a>, 
<a href="/search/math?searchtype=author&query=Mula%2C+O">Olga Mula</a>, 
<a href="/search/math?searchtype=author&query=Somacal%2C+A">Agust&#xed;n Somacal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the problem of recovering characteristic functions
$u:=\chi_\Omega$ from cell-average data on a coarse grid, and where $\Omega$ is
a compact set of $\mathbb{R}^d$. This task arises in very different contexts
such as image processing, inverse problems, and the accurate treatment of
interfaces in finite volume schemes. While linear recovery methods are known to
perform poorly, nonlinear strategies based on local reconstructions of the jump
interface $\Gamma:=\partial\Omega$ by geometrically simpler interfaces may
offer significant improvements. We study two main families of local
reconstruction schemes, the first one based on nonlinear least-squares fitting,
the second one based on the explicit computation of a polynomial-shaped curve
fitting the data, which yields simpler numerical computations and high order
geometric fitting. For each of them, we derive a general theoretical framework
which allows us to control the recovery error by the error of best
approximation up to a fixed multiplicative constant. Numerical tests in 2d
illustrate the expected approximation order of these strategies. Several
extensions are discussed, in particular the treatment of piecewise smooth
interfaces with corners.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00948" title="Abstract">arXiv:2402.00948</a> [<a href="/pdf/2402.00948" title="Download PDF">pdf</a>, <a href="/format/2402.00948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nanomechanically Induced Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diniz%2C+E+C">E. C. Diniz</a>, 
<a href="/search/cs?searchtype=author&query=de+S%C3%A1+Neto%2C+O+P">O. P. de S&#xe1; Neto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">In this paper, we investigate a nanomechanically induced transparency (NIT)
effects that arises from the coupling of a nanoelectromechanical system and a
trapped ion. By confining the ion in mesoscopic traps and capacitively coupling
it with a nanoelectromechanical system suspended as electrodes, the research is
intricately focussed on the implications of including the ion's degrees of
freedom. The Lamb--Dicke approximation is crucial to understanding the effects
of phonon exchange with electronic qubits and revealing transparency phenomena
in this unique coupling. The results underline the importance of the
Lamb--Dicke approximation in modelling the effects of transparency windows in
nanoelectromechanical systems.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00950" title="Abstract">arXiv:2402.00950</a> [<a href="/pdf/2402.00950" title="Download PDF">pdf</a>, <a href="/format/2402.00950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Semantics for Automated Web Form Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alian%2C+P">Parsa Alian</a>, 
<a href="/search/cs?searchtype=author&query=Nashid%2C+N">Noor Nashid</a>, 
<a href="/search/cs?searchtype=author&query=Shahbandeh%2C+M">Mobina Shahbandeh</a>, 
<a href="/search/cs?searchtype=author&query=Mesbah%2C+A">Ali Mesbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Automated test generation for web forms has been a longstanding challenge,
exacerbated by the intrinsic human-centric design of forms and their complex,
device-agnostic structures. We introduce an innovative approach, called
FormNexus, for automated web form test generation, which emphasizes deriving
semantic insights from individual form elements and relations among them,
utilizing textual content, DOM tree structures, and visual proximity. The
insights gathered are transformed into a new conceptual graph, the Form Entity
Relation Graph (FERG), which offers machine-friendly semantic information
extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for
generating and refining input constraints based on real-time form submission
responses. The culmination of this approach is a robust set of test cases, each
produced by methodically invalidating constraints, ensuring comprehensive
testing scenarios for web forms. This work bridges the existing gap in
automated web form testing by intertwining the capabilities of LLMs with
advanced semantic inference methods. Our evaluation demonstrates that FormNexus
combined with GPT-4 achieves 89% coverage in form submission states. This
outcome significantly outstrips the performance of the best baseline model by a
margin of 25%.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00954" title="Abstract">arXiv:2402.00954</a> [<a href="/pdf/2402.00954" title="Download PDF">pdf</a>, <a href="/ps/2402.00954" title="Download PostScript">ps</a>, <a href="/format/2402.00954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Blockchain Technologies for an Advanced and Cyber-Resilient  Automotive Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraga-Lamas%2C+P">Paula Fraga-Lamas</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Carames%2C+T+M">Tiago M. Fernandez-Carames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version accepted in IEEE Access
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Access, vol. 7, pp. 17578-17598, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the last century the automotive industry has arguably transformed society,
being one of the most complex, sophisticated and technologically advanced
industries, with innovations ranging from hybrid, electric and self-driving
smart cars to the development of IoT-connected cars. Due to its complexity, it
requires the involvement of many Industry 4.0 technologies, like robotics,
advanced manufacturing systems, cyber-physical systems or augmented reality.
One of the latest technologies that can benefit the automotive industry is
blockchain, which can enhance its data security, privacy, anonymity,
traceability, accountability, integrity, robustness, transparency,
trustworthiness and authentication, as well as provide long-term sustainability
and a higher operational efficiency to the whole industry. This review analyzes
the great potential of applying blockchain technologies to the automotive
industry emphasizing its cybersecurity features. Thus, the applicability of
blockchain is evaluated after examining the state-of-the-art and devising the
main stakeholders' current challenges. Furthermore, the article describes the
most relevant use cases, since the broad adoption of blockchain unlocks a wide
area of short- and medium-term promising automotive applications that can
create new business models and even disrupt the car-sharing economy as we know
it. Finally, after a Strengths, Weaknesses, Opportunities, and Threats (SWOT)
analysis, some recommendations are enumerated with the aim of guiding
researchers and companies in future cyber-resilient automotive industry
developments.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00955" title="Abstract">arXiv:2402.00955</a> [<a href="/pdf/2402.00955" title="Download PDF">pdf</a>, <a href="/format/2402.00955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with  Contrastive Learning in Multimodal Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+M">Malvika Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Curtin%2C+C">Catherine Curtin</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Boussard%2C+T">Tina Hernandez-Boussard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In the high-stakes realm of healthcare, ensuring fairness in predictive
models is crucial. Electronic Health Records (EHRs) have become integral to
medical decision-making, yet existing methods for enhancing model fairness
restrict themselves to unimodal data and fail to address the multifaceted
social biases intertwined with demographic factors in EHRs. To mitigate these
biases, we present FairEHR-CLP: a general framework for Fairness-aware Clinical
Predictions with Contrastive Learning in EHRs. FairEHR-CLP operates through a
two-stage process, utilizing patient demographics, longitudinal data, and
clinical notes. First, synthetic counterparts are generated for each patient,
allowing for diverse demographic identities while preserving essential health
information. Second, fairness-aware predictions employ contrastive learning to
align patient representations across sensitive attributes, jointly optimized
with an MLP classifier with a softmax layer for clinical classification tasks.
Acknowledging the unique challenges in EHRs, such as varying group sizes and
class imbalance, we introduce a novel fairness metric to effectively measure
error rate disparities across subgroups. Extensive experiments on three diverse
EHR datasets on three tasks demonstrate the effectiveness of FairEHR-CLP in
terms of fairness and utility compared with competitive baselines. FairEHR-CLP
represents an advancement towards ensuring both accuracy and equity in
predictive healthcare models.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00956" title="Abstract">arXiv:2402.00956</a> [<a href="/pdf/2402.00956" title="Download PDF">pdf</a>, <a href="/format/2402.00956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Spatial Schema Intuitions in Large Language and Vision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wicke%2C+P">Philipp Wicke</a>, 
<a href="/search/cs?searchtype=author&query=Wachowiak%2C+L">Lennart Wachowiak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the ubiquity of large language models (LLMs) in AI research, the
question of embodiment in LLMs remains underexplored, distinguishing them from
embodied systems in robotics where sensory perception directly informs physical
action. Our investigation navigates the intriguing terrain of whether LLMs,
despite their non-embodied nature, effectively capture implicit human
intuitions about fundamental, spatial building blocks of language. We employ
insights from spatial cognitive foundations developed through early
sensorimotor experiences, guiding our exploration through the reproduction of
three psycholinguistic experiments. Surprisingly, correlations between model
outputs and human responses emerge, revealing adaptability without a tangible
connection to embodied experiences. Notable distinctions include polarized
language model responses and reduced correlations in vision language models.
This research contributes to a nuanced understanding of the interplay between
language, spatial experiences, and the computations made by large language
models. More at https://cisnlp.github.io/Spatial_Schemas/
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00957" title="Abstract">arXiv:2402.00957</a> [<a href="/pdf/2402.00957" title="Download PDF">pdf</a>, <a href="/format/2402.00957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credal Learning Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Sultana%2C+M">Maryam Sultana</a>, 
<a href="/search/cs?searchtype=author&query=Elia%2C+E">Eleni Elia</a>, 
<a href="/search/cs?searchtype=author&query=Cuzzolin%2C+F">Fabio Cuzzolin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Statistical learning theory is the foundation of machine learning, providing
theoretical bounds for the risk of models learnt from a (single) training set,
assumed to issue from an unknown probability distribution. In actual
deployment, however, the data distribution may (and often does) vary, causing
domain adaptation/generalization issues. In this paper we lay the foundations
for a `credal' theory of learning, using convex sets of probabilities (credal
sets) to model the variability in the data-generating distribution. Such credal
sets, we argue, may be inferred from a finite sample of training sets. Bounds
are derived for the case of finite hypotheses spaces (both assuming
realizability or not) as well as infinite model spaces, which directly
generalize classical results.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00958" title="Abstract">arXiv:2402.00958</a> [<a href="/pdf/2402.00958" title="Download PDF">pdf</a>, <a href="/ps/2402.00958" title="Download PostScript">ps</a>, <a href="/format/2402.00958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reflection and Preservation of Properties in Coalgebraic (bi)Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos-Escrig%2C+D">David de Frutos-Escrig</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Theoretical Aspects of Computing (ICTAC) 2007. Lecture Notes in
  Computer Science volume 4711
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Our objective is to extend the standard results of preservation and
reflection of properties by bisimulations to the coalgebraic setting, as well
as to study under what conditions these results hold for simulations. The
notion of bisimulation is the classical one, while for simulations we use that
proposed by Hughes and Jacobs. As for properties, we start by using a
generalization of linear temporal logic to arbitrary coalgebras suggested by
Jacobs, and then an extension by Kurtz which includes atomic propositions too.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00962" title="Abstract">arXiv:2402.00962</a> [<a href="/pdf/2402.00962" title="Download PDF">pdf</a>, <a href="/ps/2402.00962" title="Download PostScript">ps</a>, <a href="/format/2402.00962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiset Bisimulations as a Common Framework for Ordinary and  Probabilistic Bisimulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Frutos-Escrig%2C+D">David de Frutos-Escrig</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Formal Techniques for Networked and Distributed Systems (FORTE)
  2008. Lecture Notes in Computer Science 5048. Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Our concrete objective is to present both ordinary bisimulations and
probabilistic bisimulations in a common coalgebraic framework based on multiset
bisimulations. For that we show how to relate the underlying powerset and
probabilistic distributions functors with the multiset functor by means of
adequate natural transformations. This leads us to the general topic that we
investigate in the paper: a natural transformation from a functor F to another
G transforms F-bisimulations into G-bisimulations but, in general, it is not
possible to express G-bisimulations in terms of F-bisimulations. However, they
can be characterized by considering Hughes and Jacobs' notion of simulation,
taking as the order on the functor F the equivalence induced by the epi-mono
decomposition of the natural transformation relating F and G. We also consider
the case of alternating probabilistic systems where non-deterministic and
probabilistic choices are mixed, although only in a partial way, and extend all
these results to categorical simulations.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00963" title="Abstract">arXiv:2402.00963</a> [<a href="/pdf/2402.00963" title="Download PDF">pdf</a>, <a href="/ps/2402.00963" title="Download PostScript">ps</a>, <a href="/format/2402.00963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-strongly Stable Orders Also Define Interesting Simulation Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos-Escrig%2C+D">David de Frutos-Escrig</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Algebra and Coalgebra in Computer Science (CALCO) 2009. Lecture
  Notes in Computer Science, 5728. Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present a study of the notion of coalgebraic simulation introduced by
Hughes and Jacobs. Although in their original paper they allow any functorial
order in their definition of coalgebraic simulation, for the simulation
relations to have good properties they focus their attention on functors with
orders which are strongly stable. This guarantees a so-called
"composition-preserving" property from which all the desired good properties
follow. We have noticed that the notion of strong stability not only ensures
such good properties but also "distinguishes the direction" of the simulation.
For example, the classic notion of simulation for labeled transition systems,
the relation "p is simulated by q", can be defined as a coalgebraic simulation
relation by means of a strongly stable order, whereas the opposite relation, "p
simulates q", cannot. Our study was motivated by some interesting classes of
simulations that illustrate the application of these results:
covariant-contravariant simulations and conformance simulations.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00964" title="Abstract">arXiv:2402.00964</a> [<a href="/pdf/2402.00964" title="Download PDF">pdf</a>, <a href="/ps/2402.00964" title="Download PostScript">ps</a>, <a href="/format/2402.00964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logics for Contravariant Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos-Escrig%2C+D">David de Frutos-Escrig</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Formal Techniques for Distributed Systems, Joint 12th IFIP WG 6.1
  International Conference, FMOODS 2010 and 30th IFIP WG 6.1 International
  Conference, FORTE 2010. Lecture Notes in Computer Science 6117. Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Covariant-contravariant simulation and conformance simulation are two
generalizations of the simple notion of simulation which aim at capturing the
fact that it is not always the case that "the larger the number of behaviors,
the better". Therefore, they can be considered to be more adequate to express
the fact that a system is a correct implementation of some specification. We
have previously shown that these two more elaborated notions fit well within
the categorical framework developed to study the notion of simulation in a
generic way. Now we show that their behaviors have also simple and natural
logical characterizations, though more elaborated than those for the plain
simulation semantics.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00965" title="Abstract">arXiv:2402.00965</a> [<a href="/pdf/2402.00965" title="Download PDF">pdf</a>, <a href="/format/2402.00965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Modal Machine Learning Framework for Automated Seizure Detection  in Laboratory Rats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mullen%2C+A">Aaron Mullen</a>, 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+S+E">Samuel E. Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Perdeh%2C+J">Jasmine Perdeh</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+B">Bjorn Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Talbert%2C+J">Jeffrey Talbert</a>, 
<a href="/search/cs?searchtype=author&query=Bumgardner%2C+V+K+C">V.K. Cody Bumgardner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)

</div>
<p class="mathjax">A multi-modal machine learning system uses multiple unique data sources and
types to improve its performance. This article proposes a system that combines
results from several types of models, all of which are trained on different
data signals. As an example to illustrate the efficacy of the system, an
experiment is described in which multiple types of data are collected from rats
suffering from seizures. This data includes electrocorticography readings,
piezoelectric motion sensor data, and video recordings. Separate models are
trained on each type of data, with the goal of classifying each time frame as
either containing a seizure or not. After each model has generated its
classification predictions, these results are combined. While each data signal
works adequately on its own for prediction purposes, the significant imbalance
in class labels leads to increased numbers of false positives, which can be
filtered and removed by utilizing all data sources. This paper will demonstrate
that, after postprocessing and combination techniques, classification accuracy
is improved with this multi-modal system when compared to the performance of
each individual data source.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00966" title="Abstract">arXiv:2402.00966</a> [<a href="/pdf/2402.00966" title="Download PDF">pdf</a>, <a href="/ps/2402.00966" title="Download PostScript">ps</a>, <a href="/format/2402.00966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relating Modal Refinements, Covariant-Contravariant Simulations and  Partial Bisimulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos+Escrig%2C+D">David de Frutos Escrig</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>, 
<a href="/search/cs?searchtype=author&query=Palomino%2C+M">Miguel Palomino</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Fundamentals of Software Engineering (FSEN) 2011 Lecture Notes in
  Computer Science 7141. Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This paper studies the relationships between three notions of behavioural
preorder that have been proposed in the literature: refinement over modal
transition systems, and the covariant-contravariant simulation and the partial
bisimulation preorders over labelled transition systems. It is shown that there
are mutual translations between modal transition systems and labelled
transition systems that preserve, and reflect, refinement and the
covariant-contravariant simulation preorder. The translations are also shown to
preserve the modal properties that can be expressed in the logics that
characterize those preorders. A translation from labelled transition systems
modulo the partial bisimulation preorder into the same model modulo the
covariant-contravariant simulation preorder is also offered, together with some
evidence that the former model is less expressive than the latter. In order to
gain more insight into the relationships between modal transition systems
modulo refinement and labelled transition systems modulo the
covariant-contravariant simulation preorder, their connections are also phrased
and studied in the context of institutions.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00969" title="Abstract">arXiv:2402.00969</a> [<a href="/pdf/2402.00969" title="Download PDF">pdf</a>, <a href="/format/2402.00969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPARQL Generation with Entity Pre-trained GPT for KG Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bustamante%2C+D">Diego Bustamante</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+H">Hideaki Takeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure, 2 tables. For the implementation, see <a href="https://github.com/DiegoEmilio01/SPARQL-generation-with-entity-pre-trained-GPT-for-KG-Question-Answering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Knowledge Graphs popularity has been rapidly growing in last years. All that
knowledge is available for people to query it through the many online databases
on the internet. Though, it would be a great achievement if non-programmer
users could access whatever information they want to know. There has been a lot
of effort oriented to solve this task using natural language processing tools
and creativity encouragement by way of many challenges. Our approach focuses on
assuming a correct entity linking on the natural language questions and
training a GPT model to create SPARQL queries from them. We managed to isolate
which property of the task can be the most difficult to solve at few or
zero-shot and we proposed pre-training on all entities (under CWA) to improve
the performance. We obtained a 62.703% accuracy of exact SPARQL matches on
testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of
0.009 on the question answering challenge.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00970" title="Abstract">arXiv:2402.00970</a> [<a href="/pdf/2402.00970" title="Download PDF">pdf</a>, <a href="/ps/2402.00970" title="Download PostScript">ps</a>, <a href="/format/2402.00970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Are Prime Formulae Characteristic?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=Della+Monica%2C+D">Dario Della Monica</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematical Foundations of Computer Science 2015. Lecture Notes
  in Computer Science 9234
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In the setting of the modal logic that characterizes modal refinement over
modal transition systems, Boudol and Larsen showed that the formulae for which
model checking can be reduced to preorder checking, that is, the characteristic
formulae, are exactly the consistent and prime ones. This paper presents
general, sufficient conditions guaranteeing that characteristic formulae are
exactly the consistent and prime ones. It is shown that the given conditions
apply to the logics characterizing all the semantics in van Glabbeek's
branching-time spectrum.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00971" title="Abstract">arXiv:2402.00971</a> [<a href="/pdf/2402.00971" title="Download PDF">pdf</a>, <a href="/format/2402.00971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseFormer: A Transformer for Visual and Thermal Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erdogan%2C+A">Aytekin Erdogan</a>, 
<a href="/search/cs?searchtype=author&query=Akagunduz%2C+E">Erdem Akagunduz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image fusion is the process of combining images from different sensors into a
single image that incorporates all relevant information. The majority of
state-of-the-art image fusion techniques use deep learning methods to extract
meaningful features; however, they primarily integrate local features without
considering the image's broader context. To overcome this limitation,
Transformer-based models have emerged as a promising solution, aiming to
capture general context dependencies through attention mechanisms. Since there
is no ground truth for image fusion, the loss functions are structured based on
evaluation metrics, such as the structural similarity index measure (SSIM). By
doing so, we create a bias towards the SSIM and, therefore, the input visual
band image. The objective of this study is to propose a novel methodology for
image fusion that mitigates the limitations associated with using evaluation
metrics as loss functions. Our approach integrates a transformer-based
multi-scale fusion strategy, which adeptly addresses both local and global
context information. This integration not only refines the individual
components of the image fusion process but also significantly enhances the
overall efficacy of the method. Our proposed method follows a two-stage
training approach, where an auto-encoder is initially trained to extract deep
features at multiple scales at the first stage. For the second stage, we
integrate our fusion block and change the loss function as mentioned. The
multi-scale features are fused using a combination of Convolutional Neural
Networks (CNNs) and Transformers. The CNNs are utilized to capture local
features, while the Transformer handles the integration of general context
features.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00972" title="Abstract">arXiv:2402.00972</a> [<a href="/pdf/2402.00972" title="Download PDF">pdf</a>, <a href="/format/2402.00972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closure Discovery for Coarse-Grained Partial Differential Equations  using Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Bassewitz%2C+J">Jan-Philipp von Bassewitz</a>, 
<a href="/search/cs?searchtype=author&query=Kaltenbach%2C+S">Sebastian Kaltenbach</a>, 
<a href="/search/cs?searchtype=author&query=Koumoutsakos%2C+P">Petros Koumoutsakos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Reliable predictions of critical phenomena, such as weather, wildfires and
epidemics are often founded on models described by Partial Differential
Equations (PDEs). However, simulations that capture the full range of
spatio-temporal scales in such PDEs are often prohibitively expensive.
Consequently, coarse-grained simulations that employ heuristics and empirical
closure terms are frequently utilized as an alternative. We propose a novel and
systematic approach for identifying closures in under-resolved PDEs using
Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates
inductive bias and exploits locality by deploying a central policy represented
efficiently by Convolutional Neural Networks (CNN). We demonstrate the
capabilities and limitations of MARL through numerical solutions of the
advection equation and the Burgers' equation. Our results show accurate
predictions for in- and out-of-distribution test cases as well as a significant
speedup compared to resolving all scales.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00973" title="Abstract">arXiv:2402.00973</a> [<a href="/pdf/2402.00973" title="Download PDF">pdf</a>, <a href="/ps/2402.00973" title="Download PostScript">ps</a>, <a href="/format/2402.00973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Characterisations and Compositionality of Input-Output  Conformance Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Gregorio-Rodr%C3%ADguez%2C+C">Carlos Gregorio-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Input-output conformance simulation (iocos) has been proposed by
Gregorio-Rodr\'iguez, Llana and Mart\'inez-Torres as a simulation-based
behavioural preorder underlying model-based testing. This relation is inspired
by Tretman's classic ioco relation, but has better worst-case complexity than
ioco and supports stepwise refinement. The goal of this paper is to develop the
theory of iocos by studying logical characterisations of this relation and its
compositionality. More specifically, this article presents characterisations of
iocos in terms of modal logics and compares them with an existing logical
characterisation for ioco proposed by Beohar and Mousavi. A precongruence rule
format for iocos and a rule format ensuring that operations take quiescence
properly into account are also given. Both rule formats are based on the GSOS
format by Bloom, Istrail and Meyer.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00976" title="Abstract">arXiv:2402.00976</a> [<a href="/pdf/2402.00976" title="Download PDF">pdf</a>, <a href="/ps/2402.00976" title="Download PostScript">ps</a>, <a href="/format/2402.00976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Transformers with Dynamic Halt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+J+R">Jishnu Ray Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+C">Cornelia Caragea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this paper, we study the inductive biases of two major approaches to
augmenting Transformers with a recurrent mechanism - (1) the approach of
incorporating a depth-wise recurrence similar to Universal Transformers; and
(2) the approach of incorporating a chunk-wise temporal recurrence like
Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways
to extend and combine the above methods - for example, we propose a global
mean-based dynamic halting mechanism for Universal Transformer and an
augmentation of Temporal Latent Bottleneck with elements from Universal
Transformer. We compare the models and probe their inductive biases in several
diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling,
ListOps, and Logical Inference.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00977" title="Abstract">arXiv:2402.00977</a> [<a href="/pdf/2402.00977" title="Download PDF">pdf</a>, <a href="/format/2402.00977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced fringe-to-phase framework using deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Won-Hoe Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bongjoong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Hyung-Gun Chi</a>, 
<a href="/search/cs?searchtype=author&query=Hyun%2C+J">Jae-Sang Hyun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 13 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In Fringe Projection Profilometry (FPP), achieving robust and accurate 3D
reconstruction with a limited number of fringe patterns remains a challenge in
structured light 3D imaging. Conventional methods require a set of fringe
images, but using only one or two patterns complicates phase recovery and
unwrapping. In this study, we introduce SFNet, a symmetric fusion network that
transforms two fringe images into an absolute phase. To enhance output
reliability, Our framework predicts refined phases by incorporating information
from fringe images of a different frequency than those used as input. This
allows us to achieve high accuracy with just two images. Comparative
experiments and ablation studies validate the effectiveness of our proposed
method. The dataset and code are publicly accessible on our project page
https://wonhoe-kim.github.io/SFNet.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00978" title="Abstract">arXiv:2402.00978</a> [<a href="/pdf/2402.00978" title="Download PDF">pdf</a>, <a href="/format/2402.00978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information-Theoretic Approach to Analyze NLP Classification Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Luran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M">Mark Gales</a>, 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vatsal Raina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 10 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Understanding the importance of the inputs on the output is useful across
many tasks. This work provides an information-theoretic framework to analyse
the influence of inputs for text classification tasks. Natural language
processing (NLP) tasks take either a single element input or multiple element
inputs to predict an output variable, where an element is a block of text. Each
text element has two components: an associated semantic meaning and a
linguistic realization. Multiple-choice reading comprehension (MCRC) and
sentiment classification (SC) are selected to showcase the framework. For MCRC,
it is found that the context influence on the output compared to the question
influence reduces on more challenging datasets. In particular, more challenging
contexts allow a greater variation in complexity of questions. Hence, test
creators need to carefully consider the choice of the context when designing
multiple-choice questions for assessment. For SC, it is found the semantic
meaning of the input text dominates (above 80\% for all datasets considered)
compared to its linguistic realisation when determining the sentiment. The
framework is made available at:
https://github.com/WangLuran/nlp-element-influence
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00979" title="Abstract">arXiv:2402.00979</a> [<a href="/pdf/2402.00979" title="Download PDF">pdf</a>, <a href="/format/2402.00979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of weak Galerkin mixed FEM based on the velocity--pseudostress  formulation for Navier--Stokes equation on polygonal meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gharibi%2C+Z">Zeinab Gharibi</a>, 
<a href="/search/math?searchtype=author&query=Dehghan%2C+M">Mehdi Dehghan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The present article introduces, mathematically analyzes, and numerically
validates a new weak Galerkin (WG) mixed-FEM based on Banach spaces for the
stationary Navier--Stokes equation in pseudostress-velocity formulation. More
precisely, a modified pseudostress tensor, called $ \boldsymbol{\sigma} $,
depending on the pressure, and the diffusive and convective terms has been
introduced in the proposed technique, and a dual-mixed variational formulation
has been derived where the aforementioned pseudostress tensor and the velocity,
are the main unknowns of the system, whereas the pressure is computed via a
post-processing formula. Thus, it is sufficient to provide a WG space for the
tensor variable and a space of piecewise polynomial vectors of total degree at
most 'k' for the velocity. Moreover, in order to define the weak discrete
bilinear form, whose continuous version involves the classical divergence
operator, the weak divergence operator as a well-known alternative for the
classical divergence operator in a suitable discrete subspace is proposed. The
well-posedness of the numerical solution is proven using a fixed-point approach
and the discrete versions of the Babu\v{s}ka-Brezzi theory and the
Banach-Ne\v{c}as-Babu\v{s}ka theorem. Additionally, an a priori error estimate
is derived for the proposed method. Finally, several numerical results
illustrating the method's good performance and confirming the theoretical rates
of convergence are presented.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00982" title="Abstract">arXiv:2402.00982</a> [<a href="/pdf/2402.00982" title="Download PDF">pdf</a>, <a href="/format/2402.00982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule Formats for Nominal Process Calculi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-P%C3%A9rez%2C+%C3%81">&#xc1;lvaro Garc&#xed;a-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>, 
<a href="/search/cs?searchtype=author&query=Ortega-Mall%C3%A9n%2C+Y">Yolanda Ortega-Mall&#xe9;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference version of <a href="/abs/1807.02081">arXiv:1807.02081</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The nominal transition systems (NTSs) of Parrow et al. describe the
operational semantics of nominal process calculi. We study NTSs in terms of the
nominal residual transition systems (NRTSs) that we introduce. We provide rule
formats for the specifications of NRTSs that ensure that the associated NRTS is
an NTS and apply them to the operational specification of the early
pi-calculus. Our study stems from the recent Nominal SOS of Cimini et al. and
from earlier works in nominal sets and nominal logic by Gabbay, Pitts and their
collaborators.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00986" title="Abstract">arXiv:2402.00986</a> [<a href="/pdf/2402.00986" title="Download PDF">pdf</a>, <a href="/format/2402.00986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Parallel Semantics Program Dependence Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Homerding%2C+B">Brian Homerding</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Atmn Patel</a>, 
<a href="/search/cs?searchtype=author&query=Deiana%2C+E+A">Enrico Armenio Deiana</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yian Su</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zujun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Godala%2C+B+R">Bhargav Reddy Godala</a>, 
<a href="/search/cs?searchtype=author&query=August%2C+D+I">David I. August</a>, 
<a href="/search/cs?searchtype=author&query=Campanoni%2C+S">Simone Campanoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">A compiler's intermediate representation (IR) defines a program's execution
plan by encoding its instructions and their relative order. Compiler
optimizations aim to replace a given execution plan with a
semantically-equivalent one that increases the program's performance for the
target architecture. Alternative representations of an IR, like the Program
Dependence Graph (PDG), aid this process by capturing the minimum set of
constraints that semantically-equivalent execution plans must satisfy. Parallel
programming like OpenMP extends a sequential execution plan by adding the
possibility of running instructions in parallel, creating a parallel execution
plan. Recently introduced parallel IRs, like TAPIR, explicitly encode a
parallel execution plan. These new IRs finally make it possible for compilers
to change the parallel execution plan expressed by programmers to better fit
the target parallel architecture. Unfortunately, parallel IRs do not help
compilers in identifying the set of parallel execution plans that preserve the
original semantics. In other words, we are still lacking an alternative
representation of parallel IRs to capture the minimum set of constraints that
parallel execution plans must satisfy to be semantically-equivalent.
Unfortunately, the PDG is not an ideal candidate for this task as it was
designed for sequential code. We propose the Parallel Semantics Program
Dependence Graph (PS-PDG) to precisely capture the salient program constraints
that all semantically-equivalent parallel execution plans must satisfy. This
paper defines the PS-PDG, justifies the necessity of each extension to the PDG,
and demonstrates the increased optimization power of the PS-PDG over an
existing PDG-based automatic-parallelizing compiler. Compilers can now rely on
the PS-PDG to select different parallel execution plans while maintaining the
same original semantics.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00987" title="Abstract">arXiv:2402.00987</a> [<a href="/pdf/2402.00987" title="Download PDF">pdf</a>, <a href="/format/2402.00987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Contrastive Pre-Training for Multivariate Point  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shou%2C+X">Xiao Shou</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+D">Dharmashankar Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjya%2C+D">Debarun Bhattacharjya</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tian Gao</a>, 
<a href="/search/cs?searchtype=author&query=Bennet%2C+K+P">Kristin P. Bennet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervision is one of the hallmarks of representation learning in the
increasingly popular suite of foundation models including large language models
such as BERT and GPT-3, but it has not been pursued in the context of
multivariate event streams, to the best of our knowledge. We introduce a new
paradigm for self-supervised learning for multivariate point processes using a
transformer encoder. Specifically, we design a novel pre-training strategy for
the encoder where we not only mask random event epochs but also insert randomly
sampled "void" epochs where an event does not occur; this differs from the
typical discrete-time pretext tasks such as word-masking in BERT but expands
the effectiveness of masking to better capture continuous-time dynamics. To
improve downstream tasks, we introduce a contrasting module that compares real
events to simulated void instances. The pre-trained model can subsequently be
fine-tuned on a potentially much smaller event dataset, similar conceptually to
the typical transfer of popular pre-trained language models. We demonstrate the
effectiveness of our proposed paradigm on the next-event prediction task using
synthetic datasets and 3 real applications, observing a relative performance
boost of as high as up to 20% compared to state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00989" title="Abstract">arXiv:2402.00989</a> [<a href="/pdf/2402.00989" title="Download PDF">pdf</a>, <a href="/format/2402.00989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLinO++: Single-Shot Estimation of Generic Polylines for Mapless  Automated Diving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meyer%2C+A">Annika Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+C">Christoph Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In automated driving, highly accurate maps are commonly used to support and
complement perception. These maps are costly to create and quickly become
outdated as the traffic world is permanently changing. In order to support or
replace the map of an automated system with detections from sensor data, a
perception module must be able to detect the map features. We propose a neural
network that follows the one shot philosophy of YOLO but is designed for
detection of 1D structures in images, such as lane boundaries.
<br />We extend previous ideas by a midpoint based line representation and anchor
definitions. This representation can be used to describe lane borders,
markings, but also implicit features such as centerlines of lanes. The broad
applicability of the approach is shown with the detection performance on lane
centerlines, lane borders as well as the markings both on highways and in urban
areas.
<br />Versatile lane boundaries are detected and can be inherently classified as
dashed or solid lines, curb, road boundaries, or implicit delimitation.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00994" title="Abstract">arXiv:2402.00994</a> [<a href="/pdf/2402.00994" title="Download PDF">pdf</a>, <a href="/ps/2402.00994" title="Download PostScript">ps</a>, <a href="/format/2402.00994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cost-Efficient Approach for Creating Virtual Fitting Room using  Generative Adversarial Networks (GANs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attallah%2C+K">Kirolos Attallah</a>, 
<a href="/search/cs?searchtype=author&query=Zaky%2C+G">Girgis Zaky</a>, 
<a href="/search/cs?searchtype=author&query=Abdelrhim%2C+N">Nourhan Abdelrhim</a>, 
<a href="/search/cs?searchtype=author&query=Botros%2C+K">Kyrillos Botros</a>, 
<a href="/search/cs?searchtype=author&query=Dife%2C+A">Amjad Dife</a>, 
<a href="/search/cs?searchtype=author&query=Negied%2C+N">Nermin Negied</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 15 Issue 1, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Customers all over the world want to see how the clothes fit them or not
before purchasing. Therefore, customers by nature prefer brick-and-mortar
clothes shopping so they can try on products before purchasing them. But after
the Pandemic of COVID19 many sellers either shifted to online shopping or
closed their fitting rooms which made the shopping process hesitant and
doubtful. The fact that the clothes may not be suitable for their buyers after
purchase led us to think about using new AI technologies to create an online
platform or a virtual fitting room (VFR) in the form of a mobile application
and a deployed model using a webpage that can be embedded later to any online
store where they can try on any number of cloth items without physically trying
them. Besides, it will save much searching time for their needs. Furthermore,
it will reduce the crowding and headache in the physical shops by applying the
same technology using a special type of mirror that will enable customers to
try on faster. On the other hand, from business owners' perspective, this
project will highly increase their online sales, besides, it will save the
quality of the products by avoiding physical trials issues. The main approach
used in this work is applying Generative Adversarial Networks (GANs) combined
with image processing techniques to generate one output image from two input
images which are the person image and the cloth image. This work achieved
results that outperformed the state-of-the-art approaches found in literature.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00996" title="Abstract">arXiv:2402.00996</a> [<a href="/pdf/2402.00996" title="Download PDF">pdf</a>, <a href="/format/2402.00996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mmID: High-Resolution mmWave Imaging for Human Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayaweera%2C+S+S">Sakila S. Jayaweera</a>, 
<a href="/search/cs?searchtype=author&query=Regani%2C+S+D">Sai Deepika Regani</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuqian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beibei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K+J+R">K. J. Ray Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was published in the IEEE 9th World Forum on Internet of Things
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Achieving accurate human identification through RF imaging has been a
persistent challenge, primarily attributed to the limited aperture size and its
consequent impact on imaging resolution. The existing imaging solution enables
tasks such as pose estimation, activity recognition, and human tracking based
on deep neural networks by estimating skeleton joints. In contrast to
estimating joints, this paper proposes to improve imaging resolution by
estimating the human figure as a whole using conditional generative adversarial
networks (cGAN). In order to reduce training complexity, we use an estimated
spatial spectrum using the MUltiple SIgnal Classification (MUSIC) algorithm as
input to the cGAN. Our system generates environmentally independent,
high-resolution images that can extract unique physical features useful for
human identification. We use a simple convolution layers-based classification
network to obtain the final identification result. From the experimental
results, we show that resolution of the image produced by our trained generator
is high enough to enable human identification. Our finding indicates
high-resolution accuracy with 5% mean silhouette difference to the Kinect
device. Extensive experiments in different environments on multiple testers
demonstrate that our system can achieve 93% overall test accuracy in unseen
environments for static human target identification.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00999" title="Abstract">arXiv:2402.00999</a> [<a href="/pdf/2402.00999" title="Download PDF">pdf</a>, <a href="/format/2402.00999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RDNF Oriented Analytics to Random Boolean Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aslanyan%2C+L">Levon Aslanyan</a>, 
<a href="/search/cs?searchtype=author&query=Arsenyan%2C+I">Irina Arsenyan</a>, 
<a href="/search/cs?searchtype=author&query=Karakhanyan%2C+V">Vilik Karakhanyan</a>, 
<a href="/search/cs?searchtype=author&query=Sahakyan%2C+H">Hasmik Sahakyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Dominant areas of computer science and computation systems are intensively
linked to the hypercube-related studies and interpretations. This article
presents some transformations and analytics for some example algorithms and
Boolean domain problems. Our focus is on the methodology of complexity
evaluation and integration of several types of postulations concerning special
hypercube structures. Our primary goal is to demonstrate the usual formulas and
analytics in this area, giving the necessary set of common formulas often used
for complexity estimations and approximations. The basic example under
considered is the Boolean minimization problem, in terms of the average
complexity of the so-called reduced disjunctive normal form (also referred to
as complete, prime irredundant, or Blake canonical form). In fact,
combinatorial counterparts of the disjunctive normal form complexities are
investigated in terms of sets of their maximal intervals. The results obtained
compose the basis of logical separation classification algorithmic technology
of pattern recognition. In fact, these considerations are not only general
tools of minimization investigations of Boolean functions, but they also prove
useful structures, models, and analytics for constraint logic programming,
machine learning, decision policy optimization and other domains of computer
science.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01001" title="Abstract">arXiv:2402.01001</a> [<a href="/pdf/2402.01001" title="Download PDF">pdf</a>, <a href="/format/2402.01001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensuring Data Privacy in AC Optimal Power Flow with a Distributed  Co-Simulation Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinliang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Kocher%2C+A">Alexander Kocher</a>, 
<a href="/search/cs?searchtype=author&query=Kova%C4%8Devi%C4%87%2C+J">Jovana Kova&#x10d;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Dindar%2C+B">Burak Dindar</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+C+N">Colin N. Jones</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87akmak%2C+H">H&#xfc;seyin &#xc7;akmak</a>, 
<a href="/search/cs?searchtype=author&query=Hagenmeyer%2C+V">Veit Hagenmeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">During the energy transition, the significance of collaborative management
among institutions is rising, confronting challenges posed by data privacy
concerns. Prevailing research on distributed approaches, as an alternative to
centralized management, often lacks numerical convergence guarantees or is
limited to single-machine numerical simulation. To address this, we present a
distributed approach for solving AC Optimal Power Flow (OPF) problems within a
geographically distributed environment. This involves integrating the energy
system Co-Simulation (eCoSim) module in the eASiMOV framework with the
convergence-guaranteed distributed optimization algorithm, i.e., the Augmented
Lagrangian based Alternating Direction Inexact Newton method (ALADIN).
Comprehensive evaluations across multiple system scenarios reveal a marginal
performance slowdown compared to the centralized approach and the distributed
approach executed on single machines -- a justified trade-off for enhanced data
privacy. This investigation serves as empirical validation of the successful
execution of distributed AC OPF within a geographically distributed
environment, highlighting potential directions for future research.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01002" title="Abstract">arXiv:2402.01002</a> [<a href="/pdf/2402.01002" title="Download PDF">pdf</a>, <a href="/format/2402.01002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-generated faces free from racial and gender stereotypes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlDahoul%2C+N">Nouar AlDahoul</a>, 
<a href="/search/cs?searchtype=author&query=Rahwan%2C+T">Talal Rahwan</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text-to-image generative AI models such as Stable Diffusion are used daily by
millions worldwide. However, many have raised concerns regarding how these
models amplify racial and gender stereotypes. To study this phenomenon, we
develop a classifier to predict the race, gender, and age group of any given
face image, and show that it achieves state-of-the-art performance. Using this
classifier, we quantify biases in Stable Diffusion across six races, two
genders, five age groups, 32 professions, and eight attributes. We then propose
novel debiasing solutions that outperform state-of-the-art alternatives.
Additionally, we examine the degree to which Stable Diffusion depicts
individuals of the same race as being similar to one another. This analysis
reveals a high degree of stereotyping, e.g., depicting most middle eastern
males as being dark-skinned, bearded, and wearing a traditional headdress. We
address these limitations by proposing yet another novel solution that
increases facial diversity across genders and racial groups. Our solutions are
open-sourced and made publicly available.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01007" title="Abstract">arXiv:2402.01007</a> [<a href="/pdf/2402.01007" title="Download PDF">pdf</a>, <a href="/ps/2402.01007" title="Download PostScript">ps</a>, <a href="/format/2402.01007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Municipal cyber risk modeling using cryptographic computing to inform  cyber policymaking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baral%2C+A">Avital Baral</a>, 
<a href="/search/cs?searchtype=author&query=Reynolds%2C+T">Taylor Reynolds</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+L">Lawrence Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Weitzner%2C+D+J">Daniel J. Weitzner</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Angelina Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working Draft for Presentation at the Cybersecurity Law and Policy Scholars Conference - September 29, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Municipalities are vulnerable to cyberattacks with devastating consequences,
but they lack key information to evaluate their own risk and compare their
security posture to peers. Using data from 83 municipalities collected via a
cryptographically secure computation platform about their security posture,
incidents, security control failures, and losses, we build data-driven cyber
risk models and cyber security benchmarks for municipalities. We produce
benchmarks of the security posture in a sector, the frequency of cyber
incidents, forecasted annual losses for organizations based on their defensive
posture, and a weighting of cyber controls based on their individual failure
rates and associated losses. Combined, these four items can help guide cyber
policymaking by quantifying the cyber risk in a sector, identifying gaps that
need to be addressed, prioritizing policy interventions, and tracking progress
of those interventions over time. In the case of the municipalities, these
newly derived risk measures highlight the need for continuous measured
improvement of cybersecurity readiness, show clear areas of weakness and
strength, and provide governments with some early targets for policy focus such
as security education, incident response, and focusing efforts first on
municipalities at the lowest security levels that have the highest risk
reduction per security dollar invested.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01008" title="Abstract">arXiv:2402.01008</a> [<a href="/pdf/2402.01008" title="Download PDF">pdf</a>, <a href="/format/2402.01008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CF4J: Collaborative Filtering for Java
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortega%2C+F">Fernando Ortega</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Bobadilla%2C+J">Jesus Bobadilla</a>, 
<a href="/search/cs?searchtype=author&query=Hernando%2C+A">Antonio Hernando</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Knowledge-Based Systems, 152, 94-99 (2018)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender Systems (RS) provide a relevant tool to mitigate the information
overload problem. A large number of researchers have published hundreds of
papers to improve different RS features. It is advisable to use RS frameworks
that simplify RS researchers: a) to design and implement recommendations
methods and, b) to speed up the execution time of the experiments. In this
paper, we present CF4J, a Java library designed to carry out Collaborative
Filtering based RS research experiments. CF4J has been designed from
researchers to researchers. It allows: a) RS datasets reading, b) full and easy
access to data and intermediate or final results, c) to extend their main
functionalities, d) to concurrently execute the implemented methods, and e) to
provide a thorough evaluation for the implementations by quality measures. In
summary, CF4J serves as a library specifically designed for the research trial
and error process.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01009" title="Abstract">arXiv:2402.01009</a> [<a href="/pdf/2402.01009" title="Download PDF">pdf</a>, <a href="/ps/2402.01009" title="Download PostScript">ps</a>, <a href="/format/2402.01009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Expected Cost Analysis of Functional Probabilistic  Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Amorim%2C+P+H+A">Pedro H. Azevedo de Amorim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Reasoning about resources used during the execution of programs, such as
time, is one of the fundamental questions in computer science. When programming
with probabilistic primitives, however, different samples may result in
different resource usage, making the cost of a program not a single number but
a distribution instead.
<br />The expected cost is an important metric used to quantify the efficiency of
probabilistic programs. In this work we introduce $\mathbf{cert}$, a
call-by-push-value (CBPV) metalanguage extended with primitives for
probability, cost and unbounded recursion, and give it denotational semantics
for reasoning about the average cost of programs. We justify the validity of
the semantics by presenting case-studies ranging from randomized algorithms to
stochastic processes and showing how the semantics captures their intended
cost.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01010" title="Abstract">arXiv:2402.01010</a> [<a href="/pdf/2402.01010" title="Download PDF">pdf</a>, <a href="/format/2402.01010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A generalized essentially non-hourglass total Lagrangian SPH solid  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaojing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuaihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangyu Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages, 37 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In this paper, we tackle a persistent numerical instability within the total
Lagrangian smoothed particle hydrodynamics (TLSPH) solid dynamics.
Specifically, we address the hourglass modes that may grow and eventually
deteriorate the reliability of simulation, particularly in the scenarios
characterized by large deformations. We propose a generalized essentially
non-hourglass formulation based on volumetric-deviatoric stress decomposition,
offering a general solution for elasticity, plasticity, anisotropy, and other
material models. Comparing the standard SPH formulation with the original
non-nested Laplacian operator applied in our previous work
\cite{wu2023essentially} to handle the hourglass issues in standard elasticity,
we introduce a correction for the discretization of shear stress that relies on
the discrepancy produced by a tracing-back prediction of the initial
inter-particle direction from the current deformation gradient. The present
formulation, when applied to standard elastic materials, is able to recover the
original Laplacian operator. Due to the dimensionless nature of the correction,
this formulation handles complex material models in a very straightforward way.
Furthermore, a magnitude limiter is introduced to minimize the correction in
domains where the discrepancy is less pronounced. The present formulation is
validated, with a single set of modeling parameters, through a series of
benchmark cases, confirming good stability and accuracy across elastic,
plastic, and anisotropic materials. To showcase its potential, the formulation
is employed to simulate a complex problem involving viscous plastic Oobleck
material, contacts, and very large deformation.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01011" title="Abstract">arXiv:2402.01011</a> [<a href="/pdf/2402.01011" title="Download PDF">pdf</a>, <a href="/format/2402.01011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ruling Out Low-rank Matrix Multiplication Tensor Decompositions with  Symmetries via SAT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jason Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ISSAC 2024; 8 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We analyze rank decompositions of the $3\times 3$ matrix multiplication
tensor over $\mathbb{Z}/2\mathbb{Z}$. We restrict our attention to
decompositions of rank $\le 21$, as only those decompositions will yield an
asymptotically faster algorithm for matrix multiplication than Strassen's
algorithm. To reduce search space, we also require decompositions to have
certain symmetries. Using Boolean SAT solvers, we show that under certain
symmetries, such decompositions do not exist.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01012" title="Abstract">arXiv:2402.01012</a> [<a href="/pdf/2402.01012" title="Download PDF">pdf</a>, <a href="/ps/2402.01012" title="Download PostScript">ps</a>, <a href="/format/2402.01012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> algoXSSF: Detection and analysis of cross-site request forgery (XSRF)  and cross-site scripting (XSS) attacks via Machine learning algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kshetri%2C+N">Naresh Kshetri</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dilip Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Hutson%2C+J">James Hutson</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+N">Navneet Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Osama%2C+O+F">Omar Faruq Osama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The global rise of online users and online devices has ultimately given rise
to the global internet population apart from several cybercrimes and
cyberattacks. The combination of emerging new technology and powerful
algorithms (of Artificial Intelligence, Deep Learning, and Machine Learning) is
needed to counter defense web security including attacks on several search
engines and websites. The unprecedented increase rate of cybercrime and website
attacks urged for new technology consideration to protect data and information
online. There have been recent and continuous cyberattacks on websites, web
domains with ongoing data breaches including - GitHub account hack, data leaks
on Twitter, malware in WordPress plugins, vulnerability in Tomcat server to
name just a few. We have investigated with an in-depth study apart from the
detection and analysis of two major cyberattacks (although there are many more
types): cross-site request forgery (XSRF) and cross-site scripting (XSS)
attacks. The easy identification of cyber trends and patterns with continuous
improvement is possible within the edge of machine learning and AI algorithms.
The use of machine learning algorithms would be extremely helpful to counter
(apart from detection) the XSRF and XSS attacks. We have developed the
algorithm and cyber defense framework - algoXSSF with machine learning
algorithms embedded to combat malicious attacks (including Man-in-the-Middle
attacks) on websites for detection and analysis.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01018" title="Abstract">arXiv:2402.01018</a> [<a href="/pdf/2402.01018" title="Download PDF">pdf</a>, <a href="/format/2402.01018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zicheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Cherukuri%2C+R+K">Rajesh Kumar Cherukuri</a>, 
<a href="/search/cs?searchtype=author&query=Nayyar%2C+N">Naumaan Nayyar</a>, 
<a href="/search/cs?searchtype=author&query=Malandri%2C+L">Lorenzo Malandri</a>, 
<a href="/search/cs?searchtype=author&query=Sengamedu%2C+S+H">Srinivasan H. Sengamedu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) have been reshaping
Natural Language Processing (NLP) task in several domains. Their use in the
field of Human Resources (HR) has still room for expansions and could be
beneficial for several time consuming tasks. Examples such as time-off
submissions, medical claims filing, and access requests are noteworthy, but
they are by no means the sole instances. However, the aforementioned
developments must grapple with the pivotal challenge of constructing a
high-quality training dataset. On one hand, most conversation datasets are
solving problems for customers not employees. On the other hand, gathering
conversations with HR could raise privacy concerns. To solve it, we introduce
HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR
domains to evaluate LLM Agent. Our work has the following contributions: (1) It
is the first labeled open-sourced conversation dataset in the HR domain for NLP
research. (2) It provides a detailed recipe for the data generation procedure
along with data analysis and human evaluations. The data generation pipeline is
transferable and can be easily adapted for labeled conversation data generation
in other domains. (3) The proposed data-collection pipeline is mostly based on
LLMs with minimal human involvement for annotation, which is time and
cost-efficient.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01019" title="Abstract">arXiv:2402.01019</a> [<a href="/pdf/2402.01019" title="Download PDF">pdf</a>, <a href="/format/2402.01019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Independent Deception: A New Taxonomy and Linguistic Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+R+M">Rakesh M. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Dershowitz%2C+N">Nachum Dershowitz</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+V">Victor Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Boumber%2C+D">Dainis Boumber</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages. arXiv admin note: text overlap with <a href="/abs/2207.01738">arXiv:2207.01738</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">Internet-based economies and societies are drowning in deceptive attacks.
These attacks take many forms, such as fake news, phishing, and job scams,
which we call ``domains of deception.'' Machine-learning and
natural-language-processing researchers have been attempting to ameliorate this
precarious situation by designing domain-specific detectors. Only a few recent
works have considered domain-independent deception. We collect these disparate
threads of research and investigate domain-independent deception. First, we
provide a new computational definition of deception and break down deception
into a new taxonomy. Then, we analyze the debate on linguistic cues for
deception and supply guidelines for systematic reviews. Finally, we investigate
common linguistic features and give evidence for knowledge transfer across
different forms of deception.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01020" title="Abstract">arXiv:2402.01020</a> [<a href="/pdf/2402.01020" title="Download PDF">pdf</a>, <a href="/format/2402.01020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying analogy of concepts via ologs and wiring diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+J">Jason Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Combinatorics (math.CO); Category Theory (math.CT)

</div>
<p class="mathjax">We build on the theory of ontology logs (ologs) created by Spivak and Kent,
and define a notion of wiring diagrams. In this article, a wiring diagram is a
finite directed labelled graph. The labels correspond to types in an olog; they
can also be interpreted as readings of sensors in an autonomous system. As
such, wiring diagrams can be used as a framework for an autonomous system to
form abstract concepts. We show that the graphs underlying skeleton wiring
diagrams form a category. This allows skeleton wiring diagrams to be compared
and manipulated using techniques from both graph theory and category theory. We
also extend the usual definition of graph edit distance to the case of wiring
diagrams by using operations only available to wiring diagrams, leading to a
metric on the set of all skeleton wiring diagrams. In the end, we give an
extended example on calculating the distance between two concepts represented
by wiring diagrams, and explain how to apply our framework to any application
domain.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01021" title="Abstract">arXiv:2402.01021</a> [<a href="/pdf/2402.01021" title="Download PDF">pdf</a>, <a href="/format/2402.01021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding the Challenges of Bug Localization in Deep  Learning Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahan%2C+S">Sigma Jahan</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M+B">Mehil B. Shah</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Mohammad Masudur Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software bugs cost the global economy billions of dollars annually and claim
~50\% of the programming time from software developers. Locating these bugs is
crucial for their resolution but challenging. It is even more challenging in
deep-learning systems due to their black-box nature. Bugs in these systems are
also hidden not only in the code but also in the models and training data,
which might make traditional debugging methods less effective. In this article,
we conduct a large-scale empirical study to better understand the challenges of
localizing bugs in deep-learning systems. First, we determine the bug
localization performance of four existing techniques using 2,365 bugs from
deep-learning systems and 2,913 from traditional software. We found these
techniques significantly underperform in localizing deep-learning system bugs.
Second, we evaluate how different bug types in deep learning systems impact bug
localization. We found that the effectiveness of localization techniques varies
with bug type due to their unique challenges. For example, tensor bugs were
more accessible to locate due to their structural nature, while all techniques
struggled with GPU bugs due to their external dependencies. Third, we
investigate the impact of bugs' extrinsic nature on localization in
deep-learning systems. We found that deep learning bugs are often extrinsic and
thus connected to artifacts other than source code (e.g., GPU, training data),
contributing to the poor performance of existing localization methods.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01024" title="Abstract">arXiv:2402.01024</a> [<a href="/pdf/2402.01024" title="Download PDF">pdf</a>, <a href="/format/2402.01024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the BER vs. Bandwidth-Efficiency Trade-offs in Windowed OTSM  Dispensing with Zero-Padding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zeping Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+Q">Hien Quoc Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Matthaiou%2C+M">Michail Matthaiou</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">An orthogonal time sequency multiplexing (OTSM) scheme using practical
signaling functions is proposed under strong phase noise (PHN) scenarios. By
utilizing the transform relationships between the delay-sequency (DS),
time-frequency (TF) and time-domains, we first conceive the DS-domain
input-output relationship of our OTSM system, where the conventional
zero-padding is discarded to increase the spectral efficiency. Then, the
unconditional pairwise error probability is derived, followed by deriving the
bit error ratio (BER) upper bound in closed-form. Moreover, we compare the BER
performance of our OTSM system based on several practical signaling functions.
Our simulation results demonstrate that the upper bound derived accurately
predicts the BER performance in the case of moderate to high signal-to-noise
ratios (SNRs), while harnessing practical window functions is capable of
attaining an attractive out-of-band emission (OOBE) vs. BER trade-off.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01025" title="Abstract">arXiv:2402.01025</a> [<a href="/pdf/2402.01025" title="Download PDF">pdf</a>, <a href="/format/2402.01025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Clustering for Detecting Semantic Change Across Time and  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianghe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Strube%2C+M">Michael Strube</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL2024 Camera Ready (20 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the predominance of contextualized embeddings in NLP, approaches to
detect semantic change relying on these embeddings and clustering methods
underperform simpler counterparts based on static word embeddings. This stems
from the poor quality of the clustering methods to produce sense clusters --
which struggle to capture word senses, especially those with low frequency.
This issue hinders the next step in examining how changes in word senses in one
language influence another. To address this issue, we propose a graph-based
clustering approach to capture nuanced changes in both high- and low-frequency
word senses across time and languages, including the acquisition and loss of
these senses over time. Our experimental results show that our approach
substantially surpasses previous approaches in the SemEval2020 binary
classification task across four languages. Moreover, we showcase the ability of
our approach as a versatile visualization tool to detect semantic changes in
both intra-language and inter-language setups. We make our code and data
publicly available.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01030" title="Abstract">arXiv:2402.01030</a> [<a href="/pdf/2402.01030" title="Download PDF">pdf</a>, <a href="/format/2402.01030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Executable Code Actions Elicit Better LLM Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhu Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, data, model, and demo are available at <a href="https://github.com/xingyaoww/code-act">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Model (LLM) agents, capable of performing a broad range of
actions, such as invoking tools and controlling robots, show great potential in
tackling real-world challenges. LLM agents are typically prompted to produce
actions by generating JSON or text in a pre-defined format, which is usually
limited by constrained action space (e.g., the scope of pre-defined tools) and
restricted flexibility (e.g., inability to compose multiple tools). This work
proposes to use executable Python code to consolidate LLM agents' actions into
a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct
can execute code actions and dynamically revise prior actions or emit new
actions upon new observations through multi-turn interactions. Our extensive
analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that
CodeAct outperforms widely used alternatives (up to 20% higher success rate).
The encouraging performance of CodeAct motivates us to build an open-source LLM
agent that interacts with environments by executing interpretable code and
collaborates with users using natural language. To this end, we collect an
instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn
interactions using CodeAct. We show that it can be used with existing data to
improve models in agent-oriented tasks without compromising their general
capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with
Python interpreter and uniquely tailored to perform sophisticated tasks (e.g.,
model training) using existing libraries and autonomously self-debug.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01032" title="Abstract">arXiv:2402.01032</a> [<a href="/pdf/2402.01032" title="Download PDF">pdf</a>, <a href="/format/2402.01032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repeat After Me: Transformers are Better than State Space Models at  Copying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jelassi%2C+S">Samy Jelassi</a>, 
<a href="/search/cs?searchtype=author&query=Brandfonbrener%2C+D">David Brandfonbrener</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S+M">Sham M. Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Malach%2C+E">Eran Malach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Transformers are the dominant architecture for sequence modeling, but there
is growing interest in models that use a fixed-size latent state that does not
depend on the sequence length, which we refer to as "generalized state space
models" (GSSMs). In this paper we show that while GSSMs are promising in terms
of inference-time efficiency, they are limited compared to transformer models
on tasks that require copying from the input context. We start with a
theoretical analysis of the simple task of string copying and prove that a two
layer transformer can copy strings of exponential length while GSSMs are
fundamentally limited by their fixed-size latent state. Empirically, we find
that transformers outperform GSSMs in terms of efficiency and generalization on
synthetic tasks that require copying the context. Finally, we evaluate
pretrained large language models and find that transformer models dramatically
outperform state space models at copying and retrieving information from
context. Taken together, these results suggest a fundamental gap between
transformers and GSSMs on tasks of practical interest.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01033" title="Abstract">arXiv:2402.01033</a> [<a href="/pdf/2402.01033" title="Download PDF">pdf</a>, <a href="/format/2402.01033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Deep Learning for TDD MIMO Systems in the 6G Upper Midbands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Juseong Park</a>, 
<a href="/search/cs?searchtype=author&query=Sohrabi%2C+F">Foad Sohrabi</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Amitava Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+J+G">Jeffrey G. Andrews</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper proposes and analyzes novel deep learning methods for downlink
(DL) single-user multiple-input multiple-output (SU-MIMO) and multi-user MIMO
(MU-MIMO) systems operating in time division duplex (TDD) mode. A motivating
application is the 6G upper midbands (7-24 GHz), where the base station (BS)
antenna arrays are large, user equipment (UE) array sizes are moderate, and
theoretically optimal approaches are practically infeasible for several
reasons. To deal with uplink (UL) pilot overhead and low signal power issues,
we introduce the channel-adaptive pilot, as part of an analog channel state
information feedback mechanism. Deep neural network (DNN)-generated pilots are
used to linearly transform the UL channel matrix into lower-dimensional latent
vectors. Meanwhile, the BS employs a second DNN that processes the received UL
pilots to directly generate near-optimal DL precoders. The training is
end-to-end which exploits synergies between the two DNNs. For MU-MIMO
precoding, we propose a DNN structure inspired by theoretically optimum linear
precoding. The proposed methods are evaluated against genie-aided upper bounds
and conventional approaches, using realistic upper midband datasets. Numerical
results demonstrate the potential of our approach to achieve significantly
increased sum-rate, particularly at moderate to high signal-to-noise ratio
(SNR) and when UL pilot overhead is constrained.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01035" title="Abstract">arXiv:2402.01035</a> [<a href="/pdf/2402.01035" title="Download PDF">pdf</a>, <a href="/format/2402.01035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting the most out of your tokenizer for pre-training and domain  adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagan%2C+G">Gautier Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Synnaeve%2C+G">Gabriele Synnaeve</a>, 
<a href="/search/cs?searchtype=author&query=Rozi%C3%A8re%2C+B">Baptiste Rozi&#xe8;re</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Tokenization is an understudied and often neglected component of modern LLMs.
Most published works use a single tokenizer for all experiments, often borrowed
from another model, without performing ablations or analysis to optimize
tokenization. Moreover, the tokenizer is generally kept unchanged when
fine-tuning a base model. In this paper, we show that the size,
pre-tokenization regular expression, and training data of a tokenizer can
significantly impact the model's generation speed, effective context size,
memory usage, and downstream performance. We train specialized Byte-Pair
Encoding code tokenizers, and conduct extensive ablations on the impact of
tokenizer design on the performance of LLMs for code generation tasks such as
HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters
selection and switching the tokenizer in a pre-trained LLM. We perform our
experiments on models trained from scratch and from pre-trained models,
verifying their applicability to a wide range of use-cases. We find that when
fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of
a pre-trained LLM to obtain large gains in generation speed and effective
context size.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01037" title="Abstract">arXiv:2402.01037</a> [<a href="/pdf/2402.01037" title="Download PDF">pdf</a>, <a href="/format/2402.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wireless Information Surveillance via STAR-RIS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jafarian%2C+F">Fatemeh Jafarian</a>, 
<a href="/search/cs?searchtype=author&query=Ardebilipour%2C+M">Mehrdad Ardebilipour</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+M">Mohammadali Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Matthaiou%2C+M">Michail Matthaiou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for presentation at the IEEE WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We explore the potential of a simultaneously transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) to enhance the performance of
wireless surveillance systems. The STAR-RIS is deployed between a full-duplex
(FD) multi-antenna legitimate eavesdropper (E) and a suspicious communication
pair. It reflects the suspicious signal towards the suspicious receiver (SR),
while simultaneously transmitting the same signal to E for interception
purposes. Additionally, it enables the forwarding of a jamming signal from E to
SR, which is located on the back side of the STAR-RIS. To enhance the
eavesdropping non-outage probability, we formulate a non-convex joint
optimization problem to design the beamforming vectors at E and
reflection/transmission phase shift matrices at the STAR-RIS. We adopt the
block coordinate descent (BCD) algorithm and propose an approach, mainly based
on semi-definite relaxation (SDR) and successive convex approximation (SCA),
for solving the resulting decoupled sub-problems. Finally, we compare the
performance of the proposed design against low-complexity zero-forcing
(ZF)-based beamforming designs.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01040" title="Abstract">arXiv:2402.01040</a> [<a href="/pdf/2402.01040" title="Download PDF">pdf</a>, <a href="/ps/2402.01040" title="Download PostScript">ps</a>, <a href="/format/2402.01040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everyday Uses of Music Listening and Music Technologies by Caregivers  and People with Dementia: Survey and Focus Group Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidas%2C+D">Dianna Vidas</a>, 
<a href="/search/cs?searchtype=author&query=Carrasco%2C+R">Romina Carrasco</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+R+M">Ryan M. Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Waycott%2C+J">Jenny Waycott</a>, 
<a href="/search/cs?searchtype=author&query=Tamplin%2C+J">Jeanette Tamplin</a>, 
<a href="/search/cs?searchtype=author&query=McMahon%2C+K">Kate McMahon</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+L+M">Libby M. Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Stretton-Smith%2C+P+A">Phoebe A. Stretton-Smith</a>, 
<a href="/search/cs?searchtype=author&query=Sousa%2C+T+V">Tanara Vieira Sousa</a>, 
<a href="/search/cs?searchtype=author&query=Baker%2C+F+A">Felicity A. Baker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Music is a valuable non-pharmacological tool that provides benefits for
people with dementia, and there is interest in designing technologies to
support music use in dementia care. To ensure music technologies are
appropriately designed for supporting caregivers and people living with
dementia, there remains a need to better understand how music is currently used
in everyday care at home. We aimed to understand how people with dementia and
their caregivers use music technologies in everyday caring, as well as
challenges they experience using music and technology. This study used a mixed
methods design. A survey was completed by 77 caregivers and people with
dementia to understand their use of music and technology. Of these, 18 survey
respondents (12 family caregivers, 6 people living with dementia) participated
in focus groups about their experiences of using music and technology in care.
Transcripts were analysed with reflexive thematic analysis. Most survey
respondents used music often in their daily lives, reporting a range of music
technologies such as CDs, radio, and streaming. Focus groups highlighted
benefits and challenges of music technologies in everyday care. Participants
used music and music technologies to regulate mood, provide joy, facilitate
social connection, encourage reminiscence, provide continuity before and after
diagnosis, and to make caregiving easier. Challenges of using music technology
in care included difficulties staying up to date with evolving technology, and
low self-efficacy for technology use expressed by people living with dementia.
Evidently, people living with dementia and their caregivers use music
technologies to support their everyday care needs. Results suggest
opportunities to design technologies enabling easier access to music and
supporting people living with dementia with recreational and therapeutic music
listening and music-based activities.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01045" title="Abstract">arXiv:2402.01045</a> [<a href="/pdf/2402.01045" title="Download PDF">pdf</a>, <a href="/format/2402.01045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LatticeGraphNet: A two-scale graph neural operator for simulating  lattice structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Ayush Jain</a>, 
<a href="/search/cs?searchtype=author&query=Haghighat%2C+E">Ehsan Haghighat</a>, 
<a href="/search/cs?searchtype=author&query=Nelaturi%2C+S">Sai Nelaturi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This study introduces a two-scale Graph Neural Operator (GNO), namely,
LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear
finite-element simulations of three-dimensional latticed parts and structures.
LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and
LGN-ii, learning the mapping from the reduced representation onto the
tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore
the name operator. Our approach significantly reduces inference time while
maintaining high accuracy for unseen simulations, establishing the use of GNOs
as efficient surrogate models for evaluating mechanical responses of lattices
and structures.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01047" title="Abstract">arXiv:2402.01047</a> [<a href="/pdf/2402.01047" title="Download PDF">pdf</a>, <a href="/format/2402.01047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ultra Fast Transformers on FPGAs for Particle Physics Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhixing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dennis Yin</a>, 
<a href="/search/cs?searchtype=author&query=Khoda%2C+E+E">Elham E Khoda</a>, 
<a href="/search/cs?searchtype=author&query=Loncar%2C+V">Vladimir Loncar</a>, 
<a href="/search/cs?searchtype=author&query=Govorkova%2C+E">Ekaterina Govorkova</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+E">Eric Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+P">Philip Harris</a>, 
<a href="/search/cs?searchtype=author&query=Hauck%2C+S">Scott Hauck</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+S">Shih-Chieh Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning and the Physical Sciences Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">This work introduces a highly efficient implementation of the transformer
architecture on a Field-Programmable Gate Array (FPGA) by using the
\texttt{hls4ml} tool. Given the demonstrated effectiveness of transformer
models in addressing a wide range of problems, their application in
experimental triggers within particle physics becomes a subject of significant
interest. In this work, we have implemented critical components of a
transformer model, such as multi-head attention and softmax layers. To evaluate
the effectiveness of our implementation, we have focused on a particle physics
jet flavor tagging problem, employing a public dataset. We recorded latency
under 2 $\mu$s on the Xilinx UltraScale+ FPGA, which is compatible with
hardware trigger requirements at the CERN Large Hadron Collider experiments.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01049" title="Abstract">arXiv:2402.01049</a> [<a href="/pdf/2402.01049" title="Download PDF">pdf</a>, <a href="/format/2402.01049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based  Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Zikang Leng</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Amitrajit Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Rajasekhar%2C+H">Hrudhai Rajasekhar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bruda%2C+E">Elizabeth Bruda</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Hyeokhyen Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Pl%C3%B6tz%2C+T">Thomas Pl&#xf6;tz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">One of the primary challenges in the field of human activity recognition
(HAR) is the lack of large labeled datasets. This hinders the development of
robust and generalizable models. Recently, cross modality transfer approaches
have been explored that can alleviate the problem of data scarcity. These
approaches convert existing datasets from a source modality, such as video, to
a target modality (IMU). With the emergence of generative AI models such as
large language models (LLMs) and text-driven motion synthesis models, language
has become a promising source data modality as well as shown in proof of
concepts such as IMUGPT. In this work, we conduct a large-scale evaluation of
language-based cross modality transfer to determine their effectiveness for
HAR. Based on this study, we introduce two new extensions for IMUGPT that
enhance its use for practical HAR application scenarios: a motion filter
capable of filtering out irrelevant motion sequences to ensure the relevance of
the generated virtual IMU data, and a set of metrics that measure the diversity
of the generated data facilitating the determination of when to stop generating
virtual IMU data for both effective and efficient processing. We demonstrate
that our diversity metrics can reduce the effort needed for the generation of
virtual IMU data by at least 50%, which open up IMUGPT for practical use cases
beyond a mere proof of concept.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01051" title="Abstract">arXiv:2402.01051</a> [<a href="/pdf/2402.01051" title="Download PDF">pdf</a>, <a href="/format/2402.01051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generation, Distillation and Evaluation of Motivational  Interviewing-Style Reflections with a Foundational Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+A">Andrew Brown</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiading Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Abdelwahab%2C+M">Mohamed Abdelwahab</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+A">Alec Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cindy Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rose%2C+J">Jonathan Rose</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Foundational Language Models are capable of performing many tasks at a
high level but are difficult to deploy in many applications because of their
size and proprietary ownership. Many will be motivated to distill specific
capabilities of foundational models into smaller models that can be owned and
controlled. In the development of a therapeutic chatbot, we wish to distill a
capability known as reflective listening, in which a therapist produces
reflections of client speech. These reflections either restate what a client
has said, or connect what was said to a relevant observation, idea or guess
that encourages and guides the client to continue contemplation. In this paper,
we present a method for distilling the generation of reflections from a
Foundational Language Model (GPT-4) into smaller models. We first show that
GPT-4, using zero-shot prompting, can generate reflections at near 100% success
rate, superior to all previous methods. Using reflections generated by GPT-4,
we fine-tune different sizes of the GPT-2 family. The GPT-2-small model
achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90%
success. We also show that GPT-4 can help in the labor-intensive task of
evaluating the quality of the distilled models, using it as a zero-shot
classifier. Using triple-human review as a guide, the classifier achieves a
Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01053" title="Abstract">arXiv:2402.01053</a> [<a href="/pdf/2402.01053" title="Download PDF">pdf</a>, <a href="/format/2402.01053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plan-Grounded Large Language Models for Dual Goal Conversational  Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gl%C3%B3ria-Silva%2C+D">Diogo Gl&#xf3;ria-Silva</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+R">Rafael Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+D">Diogo Tavares</a>, 
<a href="/search/cs?searchtype=author&query=Semedo%2C+D">David Semedo</a>, 
<a href="/search/cs?searchtype=author&query=Magalh%C3%A3es%2C+J">Jo&#xe3;o Magalh&#xe3;es</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Training Large Language Models (LLMs) to follow user instructions has been
shown to supply the LLM with ample capacity to converse fluently while being
aligned with humans. Yet, it is not completely clear how an LLM can lead a
plan-grounded conversation in mixed-initiative settings where instructions flow
in both directions of the conversation, i.e. both the LLM and the user provide
instructions to one another. In this paper, we tackle a dual goal
mixed-initiative conversational setting where the LLM not only grounds the
conversation on an arbitrary plan but also seeks to satisfy both a procedural
plan and user instructions. The LLM is then responsible for guiding the user
through the plan and, at the same time, adapting to new circumstances,
answering questions, and activating safety guardrails when needed. We propose a
novel LLM that grounds the dialogue on a procedural plan, can take the dialogue
initiative, and enforces guardrails on the system's behavior, while also
improving the LLM's responses to unexpected user behavior. Experiments in
controlled settings and with real users show that the best-performing model,
which we call PlanLLM, achieves a 2.1x improvement over a strong baseline.
Moreover, experiments also show good generalization to unseen domains.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01055" title="Abstract">arXiv:2402.01055</a> [<a href="/pdf/2402.01055" title="Download PDF">pdf</a>, <a href="/format/2402.01055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiclass Learning from Noisy Labels for Non-decomposable Performance  Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Shivani Agarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">There has been much interest in recent years in learning good classifiers
from data with noisy labels. Most work on learning from noisy labels has
focused on standard loss-based performance measures. However, many machine
learning problems require using non-decomposable performance measures which
cannot be expressed as the expectation or sum of a loss on individual examples;
these include for example the H-mean, Q-mean and G-mean in class imbalance
settings, and the Micro $F_1$ in information retrieval. In this paper, we
design algorithms to learn from noisy labels for two broad classes of
multiclass non-decomposable performance measures, namely, monotonic convex and
ratio-of-linear, which encompass all the above examples. Our work builds on the
Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both
cases, we develop noise-corrected versions of the algorithms under the widely
studied family of class-conditional noise models. We provide regret (excess
risk) bounds for our algorithms, establishing that even though they are trained
on noisy data, they are Bayes consistent in the sense that their performance
converges to the optimal performance w.r.t. the clean (non-noisy) distribution.
Our experiments demonstrate the effectiveness of our algorithms in handling
label noise.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01057" title="Abstract">arXiv:2402.01057</a> [<a href="/pdf/2402.01057" title="Download PDF">pdf</a>, <a href="/format/2402.01057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expert Proximity as Surrogate Rewards for Single Demonstration Imitation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Chia-Cheng Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+L">Li-Cheng Lan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei-Fang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chien Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chun-Yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we focus on single-demonstration imitation learning (IL), a
practical approach for real-world applications where obtaining numerous expert
demonstrations is costly or infeasible. In contrast to typical IL settings with
multiple demonstrations, single-demonstration IL involves an agent having
access to only one expert trajectory. We highlight the issue of sparse reward
signals in this setting and propose to mitigate this issue through our proposed
Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed
to address reward sparsity by introducing a denser surrogate reward function
that considers environmental dynamics. This surrogate reward function
encourages the agent to navigate towards states that are proximal to expert
states. In practice, TDIL trains a transition discriminator to differentiate
between valid and non-valid transitions in a given environment to compute the
surrogate rewards. The experiments demonstrate that TDIL outperforms existing
IL approaches and achieves expert-level performance in the single-demonstration
IL setting across five widely adopted MuJoCo benchmarks as well as the "Adroit
Door" environment.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01058" title="Abstract">arXiv:2402.01058</a> [<a href="/pdf/2402.01058" title="Download PDF">pdf</a>, <a href="/ps/2402.01058" title="Download PostScript">ps</a>, <a href="/format/2402.01058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Algebraic Framework For Approximating Functions Using Neural  Network Polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafi%2C+S">Shakil Rafi</a>, 
<a href="/search/cs?searchtype=author&query=Padgett%2C+J+L">Joshua Lee Padgett</a>, 
<a href="/search/cs?searchtype=author&query=Nakarmi%2C+U">Ukash Nakarmi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Combinatorics (math.CO); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We make the case for neural network objects and extend an already existing
neural network calculus explained in detail in Chapter 2 on \cite{bigbook}. Our
aim will be to show that, yes, indeed, it makes sense to talk about neural
network polynomials, neural network exponentials, sine, and cosines in the
sense that they do indeed approximate their real number counterparts subject to
limitations on certain of their parameters, $q$, and $\varepsilon$. While doing
this, we show that the parameter and depth growth are only polynomial on their
desired accuracy (defined as a 1-norm difference over $\mathbb{R}$), thereby
showing that this approach to approximating, where a neural network in some
sense has the structural properties of the function it is approximating is not
entire intractable.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01059" title="Abstract">arXiv:2402.01059</a> [<a href="/pdf/2402.01059" title="Download PDF">pdf</a>, <a href="/format/2402.01059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eco-driving under localization uncertainty for connected vehicles on  Urban roads: Data-driven approach and Experiment verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Joa%2C+E">Eunhyek Joa</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+E+Y">Eric Yongkeun Choi</a>, 
<a href="/search/eess?searchtype=author&query=Borrelli%2C+F">Francesco Borrelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE IV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper addresses the eco-driving problem for connected vehicles on urban
roads, considering localization uncertainty. Eco-driving is defined as
longitudinal speed planning and control on roads with the presence of a
sequence of traffic lights. We solve the problem by using a data-driven model
predictive control (MPC) strategy. This approach involves learning a cost-to-go
function and constraints from state-input data. The cost-to-go function
represents the remaining energy-to-spend from the given state, and the
constraints ensure that the controlled vehicle passes the upcoming traffic
light timely while obeying traffic laws. The resulting convex optimization
problem has a short horizon and is amenable for real-time implementations. We
demonstrate the effectiveness of our approach through real-world vehicle
experiments. Our method demonstrates $12\%$ improvement in energy efficiency
compared to the traditional approaches, which plan longitudinal speed by
solving a long-horizon optimal control problem and track the planned speed
using another controller, as evidenced by vehicle experiments.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01062" title="Abstract">arXiv:2402.01062</a> [<a href="/pdf/2402.01062" title="Download PDF">pdf</a>, <a href="/ps/2402.01062" title="Download PostScript">ps</a>, <a href="/format/2402.01062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic  Propulsors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M+L">Meredith L. Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Scherl%2C+I">Isabel Scherl</a>, 
<a href="/search/cs?searchtype=author&query=Gharib%2C+M">Morteza Gharib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To maintain full autonomy, autonomous robotic systems must have the ability
to self-repair. Self-repairing via compensatory mechanisms appears in nature:
for example, some fish can lose even 76% of their propulsive surface without
loss of thrust by altering stroke mechanics. However, direct transference of
these alterations from an organism to a robotic flapping propulsor may not be
optimal due to irrelevant evolutionary pressures. We instead seek to determine
what alterations to stroke mechanics are optimal for a damaged robotic system
via artificial evolution. To determine whether natural and machine-learned
optima differ, we employ a cyber-physical system using a Covariance Matrix
Adaptation Evolutionary Strategy to seek the most efficient trajectory for a
given force. We implement an online optimization with hardware-in-the-loop,
performing experimental function evaluations with an actuated flexible flat
plate. To recoup thrust production following partial amputation, the most
efficient learned strategy was to increase amplitude, increase frequency,
increase the amplitude of angle of attack, and phase shift the angle of attack
by approximately 110 degrees. In fish, only an amplitude increase is reported
by majority in the literature. To recoup side-force production, a more
challenging optimization landscape is encountered. Nesting of optimal angle of
attack traces is found in the resultant-based reference frame, but no clear
trend in amplitude or frequency are exhibited -- in contrast to the increase in
frequency reported in insect literature. These results suggest that how
mechanical flapping propulsors most efficiently adjust to damage of a flapping
propulsor may not align with natural swimmers and flyers.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01064" title="Abstract">arXiv:2402.01064</a> [<a href="/pdf/2402.01064" title="Download PDF">pdf</a>, <a href="/format/2402.01064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-Aware and Goal-Oriented Communications for Object Detection in  Wireless End-to-End Image Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Safaeipour%2C+F+Z">Fatemeh Zahra Safaeipour</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Morteza Hashemi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computing, Networking and Communications (ICNC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Semantic communication is focused on optimizing the exchange of information
by transmitting only the most relevant data required to convey the intended
message to the receiver and achieve the desired communication goal. For
example, if we consider images as the information and the goal of the
communication is object detection at the receiver side, the semantic of
information would be the objects in each image. Therefore, by only transferring
the semantics of images we can achieve the communication goal. In this paper,
we propose a design framework for implementing semantic-aware and goal-oriented
communication of images. To achieve this, we first define the baseline problem
as a set of mathematical problems that can be optimized to improve the
efficiency and effectiveness of the communication system. We consider two
scenarios in which either the data rate or the error at the receiver is the
limiting constraint. Our proposed system model and solution is inspired by the
concept of auto-encoders, where the encoder and the decoder are respectively
implemented at the transmitter and receiver to extract semantic information for
specific object detection goals. Our numerical results validate the proposed
design framework to achieve low error or near-optimal in a goal-oriented
communication system while reducing the amount of data transfers.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01065" title="Abstract">arXiv:2402.01065</a> [<a href="/pdf/2402.01065" title="Download PDF">pdf</a>, <a href="/format/2402.01065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Methodology for Large Language Models for Multilingual  Document Question and Answer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kahana%2C+A">Adar Kahana</a>, 
<a href="/search/cs?searchtype=author&query=Mathew%2C+J+S">Jaya Susan Mathew</a>, 
<a href="/search/cs?searchtype=author&query=Bleik%2C+S">Said Bleik</a>, 
<a href="/search/cs?searchtype=author&query=Reynolds%2C+J">Jeremy Reynolds</a>, 
<a href="/search/cs?searchtype=author&query=Elisha%2C+O">Oren Elisha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the widespread adoption of Large Language Models (LLMs), in this paper
we investigate the multilingual capability of these models. Our preliminary
results show that, translating the native language context, question and answer
into a high resource language produced the best results.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01070" title="Abstract">arXiv:2402.01070</a> [<a href="/pdf/2402.01070" title="Download PDF">pdf</a>, <a href="/format/2402.01070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via  Weight Shift Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jungwon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+C">Chunming Rong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minhoe Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) offers a compelling method for training machine
learning models with a focus on preserving data privacy. The presence of system
heterogeneity and statistical heterogeneity, recognized challenges in FL,
arises from the diversity of client hardware, network, and dataset
distribution. This diversity can critically affect the training pace and the
performance of models. While many studies address either system or statistical
heterogeneity by introducing communication-efficient or stable convergence
algorithms, addressing these challenges in isolation often leads to compromises
due to unaddressed heterogeneity. In response, this paper introduces FedShift,
a novel algorithm designed to enhance both the training speed and the models'
accuracy in a dual heterogeneity scenario. Our solution can improve client
engagement through quantization and mitigate the adverse effects on performance
typically associated with quantization by employing a shifting technique. This
technique has proven to enhance accuracy by an average of 3.9% in diverse
heterogeneity environments.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01071" title="Abstract">arXiv:2402.01071</a> [<a href="/pdf/2402.01071" title="Download PDF">pdf</a>, <a href="/format/2402.01071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chameleon: Foundation Models for Fairness-aware Multi-modal Data  Augmentation to Enhance Coverage of Minorities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erfanian%2C+M">Mahdi Erfanian</a>, 
<a href="/search/cs?searchtype=author&query=Jagadish%2C+H+V">H. V. Jagadish</a>, 
<a href="/search/cs?searchtype=author&query=Asudeh%2C+A">Abolfazl Asudeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Databases (cs.DB)

</div>
<p class="mathjax">The potential harms of the under-representation of minorities in training
data, particularly in multi-modal settings, is a well-recognized concern. While
there has been extensive effort in detecting such under-representation,
resolution has remained a challenge. With recent advancements in generative AI,
large language models and foundation models have emerged as versatile tools
across various domains. In this paper, we propose Chameleon, a system that
efficiently utilizes these tools to augment a data set with a minimal addition
of synthetically generated tuples, in order to enhance the coverage of the
under-represented groups. Our system follows a rejection sampling approach to
ensure the generated tuples have a high quality and follow the underlying
distribution. In order to minimize the rejection chance of the generated
tuples, we propose multiple strategies for providing a guide for the foundation
model. Our experiment results, in addition to confirming the efficiency of our
proposed algorithms, illustrate the effectiveness of our approach, as the
unfairness of the model in a downstream task significantly dropped after data
repair using Chameleon.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01074" title="Abstract">arXiv:2402.01074</a> [<a href="/pdf/2402.01074" title="Download PDF">pdf</a>, <a href="/format/2402.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Models and Algorithms for Sensorimotor Control of an Octopus Arm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tixian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Halder%2C+U">Udit Halder</a>, 
<a href="/search/eess?searchtype=author&query=Gribkova%2C+E">Ekaterina Gribkova</a>, 
<a href="/search/eess?searchtype=author&query=Gillette%2C+R">Rhanor Gillette</a>, 
<a href="/search/eess?searchtype=author&query=Gazzola%2C+M">Mattia Gazzola</a>, 
<a href="/search/eess?searchtype=author&query=Mehta%2C+P+G">Prashant G. Mehta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">In this article, a biophysically realistic model of a soft octopus arm with
internal musculature is presented. The modeling is motivated by experimental
observations of sensorimotor control where an arm localizes and reaches a
target. Major contributions of this article are: (i) development of models to
capture the mechanical properties of arm musculature, the electrical properties
of the arm peripheral nervous system (PNS), and the coupling of PNS with
muscular contractions; (ii) modeling the arm sensory system, including
chemosensing and proprioception; and (iii) algorithms for sensorimotor control,
which include a novel feedback neural motor control law for mimicking
target-oriented arm reaching motions, and a novel consensus algorithm for
solving sensing problems such as locating a food source from local chemical
sensory information (exogenous) and arm deformation information (endogenous).
Several analytical results, including rest-state characterization and stability
properties of the proposed sensing and motor control algorithms, are provided.
Numerical simulations demonstrate the efficacy of our approach. Qualitative
comparisons against observed arm rest shapes and target-oriented reaching
motions are also reported.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01076" title="Abstract">arXiv:2402.01076</a> [<a href="/pdf/2402.01076" title="Download PDF">pdf</a>, <a href="/format/2402.01076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoseGNN: Improving the Performance of Deep Learning Models in Adaptive  Dose-Volume Histogram Prediction through Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zehao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy
that facilitate treatment planning, dose evaluation, plan comparison and etc.
It helps to increase the ability to deliver precise and effective radiation
treatments while managing potential toxicities to healthy tissues as needed to
reduce the risk of complications. This paper extends recently disclosed
research findings presented on AAPM (AAPM 65th Annual Meeting $\&amp;$ Exhibition)
and includes necessary technique details. The objective is to design efficient
deep learning models for DVH prediction on general radiotherapy platform
equipped with high performance CBCT system, where input CT images and target
dose images to predict may have different origins, spacing and sizes. Deep
learning models widely-adopted in DVH prediction task are evaluated on the
novel radiotherapy platform, and graph neural networks (GNNs) are shown to be
the ideal architecture to construct a plug-and-play framework to improve
predictive performance of base deep learning models in the adaptive setting.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01077" title="Abstract">arXiv:2402.01077</a> [<a href="/pdf/2402.01077" title="Download PDF">pdf</a>, <a href="/ps/2402.01077" title="Download PostScript">ps</a>, <a href="/format/2402.01077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advances in Predictive Modeling with Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Muchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+A">Aofei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guanjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The development of electronic health records (EHR) systems has enabled the
collection of a vast amount of digitized patient data. However, utilizing EHR
data for predictive modeling presents several challenges due to its unique
characteristics. With the advancements in machine learning techniques, deep
learning has demonstrated its superiority in various applications, including
healthcare. This survey systematically reviews recent advances in deep
learning-based predictive models using EHR data. Specifically, we begin by
introducing the background of EHR data and providing a mathematical definition
of the predictive modeling task. We then categorize and summarize predictive
deep models from multiple perspectives. Furthermore, we present benchmarks and
toolkits relevant to predictive modeling in healthcare. Finally, we conclude
this survey by discussing open challenges and suggesting promising directions
for future research.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01078" title="Abstract">arXiv:2402.01078</a> [<a href="/pdf/2402.01078" title="Download PDF">pdf</a>, <a href="/format/2402.01078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low Acceptance Agreement Tests via Bounded-Degree Symplectic HDXs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dikstein%2C+Y">Yotam Dikstein</a>, 
<a href="/search/cs?searchtype=author&query=Dinur%2C+I">Irit Dinur</a>, 
<a href="/search/cs?searchtype=author&query=Lubotzky%2C+A">Alexander Lubotzky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2312.15325">arXiv:2312.15325</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Combinatorics (math.CO); Group Theory (math.GR)

</div>
<p class="mathjax">We solve the derandomized direct product testing question in the low
acceptance regime, by constructing new high dimensional expanders that have no
small connected covers. We show that our complexes have swap cocycle expansion,
which allows us to deduce the agreement theorem by relying on previous work.
<br />Derandomized direct product testing, also known as agreement testing, is the
following problem. Let X be a family of k-element subsets of [n] and let
$\{f_s:s\to\Sigma\}_{s\in X}$ be an ensemble of local functions, each defined
over a subset $s\subset [n]$. Suppose that we run the following so-called
agreement test: choose a random pair of sets $s_1,s_2\in X$ that intersect on
$\sqrt k$ elements, and accept if $f_{s_1},f_{s_2}$ agree on the elements in
$s_1\cap s_2$. We denote the success probability of this test by
$Agr(\{f_s\})$. Given that $Agr(\{f_s\})=\epsilon&gt;0$, is there a global
function $G:[n]\to\Sigma$ such that $f_s = G|_s$ for a non-negligible fraction
of $s\in X$ ?
<br />We construct a family X of k-subsets of $[n]$ such that $|X| = O(n)$ and such
that it satisfies the low acceptance agreement theorem. Namely,
<br />$Agr (\{f_s\}) &gt; \epsilon \; \; \longrightarrow$ there is a function
$G:[n]\to\Sigma$ such that $\Pr_s[f_s\overset{0.99}{\approx} G|_s]\geq
poly(\epsilon)$.
<br />A key idea is to replace the well-studied LSV complexes by symplectic high
dimensional expanders (HDXs). The family X is just the k-faces of the new
symplectic HDXs. The later serve our needs better since their fundamental group
satisfies the congruence subgroup property, which implies that they lack small
covers.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01079" title="Abstract">arXiv:2402.01079</a> [<a href="/pdf/2402.01079" title="Download PDF">pdf</a>, <a href="/format/2402.01079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Evidence-Based Syntactic Sugar Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=OBrien%2C+D">David OBrien</a>, 
<a href="/search/cs?searchtype=author&query=Dyer%2C+R">Robert Dyer</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+N">Tien N. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+H">Hridesh Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, to be published in ICSE'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Programming languages are essential tools for developers, and their evolution
plays a crucial role in supporting the activities of developers. One instance
of programming language evolution is the introduction of syntactic sugars,
which are additional syntax elements that provide alternative, more readable
code constructs. However, the process of designing and evolving a programming
language has traditionally been guided by anecdotal experiences and intuition.
Recent advances in tools and methodologies for mining open-source repositories
have enabled developers to make data-driven software engineering decisions. In
light of this, this paper proposes an approach for motivating data-driven
programming evolution by applying frequent subgraph mining techniques to a
large dataset of 166,827,154 open-source Java methods. The dataset is mined by
generalizing Java control-flow graphs to capture broad programming language
usages and instances of duplication. Frequent subgraphs are then extracted to
identify potentially impactful opportunities for new syntactic sugars. Our
diverse results demonstrate the benefits of the proposed technique by
identifying new syntactic sugars involving a variety of programming constructs
that could be implemented in Java, thus simplifying frequent code idioms. This
approach can potentially provide valuable insights for Java language designers,
and serve as a proof-of-concept for data-driven programming language design and
evolution.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01082" title="Abstract">arXiv:2402.01082</a> [<a href="/pdf/2402.01082" title="Download PDF">pdf</a>, <a href="/format/2402.01082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on  Learning With Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stevens%2C+S">Samuel Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+E">Emily Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cathy Li</a>, 
<a href="/search/cs?searchtype=author&query=Nolte%2C+N">Niklas Nolte</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+E">Eshika Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+F">Fran&#xe7;ois Charton</a>, 
<a href="/search/cs?searchtype=author&query=Lauter%2C+K">Kristin Lauter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (main text)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning with Errors (LWE) is a hard math problem underlying recently
standardized post-quantum cryptography (PQC) systems for key exchange and
digital signatures. Prior work proposed new machine learning (ML)-based attacks
on LWE problems with small, sparse secrets, but these attacks require millions
of LWE samples to train on and take days to recover secrets. We propose three
key methods -- better preprocessing, angular embeddings and model pre-training
-- to improve these attacks, speeding up preprocessing by $25\times$ and
improving model sample efficiency by $10\times$. We demonstrate for the first
time that pre-training improves and reduces the cost of ML attacks on LWE. Our
architecture improvements enable scaling to larger-dimension LWE problems: this
work is the first instance of ML attacks recovering sparse binary secrets in
dimension $n=1024$, the smallest dimension used in practice for homomorphic
encryption applications of LWE where sparse binary secrets are proposed.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01084" title="Abstract">arXiv:2402.01084</a> [<a href="/pdf/2402.01084" title="Download PDF">pdf</a>, <a href="/format/2402.01084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness and efficiency trade-off in two-sided matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sung-Ho Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+K">Kei Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kiki Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kwei-guu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhaohong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yahiro%2C+K">Kentaro Yahiro</a>, 
<a href="/search/cs?searchtype=author&query=Yokoo%2C+M">Makoto Yokoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The theory of two-sided matching has been extensively developed and applied
to many real-life application domains. As the theory has been applied to
increasingly diverse types of environments, researchers and practitioners have
encountered various forms of distributional constraints. As a mechanism can
handle a more general class of constraints, we can assign students more
flexibly to colleges to increase students' welfare. However, it turns out that
there exists a trade-off between students' welfare (efficiency) and fairness
(which means no student has justified envy). Furthermore, this trade-off
becomes sharper as the class of constraints becomes more general. The first
contribution of this paper is to clarify the boundary on whether a
strategyproof and fair mechanism can satisfy certain efficiency properties for
each class of constraints. Our second contribution is to establish a weaker
fairness requirement called envy-freeness up to $k$ peers (EF-$k$), which is
inspired by a similar concept used in the fair division of indivisible items.
EF-$k$ guarantees that each student has justified envy towards at most $k$
students. By varying $k$, EF-$k$ can represent different levels of fairness. We
investigate theoretical properties associated with EF-$k$. Furthermore, we
develop two contrasting strategyproof mechanisms that work for general
hereditary constraints, i.e., one mechanism can guarantee a strong efficiency
requirement, while the other can guarantee EF-$k$ for any fixed $k$. We
evaluate the performance of these mechanisms through computer simulation.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01086" title="Abstract">arXiv:2402.01086</a> [<a href="/pdf/2402.01086" title="Download PDF">pdf</a>, <a href="/format/2402.01086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sim-to-Real of Soft Robots with Learned Residual Physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junpeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Michelis%2C+M+Y">Mike Yan Michelis</a>, 
<a href="/search/cs?searchtype=author&query=Spielberg%2C+A">Andrew Spielberg</a>, 
<a href="/search/cs?searchtype=author&query=Katzschmann%2C+R+K">Robert K. Katzschmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurately modeling soft robots in simulation is computationally expensive
and commonly falls short of representing the real world. This well-known
discrepancy, known as the sim-to-real gap, can have several causes, such as
coarsely approximated geometry and material models, manufacturing defects,
viscoelasticity and plasticity, and hysteresis effects. Residual physics
networks learn from real-world data to augment a discrepant model and bring it
closer to reality. Here, we present a residual physics method for modeling soft
robots with large degrees of freedom. We train neural networks to learn a
residual term -- the modeling error between simulated and physical systems.
Concretely, the residual term is a force applied on the whole simulated mesh,
while real position data is collected with only sparse motion markers. The
physical prior of the analytical simulation provides a starting point for the
residual network, and the combined model is more informed than if physics were
learned tabula rasa. We demonstrate our method on 1) a silicone elastomeric
beam and 2) a soft pneumatic arm with hard-to-model, anisotropic fiber
reinforcements. Our method outperforms traditional system identification up to
60%. We show that residual physics need not be limited to low degrees of
freedom but can effectively bridge the sim-to-real gap for high dimensional
systems.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01088" title="Abstract">arXiv:2402.01088</a> [<a href="/pdf/2402.01088" title="Download PDF">pdf</a>, <a href="/format/2402.01088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Danger Of Arrogance: Welfare Equilibra As A Solution To Stackelberg  Self-Play In Non-Coincidental Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+J">Jake Levi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Willi%2C+T">Timon Willi</a>, 
<a href="/search/cs?searchtype=author&query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The increasing prevalence of multi-agent learning systems in society
necessitates understanding how to learn effective and safe policies in
general-sum multi-agent environments against a variety of opponents, including
self-play. General-sum learning is difficult because of non-stationary
opponents and misaligned incentives. Our first main contribution is to show
that many recent approaches to general-sum learning can be derived as
approximations to Stackelberg strategies, which suggests a framework for
developing new multi-agent learning algorithms. We then define non-coincidental
games as games in which the Stackelberg strategy profile is not a Nash
Equilibrium. This notably includes several canonical matrix games and provides
a normative theory for why existing algorithms fail in self-play in such games.
We address this problem by introducing Welfare Equilibria (WE) as a
generalisation of Stackelberg Strategies, which can recover desirable Nash
Equilibria even in non-coincidental games. Finally, we introduce Welfare
Function Search (WelFuSe) as a practical approach to finding desirable WE
against unknown opponents, which finds more mutually desirable solutions in
self-play, while preserving performance against naive learning opponents.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01091" title="Abstract">arXiv:2402.01091</a> [<a href="/pdf/2402.01091" title="Download PDF">pdf</a>, <a href="/format/2402.01091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Between the Tweets: Deciphering Ideological Stances of  Interconnected Mixed-Ideology Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Ashwin Rao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mokhberian%2C+N">Negar Mokhberian</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Recent advances in NLP have improved our ability to understand the nuanced
worldviews of online communities. Existing research focused on probing
ideological stances treats liberals and conservatives as separate groups.
However, this fails to account for the nuanced views of the organically formed
online communities and the connections between them. In this paper, we study
discussions of the 2020 U.S. election on Twitter to identify complex
interacting communities. Capitalizing on this interconnectedness, we introduce
a novel approach that harnesses message passing when finetuning language models
(LMs) to probe the nuanced ideologies of these communities. By comparing the
responses generated by LMs and real-world survey results, our method shows
higher alignment than existing baselines, highlighting the potential of using
LMs in revealing complex ideologies within and across interconnected
mixed-ideology communities.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01093" title="Abstract">arXiv:2402.01093</a> [<a href="/pdf/2402.01093" title="Download PDF">pdf</a>, <a href="/format/2402.01093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specialized Language Models with Cheap Inference from Limited Domain  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grangier%2C+D">David Grangier</a>, 
<a href="/search/cs?searchtype=author&query=Katharopoulos%2C+A">Angelos Katharopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>, 
<a href="/search/cs?searchtype=author&query=Hannun%2C+A">Awni Hannun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models have emerged as a versatile tool but are challenging to
apply to tasks lacking large inference budgets and large in-domain training
sets. This work formalizes these constraints and distinguishes four important
variables: the pretraining budget (for training before the target domain is
known), the specialization budget (for training after the target domain is
known), the inference budget, and the in-domain training set size. Across these
settings, we compare different approaches from the machine learning literature.
Limited by inference cost, we find better alternatives to the standard practice
of training very large vanilla transformer models. In particular, we show that
hyper-networks and mixture of experts have better perplexity for large
pretraining budgets, while small models trained on importance sampled datasets
are attractive for large specialization budgets.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01095" title="Abstract">arXiv:2402.01095</a> [<a href="/pdf/2402.01095" title="Download PDF">pdf</a>, <a href="/format/2402.01095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How many views does your deep neural network use for prediction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawano%2C+K">Keisuke Kawano</a>, 
<a href="/search/cs?searchtype=author&query=Kutsuna%2C+T">Takuro Kutsuna</a>, 
<a href="/search/cs?searchtype=author&query=Sano%2C+K">Keisuke Sano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">The generalization ability of Deep Neural Networks (DNNs) is still not fully
understood, despite numerous theoretical and empirical analyses. Recently,
Allen-Zhu &amp; Li (2023) introduced the concept of multi-views to explain the
generalization ability of DNNs, but their main target is ensemble or distilled
models, and no method for estimating multi-views used in a prediction of a
specific input is discussed. In this paper, we propose Minimal Sufficient Views
(MSVs), which is similar to multi-views but can be efficiently computed for
real images. MSVs is a set of minimal and distinct features in an input, each
of which preserves a model's prediction for the input. We empirically show that
there is a clear relationship between the number of MSVs and prediction
accuracy across models, including convolutional and transformer models,
suggesting that a multi-view like perspective is also important for
understanding the generalization ability of (non-ensemble or non-distilled)
DNNs.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01096" title="Abstract">arXiv:2402.01096</a> [<a href="/pdf/2402.01096" title="Download PDF">pdf</a>, <a href="/format/2402.01096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript accepted to ACM Computing Surveys
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01097" title="Abstract">arXiv:2402.01097</a> [<a href="/pdf/2402.01097" title="Download PDF">pdf</a>, <a href="/format/2402.01097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s Negotiate! A Survey of Negotiation Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuncheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Suraj Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+Z+S">Zhaleh Semnani Azad</a>, 
<a href="/search/cs?searchtype=author&query=Zukerman%2C+I">Ingrid Zukerman</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024 (findings). arXiv admin note: substantial text overlap with <a href="/abs/2212.09072">arXiv:2212.09072</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Negotiation is a crucial ability in human communication. Recently, there has
been a resurgent research interest in negotiation dialogue systems, whose goal
is to create intelligent agents that can assist people in resolving conflicts
or reaching agreements. Although there have been many explorations into
negotiation dialogue systems, a systematic review of this task has not been
performed to date. We aim to fill this gap by investigating recent studies in
the field of negotiation dialogue systems, and covering benchmarks, evaluations
and methodologies within the literature. We also discuss potential future
directions, including multi-modal, multi-party and cross-cultural negotiation
scenarios. Our goal is to provide the community with a systematic overview of
negotiation dialogue systems and to inspire future research.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01098" title="Abstract">arXiv:2402.01098</a> [<a href="/pdf/2402.01098" title="Download PDF">pdf</a>, <a href="/format/2402.01098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Deep Learning for Remaining Useful Life Estimation via Stein  Variational Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Della+Libera%2C+L">Luca Della Libera</a>, 
<a href="/search/cs?searchtype=author&query=Andreoli%2C+J">Jacopo Andreoli</a>, 
<a href="/search/cs?searchtype=author&query=Pezze%2C+D+D">Davide Dalle Pezze</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/cs?searchtype=author&query=Susto%2C+G+A">Gian Antonio Susto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">A crucial task in predictive maintenance is estimating the remaining useful
life of physical systems. In the last decade, deep learning has improved
considerably upon traditional model-based and statistical approaches in terms
of predictive performance. However, in order to optimally plan maintenance
operations, it is also important to quantify the uncertainty inherent to the
predictions. This issue can be addressed by turning standard frequentist neural
networks into Bayesian neural networks, which are naturally capable of
providing confidence intervals around the estimates. Several methods exist for
training those models. Researchers have focused mostly on parametric
variational inference and sampling-based techniques, which notoriously suffer
from limited approximation power and large computational burden, respectively.
In this work, we use Stein variational gradient descent, a recently proposed
algorithm for approximating intractable distributions that overcomes the
drawbacks of the aforementioned techniques. In particular, we show through
experimental studies on simulated run-to-failure turbofan engine degradation
data that Bayesian deep learning models trained via Stein variational gradient
descent consistently outperform with respect to convergence speed and
predictive performance both the same models trained via parametric variational
inference and their frequentist counterparts trained via backpropagation.
Furthermore, we propose a method to enhance performance based on the
uncertainty information provided by the Bayesian models. We release the source
code at https://github.com/lucadellalib/bdl-rul-svgd.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01103" title="Abstract">arXiv:2402.01103</a> [<a href="/pdf/2402.01103" title="Download PDF">pdf</a>, <a href="/format/2402.01103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Generative Modeling: A Single Model is Not All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Kaelbling%2C+L">Leslie Kaelbling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Large monolithic generative models trained on massive amounts of data have
become an increasingly dominant approach in AI research. In this paper, we
argue that we should instead construct large generative systems by composing
smaller generative models together. We show how such a compositional generative
approach enables us to learn distributions in a more data-efficient manner,
enabling generalization to parts of the data distribution unseen at training
time. We further show how this enables us to program and construct new
generative models for tasks completely unseen at training. Finally, we show
that in many cases, we can discover separate compositional components from
data.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01104" title="Abstract">arXiv:2402.01104</a> [<a href="/pdf/2402.01104" title="Download PDF">pdf</a>, <a href="/format/2402.01104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation Framework for Vehicle and Electric Scooter Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+Z">Zhitong He</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lingxi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by 26th IEEE International Conference on Intelligent Transportation Systems ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The number of shared micro-mobility services such as electric scooters
(e-scooters) has an increasing trend due to the advantages of high efficiency
and low cost in short-range travel in urban areas. However, due to the unique
characteristics of moving behavior, it is commonly seen that e-scooters may
share the road with other motor vehicles. The lack of protection may lead to
severe injury for e-scooter riders. The scenario where an e-scooter crosses an
intersection or makes a lane change while interacting with an approaching
vehicle was commonly seen in real-life traffic data. Such scenarios are
hazardous because the intention and behavior of the e-scooter may vary
significantly based on the traffic environment conditions. Furthermore, some
other vehicles may occlude the presence of the moving e-scooter, which can
result in an unexpected collision. In this paper, we propose a simulation
platform to mimic the interactions between vehicles and e-scooters. Several
traffic scenarios are studied via qualitative and quantitative analysis. The
proposed framework is shown to be valuable and efficient for the general risk
analysis for vehicle and e-scooter interactions (VEI).
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01105" title="Abstract">arXiv:2402.01105</a> [<a href="/pdf/2402.01105" title="Download PDF">pdf</a>, <a href="/format/2402.01105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey for Foundation Models in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Haoxiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+K">Kaiwen Long</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">The advent of foundation models has revolutionized the fields of natural
language processing and computer vision, paving the way for their application
in autonomous driving (AD). This survey presents a comprehensive review of more
than 40 research papers, demonstrating the role of foundation models in
enhancing AD. Large language models contribute to planning and simulation in
AD, particularly through their proficiency in reasoning, code generation and
translation. In parallel, vision foundation models are increasingly adapted for
critical tasks such as 3D object detection and tracking, as well as creating
realistic driving scenarios for simulation and testing. Multi-modal foundation
models, integrating diverse inputs, exhibit exceptional visual understanding
and spatial reasoning, crucial for end-to-end AD. This survey not only provides
a structured taxonomy, categorizing foundation models based on their modalities
and functionalities within the AD domain but also delves into the methods
employed in current research. It identifies the gaps between existing
foundation models and cutting-edge AD approaches, thereby charting future
research directions and proposing a roadmap for bridging these gaps.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01106" title="Abstract">arXiv:2402.01106</a> [<a href="/pdf/2402.01106" title="Download PDF">pdf</a>, <a href="/format/2402.01106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Which Side to Scan: Multi-View Informed Active Perception with  Side Scan Sonar for Autonomous Underwater Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sethuraman%2C+A+V">Advaith V. Sethuraman</a>, 
<a href="/search/cs?searchtype=author&query=Baldoni%2C+P">Philip Baldoni</a>, 
<a href="/search/cs?searchtype=author&query=Skinner%2C+K+A">Katherine A. Skinner</a>, 
<a href="/search/cs?searchtype=author&query=McMahon%2C+J">James McMahon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous underwater vehicles often perform surveys that capture multiple
views of targets in order to provide more information for human operators or
automatic target recognition algorithms. In this work, we address the problem
of choosing the most informative views that minimize survey time while
maximizing classifier accuracy. We introduce a novel active perception
framework for multi-view adaptive surveying and reacquisition using side scan
sonar imagery. Our framework addresses this challenge by using a graph
formulation for the adaptive survey task. We then use Graph Neural Networks
(GNNs) to both classify acquired sonar views and to choose the next best view
based on the collected data. We evaluate our method using simulated surveys in
a high-fidelity side scan sonar simulator. Our results demonstrate that our
approach is able to surpass the state-of-the-art in classification accuracy and
survey efficiency. This framework is a promising approach for more efficient
autonomous missions involving side scan sonar, such as underwater exploration,
marine archaeology, and environmental monitoring.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01107" title="Abstract">arXiv:2402.01107</a> [<a href="/pdf/2402.01107" title="Download PDF">pdf</a>, <a href="/format/2402.01107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation of Graph Algorithms with Looped Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Luca%2C+A+B">Artur Back de Luca</a>, 
<a href="/search/cs?searchtype=author&query=Fountoulakis%2C+K">Kimon Fountoulakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The execution of graph algorithms using neural networks has recently
attracted significant interest due to promising empirical progress. This
motivates further understanding of how neural networks can replicate reasoning
steps with relational data. In this work, we study the ability of transformer
networks to simulate algorithms on graphs from a theoretical perspective. The
architecture that we utilize is a looped transformer with extra attention heads
that interact with the graph. We prove by construction that this architecture
can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth-
and Depth-First Search, and Kosaraju's strongly connected components algorithm.
The width of the network does not increase with the size of the input graph,
which implies that the network can simulate the above algorithms for any graph.
Despite this property, we show that there is a limit to simulation in our
solution due to finite precision. Finally, we show a Turing Completeness result
with constant width when the extra attention heads are utilized.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01108" title="Abstract">arXiv:2402.01108</a> [<a href="/pdf/2402.01108" title="Download PDF">pdf</a>, <a href="/format/2402.01108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and  Human-Centered Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pezeshkpour%2C+P">Pouya Pezeshkpour</a>, 
<a href="/search/cs?searchtype=author&query=Kandogan%2C+E">Eser Kandogan</a>, 
<a href="/search/cs?searchtype=author&query=Bhutani%2C+N">Nikita Bhutani</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Sajjadur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T">Tom Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Hruschka%2C+E">Estevam Hruschka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Remarkable performance of large language models (LLMs) in a variety of tasks
brings forth many opportunities as well as challenges of utilizing them in
production settings. Towards practical adoption of LLMs, multi-agent systems
hold great promise to augment, integrate, and orchestrate LLMs in the larger
context of enterprise platforms that use existing proprietary data and models
to tackle complex real-world tasks. Despite the tremendous success of these
systems, current approaches rely on narrow, single-focus objectives for
optimization and evaluation, often overlooking potential constraints in
real-world scenarios, including restricted budgets, resources and time.
Furthermore, interpreting, analyzing, and debugging these systems requires
different components to be evaluated in relation to one another. This demand is
currently not feasible with existing methodologies. In this postion paper, we
introduce the concept of reasoning capacity as a unifying criterion to enable
integration of constraints during optimization and establish connections among
different components within the system, which also enable a more holistic and
comprehensive approach to evaluation. We present a formal definition of
reasoning capacity and illustrate its utility in identifying limitations within
each component of the system. We then argue how these limitations can be
addressed with a self-reflective process wherein human-feedback is used to
alleviate shortcomings in reasoning and enhance overall consistency of the
system.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01109" title="Abstract">arXiv:2402.01109</a> [<a href="/pdf/2402.01109" title="Download PDF">pdf</a>, <a href="/format/2402.01109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vaccine: Perturbation-aware Alignment for Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiansheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The new paradigm of finetuning-as-a-service introduces a new attack surface
for Large Language Models (LLMs): a few harmful data uploaded by users can
easily trick the finetuning to produce an alignment-broken model. We conduct an
empirical analysis and uncover a \textit{harmful embedding drift} phenomenon,
showing a probable cause of the alignment-broken effect. Inspired by our
findings, we propose Vaccine, a perturbation-aware alignment technique to
mitigate the security risk of users finetuning. The core idea of Vaccine is to
produce invariant hidden embeddings by progressively adding crafted
perturbation to them in the alignment phase. This enables the embeddings to
withstand harmful perturbation from un-sanitized user data in the finetuning
phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)
demonstrate that Vaccine can boost the robustness of alignment against harmful
prompts induced embedding drift while reserving reasoning ability towards
benign prompts. Our code is available at
\url{https://github.com/git-disl/Vaccine}.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01111" title="Abstract">arXiv:2402.01111</a> [<a href="/pdf/2402.01111" title="Download PDF">pdf</a>, <a href="/ps/2402.01111" title="Download PostScript">ps</a>, <a href="/format/2402.01111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Reinforcement Learning with Self-Play under Adaptivity  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Dan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of multi-agent reinforcement learning (MARL) with
adaptivity constraints -- a new problem motivated by real-world applications
where deployments of new policies are costly and the number of policy updates
must be minimized. For two-player zero-sum Markov Games, we design a (policy)
elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3
S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above,
$S$ denotes the number of states, $A,B$ are the number of actions for the two
players respectively, $H$ is the horizon and $K$ is the number of episodes.
Furthermore, we prove a batch complexity lower bound
$\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with
$\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to
logarithmic factors. As a byproduct, our techniques naturally extend to
learning bandit games and reward-free MARL within near optimal batch
complexity. To the best of our knowledge, these are the first line of results
towards understanding MARL with low adaptivity.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01114" title="Abstract">arXiv:2402.01114</a> [<a href="/pdf/2402.01114" title="Download PDF">pdf</a>, <a href="/format/2402.01114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double-Dip: Thwarting Label-Only Membership Inference Attacks with  Transfer Learning and Randomization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajabi%2C+A">Arezoo Rajabi</a>, 
<a href="/search/cs?searchtype=author&query=Pimple%2C+R">Reeya Pimple</a>, 
<a href="/search/cs?searchtype=author&query=Janardhanan%2C+A">Aiswarya Janardhanan</a>, 
<a href="/search/cs?searchtype=author&query=Asokraj%2C+S">Surudhi Asokraj</a>, 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+B">Bhaskar Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Transfer learning (TL) has been demonstrated to improve DNN model performance
when faced with a scarcity of training samples. However, the suitability of TL
as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is
unexplored. A class of privacy attacks called membership inference attacks
(MIAs) aim to determine whether a given sample belongs to the training dataset
(member) or not (nonmember). We introduce Double-Dip, a systematic empirical
study investigating the use of TL (Stage-1) combined with randomization
(Stage-2) to thwart MIAs on overfitted DNNs without degrading classification
accuracy. Our study examines the roles of shared feature space and parameter
values between source and target models, number of frozen layers, and
complexity of pretrained models. We evaluate Double-Dip on three (Target,
Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii)
(CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a)
VGG-19, (b) ResNet-18, (c) Swin-T, and (d) FaceNet. Our experiments demonstrate
that Stage-1 reduces adversary success while also significantly increasing
classification accuracy of nonmembers against an adversary with either
white-box or black-box DNN model access, attempting to carry out SOTA
label-only MIAs. After Stage-2, success of an adversary carrying out a
label-only MIA is further reduced to near 50%, bringing it closer to a random
guess and showing the effectiveness of Double-Dip. Stage-2 of Double-Dip also
achieves lower ASR and higher classification accuracy than regularization and
differential privacy-based methods.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01115" title="Abstract">arXiv:2402.01115</a> [<a href="/pdf/2402.01115" title="Download PDF">pdf</a>, <a href="/format/2402.01115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretation of Intracardiac Electrograms Through Textual  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+W+J">William Jongwon Han</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+D">Diana Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Alok%2C+A">Avi Alok</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Chaojing Duan</a>, 
<a href="/search/cs?searchtype=author&query=Rosenberg%2C+M+A">Michael A. Rosenberg</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+D">Douglas Weber</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emerson Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Understanding the irregular electrical activity of atrial fibrillation (AFib)
has been a key challenge in electrocardiography. For serious cases of AFib,
catheter ablations are performed to collect intracardiac electrograms (EGMs).
EGMs offer intricately detailed and localized electrical activity of the heart
and are an ideal modality for interpretable cardiac studies. Recent
advancements in artificial intelligence (AI) has allowed some works to utilize
deep learning frameworks to interpret EGMs during AFib. Additionally, language
models (LMs) have shown exceptional performance in being able to generalize to
unseen domains, especially in healthcare. In this study, we are the first to
leverage pretrained LMs for finetuning of EGM interpolation and AFib
classification via masked language modeling. We formulate the EGM as a textual
sequence and present competitive performances on AFib classification compared
against other representations. Lastly, we provide a comprehensive
interpretability study to provide a multi-perspective intuition of the model's
behavior, which could greatly benefit the clinical use.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01116" title="Abstract">arXiv:2402.01116</a> [<a href="/pdf/2402.01116" title="Download PDF">pdf</a>, <a href="/format/2402.01116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Multi-modal Model Predictive Control via Duality-based  Interaction Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hansung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+S+H">Siddharth H. Nair</a>, 
<a href="/search/cs?searchtype=author&query=Borrelli%2C+F">Francesco Borrelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Intelligent Vehicles Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">We propose a hierarchical architecture designed for scalable real-time Model
Predictive Control (MPC) in complex, multi-modal traffic scenarios. This
architecture comprises two key components: 1) RAID-Net, a novel attention-based
Recurrent Neural Network that predicts relevant interactions along the MPC
prediction horizon between the autonomous vehicle and the surrounding vehicles
using Lagrangian duality, and 2) a reduced Stochastic MPC problem that
eliminates irrelevant collision avoidance constraints, enhancing computational
efficiency. Our approach is demonstrated in a simulated traffic intersection
with interactive surrounding vehicles, showcasing a 12x speed-up in solving the
motion planning problem. A video demonstrating the proposed architecture in
multiple complex traffic scenarios can be found here:
https://youtu.be/-TcMeolCLWc
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01117" title="Abstract">arXiv:2402.01117</a> [<a href="/pdf/2402.01117" title="Download PDF">pdf</a>, <a href="/format/2402.01117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourreza%2C+M">Mohammadreza Pourreza</a>, 
<a href="/search/cs?searchtype=author&query=Rafiei%2C+D">Davood Rafiei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Leading models for the text-to-SQL task heavily rely on proprietary Large
Language Models (LLMs), posing concerns over data privacy. Closing the
performance gap between small open-source models and large proprietary models
is crucial to mitigate this reliance. To this end, we introduce a novel
two-stage fine-tuning approach that decomposes the task into two simpler tasks.
Through comprehensive evaluation on two large cross-domain datasets and two
small LLMs, we show that this approach improves execution accuracy by 3 to 7
percent, effectively aligning the performance of open-source models with their
proprietary counterparts.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01118" title="Abstract">arXiv:2402.01118</a> [<a href="/pdf/2402.01118" title="Download PDF">pdf</a>, <a href="/format/2402.01118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pok&#xe9;LLMon: A Human-Parity Agent for Pok&#xe9;mon Battles with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiansheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pok\'emon
battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies:
(i) In-context reinforcement learning that instantly consumes text-based
feedback derived from battles to iteratively refine the policy; (ii)
Knowledge-augmented generation that retrieves external knowledge to counteract
hallucination and enables the agent to act timely and properly; (iii)
Consistent action generation to mitigate the \textit{panic switching}
phenomenon when the agent faces a powerful opponent and wants to elude the
battle. We show that online battles against human demonstrates
\textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision
making, achieving 49\% of win rate in the Ladder competitions and 56\% of win
rate in the invited battles. Our implementation and playable battle logs are
available at: \url{https://github.com/git-disl/PokeLLMon}.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01122" title="Abstract">arXiv:2402.01122</a> [<a href="/pdf/2402.01122" title="Download PDF">pdf</a>, <a href="/format/2402.01122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Multi-Speed Dubins Motion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+J+P">James P. Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shalabh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wettergren%2C+T+A">Thomas A. Wettergren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The paper develops a novel motion model, called Generalized Multi-Speed
Dubins Motion Model (GMDM), which extends the Dubins model by considering
multiple speeds. While the Dubins model produces time-optimal paths under a
constant-speed constraint, these paths could be suboptimal if this constraint
is relaxed to include multiple speeds. This is because a constant speed results
in a large minimum turning radius, thus producing paths with longer maneuvers
and larger travel times. In contrast, multi-speed relaxation allows for slower
speed sharp turns, thus producing more direct paths with shorter maneuvers and
smaller travel times. Furthermore, the inability of the Dubins model to reduce
speed could result in fast maneuvers near obstacles, thus producing paths with
high collision risks.
<br />In this regard, GMDM provides the motion planners the ability to jointly
optimize time and risk by allowing the change of speed along the path. GMDM is
built upon the six Dubins path types considering the change of speed on path
segments. It is theoretically established that GMDM provides full reachability
of the configuration space for any speed selections. Furthermore, it is shown
that the Dubins model is a specific case of GMDM for constant speeds. The
solutions of GMDM are analytical and suitable for real-time applications. The
performance of GMDM in terms of solution quality (i.e., time/time-risk cost)
and computation time is comparatively evaluated against the existing motion
models in obstacle-free as well as obstacle-rich environments via extensive
Monte Carlo simulations. The results show that in obstacle-free environments,
GMDM produces near time-optimal paths with significantly lower travel times
than the Dubins model while having similar computation times. In obstacle-rich
environments, GMDM produces time-risk optimized paths with substantially lower
collision risks.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01123" title="Abstract">arXiv:2402.01123</a> [<a href="/pdf/2402.01123" title="Download PDF">pdf</a>, <a href="/format/2402.01123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Single Simple Patch is All You Need for AI-generated Image Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaxuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jieteng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Li Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent development of generative models unleashes the potential of
generating hyper-realistic fake images. To prevent the malicious usage of fake
images, AI-generated image detection aims to distinguish fake images from real
images. Nevertheless, existing methods usually suffer from poor
generalizability across different generators. In this work, we propose an
embarrassingly simple approach named SSP, i.e., feeding the noise pattern of a
Single Simple Patch (SSP) to a binary classifier, which could achieve 14.6%
relative improvement over the recent method on GenImage dataset. Our SSP method
is very robust and generalizable, which could serve as a simple and competitive
baseline for the future methods.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01124" title="Abstract">arXiv:2402.01124</a> [<a href="/pdf/2402.01124" title="Download PDF">pdf</a>, <a href="/format/2402.01124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransFR: Transferable Federated Recommendation with Pre-trained Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honglei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">He Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yidong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Federated recommendations (FRs), facilitating multiple local clients to
collectively learn a global model without disclosing user private data, have
emerged as a prevalent architecture for privacy-preserving recommendations. In
conventional FRs, a dominant paradigm is to utilize discrete identities to
represent users/clients and items, which are subsequently mapped to
domain-specific embeddings to participate in model training. Despite
considerable performance, we reveal three inherent limitations that can not be
ignored in federated settings, i.e., non-transferability across domains,
unavailability in cold-start settings, and potential privacy violations during
federated training. To this end, we propose a transferable federated
recommendation model with universal textual representations, TransFR, which
delicately incorporates the general capabilities empowered by pre-trained
language models and the personalized abilities by fine-tuning local private
data. Specifically, it first learns domain-agnostic representations of items by
exploiting pre-trained models with public textual corpora. To tailor for
federated recommendation, we further introduce an efficient federated
fine-tuning and a local training mechanism. This facilitates personalized local
heads for each client by utilizing their private behavior data. By
incorporating pre-training and fine-tuning within FRs, it greatly improves the
adaptation efficiency transferring to a new domain and the generalization
capacity to address cold-start issues. Through extensive experiments on several
datasets, we demonstrate that our TransFR model surpasses several
state-of-the-art FRs in terms of accuracy, transferability, and privacy.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01126" title="Abstract">arXiv:2402.01126</a> [<a href="/pdf/2402.01126" title="Download PDF">pdf</a>, <a href="/ps/2402.01126" title="Download PostScript">ps</a>, <a href="/format/2402.01126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing Objects in a Cluttered World: Computational Objectness from  Motion in Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poland%2C+D">Douglas Poland</a>, 
<a href="/search/cs?searchtype=author&query=Saini%2C+A">Amar Saini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures, plus 18 pages of Supplemental Information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Perception of the visually disjoint surfaces of our cluttered world as whole
objects, physically distinct from those overlapping them, is a cognitive
phenomenon called objectness that forms the basis of our visual perception.
Shared by all vertebrates and present at birth in humans, it enables
object-centric representation and reasoning about the visual world. We present
a computational approach to objectness that leverages motion cues and
spatio-temporal attention using a pair of supervised spatio-temporal
R(2+1)U-Nets. The first network detects motion boundaries and classifies the
pixels at those boundaries in terms of their local foreground-background sense.
This motion boundary sense (MBS) information is passed, along with a
spatio-temporal object attention cue, to an attentional surface perception
(ASP) module which infers the form of the attended object over a sequence of
frames and classifies its 'pixels' as visible or obscured. The spatial form of
the attention cue is flexible, but it must loosely track the attended object
which need not be visible. We demonstrate the ability of this simple but novel
approach to infer objectness from phenomenology without object models, and show
that it delivers robust perception of individual attended objects in cluttered
scenes, even with blur and camera shake. We show that our data diversity and
augmentation minimizes bias and facilitates transfer to real video. Finally, we
describe how this computational objectness capability can grow in
sophistication and anchor a robust modular video object perception framework.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01131" title="Abstract">arXiv:2402.01131</a> [<a href="/pdf/2402.01131" title="Download PDF">pdf</a>, <a href="/format/2402.01131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equilibrium preserving space in discontinuous Galerkin methods for  hyperbolic balance laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/math?searchtype=author&query=Xia%2C+Y">Yinhua Xia</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Y">Yan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we develop a general framework for the design of the arbitrary
high-order well-balanced discontinuous Galerkin (DG) method for hyperbolic
balance laws, including the compressible Euler equations with gravitation and
the shallow water equations with horizontal temperature gradients (referred to
as the Ripa model). Not only the hydrostatic equilibrium including the more
complicated isobaric steady state in Ripa system, but our scheme is also
well-balanced for the exact preservation of the moving equilibrium state. The
strategy adopted is to approximate the equilibrium variables in the DG
piecewise polynomial space, rather than the conservative variables, which is
pivotal in the well-balanced property. Our approach provides flexibility in
combination with any consistent numerical flux, and it is free of the reference
equilibrium state recovery and the special source term treatment. This approach
enables the construction of a well-balanced method for non-hydrostatic
equilibria in Euler systems. Extensive numerical examples such as moving or
isobaric equilibria validate the high order accuracy and exact equilibrium
preservation for various flows given by hyperbolic balance laws. With a
relatively coarse mesh, it is also possible to capture small perturbations at
or close to steady flow without numerical oscillations.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01134" title="Abstract">arXiv:2402.01134</a> [<a href="/pdf/2402.01134" title="Download PDF">pdf</a>, <a href="/format/2402.01134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zequan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianping Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qusheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bisheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated Aerial Triangulation (AAT), aiming to restore image pose and
reconstruct sparse points simultaneously, plays a pivotal role in earth
observation. With its rich research heritage spanning several decades in
photogrammetry, AAT has evolved into a fundamental process widely applied in
large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its
advancements, classic AAT methods still face challenges like low efficiency and
limited robustness. This paper introduces DeepAAT, a deep learning network
designed specifically for AAT of UAV imagery. DeepAAT considers both spatial
and spectral characteristics of imagery, enhancing its capability to resolve
erroneous matching pairs and accurately predict image poses. DeepAAT marks a
significant leap in AAT's efficiency, ensuring thorough scene coverage and
precision. Its processing speed outpaces incremental AAT methods by hundreds of
times and global AAT methods by tens of times while maintaining a comparable
level of reconstruction accuracy. Additionally, DeepAAT's scene clustering and
merging strategy facilitate rapid localization and pose determination for
large-scale UAV images, even under constrained computing resources. The
experimental results demonstrate DeepAAT's substantial improvements over
conventional AAT methods, highlighting its potential in the efficiency and
accuracy of UAV-based 3D reconstruction tasks. To benefit the photogrammetry
society, the code of DeepAAT will be released at:
https://github.com/WHU-USI3DV/DeepAAT.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01135" title="Abstract">arXiv:2402.01135</a> [<a href="/pdf/2402.01135" title="Download PDF">pdf</a>, <a href="/format/2402.01135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Agent Conversational Recommender System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiabao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Verberne%2C+S">Suzan Verberne</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Due to strong capabilities in conducting fluent, multi-turn conversations
with users, Large Language Models (LLMs) have the potential to further improve
the performance of Conversational Recommender System (CRS). Unlike the aimless
chit-chat that LLM excels at, CRS has a clear target. So it is imperative to
control the dialogue flow in the LLM to successfully recommend appropriate
items to the users. Furthermore, user feedback in CRS can assist the system in
better modeling user preferences, which has been ignored by existing studies.
However, simply prompting LLM to conduct conversational recommendation cannot
address the above two key challenges.
<br />In this paper, we propose Multi-Agent Conversational Recommender System
(MACRS) which contains two essential modules. First, we design a multi-agent
act planning framework, which can control the dialogue flow based on four
LLM-based agents. This cooperative multi-agent framework will generate various
candidate responses based on different dialogue acts and then choose the most
appropriate response as the system response, which can help MACRS plan suitable
dialogue acts. Second, we propose a user feedback-aware reflection mechanism
which leverages user feedback to reason errors made in previous turns to adjust
the dialogue act planning, and higher-level user information from implicit
semantics. We conduct extensive experiments based on user simulator to
demonstrate the effectiveness of MACRS in recommendation and user preferences
collection. Experimental results illustrate that MACRS demonstrates an
improvement in user interaction experience compared to directly using LLMs.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01140" title="Abstract">arXiv:2402.01140</a> [<a href="/pdf/2402.01140" title="Download PDF">pdf</a>, <a href="/format/2402.01140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Root Cause Analysis In Microservice Using Neural Granger Causal  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng-Ming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Ching Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuang-Da Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wen-Chih Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024 Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In recent years, microservices have gained widespread adoption in IT
operations due to their scalability, maintenance, and flexibility. However, it
becomes challenging for site reliability engineers (SREs) to pinpoint the root
cause due to the complex relationships in microservices when facing system
malfunctions. Previous research employed structured learning methods (e.g.,
PC-algorithm) to establish causal relationships and derive root causes from
causal graphs. Nevertheless, they ignored the temporal order of time series
data and failed to leverage the rich information inherent in the temporal
relationships. For instance, in cases where there is a sudden spike in CPU
utilization, it can lead to an increase in latency for other microservices.
However, in this scenario, the anomaly in CPU utilization occurs before the
latency increase, rather than simultaneously. As a result, the PC-algorithm
fails to capture such characteristics. To address these challenges, we propose
RUN, a novel approach for root cause analysis using neural Granger causal
discovery with contrastive learning. RUN enhances the backbone encoder by
integrating contextual information from time series, and leverages a time
series forecasting model to conduct neural Granger causal discovery. In
addition, RUN incorporates Pagerank with a personalization vector to
efficiently recommend the top-k root causes. Extensive experiments conducted on
the synthetic and real-world microservice-based datasets demonstrate that RUN
noticeably outperforms the state-of-the-art root cause analysis methods.
Moreover, we provide an analysis scenario for the sock-shop case to showcase
the practicality and efficacy of RUN in microservice-based applications. Our
code is publicly available at https://github.com/zmlin1998/RUN.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01143" title="Abstract">arXiv:2402.01143</a> [<a href="/pdf/2402.01143" title="Download PDF">pdf</a>, <a href="/format/2402.01143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Network Representations with Disentangled Graph Auto-Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Di Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chuanhou Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The (variational) graph auto-encoder is extensively employed for learning
representations of graph-structured data. However, the formation of real-world
graphs is a complex and heterogeneous process influenced by latent factors.
Existing encoders are fundamentally holistic, neglecting the entanglement of
latent factors. This not only makes graph analysis tasks less effective but
also makes it harder to understand and explain the representations. Learning
disentangled graph representations with (variational) graph auto-encoder poses
significant challenges, and remains largely unexplored in the existing
literature. In this article, we introduce the Disentangled Graph Auto-Encoder
(DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that
leverage generative models to learn disentangled representations. Specifically,
we first design a disentangled graph convolutional network with multi-channel
message-passing layers, as the encoder aggregating information related to each
disentangled latent factor. Subsequently, a component-wise flow is applied to
each channel to enhance the expressive capabilities of disentangled variational
graph auto-encoder. Additionally, we design a factor-wise decoder, considering
the characteristics of disentangled representations. In order to further
enhance the independence among representations, we introduce independence
constraints on mapping channels for different latent factors. Empirical
experiments on both synthetic and real-world datasets show the superiority of
our proposed method compared to several state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01144" title="Abstract">arXiv:2402.01144</a> [<a href="/pdf/2402.01144" title="Download PDF">pdf</a>, <a href="/ps/2402.01144" title="Download PostScript">ps</a>, <a href="/format/2402.01144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Construction of Evolving $k$-threshold Secret Sharing Scheme over A  Polynomial Ring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hongru Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Sian-Jheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The threshold secret sharing scheme allows the dealer to distribute the share
to every participant such that the secret is correctly recovered from a certain
amount of shares. The traditional $(k, n)$-threshold secret sharing scheme
requests that the number of participants $n$ is known in advance. In contrast,
the evolving secret sharing scheme allows that $n$ can be uncertain and even
ever-growing. In this paper, we consider the evolving secret sharing scenario.
Using the prefix codes and the properties of the polynomial ring, we propose a
brand-new construction of evolving $k$-threshold secret sharing scheme for an
$\ell$-bit secret over a polynomial ring, with correctness and perfect
security. The proposed schemes establish the connection between prefix codes
and the evolving schemes for $k\geq2$, and are also first evolving
$k$-threshold secret sharing schemes by generalizing Shamir's scheme onto a
polynomial ring. Specifically, the proposal also provides an unified
mathematical decryption for prior evolving $2$-threshold secret sharing
schemes. Besides, the analysis of the proposed schemes show that the size of
the $t$-th share is $(k-1)(\ell_t-1)+\ell$ bits, where $\ell_t$ denotes the
length of a binary prefix code of encoding integer $t$. In particular, when
$\delta$ code is chosen as the prefix code, the share size achieves
$(k-1)\lfloor\lg t\rfloor+2(k-1)\lfloor\lg ({\lfloor\lg t\rfloor+1})
\rfloor+\ell$, which improves the prior best result $(k-1)\lg t+6k^4\ell\lg{\lg
t}\cdot\lg{\lg {\lg t}}+ 7k^4\ell\lg k$, where $\lg$ denotes the binary
logarithm. When $k=2$, the proposed scheme also achieves the minimal share size
for single-bit secret, which is the same as the best known scheme.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01145" title="Abstract">arXiv:2402.01145</a> [<a href="/pdf/2402.01145" title="Download PDF">pdf</a>, <a href="/format/2402.01145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReEvo: Large Language Models as Hyper-Heuristics with Reflective  Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haoran Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiarui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Guojie Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The omnipresence of NP-hard combinatorial optimization problems (COPs)
compels domain experts to engage in trial-and-error heuristic design process.
The long-standing endeavor of design automation has gained new momentum with
the rise of large language models (LLMs). This paper introduces Language
Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages
LLMs for heuristic generation, featuring minimal manual intervention and
open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution
(ReEvo), a generic searching framework that emulates the reflective design
approach of human experts while far surpassing human capabilities with its
scalable LLM inference, Internet-scale domain knowledge, and powerful
evolutionary search. Evaluations across 12 COP settings show that 1) verbal
reflections for evolution lead to smoother fitness landscapes, explicit
inference of black-box COP settings, and better search results; 2) heuristics
generated by ReEvo in minutes can outperform state-of-the-art human designs and
neural solvers; 3) LHHs enable efficient algorithm design automation even when
challenged with black-box COPs, demonstrating its potential for complex and
novel real-world applications. Our code is available:
https://github.com/ai4co/LLM-as-HH.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01146" title="Abstract">arXiv:2402.01146</a> [<a href="/pdf/2402.01146" title="Download PDF">pdf</a>, <a href="/format/2402.01146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limited Memory Online Gradient Descent for Kernelized Pairwise Learning  with Dynamic Averaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlQuabeh%2C+H">Hilal AlQuabeh</a>, 
<a href="/search/cs?searchtype=author&query=de+Vazelhes%2C+W">William de Vazelhes</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pairwise learning, an important domain within machine learning, addresses
loss functions defined on pairs of training examples, including those in metric
learning and AUC maximization. Acknowledging the quadratic growth in
computation complexity accompanying pairwise loss as the sample size grows,
researchers have turned to online gradient descent (OGD) methods for enhanced
scalability. Recently, an OGD algorithm emerged, employing gradient computation
involving prior and most recent examples, a step that effectively reduces
algorithmic complexity to $O(T)$, with $T$ being the number of received
examples. This approach, however, confines itself to linear models while
assuming the independence of example arrivals. We introduce a lightweight OGD
algorithm that does not require the independence of examples and generalizes to
kernel pairwise learning. Our algorithm builds the gradient based on a random
example and a moving average representing the past data, which results in a
sub-linear regret bound with a complexity of $O(T)$. Furthermore, through the
integration of $O(\sqrt{T}{\log{T}})$ random Fourier features, the complexity
of kernel calculations is effectively minimized. Several experiments with
real-world datasets show that the proposed technique outperforms kernel and
linear algorithms in offline and online scenarios.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01147" title="Abstract">arXiv:2402.01147</a> [<a href="/pdf/2402.01147" title="Download PDF">pdf</a>, <a href="/format/2402.01147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Reinforcement Learning for Routing Jobs in Heterogeneous  Queueing Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jali%2C+N">Neharika Jali</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+G">Guannan Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weina Wang</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Performance (cs.PF)

</div>
<p class="mathjax">We consider the problem of efficiently routing jobs that arrive into a
central queue to a system of heterogeneous servers. Unlike homogeneous systems,
a threshold policy, that routes jobs to the slow server(s) when the queue
length exceeds a certain threshold, is known to be optimal for the
one-fast-one-slow two-server system. But an optimal policy for the multi-server
system is unknown and non-trivial to find. While Reinforcement Learning (RL)
has been recognized to have great potential for learning policies in such
cases, our problem has an exponentially large state space size, rendering
standard RL inefficient. In this work, we propose ACHQ, an efficient policy
gradient based algorithm with a low dimensional soft threshold policy
parameterization that leverages the underlying queueing structure. We provide
stationary-point convergence guarantees for the general case and despite the
low-dimensional parameterization prove that ACHQ converges to an approximate
global optimum for the special case of two servers. Simulations demonstrate an
improvement in expected response time of up to ~30% over the greedy policy that
routes to the fastest available server.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01149" title="Abstract">arXiv:2402.01149</a> [<a href="/pdf/2402.01149" title="Download PDF">pdf</a>, <a href="/format/2402.01149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scale Equalization for Multi-Level Feature Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B+J">Bum Jun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+W">Sang Woo Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks have exhibited remarkable performance in a variety of
computer vision fields, especially in semantic segmentation tasks. Their
success is often attributed to multi-level feature fusion, which enables them
to understand both global and local information from an image. However, we
found that multi-level features from parallel branches are on different scales.
The scale disequilibrium is a universal and unwanted flaw that leads to
detrimental gradient descent, thereby degrading performance in semantic
segmentation. We discover that scale disequilibrium is caused by bilinear
upsampling, which is supported by both theoretical and empirical evidence.
Based on this observation, we propose injecting scale equalizers to achieve
scale equilibrium across multi-level features after bilinear upsampling. Our
proposed scale equalizers are easy to implement, applicable to any
architecture, hyperparameter-free, implementable without requiring extra
computational cost, and guarantee scale equilibrium for any dataset.
Experiments showed that adopting scale equalizers consistently improved the
mIoU index across various target datasets, including ADE20K, PASCAL VOC 2012,
and Cityscapes, as well as various decoder choices, including UPerHead,
PSPHead, ASPPHead, SepASPPHead, and FCNHead.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01152" title="Abstract">arXiv:2402.01152</a> [<a href="/pdf/2402.01152" title="Download PDF">pdf</a>, <a href="/format/2402.01152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AccentFold: A Journey through African Accents for Zero-Shot ASR  Adaptation to Target Accents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Owodunni%2C+A+T">Abraham Toluwase Owodunni</a>, 
<a href="/search/cs?searchtype=author&query=Yadavalli%2C+A">Aditya Yadavalli</a>, 
<a href="/search/cs?searchtype=author&query=Emezue%2C+C+C">Chris Chinenye Emezue</a>, 
<a href="/search/cs?searchtype=author&query=Olatunji%2C+T">Tobi Olatunji</a>, 
<a href="/search/cs?searchtype=author&query=Mbataku%2C+C+C">Clinton C Mbataku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL Findings 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Despite advancements in speech recognition, accented speech remains
challenging. While previous approaches have focused on modeling techniques or
creating accented speech datasets, gathering sufficient data for the multitude
of accents, particularly in the African context, remains impractical due to
their sheer diversity and associated budget constraints. To address these
challenges, we propose \textit{AccentFold}, a method that exploits spatial
relationships between learned accent embeddings to improve downstream Automatic
Speech Recognition (ASR). Our exploratory analysis of speech embeddings
representing 100+ African accents reveals interesting spatial accent
relationships highlighting geographic and genealogical similarities, capturing
consistent phonological, and morphological regularities, all learned
empirically from speech. Furthermore, we discover accent relationships
previously uncharacterized by the Ethnologue. Through empirical evaluation, we
demonstrate the effectiveness of AccentFold by showing that, for
out-of-distribution (OOD) accents, sampling accent subsets for training based
on AccentFold information outperforms strong baselines a relative WER
improvement of 4.6%. AccentFold presents a promising approach for improving ASR
performance on accented speech, particularly in the context of African accents,
where data scarcity and budget constraints pose significant challenges. Our
findings emphasize the potential of leveraging linguistic relationships to
improve zero-shot ASR adaptation to target accents.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01154" title="Abstract">arXiv:2402.01154</a> [<a href="/pdf/2402.01154" title="Download PDF">pdf</a>, <a href="/format/2402.01154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Quantum-Safe Federated Learning via Homomorphic Encryption:  Learning with Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Guangfeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Shanxiang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+H">Hanxu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhiyong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper introduces a privacy-preserving distributed learning framework via
private-key homomorphic encryption. Thanks to the randomness of the
quantization of gradients, our learning with error (LWE) based encryption can
eliminate the error terms, thus avoiding the issue of error expansion in
conventional LWE-based homomorphic encryption. The proposed system allows a
large number of learning participants to engage in neural network-based deep
learning collaboratively over an honest-but-curious server, while ensuring the
cryptographic security of participants' uploaded gradients.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01155" title="Abstract">arXiv:2402.01155</a> [<a href="/pdf/2402.01155" title="Download PDF">pdf</a>, <a href="/format/2402.01155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CABINET: Content Relevance based Noise Reduction for Table Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+S">Sohan Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Changwal%2C+H">Heril Changwal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+M">Milan Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+S">Sumita Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+Y">Yaman Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Table understanding capability of Large Language Models (LLMs) has been
extensively studied through the task of question-answering (QA) over tables.
Typically, only a small part of the whole table is relevant to derive the
answer for a given question. The irrelevant parts act as noise and are
distracting information, resulting in sub-optimal performance due to the
vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content
RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to
enable LLMs to focus on relevant tabular data by suppressing extraneous
information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained
differentially with the QA LLM, that weighs the table content based on its
relevance to the input question before feeding it to the question-answering LLM
(QA LLM). To further aid the relevance scorer, CABINET employs a weakly
supervised module that generates a parsing statement describing the criteria of
rows and columns relevant to the question and highlights the content of
corresponding table cells. CABINET significantly outperforms various tabular
LLM baselines, as well as GPT3-based in-context learning methods, is more
robust to noise, maintains outperformance on tables of varying sizes, and
establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We
release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01156" title="Abstract">arXiv:2402.01156</a> [<a href="/pdf/2402.01156" title="Download PDF">pdf</a>, <a href="/format/2402.01156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Low Code Programming using Traditional vs Large  Language Model Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiachi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+T">Tingting Bi</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Ting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yutian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Low-code programming (LCP) refers to programming using models at higher
levels of abstraction, resulting in less manual and more efficient programming,
and reduced learning effort for amateur developers. Many LCP tools have rapidly
evolved and have benefited from the concepts of visual programming languages
(VPLs) and programming by demonstration (PBD). With huge increase in interest
in using large language models (LLMs) in software engineering, LLM-based LCP
has began to become increasingly important. However, the technical principles
and application scenarios of traditional approaches to LCP and LLM-based LCP
are significantly different. Understanding these key differences and
characteristics in the application of the two approaches to LCP by users is
crucial for LCP providers in improving existing and developing new LCP tools,
and in better assisting users in choosing the appropriate LCP technology. We
conducted an empirical study of both traditional LCP and LLM-based LCP. We
analyzed developers' discussions on Stack Overflow (SO) over the past three
years and then explored the similarities and differences between traditional
LCP and LLM-based LCP features and developer feedback. Our findings reveal that
while traditional LCP and LLM-based LCP share common primary usage scenarios,
they significantly differ in scope, limitations and usage throughout the
software development lifecycle, particularly during the implementation phase.
We also examine how LLMs impact and integrate with LCP, discussing the latest
technological developments in LLM-based LCP, such as its integration with VPLs
and the application of LLM Agents in software engineering.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01157" title="Abstract">arXiv:2402.01157</a> [<a href="/pdf/2402.01157" title="Download PDF">pdf</a>, <a href="/format/2402.01157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source-Free Unsupervised Domain Adaptation with Hypothesis Consolidation  of Prediction Rationale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yangyang Shu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaofeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziqin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Hengel%2C+A">Anton van den Hengel</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingqiao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Source-Free Unsupervised Domain Adaptation (SFUDA) is a challenging task
where a model needs to be adapted to a new domain without access to target
domain labels or source domain data. The primary difficulty in this task is
that the model's predictions may be inaccurate, and using these inaccurate
predictions for model adaptation can lead to misleading results. To address
this issue, this paper proposes a novel approach that considers multiple
prediction hypotheses for each sample and investigates the rationale behind
each hypothesis. By consolidating these hypothesis rationales, we identify the
most likely correct hypotheses, which we then use as a pseudo-labeled set to
support a semi-supervised learning procedure for model adaptation. To achieve
the optimal performance, we propose a three-step adaptation process: model
pre-adaptation, hypothesis consolidation, and semi-supervised learning.
Extensive experimental results demonstrate that our approach achieves
state-of-the-art performance in the SFUDA task and can be easily integrated
into existing approaches to improve their performance. The codes are available
at \url{https://github.com/GANPerf/HCPR}.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01158" title="Abstract">arXiv:2402.01158</a> [<a href="/pdf/2402.01158" title="Download PDF">pdf</a>, <a href="/format/2402.01158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Detector: Improving AI-Generated Chinese Text Detection with  Open-Source LLM Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruizhe Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Han Ma</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yaofei Duan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yanlan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songhua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+B">Baoyu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T">Tao Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 tables, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">ChatGPT and other general large language models (LLMs) have achieved
remarkable success, but they have also raised concerns about the misuse of
AI-generated texts. Existing AI-generated text detection models, such as based
on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor
out-of-domain (OOD) detection performance. In this paper, we first collected
Chinese text responses generated by human experts and 9 types of LLMs, for
which to multiple domains questions, and further created a dataset that mixed
human-written sentences and sentences polished by LLMs. We then proposed
LLM-Detector, a novel method for both document-level and sentence-level text
detection through Instruction Tuning of LLMs. Our method leverages the wealth
of knowledge LLMs acquire during pre-training, enabling them to detect the text
they generate. Instruction tuning aligns the model's responses with the user's
expected text detection tasks. Experimental results show that previous methods
struggle with sentence-level AI-generated text detection and OOD detection. In
contrast, our proposed method not only significantly outperforms baseline
methods in both sentence-level and document-level text detection but also
demonstrates strong generalization capabilities. Furthermore, since
LLM-Detector is trained based on open-source LLMs, it is easy to customize for
deployment.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01160" title="Abstract">arXiv:2402.01160</a> [<a href="/pdf/2402.01160" title="Download PDF">pdf</a>, <a href="/format/2402.01160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Truncated Non-Uniform Quantization for Distributed SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Guangfeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yuanzhang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Congduan Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">To address the communication bottleneck challenge in distributed learning,
our work introduces a novel two-stage quantization strategy designed to enhance
the communication efficiency of distributed Stochastic Gradient Descent (SGD).
The proposed method initially employs truncation to mitigate the impact of
long-tail noise, followed by a non-uniform quantization of the post-truncation
gradients based on their statistical characteristics. We provide a
comprehensive convergence analysis of the quantized distributed SGD,
establishing theoretical guarantees for its performance. Furthermore, by
minimizing the convergence error, we derive optimal closed-form solutions for
the truncation threshold and non-uniform quantization levels under given
communication constraints. Both theoretical insights and extensive experimental
evaluations demonstrate that our proposed algorithm outperforms existing
quantization schemes, striking a superior balance between communication
efficiency and convergence performance.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01162" title="Abstract">arXiv:2402.01162</a> [<a href="/pdf/2402.01162" title="Download PDF">pdf</a>, <a href="/format/2402.01162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2AFC Prompting of Large Multimodal Models for Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+X">Xiangjie Sui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuelin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuming Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While abundant research has been conducted on improving high-level visual
understanding and reasoning capabilities of large multimodal models~(LMMs),
their visual quality assessment~(IQA) ability has been relatively
under-explored. Here we take initial steps towards this goal by employing the
two-alternative forced choice~(2AFC) prompting, as 2AFC is widely regarded as
the most reliable way of collecting human opinions of visual quality.
Subsequently, the global quality score of each image estimated by a particular
LMM can be efficiently aggregated using the maximum a posterior estimation.
Meanwhile, we introduce three evaluation criteria: consistency, accuracy, and
correlation, to provide comprehensive quantifications and deeper insights into
the IQA capability of five LMMs. Extensive experiments show that existing LMMs
exhibit remarkable IQA ability on coarse-grained quality comparison, but there
is room for improvement on fine-grained quality discrimination. The proposed
dataset sheds light on the future development of IQA models based on LMMs. The
codes will be made publicly available at https://github.com/h4nwei/2AFC-LMMs.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01163" title="Abstract">arXiv:2402.01163</a> [<a href="/pdf/2402.01163" title="Download PDF">pdf</a>, <a href="/format/2402.01163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Urban Region Profiling with Adversarial Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+W">Weiliang Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Q">Qianqian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinbao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Urban region profiling is pivotal for smart cities, but mining fine-grained
semantics from noisy and incomplete urban data remains challenging. In
response, we propose a novel self-supervised graph collaborative filtering
model for urban region embedding called EUPAS. Specifically, region
heterogeneous graphs containing human mobility data, point of interests (POIs)
information, and geographic neighborhood details for each region are fed into
the model, which generates region embeddings that preserve intra-region and
inter-region dependencies through GCNs and multi-head attention. Meanwhile, we
introduce spatial perturbation augmentation to generate positive samples that
are semantically similar and spatially close to the anchor, preparing for
subsequent contrastive learning. Furthermore, adversarial training is employed
to construct an effective pretext task by generating strong positive pairs and
mining hard negative pairs for the region embeddings. Finally, we jointly
optimize supervised and self-supervised learning to encourage the model to
capture the high-level semantics of region embeddings while ignoring the noisy
and unimportant details. Extensive experiments on real-world datasets
demonstrate the superiority of our model over state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01166" title="Abstract">arXiv:2402.01166</a> [<a href="/pdf/2402.01166" title="Download PDF">pdf</a>, <a href="/format/2402.01166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on 3D Content Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yuenan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shixiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent years have witnessed remarkable advances in artificial intelligence
generated content(AIGC), with diverse input modalities, e.g., text, image,
video, audio and 3D. The 3D is the most close visual modality to real-world 3D
environment and carries enormous knowledge. The 3D content generation shows
both academic and practical values while also presenting formidable technical
challenges. This review aims to consolidate developments within the burgeoning
domain of 3D content generation. Specifically, a new taxonomy is proposed that
categorizes existing approaches into three types: 3D native generative methods,
2D prior-based 3D generative methods, and hybrid 3D generative methods. The
survey covers approximately 60 papers spanning the major techniques. Besides,
we discuss limitations of current 3D content generation techniques, and point
out open challenges as well as promising directions for future work.
Accompanied with this survey, we have established a project website where the
resources on 3D content generation research are provided. The project page is
available at https://github.com/hitcslj/Awesome-AIGC-3D.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01169" title="Abstract">arXiv:2402.01169</a> [<a href="/pdf/2402.01169" title="Download PDF">pdf</a>, <a href="/ps/2402.01169" title="Download PostScript">ps</a>, <a href="/format/2402.01169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Inference of Integer SWIN Transformer by Removing the GELU  Activation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tayaranian%2C+M">Mohammadreza Tayaranian</a>, 
<a href="/search/cs?searchtype=author&query=Mozafari%2C+S+H">Seyyed Hasan Mozafari</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+J+J">James J. Clark</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+B">Brett Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+W">Warren Gross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure. Submitted to Edge Intelligence Workshop III, an AAAI 2024 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">SWIN transformer is a prominent vision transformer model that has
state-of-the-art accuracy in image classification tasks. Despite this success,
its unique architecture causes slower inference compared with similar deep
neural networks. Integer quantization of the model is one of the methods used
to improve its inference latency. However, state-of-the-art has not been able
to fully quantize the model. In this work, we improve upon the inference
latency of the state-of-the-art methods by removing the floating-point
operations, which are associated with the GELU activation in Swin Transformer.
While previous work proposed to replace the non-integer operations with linear
approximation functions, we propose to replace GELU with ReLU activation. The
advantage of ReLU over previous methods is its low memory and computation
complexity. We use iterative knowledge distillation to compensate for the lost
accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN
transformer and show that on an RTX 4090 NVIDIA GPU we can improve the
inference latency of the quantized SWIN transformer by at least $11\%$ while
maintaining an accuracy drop of under $0.5\%$ on the ImageNet evaluation
dataset.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01172" title="Abstract">arXiv:2402.01172</a> [<a href="/pdf/2402.01172" title="Download PDF">pdf</a>, <a href="/format/2402.01172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Sequence Transduction through Dynamic Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weiting Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tongfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+G">Guanghui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+C">Heidi C. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Koehn%2C+P">Philipp Koehn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We introduce STAR (Stream Transduction with Anchor Representations), a novel
Transformer-based model designed for efficient sequence-to-sequence
transduction over streams. STAR dynamically segments input streams to create
compressed anchor representations, achieving nearly lossless compression (12x)
in Automatic Speech Recognition (ASR) and outperforming existing methods.
Moreover, STAR demonstrates superior segmentation and latency-quality
trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory
footprint, and quality.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01173" title="Abstract">arXiv:2402.01173</a> [<a href="/pdf/2402.01173" title="Download PDF">pdf</a>, <a href="/format/2402.01173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Prompt Caching via Embedding Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have achieved huge success in numerous natural
language process (NLP) tasks. However, it faces the challenge of significant
resource consumption during inference. In this paper, we aim to improve the
inference efficiency of LLMs by prompt caching, i.e., if the current prompt can
be answered by the same response of a previous prompt, one can directly utilize
that previous response without calling the LLM. Specifically, we focus on the
prediction accuracy of prompt caching for single-round question-answering tasks
via embedding similarity. The existing embeddings of prompts mostly focus on
whether two prompts are semantically similar, which is not necessarily
equivalent to whether the same response can answer them. Therefore, we propose
a distillation-based method to fine-tune the existing embeddings for better
caching prediction. Theoretically, we provide finite-sample guarantees for the
convergence of our method under different types of loss functions. Empirically,
we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where
the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.
We then fine-tune the above embedding model, which significantly improves the
AUC of caching prediction from 0.51 to 0.81. We also conduct simulations
demonstrating that our trained models achieve better caching efficiency than
the previous embedding model.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01176" title="Abstract">arXiv:2402.01176</a> [<a href="/pdf/2402.01176" title="Download PDF">pdf</a>, <a href="/format/2402.01176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing  External Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujia Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangchao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The advent of large language models (LLMs) has showcased their efficacy
across various domains, yet they often hallucinate, especially in
knowledge-intensive tasks that require external knowledge sources. To improve
factual accuracy of language models, retrieval-augmented generation (RAG) has
emerged as a popular solution. However, traditional retrieval modules often
rely on large-scale document indexes, which can be disconnected from generative
tasks. Through generative retrieval (GR) approach, language models can achieve
superior retrieval performance by directly generating relevant document
identifiers (DocIDs). However, the relationship between GR and downstream
tasks, as well as the potential of LLMs in GR, remains unexplored. In this
paper, we present a unified language model that utilizes external corpus to
handle various knowledge-intensive tasks by seamlessly integrating generative
retrieval, closed-book generation, and RAG. In order to achieve effective
retrieval and generation through a unified continuous decoding process, we
introduce the following mechanisms: (1) a ranking-oriented DocID decoding
strategy, which improves ranking ability by directly learning from a DocID
ranking list; (2) a continuous generation strategy to facilitate effective and
efficient RAG; (3) well-designed auxiliary DocID understanding tasks to enhance
the model's comprehension of DocIDs and their relevance to downstream tasks.
Our approach is evaluated on the widely used KILT benchmark using two variants
of backbone models: an encoder-decoder T5 model and a decoder-only LLM, Llama2.
Experimental results showcase the superior performance of our models in both
retrieval and downstream knowledge-intensive tasks.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01180" title="Abstract">arXiv:2402.01180</a> [<a href="/pdf/2402.01180" title="Download PDF">pdf</a>, <a href="/format/2402.01180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Extended Reality Video Transmission Optimization Based on  Frame-priority Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Guangjin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shugong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanzan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Multimedia (cs.MM); Signal Processing (eess.SP)

</div>
<p class="mathjax">Extended Reality (XR) is an important service in the 5G network and in future
6G networks. In contrast to traditional video on demand services, real-time XR
video is transmitted frame by frame, requiring low latency and being highly
sensitive to network fluctuations. In this paper, we model the quality of
experience (QoE) for real-time XR video transmission on a frame-by-frame basis.
Based on the proposed QoE model, we formulate an optimization problem that
maximizes QoE with constraints on wireless resources and long-term energy
consumption. We utilize Lyapunov optimization to transform the original problem
into a single-frame optimization problem and then allocate wireless
subchannels. We propose an adaptive XR video bitrate algorithm that employs a
Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video
bitrate selection. Through numerical results, we show that our proposed
algorithm outperforms the baseline algorithms, with the average QoE
improvements of 5.9% to 80.0%.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01181" title="Abstract">arXiv:2402.01181</a> [<a href="/pdf/2402.01181" title="Download PDF">pdf</a>, <a href="/format/2402.01181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Physically-based Simulation of Soft Bodies in Embodied  Environment for Surgical Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenya Yang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yonghao Long</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Q">Qi Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Surgical robot simulation platform plays a crucial role in enhancing training
efficiency and advancing research on robot learning. Much effort have been made
by scholars on developing open-sourced surgical robot simulators to facilitate
research. We also developed SurRoL formerly, an open-source, da Vinci Research
Kit (dVRK) compatible and interactive embodied environment for robot learning.
Despite its advancements, the simulation of soft bodies still remained a major
challenge within the open-source platforms available for surgical robotics. To
this end, we develop an interactive physically based soft body simulation
framework and integrate it to SurRoL. Specifically, we utilized a
high-performance adaptation of the Material Point Method (MPM) along with the
Neo-Hookean model to represent the deformable tissue. Lagrangian particles are
used to track the motion and deformation of the soft body throughout the
simulation and Eulerian grids are leveraged to discretize space and facilitate
the calculation of forces, velocities, and other physical quantities. We also
employed an efficient collision detection and handling strategy to simulate the
interaction between soft body and rigid tool of the surgical robot. By
employing the Taichi programming language, our implementation harnesses
parallel computing to boost simulation speed. Experimental results show that
our platform is able to simulate soft bodies efficiently with strong physical
interpretability and plausible visual effects. These new features in SurRoL
enable the efficient simulation of surgical tasks involving soft tissue
manipulation and pave the path for further investigation of surgical robot
learning. The code will be released in a new branch of SurRoL github repo.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01182" title="Abstract">arXiv:2402.01182</a> [<a href="/pdf/2402.01182" title="Download PDF">pdf</a>, <a href="/format/2402.01182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning for Few-Shot Nested Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meishan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+H">Hao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In nested Named entity recognition (NER), entities are nested with each
other, and thus requiring more data annotations to address. This leads to the
development of few-shot nested NER, where the prevalence of pretrained language
models with in-context learning (ICL) offers promising solutions. In this work,
we introduce an effective and innovative ICL framework for the setting of
few-shot nested NER. We improve the ICL prompt by devising a novel example
demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ
contrastive learning to perform three types of representation learning, in
terms of semantic similarity, boundary similarity, and label similarity, to
generate high-quality demonstration examples. Extensive experiments over three
nested NER and four flat NER datasets demonstrate the efficacy of our system.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01183" title="Abstract">arXiv:2402.01183</a> [<a href="/pdf/2402.01183" title="Download PDF">pdf</a>, <a href="/format/2402.01183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LINGO-Space: Language-Conditioned Incremental Grounding for Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dohyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+N">Nayoung Oh</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+D">Deokmin Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+D">Daehyung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We aim to solve the problem of spatially localizing composite instructions
referring to space: space grounding. Compared to current instance grounding,
space grounding is challenging due to the ill-posedness of identifying
locations referred to by discrete expressions and the compositional ambiguity
of referring expressions. Therefore, we propose a novel probabilistic
space-grounding methodology (LINGO-Space) that accurately identifies a
probabilistic distribution of space being referred to and incrementally updates
it, given subsequent referring expressions leveraging configurable polar
distributions. Our evaluations show that the estimation using polar
distributions enables a robot to ground locations successfully through $20$
table-top manipulation benchmark tests. We also show that updating the
distribution helps the grounding method accurately narrow the referring space.
We finally demonstrate the robustness of the space grounding with simulated
manipulation and real quadruped robot navigation tasks. Code and videos are
available at https://lingo-space.github.io.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01187" title="Abstract">arXiv:2402.01187</a> [<a href="/pdf/2402.01187" title="Download PDF">pdf</a>, <a href="/format/2402.01187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepBranchTracer: A Generally-Applicable Approach to Curvilinear  Structure Reconstruction Using Multi-Feature Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Ting Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nenggan Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, AAAI 2024 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Curvilinear structures, which include line-like continuous objects, are
fundamental geometrical elements in image-based applications. Reconstructing
these structures from images constitutes a pivotal research area in computer
vision. However, the complex topology and ambiguous image evidence render this
process a challenging task. In this paper, we introduce DeepBranchTracer, a
novel method that learns both external image features and internal geometric
characteristics to reconstruct curvilinear structures. Firstly, we formulate
the curvilinear structures extraction as a geometric attribute estimation
problem. Then, a curvilinear structure feature learning network is designed to
extract essential branch attributes, including the image features of centerline
and boundary, and the geometric features of direction and radius. Finally,
utilizing a multi-feature fusion tracing strategy, our model iteratively traces
the entire branch by integrating the extracted image and geometric features. We
extensively evaluated our model on both 2D and 3D datasets, demonstrating its
superior performance over existing segmentation and reconstruction methods in
terms of accuracy and continuity.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01188" title="Abstract">arXiv:2402.01188</a> [<a href="/pdf/2402.01188" title="Download PDF">pdf</a>, <a href="/format/2402.01188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yanfei Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> technical report, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual foundation models have achieved remarkable results in zero-shot image
classification and segmentation, but zero-shot change detection remains an open
problem. In this paper, we propose the segment any change models (AnyChange), a
new type of change detection model that supports zero-shot prediction and
generalization on unseen change types and data distributions. AnyChange is
built on the segment anything model (SAM) via our training-free adaptation
method, bitemporal latent matching. By revealing and exploiting intra-image and
inter-image semantic similarities in SAM's latent space, bitemporal latent
matching endows SAM with zero-shot change detection capabilities in a
training-free way. We also propose a point query mechanism to enable
AnyChange's zero-shot object-centric change detection capability. We perform
extensive experiments to confirm the effectiveness of AnyChange for zero-shot
change detection. AnyChange sets a new record on the SECOND benchmark for
unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$
score, and achieving comparable accuracy with negligible manual annotations (1
pixel per image) for supervised change detection.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01191" title="Abstract">arXiv:2402.01191</a> [<a href="/pdf/2402.01191" title="Download PDF">pdf</a>, <a href="/format/2402.01191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Generation of Pseudo Normal PET from MRI with Diffusion  Model for Epileptic Focus Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wentao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xichen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weimin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">[$^{18}$F]fluorodeoxyglucose (FDG) positron emission tomography (PET) has
emerged as a crucial tool in identifying the epileptic focus, especially in
cases where magnetic resonance imaging (MRI) diagnosis yields indeterminate
results. FDG PET can provide the metabolic information of glucose and help
identify abnormal areas that are not easily found through MRI. However, the
effectiveness of FDG PET-based assessment and diagnosis depends on the
selection of a healthy control group. The healthy control group typically
consists of healthy individuals similar to epilepsy patients in terms of age,
gender, and other aspects for providing normal FDG PET data, which will be used
as a reference for enhancing the accuracy and reliability of the epilepsy
diagnosis. However, significant challenges arise when a healthy PET control
group is unattainable. Yaakub \emph{et al.} have previously introduced a
Pix2PixGAN-based method for MRI to PET translation. This method used paired MRI
and FDG PET scans from healthy individuals for training, and produced pseudo
normal FDG PET images from patient MRIs that are subsequently used for lesion
detection. However, this approach requires a large amount of high-quality,
paired MRI and PET images from healthy control subjects, which may not always
be available. In this study, we investigated unsupervised learning methods for
unpaired MRI to PET translation for generating pseudo normal FDG PET for
epileptic focus localization. Two deep learning methods, CycleGAN and SynDiff,
were employed, and we found that diffusion-based method achieved improved
performance in accurately localizing the epileptic focus.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01195" title="Abstract">arXiv:2402.01195</a> [<a href="/pdf/2402.01195" title="Download PDF">pdf</a>, <a href="/format/2402.01195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Normalizing Flows for Active Learning of Coarse-Grained  Molecular Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schopmans%2C+H">Henrik Schopmans</a>, 
<a href="/search/cs?searchtype=author&query=Friederich%2C+P">Pascal Friederich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Efficient sampling of the Boltzmann distribution of molecular systems is a
long-standing challenge. Recently, instead of generating long molecular
dynamics simulations, generative machine learning methods such as normalizing
flows have been used to learn the Boltzmann distribution directly, without
samples. However, this approach is susceptible to mode collapse and thus often
does not explore the full configurational space. In this work, we address this
challenge by separating the problem into two levels, the fine-grained and
coarse-grained degrees of freedom. A normalizing flow conditioned on the
coarse-grained space yields a probabilistic connection between the two levels.
To explore the configurational space, we employ coarse-grained simulations with
active learning which allows us to update the flow and make all-atom potential
energy evaluations only when necessary. Using alanine dipeptide as an example,
we show that our methods obtain a speedup to molecular dynamics simulations of
approximately 15.9 to 216.2 compared to the speedup of 4.5 of the current
state-of-the-art machine learning approach.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01198" title="Abstract">arXiv:2402.01198</a> [<a href="/pdf/2402.01198" title="Download PDF">pdf</a>, <a href="/format/2402.01198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Layer Location Privacy in SIMO Communication Using Fake Paths  Injection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+D">Trong Duy Tran</a>, 
<a href="/search/cs?searchtype=author&query=Da+Costa%2C+M+F">Maxime Ferreira Da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+T">Linh Trung Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Fake path injection is an emerging paradigm for inducing privacy over
wireless networks. In this paper, fake paths are injected by the transmitter
into a SIMO multipath communication channel to preserve her physical location
from an eavesdropper. A novel statistical privacy metric is defined as the
ratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)
Cram\'er-Rao lower bound on the SIMO multipath channel parameters to assess the
privacy enhancements. Leveraging the spectral properties of generalized
Vandermonde matrices, bounds on the privacy margin of the proposed scheme are
derived. Specifically, it is shown that the privacy margin increases
quadratically in the inverse of the separation between the true and the fake
paths under Eve's perspective. Numerical simulations further showcase the
approach's benefit.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01201" title="Abstract">arXiv:2402.01201</a> [<a href="/pdf/2402.01201" title="Download PDF">pdf</a>, <a href="/format/2402.01201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Class-Incremental Learning with Prior Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Duo Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Menghan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao-Ping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To tackle the issues of catastrophic forgetting and overfitting in few-shot
class-incremental learning (FSCIL), previous work has primarily concentrated on
preserving the memory of old knowledge during the incremental phase. The role
of pre-trained model in shaping the effectiveness of incremental learning is
frequently underestimated in these studies. Therefore, to enhance the
generalization ability of the pre-trained model, we propose Learning with Prior
Knowledge (LwPK) by introducing nearly free prior knowledge from a few
unlabeled data of subsequent incremental classes. We cluster unlabeled
incremental class samples to produce pseudo-labels, then jointly train these
with labeled base class samples, effectively allocating embedding space for
both old and new class data. Experimental results indicate that LwPK
effectively enhances the model resilience against catastrophic forgetting, with
theoretical analysis based on empirical risk minimization and class distance
measurement corroborating its operational principles. The source code of LwPK
is publicly available at: \url{https://github.com/StevenJ308/LwPK}.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01202" title="Abstract">arXiv:2402.01202</a> [<a href="/pdf/2402.01202" title="Download PDF">pdf</a>, <a href="/format/2402.01202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Life span of SAT techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fleury%2C+M">Mathias Fleury</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+D">Daniela Kaufmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In this paper we take 4 different features of the SAT solver CaDiCaL, blocked
clause elimination, vivification, on-the-fly self subsumption, and increasing
the bound of variable elimination over the SAT Competitions benchmarks between
2009 and 2022. We study these features by both activating them one-by-one and
deactivating them one-by-one. We have three hypothesis regarding the
experiments: (i) disabling features is always harmful; (ii) the life span of
the techniques is limited; and (iii) features simulate each other. Our
experiments cannot confirm any of the hypothesis.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01203" title="Abstract">arXiv:2402.01203</a> [<a href="/pdf/2402.01203" title="Download PDF">pdf</a>, <a href="/format/2402.01203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured World Modeling via Semantic Vector Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi-Fu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minseung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural discrete representations are crucial components of modern neural
networks. However, their main limitation is that the primary strategies such as
VQ-VAE can only provide representations at the patch level. Therefore, one of
the main goals of representation learning, acquiring structured, semantic, and
compositional abstractions such as the color and shape of an object, remains
elusive. In this paper, we present the first approach to semantic neural
discrete representation learning. The proposed model, called Semantic
Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in
unsupervised object-centric learning to address this limitation. Specifically,
we observe that a simple approach quantizing at the object level poses a
significant challenge and propose constructing scene representations
hierarchically, from low-level discrete concept schemas to object
representations. Additionally, we suggest a novel method for structured
semantic world modeling by training a prior over these representations,
enabling the ability to generate images by sampling the semantic properties of
the objects in the scene. In experiments on various 2D and 3D object-centric
datasets, we find that our model achieves superior generation performance
compared to non-semantic vector quantization methods such as VQ-VAE and
previous object-centric generative models. Furthermore, we find that the
semantic discrete representations can solve downstream scene understanding
tasks that require reasoning about the properties of different objects in the
scene.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01204" title="Abstract">arXiv:2402.01204</a> [<a href="/pdf/2402.01204" title="Download PDF">pdf</a>, <a href="/format/2402.01204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei-Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Derek Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wen-Chih Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper list can be found at <a href="https://github.com/wwweiwei/awesome-self-supervised-learning-for-tabular-data">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Self-supervised learning (SSL) has been incorporated into many
state-of-the-art models in various domains, where SSL defines pretext tasks
based on unlabeled datasets to learn contextualized and robust representations.
Recently, SSL has been a new trend in exploring the representation learning
capability in the realm of tabular data, which is more challenging due to not
having explicit relations for learning descriptive representations. This survey
aims to systematically review and summarize the recent progress and challenges
of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal
definition of NS-TD and clarify its correlation to related studies. Then, these
approaches are categorized into three groups -- predictive learning,
contrastive learning, and hybrid learning, with their motivations and strengths
of representative methods within each direction. On top of this, application
issues of SSL4NS-TD are presented, including automatic data engineering,
cross-table transferability, and domain knowledge integration. In addition, we
elaborate on existing benchmarks and datasets for NS-TD applications to discuss
the performance of existing tabular models. Finally, we discuss the challenges
of SSL4NS-TD and provide potential directions for future research. We expect
our work to be useful in terms of encouraging more research on lowering the
barrier to entry SSL for the tabular domain and improving the foundations for
implicit tabular data.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01206" title="Abstract">arXiv:2402.01206</a> [<a href="/pdf/2402.01206" title="Download PDF">pdf</a>, <a href="/format/2402.01206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Evaluation of Weather Forecasting using Machine Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+S">Md Saydur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Tumpa%2C+F+A">Farhana Akter Tumpa</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Shazid Islam</a>, 
<a href="/search/cs?searchtype=author&query=Arabi%2C+A+A">Abul Al Arabi</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+S+B">Md Sanzid Bin Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+S+U">Md Saad Ul Haque</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Gaining a deeper understanding of weather and being able to predict its
future conduct have always been considered important endeavors for the growth
of our society. This research paper explores the advancements in understanding
and predicting nature's behavior, particularly in the context of weather
forecasting, through the application of machine learning algorithms. By
leveraging the power of machine learning, data mining, and data analysis
techniques, significant progress has been made in this field. This study
focuses on analyzing the contributions of various machine learning algorithms
in predicting precipitation and temperature patterns using a 20-year dataset
from a single weather station in Dhaka city. Algorithms such as Gradient
Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest,
Stacking Neural Network, and Stacking KNN are evaluated and compared based on
their performance metrics, including Confusion matrix measurements. The
findings highlight remarkable achievements and provide valuable insights into
their performances and features correlation.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01207" title="Abstract">arXiv:2402.01207</a> [<a href="/pdf/2402.01207" title="Download PDF">pdf</a>, <a href="/format/2402.01207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Causal Graph Discovery Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+T">Thomas Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoyin Chen</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+Y">Yash More</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V">Vedant Shah</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">We propose a novel framework that leverages LLMs for full causal graph
discovery. While previous LLM-based methods have used a pairwise query
approach, this requires a quadratic number of queries which quickly becomes
impractical for larger causal graphs. In contrast, the proposed framework uses
a breadth-first search (BFS) approach which allows it to use only a linear
number of queries. We also show that the proposed method can easily incorporate
observational data when available, to improve performance. In addition to being
more time and data-efficient, the proposed framework achieves state-of-the-art
results on real-world causal graphs of varying sizes. The results demonstrate
the effectiveness and efficiency of the proposed method in discovering causal
relationships, showcasing its potential for broad applicability in causal graph
discovery tasks across different domains.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01208" title="Abstract">arXiv:2402.01208</a> [<a href="/pdf/2402.01208" title="Download PDF">pdf</a>, <a href="/format/2402.01208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Location Agnostic Adaptive Rain Precipitation Prediction using Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Shazid Islam</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+S">Md Saydur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+S+U">Md Saad Ul Haque</a>, 
<a href="/search/cs?searchtype=author&query=Tumpa%2C+F+A">Farhana Akter Tumpa</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+S+B">Md Sanzid Bin Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Arabi%2C+A+A">Abul Al Arabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rain precipitation prediction is a challenging task as it depends on weather
and meteorological features which vary from location to location. As a result,
a prediction model that performs well at one location does not perform well at
other locations due to the distribution shifts. In addition, due to global
warming, the weather patterns are changing very rapidly year by year which
creates the possibility of ineffectiveness of those models even at the same
location as time passes. In our work, we have proposed an adaptive deep
learning-based framework in order to provide a solution to the aforementioned
challenges. Our method can generalize the model for the prediction of
precipitation for any location where the methods without adaptation fail. Our
method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a
deep neural network for predicting the precipitation of Paris, Los Angeles, and
Tokyo, respectively.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01212" title="Abstract">arXiv:2402.01212</a> [<a href="/pdf/2402.01212" title="Download PDF">pdf</a>, <a href="/format/2402.01212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSJNet: A Multi-modality Target and Semantic Awareness Joint-driven  Image Fusion Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y">Yuchan Jie</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yushen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaosong Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haishu Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modality image fusion involves integrating complementary information
from different modalities into a single image. Current methods primarily focus
on enhancing image fusion with a single advanced task such as incorporating
semantic or object-related information into the fusion process. This method
creates challenges in achieving multiple objectives simultaneously. We
introduce a target and semantic awareness joint-driven fusion network called
TSJNet. TSJNet comprises fusion, detection, and segmentation subnetworks
arranged in a series structure. It leverages object and semantically relevant
information derived from dual high-level tasks to guide the fusion network.
Additionally, We propose a local significant feature extraction module with a
double parallel branch structure to fully capture the fine-grained features of
cross-modal images and foster interaction among modalities, targets, and
segmentation information. We conducted extensive experiments on four publicly
available datasets (MSRS, M3FD, RoadScene, and LLVIP). The results demonstrate
that TSJNet can generate visually pleasing fused results, achieving an average
increase of 2.84% and 7.47% in object detection and segmentation mAP @0.5 and
mIoU, respectively, compared to the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01215" title="Abstract">arXiv:2402.01215</a> [<a href="/pdf/2402.01215" title="Download PDF">pdf</a>, <a href="/format/2402.01215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intraday Power Trading for Imbalance Markets: An Adaptive Risk-Averse  Strategy using Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruneel%2C+R">Robin Bruneel</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+M">Mathijs Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Patrinos%2C+P">Panagiotis Patrinos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Power Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Efficient markets are characterised by profit-driven participants
continuously refining their positions towards the latest insights. Margins for
profit generation are generally small, shaping a difficult landscape for
automated trading strategies. This paper introduces a novel, fully-automated
cross-border intraday (XBID) trading strategy tailored for single-price
imbalance energy markets. This strategy relies on a strategically devised
mixture model to predict future system imbalance prices, which, upon
benchmarking against several state-of-the-art models, outperforms its
counterparts across every metric. However, these models were fit to a finite
amount of training data typically causing them to perform worse on unseen data
when compared to their training set. To address this issue, a coherent risk
measure is added to the cost function to take additional uncertainties in the
prediction model into account. This paper introduces a methodology to select
the tuning parameter of this risk measure adaptively by continuously
quantifying the model accuracy on a window of recently observed data. The
performance of this strategy is validated with a simulation on the Belgian
energy market using real-time market data. The adaptive tuning approach enables
the strategy to achieve higher absolute profits with a reduced number of
trades.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01216" title="Abstract">arXiv:2402.01216</a> [<a href="/pdf/2402.01216" title="Download PDF">pdf</a>, <a href="/format/2402.01216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Commutation Design: Applied to Switched Reluctance Motors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=van+Meer%2C+M">Max van Meer</a>, 
<a href="/search/eess?searchtype=author&query=Witvoet%2C+G">Gert Witvoet</a>, 
<a href="/search/eess?searchtype=author&query=Oomen%2C+T">Tom Oomen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures. Initial version submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Switched Reluctance Motors (SRMs) are cost-effective electric actuators that
utilize magnetic reluctance to generate torque, with torque ripple arising from
unaccounted manufacturing defects in the rotor tooth geometry. This paper aims
to design a versatile, resource-efficient commutation function for accurate
closed-loop control of a range of SRMs, mitigating torque ripple despite
manufacturing variations across SRMs and individual rotor teeth. The developed
commutation function optimally distributes current between coils by leveraging
the variance in the torque-current-angle model and is designed with few
parameters for easy integration on affordable hardware. Monte Carlo simulations
and experimental results show a tracking error reduction of up to 31% and 11%,
respectively. The developed approach is beneficial for applications using a
single driver for multiple systems and those constrained by memory or modeling
effort, providing an economical solution for improved tracking performance and
reduced acoustic noise.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01217" title="Abstract">arXiv:2402.01217</a> [<a href="/pdf/2402.01217" title="Download PDF">pdf</a>, <a href="/format/2402.01217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaokun Li</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+C">Chao Gou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+G">Guang Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing
novel views. However, their reliance on dense inputs and scene-specific
optimization has limited their broader applicability. Generalizable NeRFs
(Gen-NeRF), while intended to address this, often produce blurring artifacts in
unobserved regions with sparse inputs, which are full of uncertainty. In this
paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.
We assume that NeRF's inability to effectively mitigate this uncertainty stems
from its inherent lack of generative capacity. Therefore, we innovatively
propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address
this uncertainty from a generative perspective by leveraging a distilled
diffusion prior as guidance. Specifically, to avoid model confusion caused by
directly regularizing with inconsistent samplings as in previous methods, our
approach introduces a strategy to indirectly inject the inherently missing
imagination into the learned implicit function through a diffusion-guided
latent space. Empirical evaluation across various benchmarks demonstrates the
superior performance of our approach in handling uncertainty with sparse
inputs.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01219" title="Abstract">arXiv:2402.01219</a> [<a href="/pdf/2402.01219" title="Download PDF">pdf</a>, <a href="/format/2402.01219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Code Generators for Security: Friend or Foe?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Natella%2C+R">Roberto Natella</a>, 
<a href="/search/cs?searchtype=author&query=Liguori%2C+P">Pietro Liguori</a>, 
<a href="/search/cs?searchtype=author&query=Improta%2C+C">Cristina Improta</a>, 
<a href="/search/cs?searchtype=author&query=Cukic%2C+B">Bojan Cukic</a>, 
<a href="/search/cs?searchtype=author&query=Cotroneo%2C+D">Domenico Cotroneo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset available at: <a href="https://github.com/dessertlab/violent-python">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Security &amp; Privacy, Early Access, February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Recent advances of artificial intelligence (AI) code generators are opening
new opportunities in software security research, including misuse by malicious
actors. We review use cases for AI code generators for security and introduce
an evaluation benchmark.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01220" title="Abstract">arXiv:2402.01220</a> [<a href="/pdf/2402.01220" title="Download PDF">pdf</a>, <a href="/format/2402.01220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delving into Decision-based Black-box Attacks on Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Z">Zhengyang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jingwen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kaixun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dingkang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yiting Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Semantic segmentation is a fundamental visual task that finds extensive
deployment in applications with security-sensitive considerations. Nonetheless,
recent work illustrates the adversarial vulnerability of semantic segmentation
models to white-box attacks. However, its adversarial robustness against
black-box attacks has not been fully explored. In this paper, we present the
first exploration of black-box decision-based attacks on semantic segmentation.
First, we analyze the challenges that semantic segmentation brings to
decision-based attacks through the case study. Then, to address these
challenges, we first propose a decision-based attack on semantic segmentation,
called Discrete Linear Attack (DLA). Based on random search and proxy index, we
utilize the discrete linear noises for perturbation exploration and calibration
to achieve efficient attack efficiency. We conduct adversarial robustness
evaluation on 5 models from Cityscapes and ADE20K under 8 attacks. DLA shows
its formidable power on Cityscapes by dramatically reducing PSPNet's mIoU from
an impressive 77.83% to a mere 2.14% with just 50 queries.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01223" title="Abstract">arXiv:2402.01223</a> [<a href="/pdf/2402.01223" title="Download PDF">pdf</a>, <a href="/ps/2402.01223" title="Download PostScript">ps</a>, <a href="/format/2402.01223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient $(3,3)$-isogenies on fast Kummer surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+C">Maria Corte-Real Santos</a> (UCL), 
<a href="/search/cs?searchtype=author&query=Costello%2C+C">Craig Costello</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+B">Benjamin Smith</a> (GRACE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Number Theory (math.NT)

</div>
<p class="mathjax">We give an alternative derivation of $(N,N)$-isogenies between fastKummer
surfaces which complements existing works based on the theory oftheta
functions. We use this framework to produce explicit formulae for thecase of $N
= 3$, and show that the resulting algorithms are more efficient thanall prior
$(3, 3)$-isogeny algorithms.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01226" title="Abstract">arXiv:2402.01226</a> [<a href="/pdf/2402.01226" title="Download PDF">pdf</a>, <a href="/format/2402.01226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HW-SW Optimization of DNNs for Privacy-preserving People Counting on  Low-resolution Infrared Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Risso%2C+M">Matteo Risso</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Daghero%2C+F">Francesco Daghero</a>, 
<a href="/search/cs?searchtype=author&query=Burrello%2C+A">Alessio Burrello</a>, 
<a href="/search/cs?searchtype=author&query=Mollaei%2C+S">Seyedmorteza Mollaei</a>, 
<a href="/search/cs?searchtype=author&query=Castellano%2C+M">Marco Castellano</a>, 
<a href="/search/cs?searchtype=author&query=Macii%2C+E">Enrico Macii</a>, 
<a href="/search/cs?searchtype=author&query=Poncino%2C+M">Massimo Poncino</a>, 
<a href="/search/cs?searchtype=author&query=Pagliari%2C+D+J">Daniele Jahier Pagliari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in the DATE 2024 conference IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Low-resolution infrared (IR) array sensors enable people counting
applications such as monitoring the occupancy of spaces and people flows while
preserving privacy and minimizing energy consumption. Deep Neural Networks
(DNNs) have been shown to be well-suited to process these sensor data in an
accurate and efficient manner. Nevertheless, the space of DNNs' architectures
is huge and its manual exploration is burdensome and often leads to sub-optimal
solutions. To overcome this problem, in this work, we propose a highly
automated full-stack optimization flow for DNNs that goes from neural
architecture search, mixed-precision quantization, and post-processing, down to
the realization of a new smart sensor prototype, including a Microcontroller
with a customized instruction set. Integrating these cross-layer optimizations,
we obtain a large set of Pareto-optimal solutions in the 3D-space of energy,
memory, and accuracy. Deploying such solutions on our hardware platform, we
improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x
code size reduction, and 15.38x energy reduction at iso-accuracy.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01227" title="Abstract">arXiv:2402.01227</a> [<a href="/pdf/2402.01227" title="Download PDF">pdf</a>, <a href="/format/2402.01227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAA-Net: A Sparse and Transferable Adversarial Attack for Speech  Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+X">Xin Jing</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+X">Xi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+T">Tanja Schultz</a>, 
<a href="/search/cs?searchtype=author&query=Schuller%2C+B+W">Bj&#xf6;rn W. Schuller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech contains rich information on the emotions of humans, and Speech
Emotion Recognition (SER) has been an important topic in the area of
human-computer interaction. The robustness of SER models is crucial,
particularly in privacy-sensitive and reliability-demanding domains like
private healthcare. Recently, the vulnerability of deep neural networks in the
audio domain to adversarial attacks has become a popular area of research.
However, prior works on adversarial attacks in the audio domain primarily rely
on iterative gradient-based techniques, which are time-consuming and prone to
overfitting the specific threat model. Furthermore, the exploration of sparse
perturbations, which have the potential for better stealthiness, remains
limited in the audio domain. To address these challenges, we propose a
generator-based attack method to generate sparse and transferable adversarial
examples to deceive SER models in an end-to-end and efficient manner. We
evaluate our method on two widely-used SER datasets, Database of Elicited Mood
in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP),
and demonstrate its ability to generate successful sparse adversarial examples
in an efficient manner. Moreover, our generated adversarial examples exhibit
model-agnostic transferability, enabling effective adversarial attacks on
advanced victim models.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01230" title="Abstract">arXiv:2402.01230</a> [<a href="/pdf/2402.01230" title="Download PDF">pdf</a>, <a href="/format/2402.01230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trees and co-trees in planar 3-connected planar graphs An easier proof  via Schnyder woods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortlieb%2C+C">Christian Ortlieb</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J+M">Jens M. Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Let $G$ be a 3-connected planar graph. Define the co-tree of a spanning tree
$T$ of $G$ as the graph induced by the dual edges of $E(G)-E(T)$. The
well-known cut-cycle duality implies that the co-tree is itself a tree. Let a
$k$-tree be a spanning tree with maximum degree $k$. In 1970, Gr\"unbaum
conjectured that every 3-connected planar graph contains a 3-tree whose co-tree
is also a 3-tree. In 2014, Biedl showed that every such graph contains a 5-tree
whose co-tree is a 5-tree. In this paper, we present an easier proof of Biedl's
result using Schnyder woods.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01231" title="Abstract">arXiv:2402.01231</a> [<a href="/pdf/2402.01231" title="Download PDF">pdf</a>, <a href="/format/2402.01231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Delay Effects in Traffic Forecasting: A Perspective from  Spatial-Temporal Delay Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Q">Qingqing Long</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traffic flow forecasting is a fundamental research issue for transportation
planning and management, which serves as a canonical and typical example of
spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and
Recurrent Neural Networks (RNNs) have achieved great success in capturing
spatial-temporal correlations for traffic flow forecasting. Yet, two
non-ignorable issues haven't been well solved: 1) The message passing in GNNs
is immediate, while in reality the spatial message interactions among
neighboring nodes can be delayed. The change of traffic flow at one node will
take several minutes, i.e., time delay, to influence its connected neighbors.
2) Traffic conditions undergo continuous changes. The prediction frequency for
traffic flow forecasting may vary based on specific scenario requirements. Most
existing discretized models require retraining for each prediction horizon,
restricting their applicability. To tackle the above issues, we propose a
neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It
includes both delay effects and continuity into a unified delay differential
equation framework, which explicitly models the time delay in spatial
information propagation. Furthermore, theoretical proofs are provided to show
its stability. Then we design a learnable traffic-graph time-delay estimator,
which utilizes the continuity of the hidden states to achieve the gradient
backward process. Finally, we propose a continuous output module, allowing us
to accurately predict traffic flow at various frequencies, which provides more
flexibility and adaptability to different scenarios. Extensive experiments show
the superiority of the proposed STDDE along with competitive computational
efficiency.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01232" title="Abstract">arXiv:2402.01232</a> [<a href="/pdf/2402.01232" title="Download PDF">pdf</a>, <a href="/format/2402.01232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scattering-Passive Structure-Preserving Finite Element Method for the  Boundary Controlled Transport Equation with a Moving Mesh
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Toledo-Zucco%2C+J">Jesus-Pablo Toledo-Zucco</a>, 
<a href="/search/math?searchtype=author&query=Matignon%2C+D">Denis Matignon</a>, 
<a href="/search/math?searchtype=author&query=Poussot-Vassal%2C+C">Charles Poussot-Vassal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">A structure-preserving Finite Element Method (FEM) for the transport equation
in one- and two-dimensional domains is presented. This Distributed Parameter
System (DPS) has non-collocated boundary control and observation, and reveals a
scattering-energy preserving structure. We show that the discretized model
preserves the aforementioned structure from the original infinite-dimensional
system. Moreover, we analyse the case of moving meshes for the one-dimensional
case. The moving mesh requires less states than the fixed one to produce
solutions with a comparable accuracy, and it can also reduce the overshoot and
oscillations of Gibbs phenomenon produced when using the FEM. Numerical
simulations are provided for the case of a one-dimensional transport equation
with fixed and moving meshes.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01238" title="Abstract">arXiv:2402.01238</a> [<a href="/pdf/2402.01238" title="Download PDF">pdf</a>, <a href="/format/2402.01238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Variational Information Bottleneck: Achieving Diverse  Compression with a Single Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudo%2C+S">Sota Kudo</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+N">Naoaki Ono</a>, 
<a href="/search/cs?searchtype=author&query=Kanaya%2C+S">Shigehiko Kanaya</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Ming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Information Bottleneck (IB) is a widely used framework that enables the
extraction of information related to a target random variable from a source
random variable. In the objective function, IB controls the trade-off between
data compression and predictiveness through the Lagrange multiplier $\beta$.
Traditionally, to find the trade-off to be learned, IB requires a search for
$\beta$ through multiple training cycles, which is computationally expensive.
In this study, we introduce Flexible Variational Information Bottleneck (FVIB),
an innovative framework for classification task that can obtain optimal models
for all values of $\beta$ with single, computationally efficient training. We
theoretically demonstrate that across all values of reasonable $\beta$, FVIB
can simultaneously maximize an approximation of the objective function for
Variational Information Bottleneck (VIB), the conventional IB method. Then we
empirically show that FVIB can learn the VIB objective as effectively as VIB.
Furthermore, in terms of calibration performance, FVIB outperforms other IB and
calibration methods by enabling continuous optimization of $\beta$. Our codes
are available at https://github.com/sotakudo/fvib.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01239" title="Abstract">arXiv:2402.01239</a> [<a href="/pdf/2402.01239" title="Download PDF">pdf</a>, <a href="/format/2402.01239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRIME: Protect Your Videos From Malicious Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the development of generative models, the quality of generated content
keeps increasing. Recently, open-source models have made it surprisingly easy
to manipulate and edit photos and videos, with just a few simple prompts. While
these cutting-edge technologies have gained popularity, they have also given
rise to concerns regarding the privacy and portrait rights of individuals.
Malicious users can exploit these tools for deceptive or illegal purposes.
Although some previous works focus on protecting photos against generative
models, we find there are still gaps between protecting videos and images in
the aspects of efficiency and effectiveness. Therefore, we introduce our
protection method, PRIME, to significantly reduce the time cost and improve the
protection performance. Moreover, to evaluate our proposed protection method,
we consider both objective metrics and human subjective metrics. Our evaluation
results indicate that PRIME only costs 8.3% GPU hours of the cost of the
previous state-of-the-art method and achieves better protection results on both
human evaluation and objective metrics. Code can be found in
https://github.com/GuanlinLee/prime.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01240" title="Abstract">arXiv:2402.01240</a> [<a href="/pdf/2402.01240" title="Download PDF">pdf</a>, <a href="/format/2402.01240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser  Web Tracker Classification in an Imbalanced Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rieder%2C+W">Wolf Rieder</a>, 
<a href="/search/cs?searchtype=author&query=Raschke%2C+P">Philip Raschke</a>, 
<a href="/search/cs?searchtype=author&query=Cory%2C+T">Thomas Cory</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The World Wide Web's connectivity is greatly attributed to the HTTP protocol,
with HTTP messages offering informative header fields that appeal to
disciplines like web security and privacy, especially concerning web tracking.
Despite existing research employing HTTP/S request messages to identify web
trackers, HTTP/S response headers are often overlooked. This study endeavors to
design effective machine learning classifiers for web tracker detection using
HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers,
obtained through the traffic monitoring browser extension T.EX, serves as our
data set. Eleven supervised models were trained on Chrome data and tested
across all browsers. The results demonstrated high accuracy, F1-score,
precision, recall, and minimal log-loss error for Chrome and Firefox, but
subpar performance on Brave, potentially due to its distinct data distribution
and feature set. The research suggests that these classifiers are viable for
detecting web trackers in Chrome and Firefox. However, real-world application
testing remains pending, and the distinction between tracker types and broader
label sources could be explored in future studies.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01241" title="Abstract">arXiv:2402.01241</a> [<a href="/pdf/2402.01241" title="Download PDF">pdf</a>, <a href="/format/2402.01241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D  Diffusion?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sbrolli%2C+C">Cristian Sbrolli</a>, 
<a href="/search/cs?searchtype=author&query=Cudrano%2C+P">Paolo Cudrano</a>, 
<a href="/search/cs?searchtype=author&query=Matteucci%2C+M">Matteo Matteucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in deep generative models, particularly with the
application of CLIP (Contrastive Language Image Pretraining) to Denoising
Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable
effectiveness in text to image generation. The well structured embedding space
of CLIP has also been extended to image to shape generation with DDPMs,
yielding notable results. Despite these successes, some fundamental questions
arise: Does CLIP ensure the best results in shape generation from images? Can
we leverage conditioning to bring explicit 3D knowledge into the generative
process and obtain better quality? This study introduces CISP (Contrastive
Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D
images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D
shapes in a shared embedding space, specifically capturing 3D characteristics
potentially overlooked by CLIP's text image focus. Our comprehensive analysis
assesses CISP's guidance performance against CLIP guided models, focusing on
generation quality, diversity, and coherence of the produced shapes with the
conditioning image. We find that, while matching CLIP in generation quality and
diversity, CISP substantially improves coherence with input images,
underscoring the value of incorporating 3D knowledge into generative models.
These findings suggest a promising direction for advancing the synthesis of 3D
visual content by integrating multimodal systems with 3D representations.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01242" title="Abstract">arXiv:2402.01242</a> [<a href="/pdf/2402.01242" title="Download PDF">pdf</a>, <a href="/format/2402.01242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Heads Are Better Than One: Boosting Graph Sparse Training via  Semantic and Topological Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guibin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yanwei Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Junfeng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yongduo Sui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dawei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) excel in various graph learning tasks but face
computational challenges when applied to large-scale graphs. A promising
solution is to remove non-essential edges to reduce the computational overheads
in GNN. Previous literature generally falls into two categories:
topology-guided and semantic-guided. The former maintains certain graph
topological properties yet often underperforms on GNNs due to low integration
with neural network training. The latter performs well at lower sparsity on
GNNs but faces performance collapse at higher sparsity levels. With this in
mind, we take the first step to propose a new research line and concept termed
Graph Sparse Training (GST), which dynamically manipulates sparsity at the data
level. Specifically, GST initially constructs a topology &amp; semantic anchor at a
low training cost, followed by performing dynamic sparse training to align the
sparse graph with the anchor. We introduce the Equilibria Sparsification
Principle to guide this process, effectively balancing the preservation of both
topological and semantic information. Ultimately, GST produces a sparse graph
with maximum topological integrity and no performance degradation. Extensive
experiments on 6 datasets and 5 backbones showcase that GST (I) identifies
subgraphs at higher graph sparsity levels (1.67%~15.85% $\uparrow$) than
state-of-the-art sparsification methods, (II) preserves more key spectral
properties, (III) achieves 1.27-3.42$\times$ speedup in GNN inference and (IV)
successfully helps graph adversarial defense and graph lottery tickets.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01244" title="Abstract">arXiv:2402.01244</a> [<a href="/pdf/2402.01244" title="Download PDF">pdf</a>, <a href="/format/2402.01244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short Systematic Codes for Correcting Random Edit Errors in DNA Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanna%2C+S+K">Serge Kas Hanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">DNA storage faces challenges in ensuring data reliability in the presence of
edit errors -- deletions, insertions, and substitutions -- that occur randomly
during various phases of the storage process. Current limitations in DNA
synthesis technology also require the use of short DNA sequences, highlighting
the particular need for short edit-correcting codes. Motivated by these
factors, we introduce a systematic code designed to correct random edits while
adhering to typical length constraints in DNA storage. We evaluate the
performance of the code through simulations and assess its effectiveness within
a DNA storage framework, revealing promising results.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01246" title="Abstract">arXiv:2402.01246</a> [<a href="/pdf/2402.01246" title="Download PDF">pdf</a>, <a href="/format/2402.01246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Daocheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wenjie Lei</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Licheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pinlong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Song Mao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+M">Min Dou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The emergence of Multimodal Large Language Models ((M)LLMs) has ushered in
new avenues in artificial intelligence, particularly for autonomous driving by
offering enhanced understanding and reasoning capabilities. This paper
introduces LimSim++, an extended version of LimSim designed for the application
of (M)LLMs in autonomous driving. Acknowledging the limitations of existing
simulation platforms, LimSim++ addresses the need for a long-term closed-loop
infrastructure supporting continuous learning and improved generalization in
autonomous driving. The platform offers extended-duration, multi-scenario
simulations, providing crucial information for (M)LLM-driven vehicles. Users
can engage in prompt engineering, model evaluation, and framework enhancement,
making LimSim++ a versatile tool for research and practice. This paper
additionally introduces a baseline (M)LLM-driven framework, systematically
validated through quantitative experiments across diverse scenarios. The
open-source resources of LimSim++ are available at:
https://pjlab-adg.github.io/limsim_plus/.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01252" title="Abstract">arXiv:2402.01252</a> [<a href="/pdf/2402.01252" title="Download PDF">pdf</a>, <a href="/format/2402.01252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target inductive methods for zero-shot regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fdez-D%C3%ADaz%2C+M">Miriam Fdez-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+J+R">Jos&#xe9; Ram&#xf3;n Quevedo</a>, 
<a href="/search/cs?searchtype=author&query=Monta%C3%B1%C3%A9s%2C+E">Elena Monta&#xf1;&#xe9;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Sciences ISSN: 0020-0255 2022 Volumen: 599 P\'aginas:
  44-63
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This research arises from the need to predict the amount of air pollutants in
meteorological stations. Air pollution depends on the location of the stations
(weather conditions and activities in the surroundings). Frequently, the
surrounding information is not considered in the learning process. This
information is known beforehand in the absence of unobserved weather conditions
and remains constant for the same station. Considering the surrounding
information as side information facilitates the generalization for predicting
pollutants in new stations, leading to a zero-shot regression scenario.
Available methods in zero-shot typically lean towards classification, and are
not easily extensible to regression. This paper proposes two zero-shot methods
for regression. The first method is a similarity based approach that learns
models from features and aggregates them using side information. However,
potential knowledge of the feature models may be lost in the aggregation. The
second method overcomes this drawback by replacing the aggregation procedure
and learning the correspondence between side information and feature-induced
models, instead. Both proposals are compared with a baseline procedure using
artificial datasets, UCI repository communities and crime datasets, and the
pollutants. Both approaches outperform the baseline method, but the parameter
learning approach manifests its superiority over the similarity based method.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01253" title="Abstract">arXiv:2402.01253</a> [<a href="/pdf/2402.01253" title="Download PDF">pdf</a>, <a href="/format/2402.01253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HimiRec: Modeling Hierarchical Multi-interest for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+H">Haolei Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yangping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yuan Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Industrial recommender systems usually consist of the retrieval stage and the
ranking stage, to handle the billion-scale of users and items. The retrieval
stage retrieves candidate items relevant to user interests for recommendations
and has attracted much attention. Frequently, users show hierarchical
multi-interests reflected in a heavy user of a certain NBA team Golden State
Warriors in Sports, who is also a light user of almost the whole Animation.
Both Sports and Animation are at the same level. However, most existing methods
implicitly learn this hierarchical difference, making more fine-grained
interest information to be averaged and limiting detailed understanding of the
user's different needs in heavy interests and other light interests. Therefore,
we propose a novel two-stage approach to explicitly modeling hierarchical
multi-interest for recommendation in this work. In the first hierarchical
multi-interest mining stage, the hierarchical clustering and transformer-based
model adaptively generate circles or sub-circles that users are interested in.
In the second stage, the partition of retrieval space allows the EBR models to
only deal with items within each circle and accurately capture user's refined
interests. Experimental results show that the proposed approach achieves
state-of-the-art performance. Our framework has also successfully deployed at
Lofter (one of the largest derivative content communities with 10 million
monthly active users) for over four months.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01254" title="Abstract">arXiv:2402.01254</a> [<a href="/pdf/2402.01254" title="Download PDF">pdf</a>, <a href="/format/2402.01254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Trajectory Model: Implicit Neural Trajectory Representation for  Trajectories Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zihan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuqing Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Trajectory planning is a fundamental problem in robotics. It facilitates a
wide range of applications in navigation and motion planning, control, and
multi-agent coordination. Trajectory planning is a difficult problem due to its
computational complexity and real-world environment complexity with
uncertainty, non-linearity, and real-time requirements. The multi-agent
trajectory planning problem adds another dimension of difficulty due to
inter-agent interaction. Existing solutions are either search-based or
optimization-based approaches with simplified assumptions of environment,
limited planning speed, and limited scalability in the number of agents. In
this work, we make the first attempt to reformulate single agent and
multi-agent trajectory planning problem as query problems over an implicit
neural representation of trajectories. We formulate such implicit
representation as Neural Trajectory Models (NTM) which can be queried to
generate nearly optimal trajectory in complex environments. We conduct
experiments in simulation environments and demonstrate that NTM can solve
single-agent and multi-agent trajectory planning problems. In the experiments,
NTMs achieve (1) sub-millisecond panning time using GPUs, (2) almost avoiding
all environment collision, (3) almost avoiding all inter-agent collision, and
(4) generating almost shortest paths. We also demonstrate that the same NTM
framework can also be used for trajectories correction and multi-trajectory
conflict resolution refining low quality and conflicting multi-agent
trajectories into nearly optimal solutions efficiently. (Open source code will
be available at https://github.com/laser2099/neural-trajectory-model)
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01255" title="Abstract">arXiv:2402.01255</a> [<a href="/pdf/2402.01255" title="Download PDF">pdf</a>, <a href="/ps/2402.01255" title="Download PostScript">ps</a>, <a href="/format/2402.01255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enumeration of linear codes with different hulls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouyuklieva%2C+S">Stefka Bouyuklieva</a>, 
<a href="/search/cs?searchtype=author&query=Bouyukliev%2C+I">Iliya Bouyukliev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The hull of a linear code $C$ is the intersection of $C$ with its dual code.
We present and analyze the number of linear $q$-ary codes of the same length
and dimension but with different dimensions for their hulls. We prove that for
given dimension $k$ and length $n\ge 2k$ the number of all $[n,k]_q$ linear
codes with hull dimension $l$ decreases as $l$ increases. We also present
classification results for binary and ternary linear codes with trivial hulls
(LCD and self-orthogonal) for some values of the length $n$ and dimension $k$,
comparing the obtained numbers with the number of all linear codes for the
given $n$ and $k$.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01257" title="Abstract">arXiv:2402.01257</a> [<a href="/pdf/2402.01257" title="Download PDF">pdf</a>, <a href="/format/2402.01257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polygonal corona limit on multigrid dual tilings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lutfalla%2C+V">Victor Lutfalla</a>, 
<a href="/search/cs?searchtype=author&query=Perrot%2C+K">K&#xe9;vin Perrot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 theorem, 13 figures, 19 pages, 17 references, 7 fundings aknowledged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The growth pattern of an invasive cell-to-cell propagation (called the
successive coronas) on the square grid is a tilted square. On the triangular
and hexagonal grids, it is an hexagon. It is remarkable that, on the aperiodic
structure of Penrose tilings, this cell-to-cell diffusion process tends to a
regular decagon (at the limit). In this article we generalize this result to
any regular multigrid dual tiling, by defining the characteristic polygon of a
multigrid and its dual tiling. Exploiting this elegant duality allows to fully
understand why such surprising phenomena, of seeing highly regular polygonal
shapes emerge from aperiodic underlying structures, happen.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01259" title="Abstract">arXiv:2402.01259</a> [<a href="/pdf/2402.01259" title="Download PDF">pdf</a>, <a href="/format/2402.01259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Aware 60 GHz mmWave Beamforming for V2V Communications  Utilizing Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mollah%2C+M+B">Muhammad Baqer Mollah</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Honggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hua Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 IEEE International Conference on Communications (ICC), Denver, CO, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Social and Information Networks (cs.SI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Beamforming techniques are considered as essential parts to compensate the
severe path loss in millimeter-wave (mmWave) communications by adopting large
antenna arrays and formulating narrow beams to obtain satisfactory received
powers. However, performing accurate beam alignment over such narrow beams for
efficient link configuration by traditional beam selection approaches, mainly
relied on channel state information, typically impose significant latency and
computing overheads, which is often infeasible in vehicle-to-vehicle (V2V)
communications like highly dynamic scenarios. In contrast, utilizing
out-of-band contextual information, such as vehicular position information, is
a potential alternative to reduce such overheads. In this context, this paper
presents a deep learning-based solution on utilizing the vehicular position
information for predicting the optimal beams having sufficient mmWave received
powers so that the best V2V line-of-sight links can be ensured proactively.
After experimental evaluation of the proposed solution on real-world measured
mmWave sensing and communications datasets, the results show that the solution
can achieve up to 84.58% of received power of link status on average, which
confirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V
communications.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01261" title="Abstract">arXiv:2402.01261</a> [<a href="/pdf/2402.01261" title="Download PDF">pdf</a>, <a href="/format/2402.01261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEDDY: Trimming Edges with Degree-based Discrimination strategY
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">Hyunjin Seo</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+J">Jihun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Eunho Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Since the pioneering work on the lottery ticket hypothesis for graph neural
networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph
lottery tickets (GLT) has become one of the pivotal focus in the GNN community,
inspiring researchers to discover sparser GLT while achieving comparable
performance to original dense networks. In parallel, the graph structure has
gained substantial attention as a crucial factor in GNN training dynamics, also
elucidated by several recent studies. Despite this, contemporary studies on
GLT, in general, have not fully exploited inherent pathways in the graph
structure and identified tickets in an iterative manner, which is
time-consuming and inefficient. To address these limitations, we introduce
TEDDY, a one-shot edge sparsification framework that leverages structural
information by incorporating edge-degree information. Following edge
sparsification, we encourage the parameter sparsity during training via simple
projected gradient descent on the $\ell_0$ ball. Given the target sparsity
levels for both the graph structure and the model parameters, our TEDDY
facilitates efficient and rapid realization of GLT within a single training.
Remarkably, our experimental results demonstrate that TEDDY significantly
surpasses conventional iterative approaches in generalization, even when
conducting one-shot sparsification that solely utilizes graph structures,
without taking node features into account.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01262" title="Abstract">arXiv:2402.01262</a> [<a href="/pdf/2402.01262" title="Download PDF">pdf</a>, <a href="/format/2402.01262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Scaling Classifier: class incremental learning with probability  scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pomponi%2C+J">Jary Pomponi</a>, 
<a href="/search/cs?searchtype=author&query=Devoto%2C+A">Alessio Devoto</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Humans are capable of acquiring new knowledge and transferring learned
knowledge into different domains, incurring a small forgetting. The same
ability, called Continual Learning, is challenging to achieve when operating
with neural networks due to the forgetting affecting past learned tasks when
learning new ones. This forgetting can be mitigated by replaying stored samples
from past tasks, but a large memory size may be needed for long sequences of
tasks; moreover, this could lead to overfitting on saved samples. In this
paper, we propose a novel regularisation approach and a novel incremental
classifier called, respectively, Margin Dampening and Cascaded Scaling
Classifier. The first combines a soft constraint and a knowledge distillation
approach to preserve past learned knowledge while allowing the model to learn
new patterns effectively. The latter is a gated incremental classifier, helping
the model modify past predictions without directly interfering with them. This
is achieved by modifying the output of the model with auxiliary scaling
functions. We empirically show that our approach performs well on multiple
benchmarks against well-established baselines, and we also study each component
of our proposal and how the combinations of such components affect the final
results.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01263" title="Abstract">arXiv:2402.01263</a> [<a href="/pdf/2402.01263" title="Download PDF">pdf</a>, <a href="/format/2402.01263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Differentiable POGLM with Forward-Backward Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yule Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The partially observable generalized linear model (POGLM) is a powerful tool
for understanding neural connectivity under the assumption of existing hidden
neurons. With spike trains only recorded from visible neurons, existing works
use variational inference to learn POGLM meanwhile presenting the difficulty of
learning this latent variable model. There are two main issues: (1) the sampled
Poisson hidden spike count hinders the use of the pathwise gradient estimator
in VI; and (2) the existing design of the variational model is neither
expressive nor time-efficient, which further affects the performance. For (1),
we propose a new differentiable POGLM, which enables the pathwise gradient
estimator, better than the score function gradient estimator used in existing
works. For (2), we propose the forward-backward message-passing sampling scheme
for the variational model. Comprehensive experiments show that our
differentiable POGLMs with our forward-backward message passing produce a
better performance on one synthetic and two real-world datasets. Furthermore,
our new method yields more interpretable parameters, underscoring its
significance in neuroscience.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01264" title="Abstract">arXiv:2402.01264</a> [<a href="/pdf/2402.01264" title="Download PDF">pdf</a>, <a href="/format/2402.01264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct side information learning for zero-shot regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fdez-D%C3%ADaz%2C+M">Miriam Fdez-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Monta%C3%B1%C3%A9s%2C+E">Elena Monta&#xf1;&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+J+R">Jos&#xe9; Ram&#xf3;n Quevedo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neurocomputing 2023 Volumen 561 126873
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Zero-shot learning provides models for targets for which instances are not
available, commonly called unobserved targets. The availability of target side
information becomes crucial in this context in order to properly induce models
for these targets. The literature is plenty of strategies to cope with this
scenario, but specifically designed on the basis of a zero-shot classification
scenario, mostly in computer vision and image classification, but they are
either not applicable or easily extensible for a zero-shot regression framework
for which a continuos value is required to be predicted rather than a label. In
fact, there is a considerable lack of methods for zero-shot regression in the
literature. Two approaches for zero-shot regression that work in a two-phase
procedure were recently proposed. They first learn the observed target models
through a classical regression learning ignoring the target side information.
Then, they aggregate those observed target models afterwards exploiting the
target side information and the models for the unobserved targets are induced.
Despite both have shown quite good performance because of the different
treatment they grant to the common features and to the side information, they
exploit features and side information separately, avoiding a global
optimization for providing the unobserved target models. The proposal of this
paper is a novel method that jointly takes features and side information in a
one-phase learning process, but treating side information properly and in a
more deserving way than as common features. A specific kernel that properly
merges features and side information is proposed for this purpose resulting in
a novel approach that exhibits better performance over both artificial and real
datasets.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01267" title="Abstract">arXiv:2402.01267</a> [<a href="/pdf/2402.01267" title="Download PDF">pdf</a>, <a href="/ps/2402.01267" title="Download PostScript">ps</a>, <a href="/format/2402.01267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Human and the Mechanical: logos, truthfulness, and ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giannakidou%2C+A">Anastasia Giannakidou</a>, 
<a href="/search/cs?searchtype=author&query=Mari%2C+A">Alda Mari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The paper addresses the question of whether it is appropriate to talk about
`mechanical minds' at all, and whether ChatGPT models can indeed be thought of
as realizations of that. Our paper adds a semantic argument to the current
debate. The act of human assertion requires the formation of a veridicality
judgment. Modification of assertions with modals (John must be at home) and the
use of subjective elements (John is obviously at home) indicate that the
speaker is manipulating her judgments and, in a cooperative context, intends
her epistemic state to be transparent to the addressee. Veridicality judgments
are formed on the basis of two components: (i) evidence that relates to reality
(exogenous evidence) and (ii) endogenous evidence, such as preferences and
private beliefs. `Mechanical minds' lack these two components: (i) they do not
relate to reality and (ii) do not have endogenous evidence. Therefore they lack
the ability to form a belief about the world and a veridicality judgments
altogether. They can only mimic that judgment, but the output is not ground in
the very foundations for it.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01269" title="Abstract">arXiv:2402.01269</a> [<a href="/pdf/2402.01269" title="Download PDF">pdf</a>, <a href="/format/2402.01269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectrum-guided Feature Enhancement Network for Event Person  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Hongchen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiuping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Nan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As a cutting-edge biosensor, the event camera holds significant potential in
the field of computer vision, particularly regarding privacy preservation.
However, compared to traditional cameras, event streams often contain noise and
possess extremely sparse semantics, posing a formidable challenge for
event-based person re-identification (event Re-ID). To address this, we
introduce a novel event person re-identification network: the Spectrum-guided
Feature Enhancement Network (SFE-Net). This network consists of two innovative
components: the Multi-grain Spectrum Attention Mechanism (MSAM) and the
Consecutive Patch Dropout Module (CPDM). MSAM employs a fourier spectrum
transform strategy to filter event noise, while also utilizing an event-guided
multi-granularity attention strategy to enhance and capture discriminative
person semantics. CPDM employs a consecutive patch dropout strategy to generate
multiple incomplete feature maps, encouraging the deep Re-ID model to equally
perceive each effective region of the person's body and capture robust person
descriptors. Extensive experiments on Event Re-ID datasets demonstrate that our
SFE-Net achieves the best performance in this task.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01274" title="Abstract">arXiv:2402.01274</a> [<a href="/pdf/2402.01274" title="Download PDF">pdf</a>, <a href="/format/2402.01274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heggan%2C+C">Calum Heggan</a>, 
<a href="/search/cs?searchtype=author&query=Budgett%2C+S">Sam Budgett</a>, 
<a href="/search/cs?searchtype=author&query=Hosepedales%2C+T">Timothy Hosepedales</a>, 
<a href="/search/cs?searchtype=author&query=Yeghoobi%2C+M">Mehrdad Yeghoobi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In recent years, self-supervised learning has excelled for its capacity to
learn robust feature representations from unlabelled data. Networks pretrained
through self-supervision serve as effective feature extractors for downstream
tasks, including Few-Shot Learning. While the evaluation of unsupervised
approaches for few-shot learning is well-established in imagery, it is notably
absent in acoustics. This study addresses this gap by assessing large-scale
self-supervised models' performance in few-shot audio classification.
Additionally, we explore the relationship between a model's few-shot learning
capability and other downstream task benchmarks. Our findings reveal
state-of-the-art performance in some few-shot problems such as
SpeechCommandsv2, as well as strong correlations between speech-based few-shot
problems and various downstream audio tasks.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01275" title="Abstract">arXiv:2402.01275</a> [<a href="/pdf/2402.01275" title="Download PDF">pdf</a>, <a href="/format/2402.01275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric-Task MAP-Elites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anne%2C+T">Timoth&#xe9;e Anne</a>, 
<a href="/search/cs?searchtype=author&query=Mouret%2C+J">Jean-Baptiste Mouret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Optimizing a set of functions simultaneously by leveraging their similarity
is called multi-task optimization. Current black-box multi-task algorithms only
solve a finite set of tasks, even when the tasks originate from a continuous
space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel
black-box algorithm to solve continuous multi-task optimization problems. This
algorithm (1) solves a new task at each iteration, effectively covering the
continuous space, and (2) exploits a new variation operator based on local
linear regression. The resulting dataset of solutions makes it possible to
create a function that maps any task parameter to its optimal solution. We show
on two parametric-task toy problems and a more realistic and challenging
robotic problem in simulation that PT-ME outperforms all baselines, including
the deep reinforcement learning algorithm PPO.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01276" title="Abstract">arXiv:2402.01276</a> [<a href="/pdf/2402.01276" title="Download PDF">pdf</a>, <a href="/format/2402.01276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Unlearning: a Perspective of Stability and Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiaqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuanyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper explores the multifaceted consequences of federated unlearning
(FU) with data heterogeneity. We introduce key metrics for FU assessment,
concentrating on verification, global stability, and local fairness, and
investigate the inherent trade-offs. Furthermore, we formulate the unlearning
process with data heterogeneity through an optimization framework. Our key
contribution lies in a comprehensive theoretical analysis of the trade-offs in
FU and provides insights into data heterogeneity's impacts on FU. Leveraging
these insights, we propose FU mechanisms to manage the trade-offs, guiding
further development for FU mechanisms. We empirically validate that our FU
mechanisms effectively balance trade-offs, confirming insights derived from our
theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01279" title="Abstract">arXiv:2402.01279</a> [<a href="/pdf/2402.01279" title="Download PDF">pdf</a>, <a href="/ps/2402.01279" title="Download PostScript">ps</a>, <a href="/format/2402.01279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Viterbi Algorithm for a Class of Optimal Binary  Convolutional Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abreu%2C+Z">Zita Abreu</a>, 
<a href="/search/cs?searchtype=author&query=Lieb%2C+J">Julia Lieb</a>, 
<a href="/search/cs?searchtype=author&query=Schaller%2C+M">Michael Schaller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The most famous error-decoding algorithm for convolutional codes is the
Viterbi algorithm. In this paper, we present a new reduced complexity version
of this algorithm which can be applied to a class of binary convolutional codes
with optimum column distances called k-partial simplex convolutional codes.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01287" title="Abstract">arXiv:2402.01287</a> [<a href="/pdf/2402.01287" title="Download PDF">pdf</a>, <a href="/format/2402.01287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking CenterNet: A Distillation-boosted Spiking Neural Network for  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodden%2C+L">Lennard Bodden</a>, 
<a href="/search/cs?searchtype=author&query=Schwaiger%2C+F">Franziska Schwaiger</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+D+B">Duc Bach Ha</a>, 
<a href="/search/cs?searchtype=author&query=Kreuzberg%2C+L">Lars Kreuzberg</a>, 
<a href="/search/cs?searchtype=author&query=Behnke%2C+S">Sven Behnke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures. Submitted to WCCI-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In the era of AI at the edge, self-driving cars, and climate change, the need
for energy-efficient, small, embedded AI is growing. Spiking Neural Networks
(SNNs) are a promising approach to address this challenge, with their
event-driven information flow and sparse activations. We propose Spiking
CenterNet for object detection on event data. It combines an SNN CenterNet
adaptation with an efficient M2U-Net-based decoder. Our model significantly
outperforms comparable previous work on Prophesee's challenging GEN1 Automotive
Detection Dataset while using less than half the energy. Distilling the
knowledge of a non-spiking teacher into our SNN further increases performance.
To the best of our knowledge, our work is the first approach that takes
advantage of knowledge distillation in the field of spiking object detection.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01289" title="Abstract">arXiv:2402.01289</a> [<a href="/pdf/2402.01289" title="Download PDF">pdf</a>, <a href="/format/2402.01289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UCVC: A Unified Contextual Video Compression Framework with Joint  P-frame and B-frame Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiayu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yongqi Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chunhui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ronggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DCC2024, CLIC2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a learned video compression method in response to video
compression track of the 6th Challenge on Learned Image Compression (CLIC), at
DCC 2024.Specifically, we propose a unified contextual video compression
framework (UCVC) for joint P-frame and B-frame coding. Each non-intra frame
refers to two neighboring decoded frames, which can be either both from the
past for P-frame compression, or one from the past and one from the future for
B-frame compression. In training stage, the model parameters are jointly
optimized with both P-frames and B-frames. Benefiting from the designs, the
framework can support both P-frame and B-frame coding and achieve comparable
compression efficiency with that specifically designed for P-frame or
B-frame.As for challenge submission, we report the optimal compression
efficiency by selecting appropriate frame types for each test sequence. Our
team name is PKUSZ-LVC.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01292" title="Abstract">arXiv:2402.01292</a> [<a href="/pdf/2402.01292" title="Download PDF">pdf</a>, <a href="/format/2402.01292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the new XAI: A Hypothesis-Driven Approach to Decision Support  Using Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thao Le</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+T">Tim Miller</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Ronal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sonenberg%2C+L">Liz Sonenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Prior research on AI-assisted human decision-making has explored several
different explainable AI (XAI) approaches. A recent paper has proposed a
paradigm shift calling for hypothesis-driven XAI through a conceptual framework
called evaluative AI that gives people evidence that supports or refutes
hypotheses without necessarily giving a decision-aid recommendation. In this
paper we describe and evaluate an approach for hypothesis-driven XAI based on
the Weight of Evidence (WoE) framework, which generates both positive and
negative evidence for a given hypothesis. Through human behavioural
experiments, we show that our hypothesis-driven approach increases decision
accuracy, reduces reliance compared to a recommendation-driven approach and an
AI-explanation-only baseline, but with a small increase in under-reliance
compared to the recommendation-driven approach. Further, we show that
participants used our hypothesis-driven approach in a materially different way
to the two baselines.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01293" title="Abstract">arXiv:2402.01293</a> [<a href="/pdf/2402.01293" title="Download PDF">pdf</a>, <a href="/format/2402.01293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can MLLMs Perform Text-to-Image In-Context Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yuchen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wonjun Kang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yicong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+H+I">Hyung Il Koo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The evolution from Large Language Models (LLMs) to Multimodal Large Language
Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to
its multimodal counterpart. Existing such studies have primarily concentrated
on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique
characteristics and potential applications, remains underexplored. To address
this gap, we formally define the task of T2I-ICL and present CoBSAT, the first
T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to
benchmark six state-of-the-art MLLMs, we uncover considerable difficulties
MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the
inherent complexity of multimodality and image generation. To overcome these
challenges, we explore strategies like fine-tuning and Chain-of-Thought
prompting, demonstrating notable improvements. Our code and dataset are
available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01294" title="Abstract">arXiv:2402.01294</a> [<a href="/pdf/2402.01294" title="Download PDF">pdf</a>, <a href="/format/2402.01294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing Regret in Billboard Advertisement under Zonal Influence  Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+D">Dildar Ali</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Suman Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+Y">Yamuna Prasad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Retrieval (cs.IR); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In a typical billboard advertisement technique, a number of digital
billboards are owned by an influence provider, and many advertisers approach
the influence provider for a specific number of views of their advertisement
content on a payment basis. If the influence provider provides the demanded or
more influence, then he will receive the full payment or else a partial
payment. In the context of an influence provider, if he provides more or less
than an advertiser's demanded influence, it is a loss for him. This is
formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to allocate the billboard slots among the
advertisers such that the total regret is minimized. In this paper, we study
this problem as a discrete optimization problem and propose four solution
approaches. The first one selects the billboard slots from the available ones
in an incremental greedy manner, and we call this method the Budget Effective
Greedy approach. In the second one, we introduce randomness with the first one,
where we perform the marginal gain computation for a sample of randomly chosen
billboard slots. The remaining two approaches are further improvements over the
second one. We analyze all the algorithms to understand their time and space
complexity. We implement them with real-life trajectory and billboard datasets
and conduct a number of experiments. It has been observed that the randomized
budget effective greedy approach takes reasonable computational time while
minimizing the regret.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01295" title="Abstract">arXiv:2402.01295</a> [<a href="/pdf/2402.01295" title="Download PDF">pdf</a>, <a href="/format/2402.01295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExtremeCast: Boosting Extreme Value Prediction for Global Weather  Forecast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanghan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tao Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lei Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data-driven weather forecast based on machine learning (ML) has experienced
rapid development and demonstrated superior performance in the global
medium-range forecast compared to traditional physics-based dynamical models.
However, most of these ML models struggle with accurately predicting extreme
weather, which is closely related to the extreme value prediction. Through
mathematical analysis, we prove that the use of symmetric losses, such as the
Mean Squared Error (MSE), leads to biased predictions and underestimation of
extreme values. To address this issue, we introduce Exloss, a novel loss
function that performs asymmetric optimization and highlights extreme values to
obtain accurate extreme weather forecast. Furthermore, we introduce a
training-free extreme value enhancement strategy named ExEnsemble, which
increases the variance of pixel values and improves the forecast robustness.
Combined with an advanced global weather forecast model, extensive experiments
show that our solution can achieve state-of-the-art performance in extreme
weather prediction, while maintaining the overall forecast accuracy comparable
to the top medium-range forecast models.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01296" title="Abstract">arXiv:2402.01296</a> [<a href="/pdf/2402.01296" title="Download PDF">pdf</a>, <a href="/format/2402.01296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Man-Jie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zheng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wei Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Privacy-preserving neural networks have attracted increasing attention in
recent years, and various algorithms have been developed to keep the balance
between accuracy, computational complexity and information security from the
cryptographic view. This work takes a different view from the input data and
structure of neural networks. We decompose the input data (e.g., some images)
into sensitive and insensitive segments according to importance and privacy.
The sensitive segment includes some important and private information such as
human faces and we take strong homomorphic encryption to keep security, whereas
the insensitive one contains some background and we add perturbations. We
propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal
with two segments, respectively, and ciphertext branch could utilize the
information from plaintext branch by unidirectional connections. We adopt
knowledge distillation for our bi-CryptoNets by transferring representations
from a well-trained teacher neural network. Empirical studies show the
effectiveness and decrease of inference latency for our bi-CryptoNets.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01297" title="Abstract">arXiv:2402.01297</a> [<a href="/pdf/2402.01297" title="Download PDF">pdf</a>, <a href="/format/2402.01297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Overfitting in Kernel Ridgeless Regression Through the  Eigenspectrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T+S">Tin Sum Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Belius%2C+D">David Belius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We derive new bounds for the condition number of kernel matrices, which we
then use to enhance existing non-asymptotic test error bounds for kernel
ridgeless regression in the over-parameterized regime for a fixed input
dimension. For kernels with polynomial spectral decay, we recover the bound
from previous work; for exponential decay, our bound is non-trivial and novel.
<br />Our conclusion on overfitting is two-fold: (i) kernel regressors whose
eigenspectrum decays polynomially must generalize well, even in the presence of
noisy labeled training data; these models exhibit so-called tempered
overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays
exponentially, then it generalizes poorly, i.e., it exhibits catastrophic
overfitting. This adds to the available characterization of kernel ridge
regressors exhibiting benign overfitting as the extremal case where the
eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new
random matrix theory (RMT) techniques with recent tools in the kernel ridge
regression (KRR) literature.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01300" title="Abstract">arXiv:2402.01300</a> [<a href="/pdf/2402.01300" title="Download PDF">pdf</a>, <a href="/format/2402.01300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Approaches to Diachronic Normalization of Polish Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dudzic%2C+K">Kacper Dudzic</a>, 
<a href="/search/cs?searchtype=author&query=Grali%C5%84ski%2C+F">Filip Grali&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Jassem%2C+K">Krzysztof Jassem</a>, 
<a href="/search/cs?searchtype=author&query=Kubis%2C+M">Marek Kubis</a>, 
<a href="/search/cs?searchtype=author&query=Wierzcho%C5%84%2C+P">Piotr Wierzcho&#x144;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the LaTeCH-CLfL 2024 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper discusses two approaches to the diachronic normalization of Polish
texts: a rule-based solution that relies on a set of handcrafted patterns, and
a neural normalization model based on the text-to-text transfer transformer
architecture. The training and evaluation data prepared for the task are
discussed in detail, along with experiments conducted to compare the proposed
normalization solutions. A quantitative and qualitative analysis is made. It is
shown that at the current stage of inquiry into the problem, the rule-based
solution outperforms the neural one on 3 out of 4 variants of the prepared
dataset, although in practice both approaches have distinct advantages and
disadvantages.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01302" title="Abstract">arXiv:2402.01302</a> [<a href="/pdf/2402.01302" title="Download PDF">pdf</a>, <a href="/format/2402.01302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Gradient-based Clustering of Distributed Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armacki%2C+A">Aleksandar Armacki</a>, 
<a href="/search/cs?searchtype=author&query=Bajovi%C4%87%2C+D">Dragana Bajovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Jakoveti%C4%87%2C+D">Du&#x161;an Jakoveti&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 5 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We develop a family of distributed clustering algorithms that work over
networks of users. In the proposed scenario, users contain a local dataset and
communicate only with their immediate neighbours, with the aim of finding a
clustering of the full, joint data. The proposed family, termed Distributed
Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$,
controling the proximity of users' center estimates, with $\mathcal{F}$
determining the clustering loss. Specialized to popular clustering losses like
$K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel
distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a
novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We
provide a unified analysis and establish several strong results, under mild
assumptions. First, the sequence of centers generated by the methods converges
to a well-defined notion of fixed point, under any center initialization and
value of $\rho$. Second, as $\rho$ increases, the family of fixed points
produced by DGC-$\mathcal{F}_\rho$ converges to a notion of consensus fixed
points. We show that consensus fixed points of DGC-$\mathcal{F}_{\rho}$ are
equivalent to fixed points of gradient clustering over the full data,
guaranteeing a clustering of the full data is produced. For the special case of
Bregman losses, we show that our fixed points converge to the set of Lloyd
points. Numerical experiments on real data confirm our theoretical findings and
demonstrate strong performance of the methods.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01303" title="Abstract">arXiv:2402.01303</a> [<a href="/pdf/2402.01303" title="Download PDF">pdf</a>, <a href="/format/2402.01303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AGILE: Approach-based Grasp Inference Learned from Element Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koosheshi%2C+M">MohammadHossein Koosheshi</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+H">Hamed Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Masouleh%2C+M+T">Mehdi Tale Masouleh</a>, 
<a href="/search/cs?searchtype=author&query=Kalhor%2C+A">Ahmad Kalhor</a>, 
<a href="/search/cs?searchtype=author&query=Yazdi%2C+M+R+H">Mohammad Reza Hairi Yazdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference Paper, ICROM 2023, 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)

</div>
<p class="mathjax">Humans, this species expert in grasp detection, can grasp objects by taking
into account hand-object positioning information. This work proposes a method
to enable a robot manipulator to learn the same, grasping objects in the most
optimal way according to how the gripper has approached the object. Built on
deep learning, the proposed method consists of two main stages. In order to
generalize the network on unseen objects, the proposed Approach-based Grasping
Inference involves an element decomposition stage to split an object into its
main parts, each with one or more annotated grasps for a particular approach of
the gripper. Subsequently, a grasp detection network utilizes the decomposed
elements by Mask R-CNN and the information on the approach of the gripper in
order to detect the element the gripper has approached and the most optimal
grasp. In order to train the networks, the study introduces a robotic grasping
dataset collected in the Coppeliasim simulation environment. The dataset
involves 10 different objects with annotated element decomposition masks and
grasp rectangles. The proposed method acquires a 90% grasp success rate on seen
objects and 78% on unseen objects in the Coppeliasim simulation environment.
Lastly, simulation-to-reality domain adaptation is performed by applying
transformations on the training set collected in simulation and augmenting the
dataset, which results in a 70% physical grasp success performance using a
Delta parallel robot and a 2 -fingered gripper.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01304" title="Abstract">arXiv:2402.01304</a> [<a href="/pdf/2402.01304" title="Download PDF">pdf</a>, <a href="/format/2402.01304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phrase Grounding-based Style Transfer for Single-Domain Generalized  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhigang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenli Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Single-domain generalized object detection aims to enhance a model's
generalizability to multiple unseen target domains using only data from a
single source domain during training. This is a practical yet challenging task
as it requires the model to address domain shift without incorporating target
domain data into training. In this paper, we propose a novel phrase
grounding-based style transfer (PGST) approach for the task. Specifically, we
first define textual prompts to describe potential objects for each unseen
target domain. Then, we leverage the grounded language-image pre-training
(GLIP) model to learn the style of these target domains and achieve style
transfer from the source to the target domain. The style-transferred source
visual features are semantically rich and could be close to imaginary
counterparts in the target domain. Finally, we employ these style-transferred
visual features to fine-tune GLIP. By introducing imaginary counterparts, the
detector could be effectively generalized to unseen target domains using only a
single source domain for training. Extensive experimental results on five
diverse weather driving benchmarks demonstrate our proposed approach achieves
state-of-the-art performance, even surpassing some domain adaptive methods that
incorporate target domain images into the training process.The source codes and
pre-trained models will be made available.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01306" title="Abstract">arXiv:2402.01306</a> [<a href="/pdf/2402.01306" title="Download PDF">pdf</a>, <a href="/format/2402.01306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KTO: Model Alignment as Prospect Theoretic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ethayarajh%2C+K">Kawin Ethayarajh</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Winnie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>, 
<a href="/search/cs?searchtype=author&query=Kiela%2C+D">Douwe Kiela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive
random variables in a biased but well-defined manner; for example, humans are
famously loss-averse. We show that objectives for aligning LLMs with human
feedback implicitly incorporate many of these biases -- the success of these
objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed
to them being $\textit{human-aware loss functions}$ (HALOs). However, the
utility functions these methods attribute to humans still differ from those in
the prospect theory literature. Using a Kahneman-Tversky model of human
utility, we propose a HALO that directly maximizes the utility of generations
instead of maximizing the log-likelihood of preferences, as current methods do.
We call this approach Kahneman-Tversky Optimization (KTO), and it matches or
exceeds the performance of preference-based methods at scales from 1B to 30B.
Crucially, KTO does not need preferences -- only a binary signal of whether an
output is desirable or undesirable for a given input. This makes it far easier
to use in the real world, where preference data is scarce and expensive.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01311" title="Abstract">arXiv:2402.01311</a> [<a href="/pdf/2402.01311" title="Download PDF">pdf</a>, <a href="/format/2402.01311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Multimodal Fusion of Data with Heterogeneous Dimensionality via  Projective Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morano%2C+J">Jos&#xe9; Morano</a>, 
<a href="/search/cs?searchtype=author&query=Aresta%2C+G">Guilherme Aresta</a>, 
<a href="/search/cs?searchtype=author&query=Grechenig%2C+C">Christoph Grechenig</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt-Erfurth%2C+U">Ursula Schmidt-Erfurth</a>, 
<a href="/search/cs?searchtype=author&query=Bogunovi%C4%87%2C+H">Hrvoje Bogunovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE Journal of Biomedical and Health Informatics (JBHI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The use of multimodal imaging has led to significant improvements in the
diagnosis and treatment of many diseases. Similar to clinical practice, some
works have demonstrated the benefits of multimodal fusion for automatic
segmentation and classification using deep learning-based methods. However,
current segmentation methods are limited to fusion of modalities with the same
dimensionality (e.g., 3D+3D, 2D+2D), which is not always possible, and the
fusion strategies implemented by classification methods are incompatible with
localization tasks. In this work, we propose a novel deep learning-based
framework for the fusion of multimodal data with heterogeneous dimensionality
(e.g., 3D+2D) that is compatible with localization tasks. The proposed
framework extracts the features of the different modalities and projects them
into the common feature subspace. The projected features are then fused and
further processed to obtain the final prediction. The framework was validated
on the following tasks: segmentation of geographic atrophy (GA), a late-stage
manifestation of age-related macular degeneration, and segmentation of retinal
blood vessels (RBV) in multimodal retinal imaging. Our results show that the
proposed method outperforms the state-of-the-art monomodal methods on GA and
RBV segmentation by up to 3.10% and 4.64% Dice, respectively.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01313" title="Abstract">arXiv:2402.01313</a> [<a href="/pdf/2402.01313" title="Download PDF">pdf</a>, <a href="/format/2402.01313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoGCN -- Towards Generic Human Activity Recognition with Neural  Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tempel%2C+F">Felix Tempel</a>, 
<a href="/search/cs?searchtype=author&query=Str%C3%BCmke%2C+I">Inga Str&#xfc;mke</a>, 
<a href="/search/cs?searchtype=author&query=Ihlen%2C+E+A+F">Espen Alexander F. Ihlen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper introduces AutoGCN, a generic Neural Architecture Search (NAS)
algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks
(GCNs). HAR has gained attention due to advances in deep learning, increased
data availability, and enhanced computational capabilities. At the same time,
GCNs have shown promising results in modeling relationships between body key
points in a skeletal graph. While domain experts often craft dataset-specific
GCN-based methods, their applicability beyond this specific context is severely
limited. AutoGCN seeks to address this limitation by simultaneously searching
for the ideal hyperparameters and architecture combination within a versatile
search space using a reinforcement controller while balancing optimal
exploration and exploitation behavior with a knowledge reservoir during the
search process. We conduct extensive experiments on two large-scale datasets
focused on skeleton-based action recognition to assess the proposed algorithm's
performance. Our experimental results underscore the effectiveness of AutoGCN
in constructing optimal GCN architectures for HAR, outperforming conventional
NAS and GCN methods, as well as random search. These findings highlight the
significance of a diverse search space and an expressive input representation
to enhance the network performance and generalizability.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01320" title="Abstract">arXiv:2402.01320</a> [<a href="/pdf/2402.01320" title="Download PDF">pdf</a>, <a href="/ps/2402.01320" title="Download PostScript">ps</a>, <a href="/format/2402.01320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the mean-field limit for Stein variational gradient descent:  stability and multilevel approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Weissmann%2C+S">Simon Weissmann</a>, 
<a href="/search/math?searchtype=author&query=Zech%2C+J">Jakob Zech</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">In this paper we propose and analyze a novel multilevel version of Stein
variational gradient descent (SVGD). SVGD is a recent particle based
variational inference method. For Bayesian inverse problems with
computationally expensive likelihood evaluations, the method can become
prohibitive as it requires to evolve a discrete dynamical system over many time
steps, each of which requires likelihood evaluations at all particle locations.
To address this, we introduce a multilevel variant that involves running
several interacting particle dynamics in parallel corresponding to different
approximation levels of the likelihood. By carefully tuning the number of
particles at each level, we prove that a significant reduction in computational
complexity can be achieved. As an application we provide a numerical experiment
for a PDE driven inverse problem, which confirms the speed up suggested by our
theoretical results.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01324" title="Abstract">arXiv:2402.01324</a> [<a href="/pdf/2402.01324" title="Download PDF">pdf</a>, <a href="/ps/2402.01324" title="Download PostScript">ps</a>, <a href="/format/2402.01324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on some bounds between cubic spline interpolants depending on the  boundary conditions: Application to a monotonicity property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=Y%C3%A1%C3%B1ez%2C+D+F">Dionisio F. Y&#xe1;&#xf1;ez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Numerical Mathematics 181 (2022) 320-325
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In the context of cubic splines, the authors have contributed to a recent
paper dealing with the computation of nonlinear derivatives at the interior
nodes so that monotonicity is enforced while keeping the order of approximation
of the spline as high as possible. During the review process of that paper, one
of the reviewers raised the question of whether a cubic spline interpolating
monotone data could be forced to preserve monotonicity by imposing suitable
values of the first derivative at the endpoints. Albeit a negative answer
appears to be intuitive, we have found no results regarding this fact. In this
short work we prove that the answer to that question is actually negative.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01326" title="Abstract">arXiv:2402.01326</a> [<a href="/pdf/2402.01326" title="Download PDF">pdf</a>, <a href="/ps/2402.01326" title="Download PostScript">ps</a>, <a href="/format/2402.01326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive multi-criteria-based load balancing technique for resource  allocation in fog-cloud environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gad-Elrab%2C+A+A+A">Ahmed A. A. Gad-Elrab</a>, 
<a href="/search/cs?searchtype=author&query=Alsharkawy%2C+A+S">Almohammady S. Alsharkawy</a>, 
<a href="/search/cs?searchtype=author&query=Embabi%2C+M+E">Mahmoud E. Embabi</a>, 
<a href="/search/cs?searchtype=author&query=Sobhi%2C+A">Ahmed Sobhi</a>, 
<a href="/search/cs?searchtype=author&query=Emara%2C+F+A">Farouk A. Emara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Recently, to deliver services directly to the network edge, fog computing, an
emerging and developing technology, acts as a layer between the cloud and the
IoT worlds. The cloud or fog computing nodes could be selected by IoTs
applications to meet their resource needs. Due to the scarce resources of fog
devices that are available, as well as the need to meet user demands for low
latency and quick reaction times, resource allocation in the fog-cloud
environment becomes a difficult problem. In this problem, the load balancing
between several fog devices is the most important element in achieving resource
efficiency and preventing overload on fog devices. In this paper, a new
adaptive resource allocation technique for load balancing in a fog-cloud
environment is proposed. The proposed technique ranks each fog device using
hybrid multi-criteria decision-making approaches Fuzzy Analytic Hierarchy
Process (FAHP) and Fuzzy Technique for Order Performance by Similarity to Ideal
Solution (FTOPSIS), then selects the most effective fog device based on the
resulting ranking set. The simulation results show that the proposed technique
outperforms existing techniques in terms of load balancing, response time,
resource utilization, and energy consumption. The proposed technique decreases
the number of fog nodes by 11%, load balancing variance by 69% and increases
resource utilization to 90% which is comparatively higher than the comparable
methods.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01327" title="Abstract">arXiv:2402.01327</a> [<a href="/pdf/2402.01327" title="Download PDF">pdf</a>, <a href="/format/2402.01327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Algorithmic Fairness in Distribution Shifts: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yujie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+M">Minglai Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Supervised fairness-aware machine learning under distribution shifts is an
emerging field that addresses the challenge of maintaining equitable and
unbiased predictions when faced with changes in data distributions from source
to target domains. In real-world applications, machine learning models are
often trained on a specific dataset but deployed in environments where the data
distribution may shift over time due to various factors. This shift can lead to
unfair predictions, disproportionately affecting certain groups characterized
by sensitive attributes, such as race and gender. In this survey, we provide a
summary of various types of distribution shifts and comprehensively investigate
existing methods based on these shifts, highlighting six commonly used
approaches in the literature. Additionally, this survey lists publicly
available datasets and evaluation metrics for empirical studies. We further
explore the interconnection with related research fields, discuss the
significant challenges, and identify potential directions for future studies.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01330" title="Abstract">arXiv:2402.01330</a> [<a href="/pdf/2402.01330" title="Download PDF">pdf</a>, <a href="/format/2402.01330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Semantic Communication with Major Object Extraction and Contextual  Video Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Haonan Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sihua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nuocheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Changchuan Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures, accepted by IEEE WCNC wksp 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This paper studies an end-to-end video semantic communication system for
massive communication. In the considered system, the transmitter must
continuously send the video to the receiver to facilitate character
reconstruction in immersive applications, such as interactive video conference.
However, transmitting the original video information with substantial amounts
of data poses a challenge to the limited wireless resources. To address this
issue, we reduce the amount of data transmitted by making the transmitter
extract and send the semantic information from the video, which refines the
major object and the correlation of time and space in the video. Specifically,
we first develop a video semantic communication system based on major object
extraction (MOE) and contextual video encoding (CVE) to achieve efficient video
transmission. Then, we design the MOE and CVE modules with convolutional neural
network based motion estimation, contextual extraction and entropy coding.
Simulation results show that compared to the traditional coding schemes, the
proposed method can reduce the amount of transmitted data by up to 25% while
increasing the peak signal-to-noise ratio (PSNR) of the reconstructed video by
up to 14%.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01331" title="Abstract">arXiv:2402.01331</a> [<a href="/pdf/2402.01331" title="Download PDF">pdf</a>, <a href="/format/2402.01331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A general framework for rotation invariant point cloud analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shuqing Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wei Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a general method for deep learning based point cloud analysis,
which is invariant to rotation on the inputs. Classical methods are vulnerable
to rotation, as they usually take aligned point clouds as input. Principle
Component Analysis (PCA) is a practical approach to achieve rotation
invariance. However, there are still some gaps between theory and practical
algorithms. In this work, we present a thorough study on designing rotation
invariant algorithms for point cloud analysis. We first formulate it as a
permutation invariant problem, then propose a general framework which can be
combined with any backbones. Our method is beneficial for further research such
as 3D pre-training and multi-modal learning. Experiments show that our method
has considerable or better performance compared to state-of-the-art approaches
on common benchmarks. Code is available at
https://github.com/luoshuqing2001/RI_framework.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01335" title="Abstract">arXiv:2402.01335</a> [<a href="/pdf/2402.01335" title="Download PDF">pdf</a>, <a href="/format/2402.01335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulator-Free Visual Domain Randomization via Video Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+C">Chintan Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Ra%C5%A1ajski%2C+N">Nemanja Ra&#x161;ajski</a>, 
<a href="/search/cs?searchtype=author&query=Makantasis%2C+K">Konstantinos Makantasis</a>, 
<a href="/search/cs?searchtype=author&query=Liapis%2C+A">Antonios Liapis</a>, 
<a href="/search/cs?searchtype=author&query=Yannakakis%2C+G+N">Georgios N. Yannakakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Domain randomization is an effective computer vision technique for improving
transferability of vision models across visually distinct domains exhibiting
similar content. Existing approaches, however, rely extensively on tweaking
complex and specialized simulation engines that are difficult to construct,
subsequently affecting their feasibility and scalability. This paper introduces
BehAVE, a video understanding framework that uniquely leverages the plethora of
existing commercial video games for domain randomization, without requiring
access to their simulation engines. Under BehAVE (1) the inherent rich visual
diversity of video games acts as the source of randomization and (2) player
behavior -- represented semantically via textual descriptions of actions --
guides the *alignment* of videos with similar content. We test BehAVE on 25
games of the first-person shooter (FPS) genre across various video and text
foundation models and we report its robustness for domain randomization. BehAVE
successfully aligns player behavioral patterns and is able to zero-shot
transfer them to multiple unseen FPS games when trained on just one FPS game.
In a more challenging setting, BehAVE manages to improve the zero-shot
transferability of foundation models to unseen FPS games (up to 22%) even when
trained on a game of a different genre (Minecraft). Code and dataset can be
found at https://github.com/nrasajski/BehAVE.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01339" title="Abstract">arXiv:2402.01339</a> [<a href="/pdf/2402.01339" title="Download PDF">pdf</a>, <a href="/format/2402.01339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Sequential Recommendations with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boz%2C+A">Artun Boz</a>, 
<a href="/search/cs?searchtype=author&query=Zorgdrager%2C+W">Wouter Zorgdrager</a>, 
<a href="/search/cs?searchtype=author&query=Kotti%2C+Z">Zoe Kotti</a>, 
<a href="/search/cs?searchtype=author&query=Harte%2C+J">Jesse Harte</a>, 
<a href="/search/cs?searchtype=author&query=Louridas%2C+P">Panos Louridas</a>, 
<a href="/search/cs?searchtype=author&query=Jannach%2C+D">Dietmar Jannach</a>, 
<a href="/search/cs?searchtype=author&query=Fragkoulis%2C+M">Marios Fragkoulis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 12 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The sequential recommendation problem has attracted considerable research
attention in the past few years, leading to the rise of numerous recommendation
models. In this work, we explore how Large Language Models (LLMs), which are
nowadays introducing disruptive effects in many AI-based applications, can be
used to build or improve sequential recommendation approaches. Specifically, we
design three orthogonal approaches and hybrids of those to leverage the power
of LLMs in different ways. In addition, we investigate the potential of each
approach by focusing on its comprising technical aspects and determining an
array of alternative choices for each one. We conduct extensive experiments on
three datasets and explore a large variety of configurations, including
different language models and baseline recommendation models, to obtain a
comprehensive picture of the performance of each approach. Among other
observations, we highlight that initializing state-of-the-art sequential
recommendation models such as BERT4Rec or SASRec with embeddings obtained from
an LLM can lead to substantial performance gains in terms of accuracy.
Furthermore, we find that fine-tuning an LLM for recommendation tasks enables
it to learn not only the tasks, but also concepts of a domain to some extent.
We also show that fine-tuning OpenAI GPT leads to considerably better
performance than fine-tuning Google PaLM 2. Overall, our extensive experiments
indicate a huge potential value of leveraging LLMs in future recommendation
approaches. We publicly share the code and data of our experiments to ensure
reproducibility.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01340" title="Abstract">arXiv:2402.01340</a> [<a href="/pdf/2402.01340" title="Download PDF">pdf</a>, <a href="/ps/2402.01340" title="Download PostScript">ps</a>, <a href="/format/2402.01340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SignSGD with Federated Defense: Harnessing Adversarial Attacks through  Gradient Sign Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanho Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Namyoon Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Signal Processing (eess.SP)

</div>
<p class="mathjax">Distributed learning is an effective approach to accelerate model training
using multiple workers. However, substantial communication delays emerge
between workers and a parameter server due to massive costs associated with
communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple
yet effective optimizer that reduces communication costs through one-bit
quantization, yet the convergence rates considerably decrease as adversarial
workers increase. In this paper, we show that the convergence rate is invariant
as the number of adversarial workers increases, provided that the number of
adversarial workers is smaller than that of benign workers. The key idea
showing this counter-intuitive result is our novel signSGD with federated
defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits
the gradient information sent by adversarial workers with the proper weights,
which are obtained through gradient sign decoding. Experimental results
demonstrate signSGD-FD achieves superior convergence rates over traditional
algorithms in various adversarial attack scenarios.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01341" title="Abstract">arXiv:2402.01341</a> [<a href="/pdf/2402.01341" title="Download PDF">pdf</a>, <a href="/ps/2402.01341" title="Download PostScript">ps</a>, <a href="/format/2402.01341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Properties of Causal Entropy and Information Gain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simoes%2C+F+N+F+Q">Francisco N. F. Q. Simoes</a>, 
<a href="/search/cs?searchtype=author&query=Dastani%2C+M">Mehdi Dastani</a>, 
<a href="/search/cs?searchtype=author&query=van+Ommen%2C+T">Thijs van Ommen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the conference CLeaR (Causal Learning and Reasoning) 2024. To appear in its proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent developments enable the quantification of causal control given a
structural causal model (SCM). This has been accomplished by introducing
quantities which encode changes in the entropy of one variable when intervening
on another. These measures, named causal entropy and causal information gain,
aim to address limitations in existing information theoretical approaches for
machine learning tasks where causality plays a crucial role. They have not yet
been properly mathematically studied. Our research contributes to the formal
understanding of the notions of causal entropy and causal information gain by
establishing and analyzing fundamental properties of these concepts, including
bounds and chain rules. Furthermore, we elucidate the relationship between
causal entropy and stochastic interventions. We also propose definitions for
causal conditional entropy and causal conditional information gain. Overall,
this exploration paves the way for enhancing causal machine learning tasks
through the study of recently-proposed information theoretic quantities
grounded in considerations about causality.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01342" title="Abstract">arXiv:2402.01342</a> [<a href="/pdf/2402.01342" title="Download PDF">pdf</a>, <a href="/format/2402.01342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-time Neuron Alignment through Permutation Subspace for  Improving Linear Mode Connectivity and Model Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zexi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chao Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In deep learning, stochastic gradient descent often yields functionally
similar yet widely scattered solutions in the weight space even under the same
initialization, causing barriers in the Linear Mode Connectivity (LMC)
landscape. Overcoming these barriers is crucial for understanding deep learning
dynamics and enhancing model-fusion algorithms. Previous studies highlight the
role of permutation symmetry in reducing post-training barriers through network
permutation. However, these post-hoc methods, demanding extra computations, are
less effective for larger, complex models (e.g., ViT, LLM) due to numerous
permutation matrices. Thus, in this paper, we study training-time neuron
alignment. Our hypothesis suggests that training-time permutation subspace can
reduce LMC barriers for free. We find that pruning at initialization supports
this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm
using a partial gradient mask during training. TNA-PFN is theoretically and
empirically validated for reducing LMC barriers. It excels in wide model fusion
applications, especially in federated learning, two algorithms based on TNA-FPN
that are proposed to show its prospects even under heterogeneous datasets.
Moreover, TNA-PFN can enhance the generalization of model soup for vision
transformers and ColD fusion for pretrained language models.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01343" title="Abstract">arXiv:2402.01343</a> [<a href="/pdf/2402.01343" title="Download PDF">pdf</a>, <a href="/format/2402.01343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shapelet-based Model-agnostic Counterfactual Local Explanations for Time  Series Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4ck%2C+T">Thomas B&#xe4;ck</a>, 
<a href="/search/cs?searchtype=author&query=van+Stein%2C+N">Niki van Stein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by the XAI4Sci workshop of AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we propose a model-agnostic instance-based post-hoc
explainability method for time series classification. The proposed algorithm,
namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual
explanations for arbitrary time series classifiers. We validate the proposed
method on several real-world univariate time series classification tasks from
the UCR Time Series Archive. The results indicate that the counterfactual
instances generated by Time-CF when compared to state-of-the-art methods,
demonstrate better performance in terms of four explainability metrics:
closeness, sensibility, plausibility, and sparsity.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01344" title="Abstract">arXiv:2402.01344</a> [<a href="/pdf/2402.01344" title="Download PDF">pdf</a>, <a href="/format/2402.01344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monotone, Bi-Lipschitz, and Polyak-&#x141;ojasiewicz Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruigang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dvijotham%2C+K">Krishnamurthy Dvijotham</a>, 
<a href="/search/cs?searchtype=author&query=Manchester%2C+I+R">Ian R. Manchester</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents a new \emph{bi-Lipschitz} invertible neural network, the
BiLipNet, which has the ability to control both its \emph{Lipschitzness}
(output sensitivity to input perturbations) and \emph{inverse Lipschitzness}
(input distinguishability from different outputs). The main contribution is a
novel invertible residual layer with certified strong monotonicity and
Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz
networks. The certification is based on incremental quadratic constraints,
which achieves much tighter bounds compared to spectral normalization.
Moreover, we formulate the model inverse calculation as a three-operator
splitting problem, for which fast algorithms are known. Based on the proposed
bi-Lipschitz network, we introduce a new scalar-output network, the PLNet,
which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn
non-convex surrogate losses with favourable properties, e.g., a unique and
efficiently-computable global minimum.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01345" title="Abstract">arXiv:2402.01345</a> [<a href="/pdf/2402.01345" title="Download PDF">pdf</a>, <a href="/format/2402.01345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip $\textbackslash n$: A simple method to reduce hallucination in  Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zongbo Han</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zechen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in large vision-language models (LVLMs) have demonstrated
impressive capability in visual information understanding with human language.
Despite these advances, LVLMs still face challenges with multimodal
hallucination, such as generating text descriptions of objects that are not
present in the visual information. However, the underlying fundamental reasons
of multimodal hallucinations remain poorly explored. In this paper, we propose
a new perspective, suggesting that the inherent biases in LVLMs might be a key
factor in hallucinations. Specifically, we systematically identify a semantic
shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'),
where the content before and after '$\textbackslash n\textbackslash n$' in the
training data frequently exhibit significant semantic changes. This pattern
leads the model to infer that the contents following '$\textbackslash
n\textbackslash n$' should be obviously different from the preceding contents
with less hallucinatory descriptions, thereby increasing the probability of
hallucinatory descriptions subsequent to the '$\textbackslash n\textbackslash
n$'. We have validated this hypothesis on multiple publicly available LVLMs.
Besides, we find that deliberately inserting '$\textbackslash n\textbackslash
n$' at the generated description can induce more hallucinations. A simple
method is proposed to effectively mitigate the hallucination of LVLMs by
skipping the output of `\textbackslash n'.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01348" title="Abstract">arXiv:2402.01348</a> [<a href="/pdf/2402.01348" title="Download PDF">pdf</a>, <a href="/format/2402.01348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CORE: Mitigating Catastrophic Forgetting in Continual Learning through  Cognitive Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianshu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yankai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Ziheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D">Dongyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces a novel perspective to significantly mitigate
catastrophic forgetting in continuous learning (CL), which emphasizes models'
capacity to preserve existing knowledge and assimilate new information. Current
replay-based methods treat every task and data sample equally and thus can not
fully exploit the potential of the replay buffer. In response, we propose
COgnitive REplay (CORE), which draws inspiration from human cognitive review
processes. CORE includes two key strategies: Adaptive Quantity Allocation and
Quality-Focused Data Selection. The former adaptively modulates the replay
buffer allocation for each task based on its forgetting rate, while the latter
guarantees the inclusion of representative data that best encapsulates the
characteristics of each task within the buffer. Our approach achieves an
average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline
method by 6.52%. Additionally, it significantly enhances the accuracy of the
poorest-performing task by 6.30% compared to the top baseline.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01349" title="Abstract">arXiv:2402.01349</a> [<a href="/pdf/2402.01349" title="Download PDF">pdf</a>, <a href="/format/2402.01349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Answers: Reviewing the Rationality of Multiple Choice  Question Answering for the Evaluation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haochun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sendong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiang%2C+Z">Zewen Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of natural language processing (NLP), Large Language Models
(LLMs) have precipitated a paradigm shift, markedly enhancing performance in
natural language generation tasks. Despite these advancements, the
comprehensive evaluation of LLMs remains an inevitable challenge for the
community. Recently, the utilization of Multiple Choice Question Answering
(MCQA) as a benchmark for LLMs has gained considerable traction. This study
investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs
genuinely understand the semantics of questions, their performance should
exhibit consistency across the varied configurations derived from the same
questions. Contrary to this expectation, our empirical findings suggest a
notable disparity in the consistency of LLM responses, which we define as
REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current
MCQA-based benchmarks may not adequately capture the true capabilities of LLMs,
which underscores the need for more robust evaluation mechanisms in assessing
the performance of LLMs.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01350" title="Abstract">arXiv:2402.01350</a> [<a href="/pdf/2402.01350" title="Download PDF">pdf</a>, <a href="/format/2402.01350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedMoE: Data-Level Personalization with Mixture of Experts for  Model-Heterogeneous Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) is widely employed for collaborative training on
decentralized data but faces challenges like data, system, and model
heterogeneity. This prompted the emergency of model-heterogeneous personalized
federated learning (MHPFL). However, concerns persist regarding data and model
privacy, model performance, communication, and computational costs in current
MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous
personalized Federated learning algorithm (FedMoE) with the Mixture of Experts
(MoE), renowned for enhancing large language models (LLMs). It assigns a shared
homogeneous small feature extractor and a local gating network for each
client's local heterogeneous large model. (1) During local training, the local
heterogeneous model's feature extractor acts as a local expert for personalized
feature (representation) extraction, while the shared homogeneous small feature
extractor serves as a global expert for generalized feature extraction. The
local gating network produces personalized weights for extracted
representations from both experts on each data sample. The three models form a
local heterogeneous MoE. The weighted mixed representation fuses global
generalized and local personalized features and is processed by the local
heterogeneous large model's header with personalized prediction information for
output. The MoE and prediction header are updated synchronously. (2) The
trained local homogeneous small feature extractors are sent to the server for
cross-client information fusion via aggregation. Briefly, FedMoE first enhances
local model personalization at a fine-grained data level while supporting model
heterogeneity.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01352" title="Abstract">arXiv:2402.01352</a> [<a href="/pdf/2402.01352" title="Download PDF">pdf</a>, <a href="/format/2402.01352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting  the Variation in Human Signals during Visuo-Linguistic Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takmaz%2C+E">Ece Takmaz</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">There is an intricate relation between the properties of an image and how
humans behave while describing the image. This behavior shows ample variation,
as manifested in human signals such as eye movements and when humans start to
describe the image. Despite the value of such signals of visuo-linguistic
variation, they are virtually disregarded in the training of current pretrained
models, which motivates further investigation. Using a corpus of Dutch image
descriptions with concurrently collected eye-tracking data, we explore the
nature of the variation in visuo-linguistic signals, and find that they
correlate with each other. Given this result, we hypothesize that variation
stems partly from the properties of the images, and explore whether image
representations encoded by pretrained vision encoders can capture such
variation. Our results indicate that pretrained models do so to a
weak-to-moderate degree, suggesting that the models lack biases about what
makes a stimulus complex for humans and what leads to variations in human
outputs.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01353" title="Abstract">arXiv:2402.01353</a> [<a href="/pdf/2402.01353" title="Download PDF">pdf</a>, <a href="/ps/2402.01353" title="Download PostScript">ps</a>, <a href="/format/2402.01353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient compilation of expressive problem space specifications to  neural network solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daggitt%2C+M+L">Matthew L. Daggitt</a>, 
<a href="/search/cs?searchtype=author&query=Kokke%2C+W">Wen Kokke</a>, 
<a href="/search/cs?searchtype=author&query=Atkey%2C+R">Robert Atkey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has described the presence of the embedding gap in neural network
verification. On one side of the gap is a high-level specification about the
network's behaviour, written by a domain expert in terms of the interpretable
problem space. On the other side are a logically-equivalent set of
satisfiability queries, expressed in the uninterpretable embedding space in a
form suitable for neural network solvers. In this paper we describe an
algorithm for compiling the former to the latter. We explore and overcome
complications that arise from targeting neural network solvers as opposed to
standard SMT solvers.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01355" title="Abstract">arXiv:2402.01355</a> [<a href="/pdf/2402.01355" title="Download PDF">pdf</a>, <a href="/format/2402.01355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FindingEmo: An Image Dataset for Emotion Recognition in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mertens%2C+L">Laurent Mertens</a>, 
<a href="/search/cs?searchtype=author&query=Yargholi%2C+E">Elahe&#x27; Yargholi</a>, 
<a href="/search/cs?searchtype=author&query=de+Beeck%2C+H+O">Hans Op de Beeck</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Stock%2C+J">Jan Van den Stock</a>, 
<a href="/search/cs?searchtype=author&query=Vennekens%2C+J">Joost Vennekens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 21 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce FindingEmo, a new image dataset containing annotations for 25k
images, specifically tailored to Emotion Recognition. Contrary to existing
datasets, it focuses on complex scenes depicting multiple people in various
naturalistic, social settings, with images being annotated as a whole, thereby
going beyond the traditional focus on faces or single individuals. Annotated
dimensions include Valence, Arousal and Emotion label, with annotations
gathered using Prolific. Together with the annotations, we release the list of
URLs pointing to the original images, as well as all associated source code.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01359" title="Abstract">arXiv:2402.01359</a> [<a href="/pdf/2402.01359" title="Download PDF">pdf</a>, <a href="/format/2402.01359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TESSERACT: Eliminating Experimental Bias in Malware Classification  across Space and Time (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kan%2C+Z">Zeliang Kan</a>, 
<a href="/search/cs?searchtype=author&query=McFadden%2C+S">Shae McFadden</a>, 
<a href="/search/cs?searchtype=author&query=Arp%2C+D">Daniel Arp</a>, 
<a href="/search/cs?searchtype=author&query=Pendlebury%2C+F">Feargus Pendlebury</a>, 
<a href="/search/cs?searchtype=author&query=Jordaney%2C+R">Roberto Jordaney</a>, 
<a href="/search/cs?searchtype=author&query=Kinder%2C+J">Johannes Kinder</a>, 
<a href="/search/cs?searchtype=author&query=Pierazzi%2C+F">Fabio Pierazzi</a>, 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+L">Lorenzo Cavallaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, submitted to ACM ToPS, under reviewing. arXiv admin note: text overlap with <a href="/abs/1807.07838">arXiv:1807.07838</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Performance (cs.PF)

</div>
<p class="mathjax">Machine learning (ML) plays a pivotal role in detecting malicious software.
Despite the high F1-scores reported in numerous studies reaching upwards of
0.99, the issue is not completely solved. Malware detectors often experience
performance decay due to constantly evolving operating systems and attack
methods, which can render previously learned knowledge insufficient for
accurate decision-making on new inputs. This paper argues that commonly
reported results are inflated due to two pervasive sources of experimental bias
in the detection task: spatial bias caused by data distributions that are not
representative of a real-world deployment; and temporal bias caused by
incorrect time splits of data, leading to unrealistic configurations. To
address these biases, we introduce a set of constraints for fair experiment
design, and propose a new metric, AUT, for classifier robustness in real-world
settings. We additionally propose an algorithm designed to tune training data
to enhance classifier performance. Finally, we present TESSERACT, an
open-source framework for realistic classifier comparison. Our evaluation
encompasses both traditional ML and deep learning methods, examining published
works on an extensive Android dataset with 259,230 samples over a five-year
span. Additionally, we conduct case studies in the Windows PE and PDF domains.
Our findings identify the existence of biases in previous studies and reveal
that significant performance enhancements are possible through appropriate,
periodic tuning. We explore how mitigation strategies may support in achieving
a more stable and better performance over time by employing multiple strategies
to delay performance decay.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01360" title="Abstract">arXiv:2402.01360</a> [<a href="/pdf/2402.01360" title="Download PDF">pdf</a>, <a href="/format/2402.01360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation  Properties for Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=W%C3%BChrl%2C+A">Amelie W&#xfc;hrl</a>, 
<a href="/search/cs?searchtype=author&query=Resendiz%2C+Y+M">Yarik Menchaca Resendiz</a>, 
<a href="/search/cs?searchtype=author&query=Grimminger%2C+L">Lara Grimminger</a>, 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Biomedical claim verification fails if no evidence can be discovered. In
these cases, the fact-checking verdict remains unknown and the claim is
unverifiable. To improve upon this, we have to understand if there are any
claim properties that impact its verifiability. In this work we assume that
entities and relations define the core variables in a biomedical claim's
anatomy and analyze if their properties help us to differentiate verifiable
from unverifiable claims. In a study with trained annotation experts we prompt
them to find evidence for biomedical claims, and observe how they refine search
queries for their evidence search. This leads to the first corpus for
scientific fact verification annotated with subject-relation-object triplets,
evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find
(1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is
particularly challenging. Further, we see that annotators process queries
mostly by adding constraints to the search and by normalizing entities to
canonical names. (2) We compare our in-house annotations with a small
crowdsourcing setting where we employ medical experts and laypeople. We find
that domain expertise does not have a substantial effect on the reliability of
annotations. Finally, (3), we demonstrate that it is possible to reliably
estimate the success of evidence retrieval purely from the claim text~(.82\F),
whereas identifying unverifiable claims proves more challenging (.27\F). The
dataset is available at <a href="http://www.ims.uni-stuttgart.de/data/bioclaim.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01361" title="Abstract">arXiv:2402.01361</a> [<a href="/pdf/2402.01361" title="Download PDF">pdf</a>, <a href="/format/2402.01361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To the Max: Reinventing Reward in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veviurko%2C+G">Grigorii Veviurko</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hmer%2C+W">Wendelin B&#xf6;hmer</a>, 
<a href="/search/cs?searchtype=author&query=de+Weerdt%2C+M">Mathijs de Weerdt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In reinforcement learning (RL), different rewards can define the same optimal
policy but result in drastically different learning performance. For some, the
agent gets stuck with a suboptimal behavior, and for others, it solves the task
efficiently. Choosing a good reward function is hence an extremely important
yet challenging problem. In this paper, we explore an alternative approach to
using rewards for learning. We introduce max-reward RL, where an agent
optimizes the maximum rather than the cumulative reward. Unlike earlier works,
our approach works for deterministic and stochastic environments and can be
easily combined with state-of-the-art RL algorithms. In the experiments, we
study the performance of max-reward RL algorithms in two goal-reaching
environments from Gymnasium-Robotics and demonstrate its benefits over standard
RL. The code is publicly available.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01363" title="Abstract">arXiv:2402.01363</a> [<a href="/pdf/2402.01363" title="Download PDF">pdf</a>, <a href="/format/2402.01363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bribe &amp; Fork: Cheap Bribing Attacks via Forking Threat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avarikioti%2C+Z">Zeta Avarikioti</a>, 
<a href="/search/cs?searchtype=author&query=K%C4%99dzior%2C+P">Pawe&#x142; K&#x119;dzior</a>, 
<a href="/search/cs?searchtype=author&query=Lizurej%2C+T">Tomasz Lizurej</a>, 
<a href="/search/cs?searchtype=author&query=Michalak%2C+T">Tomasz Michalak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this work, we reexamine the vulnerability of Payment Channel Networks
(PCNs) to bribing attacks, where an adversary incentivizes blockchain miners to
deliberately ignore a specific transaction to undermine the punishment
mechanism of PCNs. While previous studies have posited a prohibitive cost for
such attacks, we show that this cost may be dramatically reduced (to
approximately \$125), thereby increasing the likelihood of these attacks. To
this end, we introduce Bribe &amp; Fork, a modified bribing attack that leverages
the threat of a so-called feather fork which we analyze with a novel formal
model for the mining game with forking. We empirically analyze historical data
of some real-world blockchain implementations to evaluate the scale of this
cost reduction. Our findings shed more light on the potential vulnerability of
PCNs and highlight the need for robust solutions.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01364" title="Abstract">arXiv:2402.01364</a> [<a href="/pdf/2402.01364" title="Download PDF">pdf</a>, <a href="/format/2402.01364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongtong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) are not amenable to frequent re-training, due to
high training costs arising from their massive scale. However, updates are
necessary to endow LLMs with new skills and keep them up-to-date with rapidly
evolving human knowledge. This paper surveys recent works on continual learning
for LLMs. Due to the unique nature of LLMs, we catalog continue learning
techniques in a novel multi-staged categorization scheme, involving continual
pretraining, instruction tuning, and alignment. We contrast continual learning
for LLMs with simpler adaptation methods used in smaller models, as well as
with other enhancement strategies like retrieval-augmented generation and model
editing. Moreover, informed by a discussion of benchmarks and evaluation, we
identify several challenges and future work directions for this crucial task.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01366" title="Abstract">arXiv:2402.01366</a> [<a href="/pdf/2402.01366" title="Download PDF">pdf</a>, <a href="/format/2402.01366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MagicTac: A Novel High-Resolution 3D Multi-layer Grid-Based Tactile  Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dandan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurate robotic control over interactions with the environment is
fundamentally grounded in understanding tactile contacts. In this paper, we
introduce MagicTac, a novel high-resolution grid-based tactile sensor. This
sensor employs a 3D multi-layer grid-based design, inspired by the Magic Cube
structure. This structure can help increase the spatial resolution of MagicTac
to perceive external interaction contacts. Moreover, the sensor is produced
using the multi-material additive manufacturing technique, which simplifies the
manufacturing process while ensuring repeatability of production. Compared to
traditional vision-based tactile sensors, it offers the advantages of i) high
spatial resolution, ii) significant affordability, and iii)
fabrication-friendly construction that requires minimal assembly skills. We
evaluated the proposed MagicTac in the tactile reconstruction task using the
deformation field and optical flow. Results indicated that MagicTac could
capture fine textures and is sensitive to dynamic contact information. Through
the grid-based multi-material additive manufacturing technique, the
affordability and productivity of MagicTac can be enhanced with a minimum
manufacturing cost of 4.76 GBP and a minimum manufacturing time of 24.6
minutes.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01368" title="Abstract">arXiv:2402.01368</a> [<a href="/pdf/2402.01368" title="Download PDF">pdf</a>, <a href="/format/2402.01368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIR: Efficient Degradation Removal for Lightweight Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Dongqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+T">Ting Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+L">Liang Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, there have been significant advancements in Image Restoration based
on CNN and transformer. However, the inherent characteristics of the Image
Restoration task are often overlooked in many works. These works often focus on
the basic block design and stack numerous basic blocks to the model, leading to
redundant parameters and unnecessary computations and hindering the efficiency
of the image restoration. In this paper, we propose a Lightweight Image
Restoration network called LIR to efficiently remove degradation (blur, rain,
noise, haze, etc.). A key component in LIR is the Efficient Adaptive Attention
(EAA) Block, which is mainly composed of Adaptive Filters and Attention Blocks.
It is capable of adaptively sharpening contours, removing degradation, and
capturing global information in various image restoration scenes in an
efficient and computation-friendly manner. In addition, through a simple
structural design, LIR addresses the degradations existing in the local and
global residual connections that are ignored by modern networks. Extensive
experiments demonstrate that our LIR achieves comparable performance to
state-of-the-art networks on most benchmarks with fewer parameters and
computations. It is worth noting that our LIR produces better visual results
than state-of-the-art networks that are more in line with the human aesthetic.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01369" title="Abstract">arXiv:2402.01369</a> [<a href="/pdf/2402.01369" title="Download PDF">pdf</a>, <a href="/format/2402.01369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with  Multi-Modal Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dingcheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaojun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenjian Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models have been widely deployed in various image generation tasks,
demonstrating an extraordinary connection between image and text modalities.
However, they face challenges of being maliciously exploited to generate
harmful or sensitive images by appending a specific suffix to the original
prompt. Existing works mainly focus on using single-modal information to
conduct attacks, which fails to utilize multi-modal features and results in
less than satisfactory performance. Integrating multi-modal priors (MMP), i.e.
both text and image features, we propose a targeted attack method named
MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a
target object into the image content while simultaneously removing the original
object. The MMP-Attack shows a notable advantage over existing works with
superior universality and transferability, which can effectively attack
commercial text-to-image (T2I) models such as DALL-E 3. To the best of our
knowledge, this marks the first successful attempt of transfer-based attack to
commercial T2I models. Our code is publicly available at
\url{https://github.com/ydc123/MMP-Attack}.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01370" title="Abstract">arXiv:2402.01370</a> [<a href="/pdf/2402.01370" title="Download PDF">pdf</a>, <a href="/format/2402.01370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CC-VPSTO: Chance-Constrained Via-Point-based Stochastic Trajectory  Optimisation for Safe and Efficient Online Robot Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruderm%C3%BCller%2C+L">Lara Bruderm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+G">Guillaume Berger</a>, 
<a href="/search/cs?searchtype=author&query=Jankowski%2C+J">Julius Jankowski</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+R">Raunak Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Hawes%2C+N">Nick Hawes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures, submitted to IEEE Transactions on Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Safety in the face of uncertainty is a key challenge in robotics. In this
work, we propose a real-time capable framework to generate safe and
task-efficient robot trajectories for stochastic control problems. For that, we
first formulate the problem as a chance-constrained optimisation problem, in
which the probability of the controlled system to violate a safety constraint
is constrained to be below a user-defined threshold. To solve the
chance-constrained optimisation problem, we propose a Monte--Carlo
approximation relying on samples of the uncertainty to estimate the probability
of violating a safety constraint given a controller. We use this approximation
in the motion planner VP-STO to solve the sampled-based problem. Consequently,
we refer to our adapted approach as CC-VPSTO, which stands for
Chance-Constrained VP-STO. We address the crucial issue concerning the
Monte--Carlo approximation: given a predetermined number of uncertainty
samples, we propose several ways to define the sample-based problem such that
it is a reliable over-approximation of the original problem, i.e. any solution
to the sample-based problem adheres to the original chance-constrained problem
with high confidence. The strengths of our approach lie in i) its generality,
as it does not require any specific assumptions on the underlying uncertainty
distribution, the dynamics of the system, the cost function, and for some of
the proposed sample-based approximations, on the form of inequality
constraints; and ii) its applicability to MPC-settings. We demonstrate the
validity and efficiency of our approach on both simulation and real-world robot
experiments. For additional material, please visit
https://sites.google.com/oxfordrobotics.institute/cc-vpsto.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01371" title="Abstract">arXiv:2402.01371</a> [<a href="/pdf/2402.01371" title="Download PDF">pdf</a>, <a href="/format/2402.01371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critic-Actor for Average Reward MDPs with Function Approximation: A  Finite-Time Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Prashansa Panda</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+S">Shalabh Bhatnagar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, there has been a lot of research work activity focused on
carrying out asymptotic and non-asymptotic convergence analyses for
two-timescale actor critic algorithms where the actor updates are performed on
a timescale that is slower than that of the critic. In a recent work, the
critic-actor algorithm has been presented for the infinite horizon discounted
cost setting in the look-up table case where the timescales of the actor and
the critic are reversed and asymptotic convergence analysis has been presented.
In our work, we present the first critic-actor algorithm with function
approximation and in the long-run average reward setting and present the first
finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal
learning rates and prove that our algorithm achieves a sample complexity of
$\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the
critic to be upper bounded by $\epsilon$ which is better than the one obtained
for actor-critic in a similar setting. We also show the results of numerical
experiments on three benchmark settings and observe that the critic-actor
algorithm competes well with the actor-critic algorithm.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01372" title="Abstract">arXiv:2402.01372</a> [<a href="/pdf/2402.01372" title="Download PDF">pdf</a>, <a href="/format/2402.01372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Freeness Problem for Automaton Semigroups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Angeli%2C+D">Daniele D&#x27;Angeli</a>, 
<a href="/search/cs?searchtype=author&query=Rodaro%2C+E">Emanuele Rodaro</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%A4chter%2C+J+P">Jan Philipp W&#xe4;chter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Group Theory (math.GR)

</div>
<p class="mathjax">We show that the freeness problems for automaton semigroups and for automaton
monoids are undecidable by giving a reduction from Post's Correspondence
Problem. This construction seems to be quite versatile and we also immediately
obtain that the problems of testing whether a given automaton semigroup
(monoid) is (left) cancellative or whether it is equidivisible are undecidable.
We also obtain that it is undecidable whether a given map extends into a
homomorphism of automaton semigroups. Finally, we adapt our construction to
show that it is undecidable whether a given automaton generates a free monoid
whose basis is given by the states (but where we allow one state to act as the
identity). In the semigroup case, we show a weaker version of this statement.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01373" title="Abstract">arXiv:2402.01373</a> [<a href="/pdf/2402.01373" title="Download PDF">pdf</a>, <a href="/format/2402.01373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cmaes : A Simple yet Practical Python Library for CMA-ES
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nomura%2C+M">Masahiro Nomura</a>, 
<a href="/search/cs?searchtype=author&query=Shibata%2C+M">Masashi Shibata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">The covariance matrix adaptation evolution strategy (CMA-ES) has been highly
effective in black-box continuous optimization, as demonstrated by its success
in both benchmark problems and various real-world applications. To address the
need for an accessible yet potent tool in this domain, we developed cmaes, a
simple and practical Python library for CMA-ES. cmaes is characterized by its
simplicity, offering intuitive use and high code readability. This makes it
suitable for quickly using CMA-ES, as well as for educational purposes and
seamless integration into other libraries. Despite its simplistic design, cmaes
maintains enhanced functionality. It incorporates recent advancements in
CMA-ES, such as learning rate adaptation for challenging scenarios, transfer
learning, and mixed-integer optimization capabilities. These advanced features
are accessible through a user-friendly API, ensuring that cmaes can be easily
adopted in practical applications. We regard cmaes as the first choice for a
Python CMA-ES library among practitioners. The software is available under the
MIT license at https://github.com/CyberAgentAILab/cmaes.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01375" title="Abstract">arXiv:2402.01375</a> [<a href="/pdf/2402.01375" title="Download PDF">pdf</a>, <a href="/format/2402.01375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dive into the Chasm: Probing the Gap between In- and Cross-Topic  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waldis%2C+A">Andreas Waldis</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yufang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (LMs) perform well in In-Topic setups, where
training and testing data come from the same topics. However, they face
challenges in Cross-Topic scenarios where testing data is derived from distinct
topics -- such as Gun Control. This study analyzes various LMs with three
probing-based experiments to shed light on the reasons behind the In- vs.
Cross-Topic generalization gap. Thereby, we demonstrate, for the first time,
that generalization gaps and the robustness of the embedding space vary
significantly across LMs. Additionally, we assess larger LMs and underscore the
relevance of our analysis for recent models. Overall, diverse pre-training
objectives, architectural regularization, or data deduplication contribute to
more robust LMs and diminish generalization gaps. Our research contributes to a
deeper understanding and comparison of language models across different
generalization scenarios.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01376" title="Abstract">arXiv:2402.01376</a> [<a href="/pdf/2402.01376" title="Download PDF">pdf</a>, <a href="/ps/2402.01376" title="Download PostScript">ps</a>, <a href="/format/2402.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoTR: Low Tensor Rank Weight Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bershatsky%2C+D">Daniel Bershatsky</a>, 
<a href="/search/cs?searchtype=author&query=Cherniuk%2C+D">Daria Cherniuk</a>, 
<a href="/search/cs?searchtype=author&query=Daulbaev%2C+T">Talgat Daulbaev</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper we generalize and extend an idea of low-rank adaptation (LoRA)
of large language models (LLMs) based on Transformer architecture. Widely used
LoRA-like methods of fine-tuning LLMs are based on matrix factorization of
gradient update. We introduce LoTR, a novel approach for parameter-efficient
fine-tuning of LLMs which represents a gradient update to parameters in a form
of tensor decomposition. Low-rank adapter for each layer is constructed as a
product of three matrices, and tensor structure arises from sharing left and
right multipliers of this product among layers. Simultaneous compression of a
sequence of layers with low-rank tensor representation allows LoTR to archive
even better parameter efficiency then LoRA especially for deep models.
Moreover, the core tensor does not depend on original weight dimension and can
be made arbitrary small, which allows for extremely cheap and fast downstream
fine-tuning.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01379" title="Abstract">arXiv:2402.01379</a> [<a href="/pdf/2402.01379" title="Download PDF">pdf</a>, <a href="/format/2402.01379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized boosting with an increasing coefficient magnitude stop  criterion as meta-learner in hyperparameter optimization stacking ensemble
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fdez-D%C3%ADaz%2C+L">Laura Fdez-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+J+R">Jos&#xe9; Ram&#xf3;n Quevedo</a>, 
<a href="/search/cs?searchtype=author&query=Monta%C3%B1%C3%A9s%2C+E">Elena Monta&#xf1;&#xe9;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neurocomputing 2023 Volume 551 126516
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In Hyperparameter Optimization (HPO), only the hyperparameter configuration
with the best performance is chosen after performing several trials, then,
discarding the effort of training all the models with every hyperparameter
configuration trial and performing an ensemble of all them. This ensemble
consists of simply averaging the model predictions or weighting the models by a
certain probability. Recently, other more sophisticated ensemble strategies,
such as the Caruana method or the stacking strategy has been proposed. On the
one hand, the Caruana method performs well in HPO ensemble, since it is not
affected by the effects of multicollinearity, which is prevalent in HPO. It
just computes the average over a subset of predictions with replacement. But it
does not benefit from the generalization power of a learning process. On the
other hand, stacking methods include a learning procedure since a meta-learner
is required to perform the ensemble. Yet, one hardly finds advice about which
meta-learner is adequate. Besides, some meta-learners may suffer from the
effects of multicollinearity or need to be tuned to reduce them. This paper
explores meta-learners for stacking ensemble in HPO, free of hyperparameter
tuning, able to reduce the effects of multicollinearity and considering the
ensemble learning process generalization power. At this respect, the boosting
strategy seems promising as a stacking meta-learner. In fact, it completely
removes the effects of multicollinearity. This paper also proposes an implicit
regularization in the classical boosting method and a novel non-parametric stop
criterion suitable only for boosting and specifically designed for HPO. The
synergy between these two improvements over boosting exhibits competitive and
promising predictive power performance compared to other existing meta-learners
and ensemble approaches for HPO other than the stacking ensemble.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01380" title="Abstract">arXiv:2402.01380</a> [<a href="/pdf/2402.01380" title="Download PDF">pdf</a>, <a href="/format/2402.01380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate  Distortion Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guo Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Huanxiong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anni Tang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Li Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Volumetric videos, benefiting from immersive 3D realism and interactivity,
hold vast potential for various applications, while the tremendous data volume
poses significant challenges for compression. Recently, NeRF has demonstrated
remarkable potential in volumetric video compression thanks to its simple
representation and powerful 3D modeling capabilities, where a notable work is
ReRF. However, ReRF separates the modeling from compression process, resulting
in suboptimal compression efficiency. In contrast, in this paper, we propose a
volumetric video compression method based on dynamic NeRF in a more compact
manner. Specifically, we decompose the NeRF representation into the coefficient
fields and the basis fields, incrementally updating the basis fields in the
temporal domain to achieve dynamic modeling. Additionally, we perform
end-to-end joint optimization on the modeling and compression process to
further improve the compression efficiency. Extensive experiments demonstrate
that our method achieves higher compression efficiency compared to ReRF on
various datasets.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01383" title="Abstract">arXiv:2402.01383</a> [<a href="/pdf/2402.01383" title="Download PDF">pdf</a>, <a href="/format/2402.01383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based NLG Evaluation: Current Status and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+X">Xiao Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Evaluating natural language generation (NLG) is a vital but challenging
problem in artificial intelligence. Traditional evaluation metrics mainly
capturing content (e.g. n-gram) overlap between system outputs and references
are far from satisfactory, and large language models (LLMs) such as ChatGPT
have demonstrated great potential in NLG evaluation in recent years. Various
automatic evaluation methods based on LLMs have been proposed, including
metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled
evaluation data. In this survey, we first give a taxonomy of LLM-based NLG
evaluation methods, and discuss their pros and cons, respectively. We also
discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several
open problems in this area and point out future research directions.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01386" title="Abstract">arXiv:2402.01386</a> [<a href="/pdf/2402.01386" title="Download PDF">pdf</a>, <a href="/format/2402.01386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted  Approach for Qualitative Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+Z">Zeeshan Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Waseem%2C+M">Muhammad Waseem</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aakash Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Kemell%2C+K">Kai-Kristian Kemell</a>, 
<a href="/search/cs?searchtype=author&query=Xiaofeng%2C+W">Wang Xiaofeng</a>, 
<a href="/search/cs?searchtype=author&query=Duc%2C+A+N">Anh Nguyen Duc</a>, 
<a href="/search/cs?searchtype=author&query=Abrahamsson%2C+P">Pekka Abrahamsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages and 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) have enabled
collaborative human-bot interactions in Software Engineering (SE), similar to
many other professions. However, the potential benefits and implications of
incorporating LLMs into qualitative data analysis in SE have not been
completely explored. For instance, conducting qualitative data analysis
manually can be a time-consuming, effort-intensive, and error-prone task for
researchers. LLM-based solutions, such as generative AI models trained on
massive datasets, can be utilized to automate tasks in software development as
well as in qualitative data analysis. To this end, we utilized LLMs to automate
and expedite the qualitative data analysis processes. We employed a multi-agent
model, where each agent was tasked with executing distinct, individual research
related activities. Our proposed model interpreted large quantities of textual
documents and interview transcripts to perform several common tasks used in
qualitative analysis. The results show that this technical assistant speeds up
significantly the data analysis process, enabling researchers to manage larger
datasets much more effectively. Furthermore, this approach introduces a new
dimension of scalability and accuracy in qualitative research, potentially
transforming data interpretation methodologies in SE.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01389" title="Abstract">arXiv:2402.01389</a> [<a href="/pdf/2402.01389" title="Download PDF">pdf</a>, <a href="/format/2402.01389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SiMA-Hand: Boosting 3D Hand-Mesh Reconstruction by Single-to-Multi-View  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinqiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P">Pheng-Ann Heng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chi-Wing Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Estimating 3D hand mesh from RGB images is a longstanding track, in which
occlusion is one of the most challenging problems. Existing attempts towards
this task often fail when the occlusion dominates the image space. In this
paper, we propose SiMA-Hand, aiming to boost the mesh reconstruction
performance by Single-to-Multi-view Adaptation. First, we design a multi-view
hand reconstructor to fuse information across multiple views by holistically
adopting feature fusion at image, joint, and vertex levels. Then, we introduce
a single-view hand reconstructor equipped with SiMA. Though taking only one
view as input at inference, the shape and orientation features in the
single-view reconstructor can be enriched by learning non-occluded knowledge
from the extra views at training, enhancing the reconstruction precision on the
occluded regions. We conduct experiments on the Dex-YCB and HanCo benchmarks
with challenging object- and self-caused occlusion cases, manifesting that
SiMA-Hand consistently achieves superior performance over the state of the
arts. Code will be released on https://github.com/JoyboyWang/SiMA-Hand Pytorch.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01391" title="Abstract">arXiv:2402.01391</a> [<a href="/pdf/2402.01391" title="Download PDF">pdf</a>, <a href="/format/2402.01391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StepCoder: Improve Code Generation with Reinforcement Learning from  Compiler Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Haoxiang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Limao Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+E">Enyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Junjie Shan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Caishuang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaoran Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The advancement of large language models (LLMs) has significantly propelled
the field of code generation. Previous work integrated reinforcement learning
(RL) with compiler feedback for exploring the output space of LLMs to enhance
code generation quality. However, the lengthy code generated by LLMs in
response to complex human requirements makes RL exploration a challenge. Also,
since the unit tests may not cover the complicated code, optimizing LLMs by
using these unexecuted code snippets is ineffective. To tackle these
challenges, we introduce StepCoder, a novel RL framework for code generation,
consisting of two main components: CCCS addresses the exploration challenge by
breaking the long sequences code generation task into a Curriculum of Code
Completion Subtasks, while FGO only optimizes the model by masking the
unexecuted code segments to provide Fine-Grained Optimization. In addition, we
furthermore construct the APPS+ dataset for RL training, which is manually
verified to ensure the correctness of unit tests. Experimental results show
that our method improves the ability to explore the output space and
outperforms state-of-the-art approaches in corresponding benchmarks.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01393" title="Abstract">arXiv:2402.01393</a> [<a href="/pdf/2402.01393" title="Download PDF">pdf</a>, <a href="/format/2402.01393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALERT-Transformer: Bridging Asynchronous and Synchronous Machine  Learning for Real-Time Event-based Spatio-Temporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin-Turrero%2C+C">Carmen Martin-Turrero</a>, 
<a href="/search/cs?searchtype=author&query=Bouvier%2C+M">Maxence Bouvier</a>, 
<a href="/search/cs?searchtype=author&query=Breitenstein%2C+M">Manuel Breitenstein</a>, 
<a href="/search/cs?searchtype=author&query=Zanuttigh%2C+P">Pietro Zanuttigh</a>, 
<a href="/search/cs?searchtype=author&query=Parret%2C+V">Vincent Parret</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version. 8 pages, 7 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">We seek to enable classic processing of continuous ultra-sparse
spatiotemporal data generated by event-based sensors with dense machine
learning models. We propose a novel hybrid pipeline composed of asynchronous
sensing and synchronous processing that combines several ideas: (1) an
embedding based on PointNet models -- the ALERT module -- that can continuously
integrate new and dismiss old events thanks to a leakage mechanism, (2) a
flexible readout of the embedded data that allows to feed any downstream model
with always up-to-date features at any sampling rate, (3) exploiting the input
sparsity in a patch-based approach inspired by Vision Transformer to optimize
the efficiency of the method. These embeddings are then processed by a
transformer model trained for object and gesture recognition. Using this
approach, we achieve performances at the state-of-the-art with a lower latency
than competitors. We also demonstrate that our asynchronous model can operate
at any desired sampling rate.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01397" title="Abstract">arXiv:2402.01397</a> [<a href="/pdf/2402.01397" title="Download PDF">pdf</a>, <a href="/format/2402.01397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey on robustness in trajectory prediction for autonomous vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagenus%2C+J">Jeroen Hagenus</a>, 
<a href="/search/cs?searchtype=author&query=Mathiesen%2C+F+B">Frederik Baymler Mathiesen</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+J+F">Julian F. Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Zgonnikov%2C+A">Arkady Zgonnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous vehicles rely on accurate trajectory prediction to inform
decision-making processes related to navigation and collision avoidance.
However, current trajectory prediction models show signs of overfitting, which
may lead to unsafe or suboptimal behavior. To address these challenges, this
paper presents a comprehensive framework that categorizes and assesses the
definitions and strategies used in the literature on evaluating and improving
the robustness of trajectory prediction models. This involves a detailed
exploration of various approaches, including data slicing methods, perturbation
techniques, model architecture changes, and post-training adjustments. In the
literature, we see many promising methods for increasing robustness, which are
necessary for safe and reliable autonomous driving.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01399" title="Abstract">arXiv:2402.01399</a> [<a href="/pdf/2402.01399" title="Download PDF">pdf</a>, <a href="/format/2402.01399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Probabilistic Model to explain Self-Supervised Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bizeul%2C+A">Alice Bizeul</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+C">Carl Allen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Self-supervised learning (SSL) learns representations by leveraging an
auxiliary unsupervised task, such as classifying semantically related samples,
e.g. different data augmentations or modalities. Of the many approaches to SSL,
contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for
learning representations that achieve downstream performance close to that of
supervised learning. However, a theoretical understanding of the mechanism
behind these methods eludes. We propose a generative latent variable model for
the data and show that several families of discriminative self-supervised
algorithms, including contrastive methods, approximately induce its latent
structure over representations, providing a unifying theoretical framework. We
also justify links to mutual information and the use of a projection head.
Fitting our model generatively, as SimVE, improves performance over previous
VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows
the gap to discriminative methods on _content_ classification and, as our
analysis predicts, outperforms them where _style_ information is required,
taking a step toward task-agnostic representations.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01401" title="Abstract">arXiv:2402.01401</a> [<a href="/pdf/2402.01401" title="Download PDF">pdf</a>, <a href="/format/2402.01401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foster%2C+J">Jack Foster</a>, 
<a href="/search/cs?searchtype=author&query=Fogarty%2C+K">Kyle Fogarty</a>, 
<a href="/search/cs?searchtype=author&query=Schoepf%2C+S">Stefan Schoepf</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96ztireli%2C+C">Cengiz &#xd6;ztireli</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">To comply with AI and data regulations, the need to forget private or
copyrighted information from trained machine learning models is increasingly
important. The key challenge in unlearning is forgetting the necessary data in
a timely manner, while preserving model performance. In this work, we address
the zero-shot unlearning scenario, whereby an unlearning algorithm must be able
to remove data given only a trained model and the data to be forgotten. Under
such a definition, existing state-of-the-art methods are insufficient. Building
on the concepts of Lipschitz continuity, we present a method that induces
smoothing of the forget sample's output, with respect to perturbations of that
sample. We show this smoothing successfully results in forgetting while
preserving general model performance. We perform extensive empirical evaluation
of our method over a range of contemporary benchmarks, verifying that our
method achieves state-of-the-art performance under the strict constraints of
zero-shot unlearning.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01402" title="Abstract">arXiv:2402.01402</a> [<a href="/pdf/2402.01402" title="Download PDF">pdf</a>, <a href="/ps/2402.01402" title="Download PostScript">ps</a>, <a href="/format/2402.01402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comparison study of supervised learning techniques for the  approximation of high dimensional functions and feedback control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Oster%2C+M">Mathias Oster</a>, 
<a href="/search/math?searchtype=author&query=Saluzzi%2C+L">Luca Saluzzi</a>, 
<a href="/search/math?searchtype=author&query=Wenzel%2C+T">Tizian Wenzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Approximation of high dimensional functions is in the focus of machine
learning and data-based scientific computing. In many applications, empirical
risk minimisation techniques over nonlinear model classes are employed. Neural
networks, kernel methods and tensor decomposition techniques are among the most
popular model classes. We provide a numerical study comparing the performance
of these methods on various high-dimensional functions with focus on optimal
control problems, where the collection of the dataset is based on the
application of the State-Dependent Riccati Equation.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01403" title="Abstract">arXiv:2402.01403</a> [<a href="/pdf/2402.01403" title="Download PDF">pdf</a>, <a href="/ps/2402.01403" title="Download PostScript">ps</a>, <a href="/format/2402.01403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudoredundancy for the Bit-Flipping Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zumbr%C3%A4gel%2C+J">Jens Zumbr&#xe4;gel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The analysis of the decoding failure rate of the bit-flipping algorithm has
received increasing attention. For a binary linear code we consider the minimum
number of rows in a parity-check matrix such that the bit-flipping algorithm is
able to correct errors up to the minimum distance without any decoding
failures. We initiate a study of this bit-flipping redundancy, which is akin to
the stopping set, trapping set or pseudocodeword redundancy of binary linear
codes, and focus in particular on codes based on finite geometries.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01404" title="Abstract">arXiv:2402.01404</a> [<a href="/pdf/2402.01404" title="Download PDF">pdf</a>, <a href="/format/2402.01404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Measuring Context Utilization in Document-Level MT Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+W">Wafaa Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Niculae%2C+V">Vlad Niculae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document-level translation models are usually evaluated using general metrics
such as BLEU, which are not informative about the benefits of context. Current
work on context-aware evaluation, such as contrastive methods, only measure
translation accuracy on words that need context for disambiguation. Such
measures cannot reveal whether the translation model uses the correct
supporting context. We propose to complement accuracy-based evaluation with
measures of context utilization. We find that perturbation-based analysis
(comparing models' performance when provided with correct versus random
context) is an effective measure of overall context utilization. For a
finer-grained phenomenon-specific evaluation, we propose to measure how much
the supporting context contributes to handling context-dependent discourse
phenomena. We show that automatically-annotated supporting context gives
similar conclusions to human-annotated context and can be used as alternative
for cases where human annotations are not available. Finally, we highlight the
importance of using discourse-rich datasets when assessing context utilization.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01408" title="Abstract">arXiv:2402.01408</a> [<a href="/pdf/2402.01408" title="Download PDF">pdf</a>, <a href="/format/2402.01408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Climbing the Ladder of Interpretability with Counterfactual Concept  Bottleneck Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dominici%2C+G">Gabriele Dominici</a>, 
<a href="/search/cs?searchtype=author&query=Barbiero%2C+P">Pietro Barbiero</a>, 
<a href="/search/cs?searchtype=author&query=Giannini%2C+F">Francesco Giannini</a>, 
<a href="/search/cs?searchtype=author&query=Gjoreski%2C+M">Martin Gjoreski</a>, 
<a href="/search/cs?searchtype=author&query=Marra%2C+G">Giuseppe Marra</a>, 
<a href="/search/cs?searchtype=author&query=Langheinrich%2C+M">Marc Langheinrich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current deep learning models are not designed to simultaneously address three
fundamental questions: predict class labels to solve a given classification
task (the "What?"), explain task predictions (the "Why?"), and imagine
alternative scenarios that could result in different predictions (the "What
if?"). The inability to answer these questions represents a crucial gap in
deploying reliable AI agents, calibrating human trust, and deepening
human-machine interaction. To bridge this gap, we introduce CounterFactual
Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently
address the above queries all at once without the need to run post-hoc
searches. Our results show that CF-CBMs produce: accurate predictions (the
"What?"), simple explanations for task predictions (the "Why?"), and
interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or
estimate the most probable counterfactual to: (i) explain the effect of concept
interventions on tasks, (ii) show users how to get a desired class label, and
(iii) propose concept interventions via "task-driven" interventions.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01410" title="Abstract">arXiv:2402.01410</a> [<a href="/pdf/2402.01410" title="Download PDF">pdf</a>, <a href="/format/2402.01410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Correia%2C+M">Miguel Correia</a>, 
<a href="/search/cs?searchtype=author&query=Bissoto%2C+A">Alceu Bissoto</a>, 
<a href="/search/cs?searchtype=author&query=Santiago%2C+C">Carlos Santiago</a>, 
<a href="/search/cs?searchtype=author&query=Barata%2C+C">Catarina Barata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the iMIMIC Workshop @ MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Skin cancer detection through dermoscopy image analysis is a critical task.
However, existing models used for this purpose often lack interpretability and
reliability, raising the concern of physicians due to their black-box nature.
In this paper, we propose a novel approach for the diagnosis of melanoma using
an interpretable prototypical-part model. We introduce a guided supervision
based on non-expert feedback through the incorporation of: 1) binary masks,
obtained automatically using a segmentation network; and 2) user-refined
prototypes. These two distinct information pathways aim to ensure that the
learned prototypes correspond to relevant areas within the skin lesion,
excluding confounding factors beyond its boundaries. Experimental results
demonstrate that, even without expert supervision, our approach achieves
superior performance and generalization compared to non-interpretable models.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01411" title="Abstract">arXiv:2402.01411</a> [<a href="/pdf/2402.01411" title="Download PDF">pdf</a>, <a href="/format/2402.01411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodePori: Large Scale Model for Autonomous Software Development by Using  Multi-Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+Z">Zeeshan Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Waseem%2C+M">Muhammad Waseem</a>, 
<a href="/search/cs?searchtype=author&query=Saari%2C+M">Mika Saari</a>, 
<a href="/search/cs?searchtype=author&query=Syst%C3%A4%2C+K">Kari Syst&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Abrahamsson%2C+P">Pekka Abrahamsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs)
are reshaping the field of Software Engineering (SE). Existing LLM-based
multi-agent systems have successfully resolved simple dialogue tasks. However,
the potential of LLMs for more complex tasks, such as automated code generation
for large and complex projects, have been explored in only a few existing
works. This paper introduces CodePori, a novel model designed to automate code
generation for extensive and complex software projects based on natural
language prompts. We employ LLM-based multi-AI agents to handle creative and
challenging tasks in autonomous software development. Each agent engages with a
specific task, including system design, code development, code review, code
verification, and test engineering. We show in the paper that CodePori is able
to generate running code for large-scale projects, completing the entire
software development process in minutes rather than hours, and at a cost of a
few dollars. It identifies and mitigates potential security vulnerabilities and
corrects errors while maintaining a solid code performance level. We also
conducted an evaluation of CodePori against existing solutions using HumanEval
and the Massively Multitask Benchmark for Python (MBPP) benchmark. The results
indicate that CodePori improves upon the benchmarks in terms of code accuracy,
efficiency, and overall performance. For example, CodePori improves the pass@1
metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clear
improvement over the existing models. We also assessed CodePori's performance
through practitioner evaluations, with 91% expressing satisfaction with the
model's performance.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01412" title="Abstract">arXiv:2402.01412</a> [<a href="/pdf/2402.01412" title="Download PDF">pdf</a>, <a href="/format/2402.01412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bass Accompaniment Generation via Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasini%2C+M">Marco Pasini</a>, 
<a href="/search/cs?searchtype=author&query=Grachten%2C+M">Maarten Grachten</a>, 
<a href="/search/cs?searchtype=author&query=Lattner%2C+S">Stefan Lattner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The ability to automatically generate music that appropriately matches an
arbitrary input track is a challenging task. We present a novel controllable
system for generating single stems to accompany musical mixes of arbitrary
length. At the core of our method are audio autoencoders that efficiently
compress audio waveform samples into invertible latent representations, and a
conditional latent diffusion model that takes as input the latent encoding of a
mix and generates the latent encoding of a corresponding stem. To provide
control over the timbre of generated samples, we introduce a technique to
ground the latent space to a user-provided reference style during diffusion
sampling. For further improving audio quality, we adapt classifier-free
guidance to avoid distortions at high guidance strengths when generating an
unbounded latent space. We train our model on a dataset of pairs of mixes and
matching bass stems. Quantitative experiments demonstrate that, given an input
mix, the proposed system can generate basslines with user-specified timbres.
Our controllable conditional audio generation framework represents a
significant step forward in creating generative AI tools to assist musicians in
music production.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01413" title="Abstract">arXiv:2402.01413</a> [<a href="/pdf/2402.01413" title="Download PDF">pdf</a>, <a href="/format/2402.01413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Objective and subjective evaluation of speech enhancement methods in the  UDASE task of the 7th CHiME challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leglaive%2C+S">Simon Leglaive</a>, 
<a href="/search/cs?searchtype=author&query=Fraticelli%2C+M">Matthieu Fraticelli</a>, 
<a href="/search/cs?searchtype=author&query=ElGhazaly%2C+H">Hend ElGhazaly</a>, 
<a href="/search/cs?searchtype=author&query=Borne%2C+L">L&#xe9;onie Borne</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mostafa Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Wisdom%2C+S">Scott Wisdom</a>, 
<a href="/search/cs?searchtype=author&query=Pariente%2C+M">Manuel Pariente</a>, 
<a href="/search/cs?searchtype=author&query=Hershey%2C+J+R">John R. Hershey</a>, 
<a href="/search/cs?searchtype=author&query=Pressnitzer%2C+D">Daniel Pressnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Barker%2C+J+P">Jon P. Barker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Supervised models for speech enhancement are trained using artificially
generated mixtures of clean speech and noise signals. However, the synthetic
training conditions may not accurately reflect real-world conditions
encountered during testing. This discrepancy can result in poor performance
when the test domain significantly differs from the synthetic training domain.
To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to
leverage real-world noisy speech recordings from the test domain for
unsupervised domain adaptation of speech enhancement models. Specifically, this
test domain corresponds to the CHiME-5 dataset, characterized by real
multi-speaker and conversational speech recordings made in noisy and
reverberant domestic environments, for which ground-truth clean speech signals
are not available. In this paper, we present the objective and subjective
evaluations of the systems that were submitted to the CHiME-7 UDASE task, and
we provide an analysis of the results. This analysis reveals a limited
correlation between subjective ratings and several supervised nonintrusive
performance metrics recently proposed for speech enhancement. Conversely, the
results suggest that more traditional intrusive objective metrics can be used
for in-domain performance evaluation using the reverberant LibriCHiME-5 dataset
developed for the challenge. The subjective evaluation indicates that all
systems successfully reduced the background noise, but always at the expense of
increased distortion. Out of the four speech enhancement methods evaluated
subjectively, only one demonstrated an improvement in overall quality compared
to the unprocessed noisy speech, highlighting the difficulty of the task. The
tools and audio material created for the CHiME-7 UDASE task are shared with the
community.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01415" title="Abstract">arXiv:2402.01415</a> [<a href="/pdf/2402.01415" title="Download PDF">pdf</a>, <a href="/format/2402.01415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMLP: Symbolic Machine Learning Prover
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brau%C3%9Fe%2C+F">Franz Brau&#xdf;e</a>, 
<a href="/search/cs?searchtype=author&query=Khasidashvili%2C+Z">Zurab Khasidashvili</a>, 
<a href="/search/cs?searchtype=author&query=Korovin%2C+K">Konstantin Korovin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures. (submitted)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Symbolic Computation (cs.SC); Optimization and Control (math.OC)

</div>
<p class="mathjax">Symbolic Machine Learning Prover (SMLP) is a tool and a library for system
exploration based on data samples obtained by simulating or executing the
system on a number of input vectors. SMLP aims at exploring the system based on
this data by taking a grey-box approach: SMLP combines statistical methods of
data exploration with building and exploring machine learning models in close
feedback loop with the system's response, and exploring these models by
combining probabilistic and formal methods. SMLP has been applied in industrial
setting at Intel for analyzing and optimizing hardware designs at the analog
level. SMLP is a general purpose tool and can be applied to systems that can be
sampled and modeled by machine learning models.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01416" title="Abstract">arXiv:2402.01416</a> [<a href="/pdf/2402.01416" title="Download PDF">pdf</a>, <a href="/format/2402.01416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequence Shortening for Context-Aware Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C4%85ka%2C+P">Pawe&#x142; M&#x105;ka</a>, 
<a href="/search/cs?searchtype=author&query=Semerci%2C+Y+C">Yusuf Can Semerci</a>, 
<a href="/search/cs?searchtype=author&query=Scholtes%2C+J">Jan Scholtes</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of the ACL: EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Context-aware Machine Translation aims to improve translations of sentences
by incorporating surrounding sentences as context. Towards this task, two main
architectures have been applied, namely single-encoder (based on concatenation)
and multi-encoder models. In this study, we show that a special case of
multi-encoder architecture, where the latent representation of the source
sentence is cached and reused as the context in the next step, achieves higher
accuracy on the contrastive datasets (where the models have to rank the correct
translation among the provided sentences) and comparable BLEU and COMET scores
as the single- and multi-encoder approaches. Furthermore, we investigate the
application of Sequence Shortening to the cached representations. We test three
pooling-based shortening techniques and introduce two novel methods - Latent
Grouping and Latent Selecting, where the network learns to group tokens or
selects the tokens to be cached as context. Our experiments show that the two
methods achieve competitive BLEU and COMET scores and accuracies on the
contrastive datasets to the other tested methods while potentially allowing for
higher interpretability and reducing the growth of memory requirements with
increased context size.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01422" title="Abstract">arXiv:2402.01422</a> [<a href="/pdf/2402.01422" title="Download PDF">pdf</a>, <a href="/format/2402.01422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+G">Guanwen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Haoran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaoneng Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhihao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Q">Qiguang Miao</a>, 
<a href="/search/cs?searchtype=author&query=Pun%2C+C">Chi-Man Pun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Implementing fine-grained emotion control is crucial for emotion generation
tasks because it enhances the expressive capability of the generative model,
allowing it to accurately and comprehensively capture and express various
nuanced emotional states, thereby improving the emotional quality and
personalization of generated content. Generating fine-grained facial animations
that accurately portray emotional expressions using only a portrait and an
audio recording presents a challenge. In order to address this challenge, we
propose a visual attribute-guided audio decoupler. This enables the obtention
of content vectors solely related to the audio content, enhancing the stability
of subsequent lip movement coefficient predictions. To achieve more precise
emotional expression, we introduce a fine-grained emotion coefficient
prediction module. Additionally, we propose an emotion intensity control method
using a fine-grained emotion matrix. Through these, effective control over
emotional expression in the generated videos and finer classification of
emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient
generation networks are designed to predict 3D coefficients, followed by the
utilization of a rendering network to generate the final video. Our
experimental results demonstrate that our proposed method, EmoSpeaker,
outperforms existing emotional talking face generation methods in terms of
expression variation and lip synchronization. Project page:
https://peterfanfan.github.io/EmoSpeaker/
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01423" title="Abstract">arXiv:2402.01423</a> [<a href="/pdf/2402.01423" title="Download PDF">pdf</a>, <a href="/format/2402.01423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Different Tastes of Entities: Investigating Human Label Variation in  Named Entity Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Siyao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zihang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Loftus%2C+S">Sebastian Loftus</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages; Accepted at UnImplicit workshop at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named Entity Recognition (NER) is a key information extraction task with a
long-standing tradition. While recent studies address and aim to correct
annotation errors via re-labeling efforts, little is known about the sources of
human label variation, such as text ambiguity, annotation error, or guideline
divergence. This is especially the case for high-quality datasets and beyond
English CoNLL03. This paper studies disagreements in expert-annotated named
entity datasets for three languages: English, Danish, and Bavarian. We show
that text ambiguity and artificial guideline changes are dominant factors for
diverse annotations among high-quality revisions. We survey student annotations
on a subset of difficult entities and substantiate the feasibility and
necessity of manifold annotations for understanding named entity ambiguities
from a distributional perspective.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01424" title="Abstract">arXiv:2402.01424</a> [<a href="/pdf/2402.01424" title="Download PDF">pdf</a>, <a href="/format/2402.01424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Analysis of Robust Automatic Piano Transcription
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edwards%2C+D">Drew Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+S">Simon Dixon</a>, 
<a href="/search/cs?searchtype=author&query=Benetos%2C+E">Emmanouil Benetos</a>, 
<a href="/search/cs?searchtype=author&query=Maezawa%2C+A">Akira Maezawa</a>, 
<a href="/search/cs?searchtype=author&query=Kusaka%2C+Y">Yuta Kusaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Signal Processing Letters on 31 Janurary, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Algorithms for automatic piano transcription have improved dramatically in
recent years due to new datasets and modeling techniques. Recent developments
have focused primarily on adapting new neural network architectures, such as
the Transformer and Perceiver, in order to yield more accurate systems. In this
work, we study transcription systems from the perspective of their training
data. By measuring their performance on out-of-distribution annotated piano
data, we show how these models can severely overfit to acoustic properties of
the training data. We create a new set of audio for the MAESTRO dataset,
captured automatically in a professional studio recording environment via
Yamaha Disklavier playback. Using various data augmentation techniques when
training with the original and re-performed versions of the MAESTRO dataset, we
achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS
dataset, without seeing any of its training data. We subsequently analyze these
data augmentation techniques in a series of ablation studies to better
understand their influence on the resulting models.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01426" title="Abstract">arXiv:2402.01426</a> [<a href="/pdf/2402.01426" title="Download PDF">pdf</a>, <a href="/ps/2402.01426" title="Download PostScript">ps</a>, <a href="/format/2402.01426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pilot Length Optimization with RS-LS Channel Estimation for Extremely  Large Aperture Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al%C4%B1c%C4%B1o%C4%9Flu%2C+M">Mert Al&#x131;c&#x131;o&#x11f;lu</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+%C3%96+T">&#xd6;zlem Tu&#x11f;fe Demir</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rnson%2C+E">Emil Bj&#xf6;rnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be presented in IEEE WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Extremely large aperture arrays can enable unprecedented spatial multiplexing
in beyond 5G systems due to their extremely narrow beamfocusing capabilities.
However, acquiring the spatial correlation matrix to enable efficient channel
estimation is a complex task due to the vast number of antenna dimensions.
Recently, a new estimation method called the "reduced-subspace least squares
(RS-LS) estimator" has been proposed for densely packed arrays. This method
relies solely on the geometry of the array to limit the estimation resources.
In this paper, we address a gap in the existing literature by deriving the
average spectral efficiency for a certain distribution of user equipments (UEs)
and a lower bound on it when using the RS-LS estimator. This bound is
determined by the channel gain and the statistics of the normalized spatial
correlation matrices of potential UEs but, importantly, does not require
knowledge of a specific UE's spatial correlation matrix. We establish that
there exists a pilot length that maximizes this expression. Additionally, we
derive an approximate expression for the optimal pilot length under low
signal-to-noise ratio (SNR) conditions. Simulation results validate the
tightness of the derived lower bound and the effectiveness of using the
optimized pilot length.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01427" title="Abstract">arXiv:2402.01427</a> [<a href="/pdf/2402.01427" title="Download PDF">pdf</a>, <a href="/format/2402.01427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effect of diversity on group decision-making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karadzhov%2C+G">Georgi Karadzhov</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+A">Andreas Vlachos</a>, 
<a href="/search/cs?searchtype=author&query=Stafford%2C+T">Tom Stafford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We explore different aspects of cognitive diversity and its effect on the
success of group deliberation. To evaluate this, we use 500 dialogues from
small, online groups discussing the Wason Card Selection task - the DeliData
corpus. Leveraging the corpus, we perform quantitative analysis evaluating
three different measures of cognitive diversity. First, we analyse the effect
of group size as a proxy measure for diversity. Second, we evaluate the effect
of the size of the initial idea pool. Finally, we look into the content of the
discussion by analysing discussed solutions, discussion patterns, and how
conversational probing can improve those characteristics.
<br />Despite the reputation of groups for compounding bias, we show that small
groups can, through dialogue, overcome intuitive biases and improve individual
decision-making. Across a large sample and different operationalisations, we
consistently find that greater cognitive diversity is associated with more
successful group deliberation. Code and data used for the analysis are
available in the anonymised repository: https://anonymous.4open.science/
r/cogsci24-FD6D
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01428" title="Abstract">arXiv:2402.01428</a> [<a href="/pdf/2402.01428" title="Download PDF">pdf</a>, <a href="/ps/2402.01428" title="Download PostScript">ps</a>, <a href="/format/2402.01428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adjoint Natural Deduction (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Junyoung Jang</a>, 
<a href="/search/cs?searchtype=author&query=Roshal%2C+S">Sophia Roshal</a>, 
<a href="/search/cs?searchtype=author&query=Pfenning%2C+F">Frank Pfenning</a>, 
<a href="/search/cs?searchtype=author&query=Pientka%2C+B">Brigitte Pientka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Adjoint logic is a general approach to combining multiple logics with
different structural properties, including linear, affine, strict, and
(ordinary) intuitionistic logics, where each proposition has an intrinsic mode
of truth. It has been defined in the form of a sequent calculus because the
central concept of independence is most clearly understood in this form, and
because it permits a proof of cut elimination following standard techniques.
<br />In this paper we present a natural deduction formulation of adjoint logic and
show how it is related to the sequent calculus. As a consequence, every
provable proposition has a verification (sometimes called a long normal form).
We also give a computational interpretation of adjoint logic in the form of a
functional language and prove properties of computations that derive from the
structure of modes, including freedom from garbage (for modes without weakening
and contraction), strictness (for modes disallowing weakening), and erasure
(based on a preorder between modes). Finally, we present a surprisingly subtle
algorithm for type checking.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01431" title="Abstract">arXiv:2402.01431</a> [<a href="/pdf/2402.01431" title="Download PDF">pdf</a>, <a href="/format/2402.01431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Control for Continuous-Time POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eich%2C+Y">Yannick Eich</a>, 
<a href="/search/cs?searchtype=author&query=Alt%2C+B">Bastian Alt</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">This work proposes a decision-making framework for partially observable
systems in continuous time with discrete state and action spaces. As optimal
decision-making becomes intractable for large state spaces we employ
approximation methods for the filtering and the control problem that scale well
with an increasing number of states. Specifically, we approximate the
high-dimensional filtering distribution by projecting it onto a parametric
family of distributions, and integrate it into a control heuristic based on the
fully observable system to obtain a scalable policy. We demonstrate the
effectiveness of our approach on several partially observed systems, including
queueing systems and chemical reaction networks.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01438" title="Abstract">arXiv:2402.01438</a> [<a href="/pdf/2402.01438" title="Download PDF">pdf</a>, <a href="/ps/2402.01438" title="Download PostScript">ps</a>, <a href="/format/2402.01438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Effect of Multiple Natural Languages on Code Suggestion  Using GitHub Copilot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koyanagi%2C+K">Kei Koyanagi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Noguchi%2C+K">Kotaro Noguchi</a>, 
<a href="/search/cs?searchtype=author&query=Kondo%2C+M">Masanari Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Serebrenik%2C+A">Alexander Serebrenik</a>, 
<a href="/search/cs?searchtype=author&query=Kamei%2C+Y">Yasutaka Kamei</a>, 
<a href="/search/cs?searchtype=author&query=Ubayashi%2C+N">Naoyasu Ubayashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">GitHub Copilot is an AI-enabled tool that automates program synthesis. It has
gained significant attention since its launch in 2021. Recent studies have
extensively examined Copilot's capabilities in various programming tasks, as
well as its security issues. However, little is known about the effect of
different natural languages on code suggestion. Natural language is considered
a social bias in the field of NLP, and this bias could impact the diversity of
software engineering. To address this gap, we conducted an empirical study to
investigate the effect of three popular natural languages (English, Japanese,
and Chinese) on Copilot. We used 756 questions of varying difficulty levels
from AtCoder contests for evaluation purposes. The results highlight that the
capability varies across natural languages, with Chinese achieving the worst
performance. Furthermore, regardless of the type of natural language, the
performance decreases significantly as the difficulty of questions increases.
Our work represents the initial step in comprehending the significance of
natural languages in Copilot's capability and introduces promising
opportunities for future endeavors.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01439" title="Abstract">arXiv:2402.01439</a> [<a href="/pdf/2402.01439" title="Download PDF">pdf</a>, <a href="/format/2402.01439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Words to Molecules: A Survey of Large Language Models in Chemistry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yemin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Ying Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCAI 2024 survey track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">In recent years, Large Language Models (LLMs) have achieved significant
success in natural language processing (NLP) and various interdisciplinary
areas. However, applying LLMs to chemistry is a complex task that requires
specialized domain knowledge. This paper provides a thorough exploration of the
nuanced methodologies employed in integrating LLMs into the field of chemistry,
delving into the complexities and innovations at this interdisciplinary
juncture. Specifically, our analysis begins with examining how molecular
information is fed into LLMs through various representation and tokenization
methods. We then categorize chemical LLMs into three distinct groups based on
the domain and modality of their input data, and discuss approaches for
integrating these inputs for LLMs. Furthermore, this paper delves into the
pretraining objectives with adaptations to chemical LLMs. After that, we
explore the diverse applications of LLMs in chemistry, including novel
paradigms for their application in chemistry tasks. Finally, we identify
promising research directions, including further integration with chemical
knowledge, advancements in continual learning, and improvements in model
interpretability, paving the way for groundbreaking developments in the field.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01440" title="Abstract">arXiv:2402.01440</a> [<a href="/pdf/2402.01440" title="Download PDF">pdf</a>, <a href="/format/2402.01440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Learning on Graphs: from Meta-learning to Pre-training and  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xingtong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zemin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhihao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Bo%2C+J">Jianyuan Bo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hoi%2C+S+C+H">Steven C.H. Hoi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph representation learning, a critical step in graph-centric tasks, has
seen significant advancements. Earlier techniques often operate in an
end-to-end setting, where performance heavily relies on the availability of
ample labeled data. This constraint has spurred the emergence of few-shot
learning on graphs, where only a few task-specific labels are available for
each task. Given the extensive literature in this field, this survey endeavors
to synthesize recent developments, provide comparative insights, and identify
future directions. We systematically categorize existing studies into three
major families: meta-learning approaches, pre-training approaches, and hybrid
approaches, with a finer-grained classification in each family to aid readers
in their method selection process. Within each category, we analyze the
relationships among these methods and compare their strengths and limitations.
Finally, we outline prospective future directions for few-shot learning on
graphs to catalyze continued innovation in this field.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01442" title="Abstract">arXiv:2402.01442</a> [<a href="/pdf/2402.01442" title="Download PDF">pdf</a>, <a href="/format/2402.01442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized framework for admissibility preserving Lax-Wendroff Flux  Reconstruction for hyperbolic conservation laws with source terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Babbar%2C+A">Arpit Babbar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekar%2C+P">Praveen Chandrashekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures, has been submitted to proceedings for ICOSAHOM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order,
quadrature free method for solving hyperbolic conservation laws. We perform a
cell average decomposition of the LWFR scheme that is similar to the one used
in the admissibility preserving framework of Zhang and Shu (2010). By
performing a flux limiting of the time averaged numerical flux, the
decomposition is used to obtain an admissibility preserving LWFR scheme. The
admissibility preservation framework is further extended to a newly proposed
extension of LWFR scheme for conservation laws with source terms. This is the
first extension of the high order LW scheme that can handle source terms. The
admissibility and accuracy are verified by numerical experiments on the Ten
Moment equations of Livermore et al.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01443" title="Abstract">arXiv:2402.01443</a> [<a href="/pdf/2402.01443" title="Download PDF">pdf</a>, <a href="/format/2402.01443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frenetix Motion Planner: High-Performance and Modular Trajectory  Planning Algorithm for Complex Autonomous Driving Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moller%2C+K">Korbinian Moller</a>, 
<a href="/search/cs?searchtype=author&query=Trauth%2C+R">Rainer Trauth</a>, 
<a href="/search/cs?searchtype=author&query=Wuersching%2C+G">Gerald Wuersching</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages. Submitted to IEEE IV 2024 Korea Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Our work aims to present a high-performance and modular sampling-based
trajectory planning algorithm for autonomous vehicles. This algorithm is
tailored to address the complex challenges in solution space construction and
optimization problem formulation within the path planning domain. Our method
employs a multi-objective optimization strategy for efficient navigation in
static and highly dynamic environments, focusing on optimizing trajectory
comfort, safety, and path precision. This algorithm was then used to analyze
the algorithm performance and success rate in 1750 virtual complex urban and
highway scenarios. Our results demonstrate fast calculation times (8ms for 800
trajectories), a high success rate in complex scenarios (88%), and easy
adaptability with different modules presented. The most noticeable difference
exhibited was the fast trajectory sampling, feasibility check, and cost
evaluation step across various trajectory counts. While our study presents
promising results, it's important to note that our assessments have been
conducted exclusively in simulated environments, and real-world testing is
required to fully validate our findings. The code and the additional modules
used in this research are publicly available as open-source software and can be
accessed at the following link:
https://github.com/TUM-AVS/Frenetix-Motion-Planner.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01444" title="Abstract">arXiv:2402.01444</a> [<a href="/pdf/2402.01444" title="Download PDF">pdf</a>, <a href="/format/2402.01444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mission Critical -- Satellite Data is a Distinct Modality in Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rolf%2C+E">Esther Rolf</a>, 
<a href="/search/cs?searchtype=author&query=Klemmer%2C+K">Konstantin Klemmer</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C">Caleb Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Kerner%2C+H">Hannah Kerner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Satellite data has the potential to inspire a seismic shift for machine
learning -- one in which we rethink existing practices designed for traditional
data modalities. As machine learning for satellite data (SatML) gains traction
for its real-world impact, our field is at a crossroads. We can either continue
applying ill-suited approaches, or we can initiate a new research agenda that
centers around the unique characteristics and challenges of satellite data.
This position paper argues that satellite data constitutes a distinct modality
for machine learning research and that we must recognize it as such to advance
the quality and impact of SatML research across theory, methods, and
deployment. We outline critical discussion questions and actionable suggestions
to transform SatML from merely an intriguing application area to a dedicated
research discipline that helps move the needle on big challenges for machine
learning and society.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01446" title="Abstract">arXiv:2402.01446</a> [<a href="/pdf/2402.01446" title="Download PDF">pdf</a>, <a href="/format/2402.01446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guidance Graph Optimization for Lifelong Multi-Agent Path Finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">He Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+V">Varun Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaidis%2C+S">Stefanos Nikolaidis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoyang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">We study how to use guidance to improve the throughput of lifelong
Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while
incorporating guidance, such as highways, can accelerate MAPF algorithms, this
often results in a trade-off with solution quality. In addition, how to
generate good guidance automatically remains largely unexplored, with current
methods falling short of surpassing manually designed ones. In this work, we
introduce the directed guidance graph as a versatile representation of guidance
for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of
optimizing its edge weights. We present two GGO algorithms to automatically
generate guidance for arbitrary lifelong MAPF algorithms and maps. The first
method directly solves GGO by employing CMA-ES, a black-box optimization
algorithm. The second method, PIU, optimizes an update model capable of
generating guidance, demonstrating the ability to transfer optimized guidance
graphs to larger maps with similar layouts. Empirically, we show that (1) our
guidance graphs improve the throughput of three representative lifelong MAPF
algorithms in four benchmark maps, and (2) our update model can generate
guidance graphs for as large as $93 \times 91$ maps and as many as 3000 agents.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01450" title="Abstract">arXiv:2402.01450</a> [<a href="/pdf/2402.01450" title="Download PDF">pdf</a>, <a href="/format/2402.01450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving importance estimation in covariate shift for providing  accurate prediction error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fdez-D%C3%ADaz%2C+L">Laura Fdez-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Tomillo%2C+S+G">Sara Gonz&#xe1;lez Tomillo</a>, 
<a href="/search/cs?searchtype=author&query=Monta%C3%B1%C3%A9s%2C+E">Elena Monta&#xf1;&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+J+R">Jos&#xe9; Ram&#xf3;n Quevedo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems With Applications 2022 Volume 193 116376
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In traditional Machine Learning, the algorithms predictions are based on the
assumption that the data follows the same distribution in both the training and
the test datasets. However, in real world data this condition does not hold
and, for instance, the distribution of the covariates changes whereas the
conditional distribution of the targets remains unchanged. This situation is
called covariate shift problem where standard error estimation may be no longer
accurate. In this context, the importance is a measure commonly used to
alleviate the influence of covariate shift on error estimations. The main
drawback is that it is not easy to compute. The Kullback-Leibler Importance
Estimation Procedure (KLIEP) is capable of estimating importance in a promising
way. Despite its good performance, it fails to ignore target information, since
it only includes the covariates information for computing the importance. In
this direction, this paper explores the potential performance improvement if
target information is considered in the computation of the importance. Then, a
redefinition of the importance arises in order to be generalized in this way.
Besides the potential improvement in performance, including target information
make possible the application to a real application about plankton
classification that motivates this research and characterized by its great
dimensionality, since considering targets rather than covariates reduces the
computation and the noise in the covariates. The impact of taking target
information is also explored when Logistic Regression (LR), Kernel Mean
Matching (KMM), Ensemble Kernel Mean Matching (EKMM) and the naive predecessor
of KLIEP called Kernel Density Estimation (KDE) methods estimate the
importance. The experimental results lead to a more accurate error estimation
using target information, especially in case of the more promising method
KLIEP.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01451" title="Abstract">arXiv:2402.01451</a> [<a href="/pdf/2402.01451" title="Download PDF">pdf</a>, <a href="/format/2402.01451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divergence conforming finite element methods for flow-transport coupling  with osmotic effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khan%2C+A">Arbaz Khan</a>, 
<a href="/search/math?searchtype=author&query=Mora%2C+D">David Mora</a>, 
<a href="/search/math?searchtype=author&query=Ru%C3%ADz-Baier%2C+R">Ricardo Ru&#xed;z-Baier</a>, 
<a href="/search/math?searchtype=author&query=Vellojin%2C+J">Jesus Vellojin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a model for the coupling of flow and transport equations with
porous membrane-type conditions on part of the boundary. The governing
equations consist of the incompressible Navier--Stokes equations coupled with
an advection-diffusion equation, and we employ a Lagrange multiplier to enforce
the coupling between penetration velocity and transport on the membrane, while
mixed boundary conditions are considered in the remainder of the boundary. We
show existence and uniqueness of the continuous problem using a fixed-point
argument. Next, an H(div)-conforming finite element formulation is proposed,
and we address its a priori error analysis. The method uses an upwind approach
that provides stability in the convection-dominated regime. We showcase a set
of numerical examples validating the theory and illustrating the use of the new
methods in the simulation of reverse osmosis processes.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01453" title="Abstract">arXiv:2402.01453</a> [<a href="/pdf/2402.01453" title="Download PDF">pdf</a>, <a href="/format/2402.01453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Queen of England is not England&#x27;s Queen: On the Lack of Factual  Coherency in PLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Youssef%2C+P">Paul Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6tterer%2C+J">J&#xf6;rg Schl&#xf6;tterer</a>, 
<a href="/search/cs?searchtype=author&query=Seifert%2C+C">Christin Seifert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL Findings 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches
their representations and justifies their use as knowledge bases. Previous work
has focused on probing PLMs for factual knowledge by measuring how often they
can correctly predict an object entity given a subject and a relation, and
improving fact retrieval by optimizing the prompts used for querying PLMs. In
this work, we consider a complementary aspect, namely the coherency of factual
knowledge in PLMs, i.e., how often can PLMs predict the subject entity given
its initial prediction of the object entity. This goes beyond evaluating how
much PLMs know, and focuses on the internal state of knowledge inside them. Our
results indicate that PLMs have low coherency using manually written, optimized
and paraphrased prompts, but including an evidence paragraph leads to
substantial improvement. This shows that PLMs fail to model inverse relations
and need further enhancements to be able to handle retrieving facts from their
parameters in a coherent manner, and to be considered as knowledge bases.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01454" title="Abstract">arXiv:2402.01454</a> [<a href="/pdf/2402.01454" title="Download PDF">pdf</a>, <a href="/format/2402.01454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Large Language Models in Causal Discovery: A Statistical  Causal Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takayama%2C+M">Masayuki Takayama</a>, 
<a href="/search/cs?searchtype=author&query=Okuda%2C+T">Tadahisa Okuda</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Thong Pham</a>, 
<a href="/search/cs?searchtype=author&query=Ikenoue%2C+T">Tatsuyoshi Ikenoue</a>, 
<a href="/search/cs?searchtype=author&query=Fukuma%2C+S">Shingo Fukuma</a>, 
<a href="/search/cs?searchtype=author&query=Shimizu%2C+S">Shohei Shimizu</a>, 
<a href="/search/cs?searchtype=author&query=Sannai%2C+A">Akiyoshi Sannai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">In practical statistical causal discovery (SCD), embedding domain expert
knowledge as constraints into the algorithm is widely accepted as significant
for creating consistent meaningful causal models, despite the recognized
challenges in systematic acquisition of the background knowledge. To overcome
these challenges, this paper proposes a novel methodology for causal inference,
in which SCD methods and knowledge based causal inference (KBCI) with a large
language model (LLM) are synthesized through "statistical causal prompting
(SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have
revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result
with prior knowledge from LLM-KBCI to approach the ground truth, and that the
SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has
been clarified that an LLM can improve SCD with its background knowledge, even
if the LLM does not contain information on the dataset. The proposed approach
can thus address challenges such as dataset biases and limitations,
illustrating the potential of LLMs to improve data-driven causal inference
across diverse scientific domains.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01456" title="Abstract">arXiv:2402.01456</a> [<a href="/pdf/2402.01456" title="Download PDF">pdf</a>, <a href="/format/2402.01456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolution kernel adaptation to calibrated fisheye
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berenguel-Baeta%2C+B">Bruno Berenguel-Baeta</a>, 
<a href="/search/cs?searchtype=author&query=Santos-Villafranca%2C+M">Maria Santos-Villafranca</a>, 
<a href="/search/cs?searchtype=author&query=Bermudez-Cameo%2C+J">Jesus Bermudez-Cameo</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Yus%2C+A">Alejandro Perez-Yus</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+J+J">Jose J. Guerrero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Previously presented at BMVC: <a href="https://proceedings.bmvc2023.org/721/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Convolution kernels are the basic structural component of convolutional
neural networks (CNNs). In the last years there has been a growing interest in
fisheye cameras for many applications. However, the radially symmetric
projection model of these cameras produces high distortions that affect the
performance of CNNs, especially when the field of view is very large. In this
work, we tackle this problem by proposing a method that leverages the
calibration of cameras to deform the convolution kernel accordingly and adapt
to the distortion. That way, the receptive field of the convolution is similar
to standard convolutions in perspective images, allowing us to take advantage
of pre-trained networks in large perspective datasets. We show how, with just a
brief fine-tuning stage in a small dataset, we improve the performance of the
network for the calibrated fisheye with respect to standard convolutions in
depth estimation and semantic segmentation.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01459" title="Abstract">arXiv:2402.01459</a> [<a href="/pdf/2402.01459" title="Download PDF">pdf</a>, <a href="/format/2402.01459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waczy%C5%84ska%2C+J">Joanna Waczy&#x144;ska</a>, 
<a href="/search/cs?searchtype=author&query=Borycki%2C+P">Piotr Borycki</a>, 
<a href="/search/cs?searchtype=author&query=Tadeja%2C+S">S&#x142;awomir Tadeja</a>, 
<a href="/search/cs?searchtype=author&query=Tabor%2C+J">Jacek Tabor</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, a range of neural network-based methods for image rendering
have been introduced. For instance, widely-researched neural radiance fields
(NeRF) rely on a neural network to represent 3D scenes, allowing for realistic
view synthesis from a small number of 2D images. However, most NeRF models are
constrained by long training and inference times. In comparison, Gaussian
Splatting (GS) is a novel, state-of-theart technique for rendering points in a
3D scene by approximating their contribution to image pixels through Gaussian
distributions, warranting fast training and swift, real-time rendering. A
drawback of GS is the absence of a well-defined approach for its conditioning
due to the necessity to condition several hundred thousand Gaussian components.
To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of
mesh and a Gaussian distribution, that pin all Gaussians splats on the object
surface (mesh). The unique contribution of our methods is defining Gaussian
splats solely based on their location on the mesh, allowing for automatic
adjustments in position, scale, and rotation during animation. As a result, we
obtain high-quality renders in the real-time generation of high-quality views.
Furthermore, we demonstrate that in the absence of a predefined mesh, it is
possible to fine-tune the initial mesh during the learning process.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01461" title="Abstract">arXiv:2402.01461</a> [<a href="/pdf/2402.01461" title="Download PDF">pdf</a>, <a href="/format/2402.01461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Gyroscope: Combination of Deep Learning Features and Direct  Alignment for Panoramic Stabilization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berenguel-Baeta%2C+B">Bruno Berenguel-Baeta</a>, 
<a href="/search/cs?searchtype=author&query=Andre%2C+A+N">Antoine N. Andre</a>, 
<a href="/search/cs?searchtype=author&query=Caron%2C+G">Guillaume Caron</a>, 
<a href="/search/cs?searchtype=author&query=Bermudez-Cameo%2C+J">Jesus Bermudez-Cameo</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+J+J">Jose J. Guerrero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/CVF Conference on Computer Vision and Pattern Recognition pp.
  6444-6447, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this article we present a visual gyroscope based on equirectangular
panoramas. We propose a new pipeline where we take advantage of combining three
different methods to obtain a robust and accurate estimation of the attitude of
the camera. We quantitatively and qualitatively validate our method on two
image sequences taken with a $360^\circ$ dual-fisheye camera mounted on
different aerial vehicles.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01462" title="Abstract">arXiv:2402.01462</a> [<a href="/pdf/2402.01462" title="Download PDF">pdf</a>, <a href="/format/2402.01462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine  Mesh Models Using Local Anatomical Vertebral Axes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kramer%2C+I">Ivanna Kramer</a>, 
<a href="/search/cs?searchtype=author&query=Rittel%2C+V">Vinzent Rittel</a>, 
<a href="/search/cs?searchtype=author&query=Blomenkamp%2C+L">Lara Blomenkamp</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Sabine Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Paulus%2C+D">Dietrich Paulus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vertebral morphological measurements are important across various
disciplines, including spinal biomechanics and clinical applications, pre- and
post-operatively. These measurements also play a crucial role in
anthropological longitudinal studies, where spinal metrics are repeatedly
documented over extended periods. Traditionally, such measurements have been
manually conducted, a process that is time-consuming. In this study, we
introduce a novel, fully automated method for measuring vertebral morphology
using 3D meshes of lumbar and thoracic spine models.Our experimental results
demonstrate the method's capability to accurately measure low-resolution
patient-specific vertebral meshes with mean absolute error (MAE) of 1.09 mm and
those derived from artificially created lumbar spines, where the average MAE
value was 0.7 mm. Our qualitative analysis indicates that measurements obtained
using our method on 3D spine models can be accurately reprojected back onto the
original medical images if these images are available.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01465" title="Abstract">arXiv:2402.01465</a> [<a href="/pdf/2402.01465" title="Download PDF">pdf</a>, <a href="/format/2402.01465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reinforcement Learning-Boosted Motion Planning Framework:  Comprehensive Generalization Performance in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trauth%2C+R">Rainer Trauth</a>, 
<a href="/search/cs?searchtype=author&query=Hobmeier%2C+A">Alexander Hobmeier</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages. Submitted in Conference IEEE IV 2024 Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This study introduces a novel approach to autonomous motion planning,
informing an analytical algorithm with a reinforcement learning (RL) agent
within a Frenet coordinate system. The combination directly addresses the
challenges of adaptability and safety in autonomous driving. Motion planning
algorithms are essential for navigating dynamic and complex scenarios.
Traditional methods, however, lack the flexibility required for unpredictable
environments, whereas machine learning techniques, particularly reinforcement
learning (RL), offer adaptability but suffer from instability and a lack of
explainability. Our unique solution synergizes the predictability and stability
of traditional motion planning algorithms with the dynamic adaptability of RL,
resulting in a system that efficiently manages complex situations and adapts to
changing environmental conditions. Evaluation of our integrated approach shows
a significant reduction in collisions, improved risk management, and improved
goal success rates across multiple scenarios. The code used in this research is
publicly available as open-source software and can be accessed at the following
link: https://github.com/TUM-AVS/Frenetix-RL.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01466" title="Abstract">arXiv:2402.01466</a> [<a href="/pdf/2402.01466" title="Download PDF">pdf</a>, <a href="/format/2402.01466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaled 360 layouts: Revisiting non-central panoramas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berenguel-Baeta%2C+B">Bruno Berenguel-Baeta</a>, 
<a href="/search/cs?searchtype=author&query=Bermudez-Cameo%2C+J">Jesus Bermudez-Cameo</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+J+J">Jose J. Guerrero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2401.17058">arXiv:2401.17058</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (pp. 3702-3705) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">From a non-central panorama, 3D lines can be recovered by geometric
reasoning. However, their sensitivity to noise and the complex geometric
modeling required has led these panoramas being very little investigated. In
this work we present a novel approach for 3D layout recovery of indoor
environments using single non-central panoramas. We obtain the boundaries of
the structural lines of the room from a non-central panorama using deep
learning and exploit the properties of non-central projection systems in a new
geometrical processing to recover the scaled layout. We solve the problem for
Manhattan environments, handling occlusions, and also for Atlanta environments
in an unified method. The experiments performed improve the state-of-the-art
methods for 3D layout recovery from a single panorama. Our approach is the
first work using deep learning with non-central panoramas and recovering the
scale of single panorama layouts.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01467" title="Abstract">arXiv:2402.01467</a> [<a href="/pdf/2402.01467" title="Download PDF">pdf</a>, <a href="/format/2402.01467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiyi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+L">Likai Tang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Huimiao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Sen Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Can replay, as a widely observed neural activity pattern in brain regions,
particularly in the hippocampus and neocortex, emerge in an artificial agent?
If yes, does it contribute to the tasks? In this work, without heavy dependence
on complex assumptions, we discover naturally emergent replay under
task-optimized paradigm using a recurrent neural network-based reinforcement
learning model, which mimics the hippocampus and prefrontal cortex, as well as
their intercommunication and the sensory cortex input. The emergent replay in
the hippocampus, which results from the episodic memory and cognitive map as
well as environment observations, well resembles animal experimental data and
serves as an effective indicator of high task performance. The model also
successfully reproduces local and nonlocal replay, which matches the human
experimental data. Our work provides a new avenue for understanding the
mechanisms behind replay.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01469" title="Abstract">arXiv:2402.01469</a> [<a href="/pdf/2402.01469" title="Download PDF">pdf</a>, <a href="/format/2402.01469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through  Process Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jian Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zujie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The notable success of large language models (LLMs) has sparked an upsurge in
building language agents to complete various complex tasks. We present AMOR, an
agent framework based on open-source LLMs, which reasons with external
knowledge bases and adapts to specific domains through human supervision to the
reasoning process. AMOR builds reasoning logic over a finite state machine
(FSM) that solves problems through autonomous executions and transitions over
disentangled modules. This allows humans to provide direct feedback to the
individual modules, and thus naturally forms process supervision. Based on this
reasoning and feedback framework, we develop AMOR through two-stage
fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with
examples automatically constructed from various public datasets and enables
AMOR to generalize across different knowledge environments, while the latter
tailors AMOR to specific domains using process feedback. Extensive experiments
across multiple domains demonstrate the advantage of AMOR to strong baselines,
thanks to its FSM-based reasoning and process feedback mechanism.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01472" title="Abstract">arXiv:2402.01472</a> [<a href="/pdf/2402.01472" title="Download PDF">pdf</a>, <a href="/format/2402.01472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Data for the Mitigation of Demographic Biases in Face  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melzi%2C+P">Pietro Melzi</a>, 
<a href="/search/cs?searchtype=author&query=Rathgeb%2C+C">Christian Rathgeb</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana%2C+R">Ruben Tolosana</a>, 
<a href="/search/cs?searchtype=author&query=Vera-Rodriguez%2C+R">Ruben Vera-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+A">Aythami Morales</a>, 
<a href="/search/cs?searchtype=author&query=Lawatsch%2C+D">Dominik Lawatsch</a>, 
<a href="/search/cs?searchtype=author&query=Domin%2C+F">Florian Domin</a>, 
<a href="/search/cs?searchtype=author&query=Schaubert%2C+M">Maxim Schaubert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the International Joint Conference on Biometrics
  2023, special session on "Synthetic Data in Biometrics"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This study investigates the possibility of mitigating the demographic biases
that affect face recognition technologies through the use of synthetic data.
Demographic biases have the potential to impact individuals from specific
demographic groups, and can be identified by observing disparate performance of
face recognition systems across demographic groups. They primarily arise from
the unequal representations of demographic groups in the training data. In
recent times, synthetic data have emerged as a solution to some problems that
affect face recognition systems. In particular, during the generation process
it is possible to specify the desired demographic and facial attributes of
images, in order to control the demographic distribution of the synthesized
dataset, and fairly represent the different demographic groups. We propose to
fine-tune with synthetic data existing face recognition systems that present
some demographic biases. We use synthetic datasets generated with GANDiffFace,
a novel framework able to synthesize datasets for face recognition with
controllable demographic distribution and realistic intra-class variations. We
consider multiple datasets representing different demographic groups for
training and evaluation. Also, we fine-tune different face recognition systems,
and evaluate their demographic fairness with different metrics. Our results
support the proposed approach and the use of synthetic data to mitigate
demographic biases in face recognition.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01473" title="Abstract">arXiv:2402.01473</a> [<a href="/pdf/2402.01473" title="Download PDF">pdf</a>, <a href="/format/2402.01473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On approximate implicit Taylor methods for ordinary differential  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=B%C3%BCrger%2C+R">Raimund B&#xfc;rger</a>, 
<a href="/search/math?searchtype=author&query=del+Carmen+Mart%C3%AD%2C+M">Mar&#xed;a del Carmen Mart&#xed;</a>, 
<a href="/search/math?searchtype=author&query=Mulet%2C+P">Pep Mulet</a>, 
<a href="/search/math?searchtype=author&query=Zor%C3%ADo%2C+D">David Zor&#xed;o</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Comput. Appl. Math. 39 (2020), no. 4, Paper No. 304, 21 pp
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">An efficient approximate version of implicit Taylor methods for initial-value
problems of systems of ordinary differential equations (ODEs) is introduced.
The approach, based on an approximate formulation of Taylor methods, produces a
method that requires less evaluations of the function that defines the ODE and
its derivatives than the usual version. On the other hand, an efficient
numerical solution of the equation that arises from the discretization by means
of Newton's method is introduced for an implicit scheme of any order. Numerical
experiments illustrate that the resulting algorithm is simpler to implement and
has better performance than its exact counterpart.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01476" title="Abstract">arXiv:2402.01476</a> [<a href="/pdf/2402.01476" title="Download PDF">pdf</a>, <a href="/format/2402.01476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Q">Qinghua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Tonin%2C+F">Francesco Tonin</a>, 
<a href="/search/cs?searchtype=author&query=Suykens%2C+J+A+K">Johan A.K. Suykens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernel is tackled by KSVD and a reduced time complexity is acquired
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">While the great capability of Transformers significantly boosts prediction
accuracy, it could also yield overconfident predictions and require calibrated
uncertainty estimation, which can be commonly tackled by Gaussian processes
(GPs). Existing works apply GPs with symmetric kernels under variational
inference to the attention kernel; however, omitting the fact that attention
kernels are in essence asymmetric. Moreover, the complexity of deriving the GP
posteriors remains high for large-scale data. In this work, we propose
Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building
uncertainty-aware self-attention where the asymmetry of attention kernels is
tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through
KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from
KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using
only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP
posteriors can be based on the inversion of a diagonal matrix containing
singular values, contributing to a reduction in time complexity; iii) an
evidence lower bound is derived so that variational parameters can be optimized
towards this objective. Experiments verify our excellent performances and
efficiency on in-distribution, distribution-shift and out-of-distribution
benchmarks.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01477" title="Abstract">arXiv:2402.01477</a> [<a href="/pdf/2402.01477" title="Download PDF">pdf</a>, <a href="/format/2402.01477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modular Aerial System Based on Homogeneous Quadrotors with  Fault-Tolerant Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kai Cui</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The standard quadrotor is one of the most popular and widely used aerial
vehicle of recent decades, offering great maneuverability with mechanical
simplicity. However, the under-actuation characteristic limits its
applications, especially when it comes to generating desired wrench with six
degrees of freedom (DOF). Therefore, existing work often compromises between
mechanical complexity and the controllable DOF of the aerial system. To take
advantage of the mechanical simplicity of a standard quadrotor, we propose a
modular aerial system, IdentiQuad, that combines only homogeneous
quadrotor-based modules. Each IdentiQuad can be operated alone like a standard
quadrotor, but at the same time allows task-specific assembly, increasing the
controllable DOF of the system. Each module is interchangeable within its
assembly. We also propose a general controller for different configurations of
assemblies, capable of tolerating rotor failures and balancing the energy
consumption of each module. The functionality and robustness of the system and
its controller are validated using physics-based simulations for different
assembly configurations.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01480" title="Abstract">arXiv:2402.01480</a> [<a href="/pdf/2402.01480" title="Download PDF">pdf</a>, <a href="/format/2402.01480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selenium-Jupiter: A JUnit 5 extension for Selenium WebDriver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+B">Boni Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Kloos%2C+C+D">Carlos Delgado Kloos</a>, 
<a href="/search/cs?searchtype=author&query=Alario-Hoyos%2C+C">Carlos Alario-Hoyos</a>, 
<a href="/search/cs?searchtype=author&query=Munoz-Organero%2C+M">Mario Munoz-Organero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Systems and Software, 2022, 189, 111298
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">Selenium WebDriver is a library that allows controlling web browsers (e.g.,
Chrome, Firefox, etc.) programmatically. It provides a cross-browser
programming interface in several languages used primarily to implement
end-to-end tests for web applications. JUnit is a popular unit testing
framework for Java. Its latest version (i.e., JUnit 5) provides a programming
and extension model called Jupiter. This paper presents Selenium-Jupiter, an
open-source JUnit 5 extension for Selenium WebDriver. Selenium-Jupiter aims to
ease the development of Selenium WebDriver tests thanks to an automated driver
management process implemented in conjunction with the Jupiter parameter
resolution mechanism. Moreover, Selenium-Jupiter provides seamless integration
with Docker, allowing the use of different web browsers in Docker containers
out of the box. This feature enables cross-browser testing, load testing, and
troubleshooting (e.g., configurable session recordings). This paper presents an
example case in which Selenium-Jupiter is used to evaluate the performance of
video conferencing systems based on WebRTC. This example case shows that
Selenium-Jupiter can build and maintain the required infrastructure for complex
tests effortlessly.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01481" title="Abstract">arXiv:2402.01481</a> [<a href="/pdf/2402.01481" title="Download PDF">pdf</a>, <a href="/format/2402.01481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level protein pre-training with Vabs-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiale Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+W">Wanru Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jia Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuqi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">In recent years, there has been a surge in the development of 3D
structure-based pre-trained protein models, representing a significant
advancement over pre-trained protein language models in various downstream
tasks. However, most existing structure-based pre-trained models primarily
focus on the residue level, i.e., alpha carbon atoms, while ignoring other
atoms like side chain atoms. We argue that modeling proteins at both residue
and atom levels is important since the side chain atoms can also be crucial for
numerous downstream tasks, for example, molecular docking. Nevertheless, we
find that naively combining residue and atom information during pre-training
typically fails. We identify a key reason is the information leakage caused by
the inclusion of atom structure in the input, which renders residue-level
pre-training tasks trivial and results in insufficiently expressive residue
representations. To address this issue, we introduce a span mask pre-training
strategy on 3D protein chains to learn meaningful representations of both
residues and atoms. This leads to a simple yet effective approach to learning
protein representation suitable for diverse downstream tasks. Extensive
experimental results on binding site prediction and function prediction tasks
demonstrate our proposed pre-training approach significantly outperforms other
methods. Our code will be made public.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01484" title="Abstract">arXiv:2402.01484</a> [<a href="/pdf/2402.01484" title="Download PDF">pdf</a>, <a href="/format/2402.01484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting the Dots: Is Mode-Connectedness the Key to Feasible  Sample-Based Inference in Bayesian Neural Networks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sommer%2C+E">Emanuel Sommer</a>, 
<a href="/search/cs?searchtype=author&query=Wimmer%2C+L">Lisa Wimmer</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Bothmann%2C+L">Ludwig Bothmann</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCgamer%2C+D">David R&#xfc;gamer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">A major challenge in sample-based inference (SBI) for Bayesian neural
networks is the size and structure of the networks' parameter space. Our work
shows that successful SBI is possible by embracing the characteristic
relationship between weight and function space, uncovering a systematic link
between overparameterization and the difficulty of the sampling problem.
Through extensive experiments, we establish practical guidelines for sampling
and convergence diagnosis. As a result, we present a Bayesian deep ensemble
approach as an effective solution with competitive performance and uncertainty
quantification.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01485" title="Abstract">arXiv:2402.01485</a> [<a href="/pdf/2402.01485" title="Download PDF">pdf</a>, <a href="/format/2402.01485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown  Relative Poses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asadi%2C+M">Mahboubeh Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Zareinia%2C+K">Kourosh Zareinia</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi%2C+S">Sajad Saeedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 figures, Submitted to IEEE-RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Collaborative mapping of unknown environments can be done faster and more
robustly than a single robot. However, a collaborative approach requires a
distributed paradigm to be scalable and deal with communication issues. This
work presents a fully distributed algorithm enabling a group of robots to
collectively optimize the parameters of a Neural Radiance Field (NeRF). The
algorithm involves the communication of each robot's trained NeRF parameters
over a mesh network, where each robot trains its NeRF and has access to its own
visual data only. Additionally, the relative poses of all robots are jointly
optimized alongside the model parameters, enabling mapping with unknown
relative camera poses. We show that multi-robot systems can benefit from
differentiable and robust 3D reconstruction optimized from multiple NeRFs.
Experiments on real-world and synthetic data demonstrate the efficiency of the
proposed algorithm. See the website of the project for videos of the
experiments and supplementary
material(https://sites.google.com/view/di-nerf/home).
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01488" title="Abstract">arXiv:2402.01488</a> [<a href="/pdf/2402.01488" title="Download PDF">pdf</a>, <a href="/format/2402.01488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Occupancy Grids for Object Detection: A Radar-Centric Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ronecker%2C+M+P">Max Peter Ronecker</a>, 
<a href="/search/cs?searchtype=author&query=Schratter%2C+M">Markus Schratter</a>, 
<a href="/search/cs?searchtype=author&query=Kuschnig%2C+L">Lukas Kuschnig</a>, 
<a href="/search/cs?searchtype=author&query=Watzenig%2C+D">Daniel Watzenig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Dynamic Occupancy Grid Mapping is a technique used to generate a local map of
the environment containing both static and dynamic information. Typically,
these maps are primarily generated using lidar measurements. However, with
improvements in radar sensing, resulting in better accuracy and higher
resolution, radar is emerging as a viable alternative to lidar as the primary
sensor for mapping. In this paper, we propose a radar-centric dynamic occupancy
grid mapping algorithm with adaptations to the state computation, inverse
sensor model, and field-of-view computation tailored to the specifics of radar
measurements. We extensively evaluate our approach using real data to
demonstrate its effectiveness and establish the first benchmark for radar-based
dynamic occupancy grid mapping using the publicly available Radarscenes
dataset.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01494" title="Abstract">arXiv:2402.01494</a> [<a href="/pdf/2402.01494" title="Download PDF">pdf</a>, <a href="/format/2402.01494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating UAV Path Planning Algorithms for Realistic Maritime Search  and Rescue Missions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Messmer%2C+M">Martin Messmer</a>, 
<a href="/search/cs?searchtype=author&query=Zell%2C+A">Andreas Zell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Unmanned Aerial Vehicles (UAVs) are emerging as very important tools in
search and rescue (SAR) missions at sea, enabling swift and efficient
deployment for locating individuals or vessels in distress. The successful
execution of these critical missions heavily relies on effective path planning
algorithms that navigate UAVs through complex maritime environments while
considering dynamic factors such as water currents and wind flow. Furthermore,
they need to account for the uncertainty in search target locations. However,
existing path planning methods often fail to address the inherent uncertainty
associated with the precise location of search targets and the uncertainty of
oceanic forces. In this paper, we develop a framework to develop and
investigate trajectory planning algorithms for maritime SAR scenarios employing
UAVs. We adopt it to compare multiple planning strategies, some of them used in
practical applications by the United States Coast Guard. Furthermore, we
propose a novel planner that aims at bridging the gap between computation
heavy, precise algorithms and lightweight strategies applicable to real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01495" title="Abstract">arXiv:2402.01495</a> [<a href="/pdf/2402.01495" title="Download PDF">pdf</a>, <a href="/format/2402.01495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Conversational Large Language Models in  Knowledge-Based Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+P">Phillip Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Klettner%2C+M">Manuel Klettner</a>, 
<a href="/search/cs?searchtype=author&query=Simperl%2C+E">Elena Simperl</a>, 
<a href="/search/cs?searchtype=author&query=Matthes%2C+F">Florian Matthes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating natural language text from graph-structured data is essential for
conversational information seeking. Semantic triples derived from knowledge
graphs can serve as a valuable source for grounding responses from
conversational agents by providing a factual basis for the information they
communicate. This is especially relevant in the context of large language
models, which offer great potential for conversational interaction but are
prone to hallucinating, omitting, or producing conflicting information. In this
study, we conduct an empirical analysis of conversational large language models
in generating natural language text from semantic triples. We compare four
large language models of varying sizes with different prompting techniques.
Through a series of benchmark experiments on the WebNLG dataset, we analyze the
models' performance and identify the most common issues in the generated
predictions. Our findings show that the capabilities of large language models
in triple verbalization can be significantly improved through few-shot
prompting, post-processing, and efficient fine-tuning techniques, particularly
for smaller models that exhibit lower zero-shot performance.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01499" title="Abstract">arXiv:2402.01499</a> [<a href="/pdf/2402.01499" title="Download PDF">pdf</a>, <a href="/format/2402.01499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing and Evaluating a Design Method for Positive Artificial  Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Maden%2C+W">Willem van der Maden</a>, 
<a href="/search/cs?searchtype=author&query=Lomas%2C+D">Derek Lomas</a>, 
<a href="/search/cs?searchtype=author&query=Hekkert%2C+P">Paul Hekkert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As artificial intelligence (AI) continues advancing, ensuring positive
societal impacts becomes critical, especially as AI systems become increasingly
ubiquitous in various aspects of life. However, developing "AI for good" poses
substantial challenges around aligning systems with complex human values.
Presently, we lack mature methods for addressing these challenges. This article
presents and evaluates the Positive AI design method aimed at addressing this
gap. The method provides a human-centered process to translate wellbeing
aspirations into concrete practices. First, we explain the method's four key
steps: contextualizing, operationalizing, optimizing, and implementing
wellbeing supported by continuous measurement for feedback cycles. We then
present a multiple case study where novice designers applied the method,
revealing strengths and weaknesses related to efficacy and usability. Next, an
expert evaluation study assessed the quality of the resulting concepts, rating
them moderately high for feasibility, desirability, and plausibility of
achieving intended wellbeing benefits. Together, these studies provide
preliminary validation of the method's ability to improve AI design, while
surfacing areas needing refinement like developing support for complex steps.
Proposed adaptations such as examples and evaluation heuristics could address
weaknesses. Further research should examine sustained application over multiple
projects. This human-centered approach shows promise for realizing the vision
of 'AI for Wellbeing' that does not just avoid harm, but actively benefits
humanity.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01501" title="Abstract">arXiv:2402.01501</a> [<a href="/pdf/2402.01501" title="Download PDF">pdf</a>, <a href="/ps/2402.01501" title="Download PostScript">ps</a>, <a href="/format/2402.01501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Satisfiability Modulo Exponential Integer Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frohn%2C+F">Florian Frohn</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">SMT solvers use sophisticated techniques for polynomial (linear or
non-linear) integer arithmetic. In contrast, non-polynomial integer arithmetic
has mostly been neglected so far. However, in the context of program
verification, polynomials are often insufficient to capture the behavior of the
analyzed system without resorting to approximations. In the last years,
incremental linearization has been applied successfully to satisfiability
modulo real arithmetic with transcendental functions. We adapt this approach to
an extension of polynomial integer arithmetic with exponential functions. Here,
the key challenge is to compute suitable lemmas that eliminate the current
model from the search space if it violates the semantics of exponentiation. An
empirical evaluation of our implementation shows that our approach is highly
effective in practice.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01505" title="Abstract">arXiv:2402.01505</a> [<a href="/pdf/2402.01505" title="Download PDF">pdf</a>, <a href="/format/2402.01505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code-Switched Language Identification is Harder Than You Think
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burchell%2C+L">Laurie Burchell</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+R+P">Robert P. Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Heafield%2C+K">Kenneth Heafield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Code switching (CS) is a very common phenomenon in written and spoken
communication but one that is handled poorly by many natural language
processing applications. Looking to the application of building CS corpora, we
explore CS language identification (LID) for corpus building. We make the task
more realistic by scaling it to more languages and considering models with
simpler architectures for faster inference. We also reformulate the task as a
sentence-level multi-label tagging problem to make it more tractable. Having
defined the task, we investigate three reasonable models for this task and
define metrics which better reflect desired performance. We present empirical
evidence that no current approach is adequate and finally provide
recommendations for future work in this area.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01507" title="Abstract">arXiv:2402.01507</a> [<a href="/pdf/2402.01507" title="Download PDF">pdf</a>, <a href="/format/2402.01507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Blind Spots: Occlusion Considerations for Improved Autonomous  Driving Safety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moller%2C+K">Korbinian Moller</a>, 
<a href="/search/cs?searchtype=author&query=Trauth%2C+R">Rainer Trauth</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages. Submitted to IEEE IV Conference, Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Our work introduces a module for assessing the trajectory safety of
autonomous vehicles in dynamic environments marked by high uncertainty. We
focus on occluded areas and occluded traffic participants with limited
information about surrounding obstacles. To address this problem, we propose a
software module that handles blind spots (BS) created by static and dynamic
obstacles in urban environments. We identify potential occluded traffic
participants, predict their movement, and assess the ego vehicle's trajectory
using various criticality metrics. The method offers a straightforward and
modular integration into motion planner algorithms. We present critical
real-world scenarios to evaluate our module and apply our approach to a
publicly available trajectory planning algorithm. Our results demonstrate that
safe yet efficient driving with occluded road users can be achieved by
incorporating safety assessments into the planning process. The code used in
this research is publicly available as open-source software and can be accessed
at the following link: https://github.com/TUM-AVS/Frenetix-Occlusion.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01510" title="Abstract">arXiv:2402.01510</a> [<a href="/pdf/2402.01510" title="Download PDF">pdf</a>, <a href="/ps/2402.01510" title="Download PostScript">ps</a>, <a href="/format/2402.01510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Strategy for Chat Transcript Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+P+K">Pratik K. Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal Paper (13 Pages, 7 Figures, 4 Tables). arXiv admin note: text overlap with <a href="/abs/2103.10599">arXiv:2103.10599</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text summarization is the process of condensing a piece of text to fewer
sentences, while still preserving its content. Chat transcript, in this
context, is a textual copy of a digital or online conversation between a
customer (caller) and agent(s). This paper presents an indigenously (locally)
developed hybrid method that first combines extractive and abstractive
summarization techniques in compressing ill-punctuated or un-punctuated chat
transcripts to produce more readable punctuated summaries and then optimizes
the overall quality of summarization through reinforcement learning. Extensive
testing, evaluations, comparisons, and validation have demonstrated the
efficacy of this approach for large-scale deployment of chat transcript
summarization, in the absence of manually generated reference (annotated)
summaries.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01511" title="Abstract">arXiv:2402.01511</a> [<a href="/pdf/2402.01511" title="Download PDF">pdf</a>, <a href="/format/2402.01511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-based optimization of a production system topology -- a  neural network-assisted genetic algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paape%2C+N">N. Paape</a>, 
<a href="/search/cs?searchtype=author&query=van+Eekelen%2C+J+A+W+M">J.A.W.M. van Eekelen</a>, 
<a href="/search/cs?searchtype=author&query=Reniers%2C+M+A">M.A. Reniers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint - under peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">There is an abundance of prior research on the optimization of production
systems, but there is a research gap when it comes to optimizing which
components should be included in a design, and how they should be connected. To
overcome this gap, a novel approach is presented for topology optimization of
production systems using a genetic algorithm (GA). This GA employs
similarity-based mutation and recombination for the creation of offspring, and
discrete-event simulation for fitness evaluation. To reduce computational cost,
an extension to the GA is presented in which a neural network functions as a
surrogate model for simulation. Three types of neural networks are compared,
and the type most effective as a surrogate model is chosen based on its
optimization performance and computational cost.
<br />Both the unassisted GA and neural network-assisted GA are applied to an
industrial case study and a scalability case study. These show that both
approaches are effective at finding the optimal solution in industrial
settings, and both scale well as the number of potential solutions increases,
with the neural network-assisted GA having the better scalability of the two.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01512" title="Abstract">arXiv:2402.01512</a> [<a href="/pdf/2402.01512" title="Download PDF">pdf</a>, <a href="/format/2402.01512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distractor Generation for Multiple-Choice Questions: A Survey of  Methods, Datasets, and Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhazmi%2C+E">Elaf Alhazmi</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q+Z">Quan Z. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W+E">Wei Emma Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zaib%2C+M">Munazza Zaib</a>, 
<a href="/search/cs?searchtype=author&query=Alhazmi%2C+A">Ahoud Alhazmi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Distractors are important in learning evaluation. This paper surveys
distractor generation tasks using English multiple-choice question datasets for
textual and multimodal contexts. In particular, this paper presents a thorough
literature review of the recent studies on distractor generation tasks,
discusses multiple choice components and their characteristics, analyzes the
related datasets, and summarizes the evaluation metrics of distractor
generation. Our investigation reveals that more than half of datasets are
human-generated from educational sources in specific domains such as Science
and English, which are largely text-based, with a lack of open domain and
multimodal datasets.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01513" title="Abstract">arXiv:2402.01513</a> [<a href="/pdf/2402.01513" title="Download PDF">pdf</a>, <a href="/format/2402.01513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Gradient Word-Order Typology from Universal Dependencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baylor%2C+E">Emi Baylor</a>, 
<a href="/search/cs?searchtype=author&query=Ploeger%2C+E">Esther Ploeger</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While information from the field of linguistic typology has the potential to
improve performance on NLP tasks, reliable typological data is a prerequisite.
Existing typological databases, including WALS and Grambank, suffer from
inconsistencies primarily caused by their categorical format. Furthermore,
typological categorisations by definition differ significantly from the
continuous nature of phenomena, as found in natural language corpora. In this
paper, we introduce a new seed dataset made up of continuous-valued data,
rather than categorical data, that can better reflect the variability of
language. While this initial dataset focuses on word-order typology, we also
present the methodology used to create the dataset, which can be easily adapted
to generate data for a broader set of features and languages.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01514" title="Abstract">arXiv:2402.01514</a> [<a href="/pdf/2402.01514" title="Download PDF">pdf</a>, <a href="/format/2402.01514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping the Multiverse of Latent Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wayland%2C+J">Jeremy Wayland</a>, 
<a href="/search/cs?searchtype=author&query=Coupette%2C+C">Corinna Coupette</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Echoing recent calls to counter reliability and robustness concerns in
machine learning via multiverse analysis, we present PRESTO, a principled
framework for mapping the multiverse of machine-learning models that rely on
latent representations. Although such models enjoy widespread adoption, the
variability in their embeddings remains poorly understood, resulting in
unnecessary complexity and untrustworthy representations. Our framework uses
persistent homology to characterize the latent spaces arising from different
combinations of diverse machine-learning methods, (hyper)parameter
configurations, and datasets, allowing us to measure their pairwise
(dis)similarity and statistically reason about their distributions. As we
demonstrate both theoretically and empirically, our pipeline preserves
desirable properties of collections of latent representations, and it can be
leveraged to perform sensitivity analysis, detect anomalous embeddings, or
efficiently and effectively navigate hyperparameter search spaces.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01515" title="Abstract">arXiv:2402.01515</a> [<a href="/pdf/2402.01515" title="Download PDF">pdf</a>, <a href="/format/2402.01515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Stochastic Gradient Descent: A Unified Framework and Novel  Acceleration Methods for Faster Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yichuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chiwun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Based on SGD, previous works have proposed many algorithms that have improved
convergence speed and generalization in stochastic optimization, such as SGDm,
AdaGrad, Adam, etc. However, their convergence analysis under non-convex
conditions is challenging. In this work, we propose a unified framework to
address this issue. For any first-order methods, we interpret the updated
direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and
an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t)
\rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing
$\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have
discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating}
and \textbf{Random Vector Accelerating}, we theoretically demonstrate that
these two methods can directly lead to an improvement in convergence rate.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01516" title="Abstract">arXiv:2402.01516</a> [<a href="/pdf/2402.01516" title="Download PDF">pdf</a>, <a href="/format/2402.01516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-view Masked Diffusion Transformers for Person Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+T+X">Trung X. Pham</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zhang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+C+D">Chang D. Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present X-MDPT (Cross-view Masked Diffusion Prediction Transformers), a
novel diffusion model designed for pose-guided human image generation. X-MDPT
distinguishes itself by employing masked diffusion transformers that operate on
latent patches, a departure from the commonly-used Unet structures in existing
works. The model comprises three key modules: 1) a denoising diffusion
Transformer, 2) an aggregation network that consolidates conditions into a
single vector for the diffusion process, and 3) a mask cross-prediction module
that enhances representation learning with semantic information from the
reference image. X-MDPT demonstrates scalability, improving FID, SSIM, and
LPIPS with larger models. Despite its simple design, our model outperforms
state-of-the-art approaches on the DeepFashion dataset while exhibiting
efficiency in terms of training parameters, training time, and inference speed.
Our compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent
diffusion approach (FID 8.07) using only $11\times$ fewer parameters. Our best
model surpasses the pixel-based diffusion with $\frac{2}{3}$ of the parameters
and achieves $5.43 \times$ faster inference.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01518" title="Abstract">arXiv:2402.01518</a> [<a href="/pdf/2402.01518" title="Download PDF">pdf</a>, <a href="/format/2402.01518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadrotor Takeoff Trajectory Planning in a One-Dimensional Uncertain  Wind-field Aided by Wind-Sensing Infrastructure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kakavitsas%2C+N">Nicholas Kakavitsas</a>, 
<a href="/search/eess?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, AIAA SciTech 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper investigates optimal takeoff trajectory planning for a quadrotor
modeled with vertical-plane rigid body dynamics in an uncertain,
one-dimensional wind-field. The wind-field varies horizontally and propagates
across an operating region with a known fixed speed. The operating area of the
quadrotor is equipped with wind-sensing infrastructure that shares noisy
anemometer measurements with a centralized trajectory planner. The measurements
are assimilated via Gaussian process regression to predict the wind at
unsampled locations and future time instants. A minimum-time optimal control
problem is formulated for the quadrotor to take off and reach a desired
vertical-plane position in the presence of the predicted wind-field. The
problem is solved using numerical optimal control. Several examples illustrate
and compare the performance of the trajectory planner under varying wind
conditions and sensing characteristics.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01520" title="Abstract">arXiv:2402.01520</a> [<a href="/pdf/2402.01520" title="Download PDF">pdf</a>, <a href="/ps/2402.01520" title="Download PostScript">ps</a>, <a href="/format/2402.01520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Resource Cross-Domain Singing Voice Synthesis via Reduced  Self-Supervised Speech Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kakoulidis%2C+P">Panos Kakoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Ellinas%2C+N">Nikolaos Ellinas</a>, 
<a href="/search/cs?searchtype=author&query=Vamvoukakis%2C+G">Georgios Vamvoukakis</a>, 
<a href="/search/cs?searchtype=author&query=Christidou%2C+M">Myrsini Christidou</a>, 
<a href="/search/cs?searchtype=author&query=Vioni%2C+A">Alexandra Vioni</a>, 
<a href="/search/cs?searchtype=author&query=Maniati%2C+G">Georgia Maniati</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Junkwang Oh</a>, 
<a href="/search/cs?searchtype=author&query=Jho%2C+G">Gunu Jho</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+I">Inchul Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Tsiakoulis%2C+P">Pirros Tsiakoulis</a>, 
<a href="/search/cs?searchtype=author&query=Chalamandaris%2C+A">Aimilios Chalamandaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ICASSP SASB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that
is trained only on text and speech data as a typical multi-speaker acoustic
model. It is a low-resource pipeline that does not utilize any singing data
end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is
conditioned by self-supervised speech representations in an unsupervised
manner. We preprocess these representations by selecting only a subset of their
task-correlated dimensions. The conditioning module is indirectly guided to
capture style information during training by multi-tasking. This is achieved
with a Conformer-based module, which predicts the pitch from the acoustic
model's output. Thus, Karaoker-SSL allows singing voice synthesis without
reliance on hand-crafted and domain-specific features. There are also no
requirements for text alignments or lyrics timestamps. To refine the voice
quality, we employ a U-Net discriminator that is conditioned on the target
speaker and follows a Diffusion GAN training scheme.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01521" title="Abstract">arXiv:2402.01521</a> [<a href="/pdf/2402.01521" title="Download PDF">pdf</a>, <a href="/format/2402.01521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-Level Reasoning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yadong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shaoguang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+M">Man Lan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Large Language Models (LLMs) have demonstrated their proficiency in
complex reasoning tasks, their performance in dynamic, interactive, and
competitive scenarios - such as business strategy and stock market analysis -
remains underexplored. To bridge this gap, we formally explore the dynamic
reasoning capabilities of LLMs for decision-making in rapidly evolving
environments. We introduce two game theory-based pilot challenges that mirror
the complexities of real-world dynamic decision-making. These challenges are
well-defined, enabling clear, controllable, and precise evaluation of LLMs'
dynamic reasoning abilities. Through extensive experiments, we find that
existing reasoning methods tend to falter in dynamic settings that require
k-level thinking - a key concept not tackled by previous works. To address
this, we propose a novel reasoning approach for LLMs, named "K-Level
Reasoning". This approach adopts the perspective of rivals to recursively
employ k-level thinking based on available historical information, which
significantly improves the prediction accuracy of rivals' subsequent moves and
informs more strategic decision-making. This research not only sets a robust
quantitative benchmark for the assessment of dynamic reasoning but also
markedly enhances the proficiency of LLMs in dynamic contexts.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01523" title="Abstract">arXiv:2402.01523</a> [<a href="/pdf/2402.01523" title="Download PDF">pdf</a>, <a href="/ps/2402.01523" title="Download PostScript">ps</a>, <a href="/format/2402.01523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Support of Inverters for Improving Short-Term Voltage Security in  100% IBRsPenetrated Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yinhong Lin</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Q">Qinglai Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+H">Haotian Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">Hongbin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Due to the energy crisis and environmental pollution, the installed capacity
of inverter-based resources (IBRs) in power grids is rapidly increasing, and
grid-following control (GFL) is the most prevalent at present. Meanwhile,
grid-forming control-based (GFM) devices have been installed in the grid to
provide active support for frequency and voltage. In the future GFL devices
combined with GFM will be promising, especially in power systems with high
penetration or 100% IBRs. When a short-circuit fault occurs in the grid, the
controlled current source characteristic of the GFL devices leads to
insufficient dynamic voltage support (DVS), while the GFM devices usually
reduce the internal voltage to limit the current. Thus, deep voltage sags and
undesired disconnections of IBRs may occur. Moreover, due to the dispersed
locations and the control strategies' diversity of IBRs, the voltage support of
different devices may not be fully coordinated, which is not conducive to
short-term voltage security (STVS). To address this issue, a control scheme
based on the simulation of transient characteristics of synchronous machines
(SMs) is proposed. Then, a new fault ride-through strategy (FRT) is proposed
based on the characteristic differences between GFL and GFM devices, and an
optimization model of multi-device control parameters is formulated to meet the
short-term voltage security constraints (SVSCs) and device capacity
constraints. Finally, a fast solution method based on analytical modeling is
proposed for the model. Test results based on the doublegenerator-one-load
system, the IEEE 14-bus system, and other systems of different sizes show that
the proposed method can effectively enhance the active support capability of
GFL and GFM to the grid voltage, and avoid the large-scale disconnection of
IBRs
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01524" title="Abstract">arXiv:2402.01524</a> [<a href="/pdf/2402.01524" title="Download PDF">pdf</a>, <a href="/format/2402.01524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Batorski%2C+P">Pawe&#x142; Batorski</a>, 
<a href="/search/cs?searchtype=author&query=Malarz%2C+D">Dawid Malarz</a>, 
<a href="/search/cs?searchtype=author&query=Przewi%C4%99%C5%BAlikowski%2C+M">Marcin Przewi&#x119;&#x17a;likowski</a>, 
<a href="/search/cs?searchtype=author&query=Mazur%2C+M">Marcin Mazur</a>, 
<a href="/search/cs?searchtype=author&query=Tadeja%2C+S">S&#x142;awomir Tadeja</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01525" title="Abstract">arXiv:2402.01525</a> [<a href="/pdf/2402.01525" title="Download PDF">pdf</a>, <a href="/format/2402.01525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Linear Analog Processing Gains in Task-Based Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+M+T">Marian Temprana Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Shirani%2C+F">Farhad Shirani</a>, 
<a href="/search/cs?searchtype=author&query=Bernardo%2C+N+I">Neil Irwin Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In task-based quantization, a multivariate analog signal is transformed into
a digital signal using a limited number of low-resolution analog-to-digital
converters (ADCs). This process aims to minimize a fidelity criterion, which is
assessed against an unobserved task variable that is correlated with the analog
signal. The scenario models various applications of interest such as channel
estimation, medical imaging applications, and object localization. This work
explores the integration of analog processing components -- such as analog
delay elements, polynomial operators, and envelope detectors -- prior to ADC
quantization. Specifically, four scenarios, involving different collections of
analog processing operators are considered: (i) arbitrary polynomial operators
with analog delay elements, (ii) limited-degree polynomial operators, excluding
delay elements, (iii) sequences of envelope detectors, and (iv) a combination
of analog delay elements and linear combiners. For each scenario, the minimum
achievable distortion is quantified through derivation of computable
expressions in various statistical settings. It is shown that analog processing
can significantly reduce the distortion in task reconstruction. Numerical
simulations in a Gaussian example are provided to give further insights into
the aforementioned analog processing gains.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01526" title="Abstract">arXiv:2402.01526</a> [<a href="/pdf/2402.01526" title="Download PDF">pdf</a>, <a href="/format/2402.01526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Central WENO schemes through a global average weight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=B%C3%BCrger%2C+R">Raimund B&#xfc;rger</a>, 
<a href="/search/math?searchtype=author&query=Mulet%2C+P">Pep Mulet</a>, 
<a href="/search/math?searchtype=author&query=Zor%C3%ADo%2C+D">David Zor&#xed;o</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J Sci Comput 78, 499-530 (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A novel central weighted essentially non-oscillatory (central WENO;
CWENO)-type scheme for the construction of high-resolution approximations to
discontinuous solutions to hyperbolic systems of conservation laws is
presented. This procedure is based on the construction of a global average
weight using the whole set of Jiang-Shu smoothness indicators associated to
every candidate stencil. By this device one does not to have to rely on ideal
weights, which, under certain stencil arrangements and interpolating point
locations, do not define a convex combination of the lower-degree interpolating
polynomials of the corresponding sub-stencils. Moreover, this procedure also
prevents some cases of accuracy loss near smooth extrema that are experienced
by classical WENO and CWENO schemes. These properties result in a more flexible
scheme that overcomes these issues, at the cost of only a few additional
computations with respect to classical WENO schemes and with a smaller cost
than classical CWENO schemes. Numerical examples illustrate that the proposed
CWENO schemes outperform both the traditional WENO and the original CWENO
schemes.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01528" title="Abstract">arXiv:2402.01528</a> [<a href="/pdf/2402.01528" title="Download PDF">pdf</a>, <a href="/format/2402.01528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Minghao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Saurabh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Venkataraman%2C+S">Shivaram Venkataraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Speculative Decoding is a widely used technique to speed up inference for
Large Language Models (LLMs) without modifying its outcome. When performing
inference on an LLM, speculative decoding uses a smaller draft model which
generates speculative tokens and then uses the target LLM to verify those draft
tokens. The speedup provided by speculative decoding heavily depends on the
choice of the draft model. It has been widely suggested to select a draft model
that provides a high probability of the generated token being accepted by the
LLM to achieve the highest throughput. However, our experiments indicate the
contrary with throughput diminishing as the probability of generated tokens to
be accepted by the target model increases. To understand this phenomenon, we
perform extensive experiments to characterize the different factors that affect
speculative decoding and how those factors interact and affect the speedups.
Based on our experiments we describe an analytical model which can be used to
decide the right draft model for a given workload. Further, using our insights
we design a new draft model for LLaMA-65B which can provide 30% higher
throughput than existing draft models.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01533" title="Abstract">arXiv:2402.01533</a> [<a href="/pdf/2402.01533" title="Download PDF">pdf</a>, <a href="/format/2402.01533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Effective Time-Series Forecasting with Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yansen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongqi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Spiking neural networks (SNNs), inspired by the spiking behavior of
biological neurons, provide a unique pathway for capturing the intricacies of
temporal data. However, applying SNNs to time-series forecasting is challenging
due to difficulties in effective temporal alignment, complexities in encoding
processes, and the absence of standardized guidelines for model selection. In
this paper, we propose a framework for SNNs in time-series forecasting tasks,
leveraging the efficiency of spiking neurons in processing temporal
information. Through a series of experiments, we demonstrate that our proposed
SNN-based approaches achieve comparable or superior results to traditional
time-series forecasting methods on diverse benchmarks with much less energy
consumption. Furthermore, we conduct detailed analysis experiments to assess
the SNN's capacity to capture temporal dependencies within time-series data,
offering valuable insights into its nuanced strengths and effectiveness in
modeling the intricate dynamics of temporal data. Our study contributes to the
expanding field of SNNs and offers a promising alternative for time-series
forecasting tasks, presenting a pathway for the development of more
biologically inspired and temporally aware forecasting models.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01535" title="Abstract">arXiv:2402.01535</a> [<a href="/pdf/2402.01535" title="Download PDF">pdf</a>, <a href="/format/2402.01535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Analysis of Diversity in Argument Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Meer%2C+M">Michiel van der Meer</a>, 
<a href="/search/cs?searchtype=author&query=Vossen%2C+P">Piek Vossen</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+C+M">Catholijn M. Jonker</a>, 
<a href="/search/cs?searchtype=author&query=Murukannaiah%2C+P+K">Pradeep K. Murukannaiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL2024 (main proceedings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Presenting high-level arguments is a crucial task for fostering participation
in online societal discussions. Current argument summarization approaches miss
an important facet of this task -- capturing diversity -- which is important
for accommodating multiple perspectives. We introduce three aspects of
diversity: those of opinions, annotators, and sources. We evaluate approaches
to a popular argument summarization task called Key Point Analysis, which shows
how these approaches struggle to (1) represent arguments shared by few people,
(2) deal with data from various sources, and (3) align with subjectivity in
human-provided annotations. We find that both general-purpose LLMs and
dedicated KPA models exhibit this behavior, but have complementary strengths.
Further, we observe that diversification of training data may ameliorate
generalization. Addressing diversity in argument summarization requires a mix
of strategies to deal with subjectivity.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01536" title="Abstract">arXiv:2402.01536</a> [<a href="/pdf/2402.01536" title="Download PDF">pdf</a>, <a href="/format/2402.01536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homogenization Effects of Large Language Models on Human Creative  Ideation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anderson%2C+B+R">Barrett R. Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J+H">Jash Hemant Shah</a>, 
<a href="/search/cs?searchtype=author&query=Kreminski%2C+M">Max Kreminski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are now being used in a wide variety of
contexts, including as creativity support tools (CSTs) intended to help their
users come up with new ideas. But do LLMs actually support user creativity? We
hypothesized that the use of an LLM as a CST might make the LLM's users feel
more creative, and even broaden the range of ideas suggested by each individual
user, but also homogenize the ideas suggested by different users. We conducted
a 36-participant comparative user study and found, in accordance with the
homogenization hypothesis, that different users tended to produce less
semantically distinct ideas with ChatGPT than with an alternative CST.
Additionally, ChatGPT users generated a greater number of more detailed ideas,
but felt less responsible for the ideas they generated. We discuss potential
implications of these findings for users, designers, and developers of
LLM-based CSTs.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01537" title="Abstract">arXiv:2402.01537</a> [<a href="/pdf/2402.01537" title="Download PDF">pdf</a>, <a href="/format/2402.01537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing  Trimodal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stippel%2C+C">Christian Stippel</a>, 
<a href="/search/cs?searchtype=author&query=Heitzinger%2C+T">Thomas Heitzinger</a>, 
<a href="/search/cs?searchtype=author&query=Sterzinger%2C+R">Rafael Sterzinger</a>, 
<a href="/search/cs?searchtype=author&query=Kampel%2C+M">Martin Kampel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In pervasive machine learning, especially in Human Behavior Analysis (HBA),
RGB has been the primary modality due to its accessibility and richness of
information. However, linked with its benefits are challenges, including
sensitivity to lighting conditions and privacy concerns. One possibility to
overcome these vulnerabilities is to resort to different modalities. For
instance, thermal is particularly adept at accentuating human forms, while
depth adds crucial contextual layers. Despite their known benefits, only a few
HBA-specific datasets that integrate these modalities exist. To address this
shortage, our research introduces a novel generative technique for creating
trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique
capitalizes on human segmentation masks derived from RGB images, combined with
thermal and depth backgrounds that are sourced automatically. With these two
ingredients, we synthesize depth and thermal counterparts from existing RGB
data utilizing conditional image-to-image translation. By employing this
approach, we generate trimodal data that can be leveraged to train models for
settings with limited data, bad lightning conditions, or privacy-sensitive
areas.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01539" title="Abstract">arXiv:2402.01539</a> [<a href="/pdf/2402.01539" title="Download PDF">pdf</a>, <a href="/ps/2402.01539" title="Download PostScript">ps</a>, <a href="/format/2402.01539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backward Responsibility in Transition Systems Using General Power  Indices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baier%2C+C">Christel Baier</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Bossche%2C+R">Roxane van den Bossche</a>, 
<a href="/search/cs?searchtype=author&query=Kl%C3%BCppelholz%2C+S">Sascha Kl&#xfc;ppelholz</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+J">Johannes Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Piribauer%2C+J">Jakob Piribauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">To improve reliability and the understanding of AI systems, there is
increasing interest in the use of formal methods, e.g. model checking. Model
checking tools produce a counterexample when a model does not satisfy a
property. Understanding these counterexamples is critical for efficient
debugging, as it allows the developer to focus on the parts of the program that
caused the issue.
<br />To this end, we present a new technique that ascribes a responsibility value
to each state in a transition system that does not satisfy a given safety
property. The value is higher if the non-deterministic choices in a state have
more power to change the outcome, given the behaviour observed in the
counterexample. For this, we employ a concept from cooperative game theory --
namely general power indices, such as the Shapley value -- to compute the
responsibility of the states.
<br />We present an optimistic and pessimistic version of responsibility that
differ in how they treat the states that do not lie on the counterexample. We
give a characterisation of optimistic responsibility that leads to an efficient
algorithm for it and show computational hardness of the pessimistic version. We
also present a tool to compute responsibility and show how a stochastic
algorithm can be used to approximate responsibility in larger models. These
methods can be deployed in the design phase, at runtime and at inspection time
to gain insights on causal relations within the behavior of AI systems.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01543" title="Abstract">arXiv:2402.01543</a> [<a href="/pdf/2402.01543" title="Download PDF">pdf</a>, <a href="/format/2402.01543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Optimization for Prediction with Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertsimas%2C+D">Dimitris Bertsimas</a>, 
<a href="/search/cs?searchtype=author&query=Delarue%2C+A">Arthur Delarue</a>, 
<a href="/search/cs?searchtype=author&query=Pauphilet%2C+J">Jean Pauphilet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2104.03158">arXiv:2104.03158</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">When training predictive models on data with missing entries, the most widely
used and versatile approach is a pipeline technique where we first impute
missing entries and then compute predictions. In this paper, we view prediction
with missing data as a two-stage adaptive optimization problem and propose a
new class of models, adaptive linear regression models, where the regression
coefficients adapt to the set of observed features. We show that some adaptive
linear regression models are equivalent to learning an imputation rule and a
downstream linear regression model simultaneously instead of sequentially. We
leverage this joint-impute-then-regress interpretation to generalize our
framework to non-linear models. In settings where data is strongly not missing
at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01546" title="Abstract">arXiv:2402.01546</a> [<a href="/pdf/2402.01546" title="Download PDF">pdf</a>, <a href="/format/2402.01546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Distributed Learning for Residential Short-Term Load  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gama%2C+M">Mariana Gama</a>, 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+M+A">Mustafa A. Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Deconinck%2C+G">Geert Deconinck</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">In the realm of power systems, the increasing involvement of residential
users in load forecasting applications has heightened concerns about data
privacy. Specifically, the load data can inadvertently reveal the daily
routines of residential users, thereby posing a risk to their property
security. While federated learning (FL) has been employed to safeguard user
privacy by enabling model training without the exchange of raw data, these FL
models have shown vulnerabilities to emerging attack techniques, such as Deep
Leakage from Gradients and poisoning attacks. To counteract these, we initially
employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty
computation cryptographic techniques to mitigate the risk of gradient leakage.
However, the introduction of SecAgg necessitates the deployment of additional
sub-center servers for executing the multiparty computation protocol, thereby
escalating computational complexity and reducing system robustness, especially
in scenarios where one or more sub-centers are unavailable. To address these
challenges, we introduce a Markovian Switching-based distributed training
framework, the convergence of which is substantiated through rigorous
theoretical analysis. The Distributed Markovian Switching (DMS) topology shows
strong robustness towards the poisoning attacks as well. Case studies employing
real-world power system load data validate the efficacy of our proposed
algorithm. It not only significantly minimizes communication complexity but
also maintains accuracy levels comparable to traditional FL methods, thereby
enhancing the scalability of our load forecasting algorithm.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01547" title="Abstract">arXiv:2402.01547</a> [<a href="/pdf/2402.01547" title="Download PDF">pdf</a>, <a href="/format/2402.01547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contingency Detection in Modern Power Systems: A Stochastic Hybrid  System Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yuan%2C+S">Shuo Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L+Y">Le Yi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+G">George Yin</a>, 
<a href="/search/eess?searchtype=author&query=Nazari%2C+M+H">Masoud H. Nazari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures. arXiv admin note: text overlap with <a href="/abs/2401.16568">arXiv:2401.16568</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a new stochastic hybrid system (SHS) framework for
contingency detection in modern power systems (MPS). The framework uses
stochastic hybrid system representations in state space models to expand and
facilitate capability of contingency detection. In typical microgrids (MGs),
buses may contain various synchronous generators, renewable generators,
controllable loads, battery systems, regular loads, etc. For development of SHS
models in power systems, this paper introduces the concept of dynamic and
non-dynamic buses. By converting a physical power grid into a virtual
linearized state space model and representing contingencies as random switching
of system structures and parameters, this paper formulates the contingency
detection problem as a joint estimation problem of discrete event and
continuous states in stochastic hybrid systems. This method offers unique
advantages, including using common measurement signals on voltage and current
synchrophasors to detect different types and locations of contingencies,
avoiding expensive local direct fault measurements and detecting certain
contingencies that cannot be directly measured. The method employs a small and
suitably-designed probing signal to sustain the ability of persistent
contingency detection. Joint estimation algorithms are presented with their
proven convergence and reliability properties. Examples that use an IEEE 5-bus
system demonstrate the main ideas and derivation steps. Simulation case studies
on an IEEE 33-bus system are used for detecting transmission line faults and
sensor interruptions.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01551" title="Abstract">arXiv:2402.01551</a> [<a href="/pdf/2402.01551" title="Download PDF">pdf</a>, <a href="/format/2402.01551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping Acceptance: Assessing Emerging Technologies and Concepts through  Micro Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brauner%2C+P">Philipp Brauner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">As technology evolves rapidly, understanding public perception becomes
increasingly crucial. This article introduces an integrative method for
evaluating mental models and social acceptance of various technologies. Our
approach utilizes micro scenarios coupled with visual-spatial mapping, offering
a comprehensive perspective that contrasts with traditional methods focused on
detailed assessments of limited scenarios. This methodology allows for
simultaneous quantitative evaluation of multiple technologies on visio-spatial
maps, facilitating a comparative ranking based on diverse criteria and an
exploration of the interplay between individual factors and technology
attributes in shaping public opinion. Our approach provides a framework for
researchers and policymakers to gauge critical issues and to identify factors
pivotal to acceptance. We illustrate this methodology with examples from our
research, offering practical guidelines and R code to enable others in
conducting similar studies. This paper aims to bridge the gap between
technological advancement and societal perception, offering a tool for more
informed decision-making in the realm of technology development and policy.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01552" title="Abstract">arXiv:2402.01552</a> [<a href="/pdf/2402.01552" title="Download PDF">pdf</a>, <a href="/format/2402.01552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hardware Trojans in Quantum Circuits, Their Impacts, and Defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+R">Rupshali Roy</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Subrata Das</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures, ISQED 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The reliability of the outcome of a quantum circuit in near-term noisy
quantum computers depends on the gate count and depth for a given problem.
Circuits with a short depth and lower gate count can yield the correct solution
more often than the variant with a higher gate count and depth. To work
successfully for Noisy Intermediate Scale Quantum (NISQ) computers, quantum
circuits need to be optimized efficiently using a compiler that decomposes
high-level gates to native gates of the hardware. Many 3rd party compilers are
being developed for lower compilation time, reduced circuit depth, and lower
gate count for large quantum circuits. Such compilers, or even a specific
release version of a compiler that is otherwise trustworthy, may be unreliable
and give rise to security risks such as insertion of a quantum trojan during
compilation that evades detection due to the lack of a golden/Oracle model in
quantum computing. Trojans may corrupt the functionality to give flipped
probabilities of basis states, or result in a lower probability of correct
basis states in the output. In this paper, we investigate and discuss the
impact of a single qubit Trojan (we have chosen a Hadamard gate and a NOT gate)
inserted one at a time at various locations in benchmark quantum circuits
without changing the the depth of the circuit. Results indicate an average of
16.18% degradation for the Hadamard Trojan without noise, and 7.78% with noise.
For the NOT Trojan (with noise) there is 14.6% degradation over all possible
inputs. We then discuss the detection of such Trojans in a quantum circuit
using CNN-based classifier achieving an accuracy of 90%.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01555" title="Abstract">arXiv:2402.01555</a> [<a href="/pdf/2402.01555" title="Download PDF">pdf</a>, <a href="/format/2402.01555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLYKLatent, a Learning Framework for Facial Features Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adebayo%2C+S">Samuel Adebayo</a>, 
<a href="/search/cs?searchtype=author&query=Dessing%2C+J+C">Joost C. Dessing</a>, 
<a href="/search/cs?searchtype=author&query=McLoone%2C+S">Se&#xe1;n McLoone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In this research, we present SLYKLatent, a novel approach for enhancing gaze
estimation by addressing appearance instability challenges in datasets due to
aleatoric uncertainties, covariant shifts, and test domain generalization.
SLYKLatent utilizes Self-Supervised Learning for initial training with facial
expression datasets, followed by refinement with a patch-based tri-branch
network and an inverse explained variance-weighted training loss function. Our
evaluation on benchmark datasets achieves an 8.7% improvement on Gaze360,
rivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%,
surpassing existing methods by significant margins. Adaptability tests on
RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation
studies confirm the effectiveness of SLYKLatent's novel components. This
approach has strong potential in human-robot interaction.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01557" title="Abstract">arXiv:2402.01557</a> [<a href="/pdf/2402.01557" title="Download PDF">pdf</a>, <a href="/format/2402.01557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Continuous Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tomen%2C+N">Nergis Tomen</a>, 
<a href="/search/cs?searchtype=author&query=Pintea%2C+S+L">Silvia L. Pintea</a>, 
<a href="/search/cs?searchtype=author&query=van+Gemert%2C+J+C">Jan C. van Gemert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at ICML 2021
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In International Conference on Machine Learning 2021 Jul 1 (pp.
  10324-10335). PMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">CNNs and computational models of biological vision share some fundamental
principles, which opened new avenues of research. However, fruitful cross-field
research is hampered by conventional CNN architectures being based on spatially
and depthwise discrete representations, which cannot accommodate certain
aspects of biological complexity such as continuously varying receptive field
sizes and dynamics of neuronal responses. Here we propose deep continuous
networks (DCNs), which combine spatially continuous filters, with the
continuous depth framework of neural ODEs. This allows us to learn the spatial
support of the filters during training, as well as model the continuous
evolution of feature maps, linking DCNs closely to biological models. We show
that DCNs are versatile and highly applicable to standard image classification
and reconstruction problems, where they improve parameter and data efficiency,
and allow for meta-parametrization. We illustrate the biological plausibility
of the scale distributions learned by DCNs and explore their performance in a
neuroscientifically inspired pattern completion task. Finally, we investigate
an efficient implementation of DCNs by changing input contrast.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01566" title="Abstract">arXiv:2402.01566</a> [<a href="/pdf/2402.01566" title="Download PDF">pdf</a>, <a href="/format/2402.01566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boximator: Generating Rich and Controllable Motions for Video Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiawei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jiaxin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guoqiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liping Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generating rich and controllable motion is a pivotal challenge in video
synthesis. We propose Boximator, a new approach for fine-grained motion
control. Boximator introduces two constraint types: hard box and soft box.
Users select objects in the conditional frame using hard boxes and then use
either type of boxes to roughly or rigorously define the object's position,
shape, or motion path in future frames. Boximator functions as a plug-in for
existing video diffusion models. Its training process preserves the base
model's knowledge by freezing the original weights and training only the
control module. To address training challenges, we introduce a novel
self-tracking technique that greatly simplifies the learning of box-object
correlations. Empirically, Boximator achieves state-of-the-art video quality
(FVD) scores, improving on two base models, and further enhanced after
incorporating box constraints. Its robust motion controllability is validated
by drastic increases in the bounding box alignment metric. Human evaluation
also shows that users favor Boximator generation results over the base model.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01567" title="Abstract">arXiv:2402.01567</a> [<a href="/pdf/2402.01567" title="Download PDF">pdf</a>, <a href="/format/2402.01567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Adam Optimizer via Online Learning of Updates: Adam is  FTRL in Disguise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kook%2C+Y">Yunbum Kook</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yan Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments would be appreciated!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Despite the success of the Adam optimizer in practice, the theoretical
understanding of its algorithmic components still remains limited. In
particular, most existing analyses of Adam show the convergence rate that can
be simply achieved by non-adative algorithms like SGD. In this work, we provide
a different perspective based on online learning that underscores the
importance of Adam's algorithmic components. Inspired by Cutkosky et al.
(2023), we consider the framework called online learning of updates, where we
choose the updates of an optimizer based on an online learner. With this
framework, the design of a good optimizer is reduced to the design of a good
online learner. Our main observation is that Adam corresponds to a principled
online learning framework called Follow-the-Regularized-Leader (FTRL). Building
on this observation, we study the benefits of its algorithmic components from
the online learning perspective.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01571" title="Abstract">arXiv:2402.01571</a> [<a href="/pdf/2402.01571" title="Download PDF">pdf</a>, <a href="/format/2402.01571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking Music: Audio Compression with Event Based Auto-encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lisboa%2C+M">Martim Lisboa</a>, 
<a href="/search/cs?searchtype=author&query=Bellec%2C+G">Guillaume Bellec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Neurons in the brain communicate information via punctual events called
spikes. The timing of spikes is thought to carry rich information, but it is
not clear how to leverage this in digital systems. We demonstrate that
event-based encoding is efficient for audio compression. To build this
event-based representation we use a deep binary auto-encoder, and under high
sparsity pressure, the model enters a regime where the binary event matrix is
stored more efficiently with sparse matrix storage algorithms. We test this on
the large MAESTRO dataset of piano recordings against vector quantized
auto-encoders. Not only does our "Spiking Music compression" algorithm achieve
a competitive compression/reconstruction trade-off, but selectivity and
synchrony between encoded events and piano key strikes emerge without
supervision in the sparse regime.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01573" title="Abstract">arXiv:2402.01573</a> [<a href="/pdf/2402.01573" title="Download PDF">pdf</a>, <a href="/format/2402.01573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Actionable Framework for Understanding and Improving Talent Retention  as a Competitive Advantage in IT Organizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costa%2C+L+A">Luiz Alexandre Costa</a>, 
<a href="/search/cs?searchtype=author&query=Dias%2C+E">Edson Dias</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+D">Danilo Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Font%C3%A3o%2C+A">Awdren Font&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+G">Gustavo Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+R+P+d">Rodrigo Pereira dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Serebrenik%2C+A">Alexander Serebrenik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.06352">arXiv:2205.06352</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the rapidly evolving global business landscape, the demand for software
has intensified competition among organizations, leading to challenges in
retaining highly qualified IT members in software organizations. One of the
problems faced by IT organizations is the retention of these strategic
professionals, also known as talent. This work presents an actionable framework
for Talent Retention (TR) used in IT organizations. It is based on our findings
from interviews performed with 21 IT managers. The TR Framework is our main
research outcome. Our framework encompasses a set of factors, contextual
characteristics, barriers, strategies, and coping mechanisms.
<br />Our findings indicated that software engineers can be differentiated from
other professional groups, and beyond competitive salaries, other elements for
retaining talent in IT organizations should be considered, such as
psychological safety, work-life balance, a positive work environment,
innovative and challenging projects, and flexible work. A better understanding
of factors could guide IT managers in improving talent management processes by
addressing Software Engineering challenges, identifying important elements, and
exploring strategies at the individual, team, and organizational levels.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01575" title="Abstract">arXiv:2402.01575</a> [<a href="/pdf/2402.01575" title="Download PDF">pdf</a>, <a href="/format/2402.01575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Interaction-Aware Trajectory Planning for Autonomous  Vehicles with Particle Swarm Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lin Song</a>, 
<a href="/search/cs?searchtype=author&query=Isele%2C+D">David Isele</a>, 
<a href="/search/cs?searchtype=author&query=Hovakimyan%2C+N">Naira Hovakimyan</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sangjae Bae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces a novel numerical approach to achieving smooth
lane-change trajectories in autonomous driving scenarios. Our trajectory
generation approach leverages particle swarm optimization (PSO) techniques,
incorporating Neural Network (NN) predictions for trajectory refinement. The
generation of smooth and dynamically feasible trajectories for the lane change
maneuver is facilitated by combining polynomial curve fitting with particle
propagation, which can account for vehicle dynamics. The proposed planning
algorithm is capable of determining feasible trajectories with real-time
computation capability. We conduct comparative analyses with two baseline
methods for lane changing, involving analytic solutions and heuristic
techniques in numerical simulations. The simulation results validate the
efficacy and effectiveness of our proposed approach.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01576" title="Abstract">arXiv:2402.01576</a> [<a href="/pdf/2402.01576" title="Download PDF">pdf</a>, <a href="/format/2402.01576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Adversarial yet Safe Agent to Characterize Safety Performance  of Highly Automated Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sidhu%2C+A">Anmol Sidhu</a>, 
<a href="/search/cs?searchtype=author&query=Redmill%2C+K+A">Keith A. Redmill</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper focuses on safety performance testing and characterization of
black-box highly automated vehicles (HAV). Existing testing approaches
typically obtain the testing outcomes by deploying the HAV into a specific
testing environment. Such a testing environment can involve various passively
given testing strategies presented by other traffic participants such as (i)
the naturalistic driving policy learned from human drivers, (ii) extracted
concrete scenarios from real-world driving data, and (iii) model-based or
data-driven adversarial testing methodologies focusing on forcing
safety-critical events. The safety performance of HAV is further characterized
by analyzing the obtained testing outcomes with a particular selected measure,
such as the observed collision risk. The aforementioned testing practices
suffer from the scarcity of safety-critical events, have limited operational
design domain (ODD) coverage, or are biased toward long-tail unsafe cases. This
paper presents a novel and informative testing strategy that differs from these
existing practices. The proposal is inspired by the intuition that a relatively
safer HAV driving policy would allow the traffic vehicles to exhibit a higher
level of aggressiveness to achieve a certain fixed level of an overall safe
outcome. One can specifically characterize such a HAV and traffic interactive
strategy and use it as a safety performance indicator for the HAV. Under the
proposed testing scheme, the HAV is evaluated under its full ODD with a reward
function that represents a trade-off between safety and adversity in generating
safety-critical events. The proposed methodology is demonstrated in simulation
with various HAV designs under different operational design domains.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01577" title="Abstract">arXiv:2402.01577</a> [<a href="/pdf/2402.01577" title="Download PDF">pdf</a>, <a href="/format/2402.01577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Active Learning for Data Mining from Conflict Text Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Croicu%2C+M">Mihai Croicu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 6 figures. Paper presented at the Using LLMs and Text-as-Data in Political Science Research Workshop at the University of Barcelona, 29 January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">High-resolution event data on armed conflict and related processes have
revolutionized the study of political contention with datasets like UCDP GED,
ACLED etc. However, most of these datasets limit themselves to collecting
spatio-temporal (high-resolution) and intensity data. Information on dynamics,
such as targets, tactics, purposes etc. are rarely collected owing to the
extreme workload of collecting data. However, most datasets rely on a rich
corpus of textual data allowing further mining of further information connected
to each event. This paper proposes one such approach that is inexpensive and
high performance, leveraging active learning - an iterative process of
improving a machine learning model based on sequential (guided) human input.
Active learning is employed to then step-wise train (fine-tuning) of a large,
encoder-only language model adapted for extracting sub-classes of events
relating to conflict dynamics. The approach shows performance similar to human
(gold-standard) coding while reducing the amount of required human annotation
by as much as 99%.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01580" title="Abstract">arXiv:2402.01580</a> [<a href="/pdf/2402.01580" title="Download PDF">pdf</a>, <a href="/format/2402.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Education (GAIED): Advances, Opportunities, and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Heffernan%2C+N+T">Neil T. Heffernan</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4ser%2C+T">Tanja K&#xe4;ser</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+S">Steven Moore</a>, 
<a href="/search/cs?searchtype=author&query=Rafferty%2C+A+N">Anna N. Rafferty</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Adish Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This survey article has grown out of the GAIED (pronounced "guide") workshop
organized by the authors at the NeurIPS 2023 conference. We organized the GAIED
workshop as part of a community-building effort to bring together researchers,
educators, and practitioners to explore the potential of generative AI for
enhancing education. This article aims to provide an overview of the workshop
activities and highlight several future research directions in the area of
GAIED.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01582" title="Abstract">arXiv:2402.01582</a> [<a href="/pdf/2402.01582" title="Download PDF">pdf</a>, <a href="/ps/2402.01582" title="Download PostScript">ps</a>, <a href="/format/2402.01582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating Sound Change Prediction for Phylogenetic Inference: A  Tukanoan Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kalvin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+N+R">Nathaniel R. Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+A">Anna Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Ting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Annie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D+R">David R. Mortensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LChange 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We describe a set of new methods to partially automate linguistic
phylogenetic inference given (1) cognate sets with their respective protoforms
and sound laws, (2) a mapping from phones to their articulatory features and
(3) a typological database of sound changes. We train a neural network on these
sound change data to weight articulatory distances between phones and predict
intermediate sound change steps between historical protoforms and their modern
descendants, replacing a linguistic expert in part of a parsimony-based
phylogenetic inference algorithm. In our best experiments on Tukanoan
languages, this method produces trees with a Generalized Quartet Distance of
0.12 from a tree that used expert annotations, a significant improvement over
other semi-automated baselines. We discuss potential benefits and drawbacks to
our neural approach and parsimony-based tree prediction. We also experiment
with a minimal generalization learner for automatic sound law induction,
finding it comparably effective to sound laws from expert annotation. Our code
is publicly available at https://github.com/cmu-llab/aiscp.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01583" title="Abstract">arXiv:2402.01583</a> [<a href="/pdf/2402.01583" title="Download PDF">pdf</a>, <a href="/format/2402.01583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the efficient computation of smoothness indicators for a class of  WENO reconstructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=B%C3%BCrger%2C+R">Raimund B&#xfc;rger</a>, 
<a href="/search/math?searchtype=author&query=Mulet%2C+P">Pep Mulet</a>, 
<a href="/search/math?searchtype=author&query=Zor%C3%ADo%2C+D">David Zor&#xed;o</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J Sci Comput 80, 1240-1263 (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Common smoothness indicators used in Weighted Essentially
Non\--Os\-cil\-la\-to\-ry (WENO) reconstructions [Jiang, G.S., Shu, C.W.:
Efficient implementation of {Weighted} {ENO} schemes, J.\ Comput.\ Phys.
\textbf{126}, 202--228 (1996)] have quadratic cost with respect to the order. A
set of novel smoothness indicators with linear cost of computation with respect
to the order is presented. These smoothness indicators can be used in the
context of schemes of the type introduced by Yamaleev and Carpenter [Yamaleev,
N.K., Carpenter, M.H.: A systematic methodology to for constructing high-order
energy stable WENO schemes. J. Comput. Phys. \textbf{228}(11), 4248-4272
(2009)]. The accuracy properties of the resulting non-linear weights are the
same as those arising from using the traditional Jiang-Shu smoothness
indicators in Yamaleev-Carpenter-type reconstructions. The increase of the
efficiency and ease of implementation are shown.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01586" title="Abstract">arXiv:2402.01586</a> [<a href="/pdf/2402.01586" title="Download PDF">pdf</a>, <a href="/format/2402.01586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent  Constitution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zelong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Cheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 5 tables, comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The emergence of LLM-based agents has garnered considerable attention, yet
their trustworthiness remains an under-explored area. As agents can directly
interact with the physical environment, their reliability and safety is
critical. This paper presents an Agent-Constitution-based agent framework,
TrustAgent, an initial investigation into improving the safety dimension of
trustworthiness in LLM-based agents. This framework consists of threefold
strategies: pre-planning strategy which injects safety knowledge to the model
prior to plan generation, in-planning strategy which bolsters safety during
plan generation, and post-planning strategy which ensures safety by
post-planning inspection. Through experimental analysis, we demonstrate how
these approaches can effectively elevate an LLM agent's safety by identifying
and preventing potential dangers. Furthermore, we explore the intricate
relationships between safety and helpfulness, and between the model's reasoning
ability and its efficacy as a safe agent. This paper underscores the imperative
of integrating safety awareness and trustworthiness into the design and
deployment of LLM-based agents, not only to enhance their performance but also
to ensure their responsible integration into human-centric environments. Data
and code are available at https://github.com/agiresearch/TrustAgent.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01589" title="Abstract">arXiv:2402.01589</a> [<a href="/pdf/2402.01589" title="Download PDF">pdf</a>, <a href="/ps/2402.01589" title="Download PostScript">ps</a>, <a href="/format/2402.01589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bisimulations and Logics for Higher-Dimensional Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zouari%2C+S">Safa Zouari</a>, 
<a href="/search/cs?searchtype=author&query=Ziemianski%2C+K">Krzysztof Ziemianski</a>, 
<a href="/search/cs?searchtype=author&query=Fahrenberg%2C+U">Uli Fahrenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Higher-dimensional automata (HDAs) are models of non-in\-ter\-leav\-ing
concurrency for analyzing concurrent systems. There is a rich literature that
deals with bisimulations for concurrent systems, and some of them have been
extended to HDAs. However, no logical characterizations of these relations are
currently available for HDAs. In this work, we address this gap by introducing
Ipomset modal logic, a Hennessy-Milner type logic over HDAs, and show that it
characterizes Path-bisimulation, a variant of ST-bisimulation existing in the
literature. We also define a notion of Cell-bisimulation, using the open-maps
framework of Joyal, Nielsen, and Winskel, and establish the relationship
between these bisimulations (and also their "strong" variants, which take
restrictions into account). In our work, we rely on the new categorical
definition of HDAs as presheaves over concurrency lists and on track objects.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01590" title="Abstract">arXiv:2402.01590</a> [<a href="/pdf/2402.01590" title="Download PDF">pdf</a>, <a href="/format/2402.01590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zijiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the pursuit to understand the intricacies of human brain's visual
processing, reconstructing dynamic visual experiences from brain activities
emerges as a challenging yet fascinating endeavor. While recent advancements
have achieved success in reconstructing static images from non-invasive brain
recordings, the domain of translating continuous brain activities into video
format remains underexplored. In this work, we introduce NeuroCine, a novel
dual-phase framework to targeting the inherent challenges of decoding fMRI
data, such as noises, spatial redundancy and temporal lags. This framework
proposes spatial masking and temporal interpolation-based augmentation for
contrastive learning fMRI representations and a diffusion model enhanced by
dependent prior noise for video generation. Tested on a publicly available fMRI
dataset, our method shows promising results, outperforming the previous
state-of-the-art models by a notable margin of ${20.97\%}$, ${31.00\%}$ and
${12.30\%}$ respectively on decoding the brain activities of three subjects in
the fMRI dataset, as measured by SSIM. Additionally, our attention analysis
suggests that the model aligns with existing brain structures and functions,
indicating its biological plausibility and interpretability.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01592" title="Abstract">arXiv:2402.01592</a> [<a href="/pdf/2402.01592" title="Download PDF">pdf</a>, <a href="/format/2402.01592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Sustainable Workplace Mental Health: A Novel Approach to Early  Intervention and Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vinson%2C+D+W">David W. Vinson</a>, 
<a href="/search/cs?searchtype=author&query=Arcan%2C+M">Mihael Arcan</a>, 
<a href="/search/cs?searchtype=author&query=Niland%2C+D">David-Paul Niland</a>, 
<a href="/search/cs?searchtype=author&query=Delahunty%2C+F">Fionn Delahunty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Employee well-being is a critical concern in the contemporary workplace, as
highlighted by the American Psychological Association's 2021 report, indicating
that 71% of employees experience stress or tension. This stress contributes
significantly to workplace attrition and absenteeism, with 61% of attrition and
16% of sick days attributed to poor mental health. A major challenge for
employers is that employees often remain unaware of their mental health issues
until they reach a crisis point, resulting in limited utilization of corporate
well-being benefits. This research addresses this challenge by presenting a
groundbreaking stress detection algorithm that provides real-time support
preemptively. Leveraging automated chatbot technology, the algorithm
objectively measures mental health levels by analyzing chat conversations,
offering personalized treatment suggestions in real-time based on linguistic
biomarkers. The study explores the feasibility of integrating these innovations
into practical learning applications within real-world contexts and introduces
a chatbot-style system integrated into the broader employee experience
platform. This platform, encompassing various features, aims to enhance overall
employee well-being, detect stress in real time, and proactively engage with
individuals to improve support effectiveness, demonstrating a 22% increase when
assistance is provided early. Overall, the study emphasizes the importance of
fostering a supportive workplace environment for employees' mental health.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01593" title="Abstract">arXiv:2402.01593</a> [<a href="/pdf/2402.01593" title="Download PDF">pdf</a>, <a href="/ps/2402.01593" title="Download PostScript">ps</a>, <a href="/format/2402.01593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Accuracy of Approximate Filtering Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carrillo%2C+J+A">J. A. Carrillo</a>, 
<a href="/search/math?searchtype=author&query=Hoffmann%2C+F">F. Hoffmann</a>, 
<a href="/search/math?searchtype=author&query=Stuart%2C+A+M">A. M. Stuart</a>, 
<a href="/search/math?searchtype=author&query=Vaes%2C+U">U. Vaes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SIAM News
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">Estimating the statistics of the state of a dynamical system, from partial
and noisy observations, is both mathematically challenging and finds wide
application. Furthermore, the applications are of great societal importance,
including problems such as probabilistic weather forecasting and prediction of
epidemics. Particle filters provide a well-founded approach to the problem,
leading to provably accurate approximations of the statistics. However these
methods perform poorly in high dimensions. In 1994 the idea of ensemble Kalman
filtering was introduced by Evensen, leading to a methodology that has been
widely adopted in the geophysical sciences and also finds application to quite
general inverse problems. However, ensemble Kalman filters have defied rigorous
analysis of their statistical accuracy, except in the linear Gaussian setting.
In this article we describe recent work which takes first steps to analyze the
statistical accuracy of ensemble Kalman filters beyond the linear Gaussian
setting. The subject is inherently technical, as it involves the evolution of
probability measures according to a nonlinear and nonautonomous dynamical
system; and the approximation of this evolution. It can nonetheless be
presented in a fairly accessible fashion, understandable with basic knowledge
of dynamical systems, numerical analysis and probability.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01602" title="Abstract">arXiv:2402.01602</a> [<a href="/pdf/2402.01602" title="Download PDF">pdf</a>, <a href="/format/2402.01602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model Sherpas: Guiding Foundation Models through Knowledge  and Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharjya%2C+D">Debarun Bhattacharjya</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junkyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Agravante%2C+D+J">Don Joven Agravante</a>, 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+B">Balaji Ganesan</a>, 
<a href="/search/cs?searchtype=author&query=Marinescu%2C+R">Radu Marinescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Foundation models (FMs) such as large language models have revolutionized the
field of AI by showing remarkable performance in various tasks. However, they
exhibit numerous limitations that prevent their broader adoption in many
real-world systems, which often require a higher bar for trustworthiness and
usability. Since FMs are trained using loss functions aimed at reconstructing
the training corpus in a self-supervised manner, there is no guarantee that the
model's output aligns with users' preferences for a specific task at hand. In
this survey paper, we propose a conceptual framework that encapsulates
different modes by which agents could interact with FMs and guide them suitably
for a set of tasks, particularly through knowledge augmentation and reasoning.
Our framework elucidates agent role categories such as updating the underlying
FM, assisting with prompting the FM, and evaluating the FM output. We also
categorize several state-of-the-art approaches into agent interaction
protocols, highlighting the nature and extent of involvement of the various
agent roles. The proposed framework provides guidance for future directions to
further realize the power of FMs in practical AI systems.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01607" title="Abstract">arXiv:2402.01607</a> [<a href="/pdf/2402.01607" title="Download PDF">pdf</a>, <a href="/format/2402.01607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Counterfactuals With Necessary Backtracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+G">Guang-Yuan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Counterfactual reasoning is pivotal in human cognition and especially
important for providing explanations and making decisions. While Judea Pearl's
influential approach is theoretically elegant, its generation of a
counterfactual scenario often requires interventions that are too detached from
the real scenarios to be feasible. In response, we propose a framework of
natural counterfactuals and a method for generating counterfactuals that are
natural with respect to the actual world's data distribution. Our methodology
refines counterfactual reasoning, allowing changes in causally preceding
variables to minimize deviations from realistic scenarios. To generate natural
counterfactuals, we introduce an innovative optimization framework that permits
but controls the extent of backtracking with a naturalness criterion. Empirical
experiments indicate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01608" title="Abstract">arXiv:2402.01608</a> [<a href="/pdf/2402.01608" title="Download PDF">pdf</a>, <a href="/ps/2402.01608" title="Download PostScript">ps</a>, <a href="/format/2402.01608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contingency Analysis of a Grid of Connected EVs for Primary Frequency  Control of an Industrial Microgrid Using Efficient Control Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabhahit%2C+J+N">J.N. Sabhahit</a>, 
<a href="/search/cs?searchtype=author&query=Solanke%2C+S+S">S.S. Solanke</a>, 
<a href="/search/cs?searchtype=author&query=Jadoun%2C+V+K">V.K. Jadoun</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+H">H. Malik</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A1rquez%2C+F+P+G">F.P. Garc&#xed;a M&#xe1;rquez</a>, 
<a href="/search/cs?searchtype=author&query=Pinar-P%C3%A9rez%2C+J+M">J.M. Pinar-P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in energies (MDPI) 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Energies 2022, 15, 3102
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">After over a century of internal combustion engines ruling the transport
sector, electric vehicles appear to be on the verge of gaining traction due to
a slew of advantages, including lower operating costs and lower CO2 emissions.
By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are
utilized as load) approach, EVs can operate as both a load and a source.
Primary frequency regulation and congestion management are two essential
characteristics of this technology that are added to an industrial microgrid.
Industrial Microgrids are made up of different energy sources such as wind
farms and PV farms, storage systems, and loads. EVs have gained a lot of
interest as a technique for frequency management because of their ability to
regulate quickly. Grid reliability depends on this quick reaction. Different
contingency, state of charge of the electric vehicles, and a varying number of
EVs in an EV fleet are considered in this work, and a proposed control scheme
for frequency management is presented. This control scheme enables
bidirectional power flow, allowing for primary frequency regulation during the
various scenarios that an industrial microgrid may encounter over the course of
a 24-h period. The presented controller will provide dependable frequency
regulation support to the industrial microgrid during contingencies, as will be
demonstrated by simulation results, achieving a more reliable system. However,
simulation results will show that by increasing a number of the EVs in a fleet
for the Vehicle-to-Grid approach, an industrial microgrid\'s frequency can be
enhanced even further.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01610" title="Abstract">arXiv:2402.01610</a> [<a href="/pdf/2402.01610" title="Download PDF">pdf</a>, <a href="/format/2402.01610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runtime phylogenetic analysis enables extreme subsampling for test-based  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lalejini%2C+A">Alexander Lalejini</a>, 
<a href="/search/cs?searchtype=author&query=Sanson%2C+M">Marcos Sanson</a>, 
<a href="/search/cs?searchtype=author&query=Garbus%2C+J">Jack Garbus</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+M+A">Matthew Andres Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Dolson%2C+E">Emily Dolson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">A phylogeny describes the evolutionary history of an evolving population.
Evolutionary search algorithms can perfectly track the ancestry of candidate
solutions, illuminating a population's trajectory through the search space.
However, phylogenetic analyses are typically limited to post-hoc studies of
search performance. We introduce phylogeny-informed subsampling, a new class of
subsampling methods that exploit runtime phylogenetic analyses for solving
test-based problems. Specifically, we assess two phylogeny-informed subsampling
methods -- individualized random subsampling and ancestor-based subsampling --
on three diagnostic problems and ten genetic programming (GP) problems from
program synthesis benchmark suites. Overall, we found that phylogeny-informed
subsampling methods enable problem-solving success at extreme subsampling
levels where other subsampling methods fail. For example, phylogeny-informed
subsampling methods more reliably solved program synthesis problems when
evaluating just one training case per-individual, per-generation. However, at
moderate subsampling levels, phylogeny-informed subsampling generally performed
no better than random subsampling on GP problems. Our diagnostic experiments
show that phylogeny-informed subsampling improves diversity maintenance
relative to random subsampling, but its effects on a selection scheme's
capacity to rapidly exploit fitness gradients varied by selection scheme.
Continued refinements of phylogeny-informed subsampling techniques offer a
promising new direction for scaling up evolutionary systems to handle problems
with many expensive-to-evaluate fitness criteria.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01613" title="Abstract">arXiv:2402.01613</a> [<a href="/pdf/2402.01613" title="Download PDF">pdf</a>, <a href="/format/2402.01613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nomic Embed: Training a Reproducible Long Context Text Embedder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nussbaum%2C+Z">Zach Nussbaum</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+J+X">John X. Morris</a>, 
<a href="/search/cs?searchtype=author&query=Duderstadt%2C+B">Brandon Duderstadt</a>, 
<a href="/search/cs?searchtype=author&query=Mulyar%2C+A">Andriy Mulyar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This technical report describes the training of nomic-embed-text-v1, the
first fully reproducible, open-source, open-weights, open-data, 8192 context
length English text embedding model that outperforms both OpenAI Ada-002 and
OpenAI text-embedding-3-small on short and long-context tasks. We release the
training code and model weights under an Apache 2 license. In contrast with
other open-source models, we release a training data loader with 235 million
curated text pairs that allows for the full replication of nomic-embed-text-v1.
You can find code and data to replicate the model at
https://github.com/nomic-ai/contrastors
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01614" title="Abstract">arXiv:2402.01614</a> [<a href="/pdf/2402.01614" title="Download PDF">pdf</a>, <a href="/format/2402.01614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L2G2G: a Scalable Local-to-Global Network Embedding with Graph  Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+R">Ruikang Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+A">Andrew Elliott</a>, 
<a href="/search/cs?searchtype=author&query=Limnios%2C+S">Stratis Limnios</a>, 
<a href="/search/cs?searchtype=author&query=Cucuringu%2C+M">Mihai Cucuringu</a>, 
<a href="/search/cs?searchtype=author&query=Reinert%2C+G">Gesine Reinert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures, Complex Networks 2023, Volume I, SCI 1141
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
<p class="mathjax">For analysing real-world networks, graph representation learning is a popular
tool. These methods, such as a graph autoencoder (GAE), typically rely on
low-dimensional representations, also called embeddings, which are obtained
through minimising a loss function; these embeddings are used with a decoder
for downstream tasks such as node classification and edge prediction. While
GAEs tend to be fairly accurate, they suffer from scalability issues. For
improved speed, a Local2Global approach, which combines graph patch embeddings
based on eigenvector synchronisation, was shown to be fast and achieve good
accuracy. Here we propose L2G2G, a Local2Global method which improves GAE
accuracy without sacrificing scalability. This improvement is achieved by
dynamically synchronising the latent node representations, while training the
GAEs. It also benefits from the decoder computing an only local patch loss.
Hence, aligning the local embeddings in each epoch utilises more information
from the graph than a single post-training alignment does, while maintaining
scalability. We illustrate on synthetic benchmarks, as well as real-world
examples, that L2G2G achieves higher accuracy than the standard Local2Global
approach and scales efficiently on the larger data sets. We find that for large
and dense networks, it even outperforms the slow, but assumed more accurate,
GAEs.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01617" title="Abstract">arXiv:2402.01617</a> [<a href="/pdf/2402.01617" title="Download PDF">pdf</a>, <a href="/format/2402.01617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A GP-based Robust Motion Planning Framework for Agile Autonomous Robot  Navigation and Recovery in Unknown Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+N">Nicholas Mohammad</a>, 
<a href="/search/cs?searchtype=author&query=Higgins%2C+J">Jacob Higgins</a>, 
<a href="/search/cs?searchtype=author&query=Bezzo%2C+N">Nicola Bezzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in 2024 IEEE/RSJ International Conference on Robotics and Automation (ICRA), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">For autonomous mobile robots, uncertainties in the environment and system
model can lead to failure in the motion planning pipeline, resulting in
potential collisions. In order to achieve a high level of robust autonomy,
these robots should be able to proactively predict and recover from such
failures. To this end, we propose a Gaussian Process (GP) based model for
proactively detecting the risk of future motion planning failure. When this
risk exceeds a certain threshold, a recovery behavior is triggered that
leverages the same GP model to find a safe state from which the robot may
continue towards the goal. The proposed approach is trained in simulation only
and can generalize to real world environments on different robotic platforms.
Simulations and physical experiments demonstrate that our framework is capable
of both predicting planner failures and recovering the robot to states where
planner success is likely, all while producing agile motion.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01618" title="Abstract">arXiv:2402.01618</a> [<a href="/pdf/2402.01618" title="Download PDF">pdf</a>, <a href="/format/2402.01618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style Vectors for Steering Generative Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konen%2C+K">Kai Konen</a>, 
<a href="/search/cs?searchtype=author&query=Jentzsch%2C+S">Sophie Jentzsch</a>, 
<a href="/search/cs?searchtype=author&query=Diallo%2C+D">Diaoul&#xe9; Diallo</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtt%2C+P">Peer Sch&#xfc;tt</a>, 
<a href="/search/cs?searchtype=author&query=Bensch%2C+O">Oliver Bensch</a>, 
<a href="/search/cs?searchtype=author&query=Baff%2C+R+E">Roxanne El Baff</a>, 
<a href="/search/cs?searchtype=author&query=Opitz%2C+D">Dominik Opitz</a>, 
<a href="/search/cs?searchtype=author&query=Hecking%2C+T">Tobias Hecking</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will be published as findings paper at EACL2024 - 18th Conference of the European Chapter of the Association for Computational Linguistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This research explores strategies for steering the output of large language
models (LLMs) towards specific styles, such as sentiment, emotion, or writing
style, by adding style vectors to the activations of hidden layers during text
generation. We show that style vectors can be simply computed from recorded
layer activations for input texts in a specific style in contrast to more
complex training-based approaches. Through a series of experiments, we
demonstrate the effectiveness of activation engineering using such style
vectors to influence the style of generated text in a nuanced and
parameterisable way, distinguishing it from prompt engineering. The presented
research constitutes a significant step towards developing more adaptive and
effective AI-empowered interactive systems.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01619" title="Abstract">arXiv:2402.01619</a> [<a href="/pdf/2402.01619" title="Download PDF">pdf</a>, <a href="/format/2402.01619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce  Programs over Low-resourced Knowledge Bases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shulin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Linmei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Ling Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Program induction (PI) has become a promising paradigm for using knowledge
bases (KBs) to help large language models (LLMs) answer complex
knowledge-intensive questions. Nonetheless, PI typically relies on a large
number of parallel question-program pairs to make the LLM aware of the schema
of the given KB, and is thus challenging for many low-resourced KBs that lack
annotated data. To this end, we propose KB-Plugin, a plug-and-play framework
that enables LLMs to induce programs over any low-resourced KB. Firstly,
KB-Plugin adopts self-supervised learning to encode the detailed schema
information of a given KB into a pluggable module, namely schema plugin.
Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB
to train another pluggable module, namely PI plugin, which can help the LLM
extract question-relevant schema information from the schema plugin of any KB
and utilize this information to induce programs over this KB. Experiments on
five heterogeneous KBQA datasets show that KB-Plugin achieves better or
comparable performance with 25$\times$ smaller backbone LLM compared to SoTA PI
methods for low-resourced KBs, and even approaches the performance of
supervised methods. Our code and data are available at
https://github.com/THU-KEG/KB-Plugin.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01620" title="Abstract">arXiv:2402.01620</a> [<a href="/pdf/2402.01620" title="Download PDF">pdf</a>, <a href="/format/2402.01620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAGDi: Structured Distillation of Multi-Agent Interaction Graphs  Improves Reasoning in Smaller Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+C">Justin Chih-Yao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swarnadeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages; First two authors contributed equally; GitHub: <a href="https://github.com/dinobby/MAGDi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01621" title="Abstract">arXiv:2402.01621</a> [<a href="/pdf/2402.01621" title="Download PDF">pdf</a>, <a href="/format/2402.01621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Two Points Method for Deep Model Zeroth-order Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yijiang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayu Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large foundation models, such as large language models, have performed
exceptionally well in various application scenarios. Building or fully
fine-tuning such large models is usually prohibitive due to either hardware
budget or lack of access to backpropagation. The zeroth-order methods offer a
promising direction for tackling this challenge, where only forward passes are
needed to update the model. This paper introduces an efficient Stochastic
Two-Point (S2P) approach within the gradient-free regime. We present the
theoretical convergence properties of S2P under the general and relaxed
smoothness assumptions. The theoretical properties also shed light on a faster
and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new
convergence properties that better represent the dynamics of deep models in
training. Our comprehensive empirical results show that AS2P is highly
effective in optimizing objectives for large deep models, including language
models, and outperforms standard methods across various model types and scales,
with 2 $\times$ speed-up in training over most conducted tasks.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01622" title="Abstract">arXiv:2402.01622</a> [<a href="/pdf/2402.01622" title="Download PDF">pdf</a>, <a href="/format/2402.01622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TravelPlanner: A Benchmark for Real-World Planning with Language Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tinghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+R">Renze Lou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01629" title="Abstract">arXiv:2402.01629</a> [<a href="/pdf/2402.01629" title="Download PDF">pdf</a>, <a href="/ps/2402.01629" title="Download PostScript">ps</a>, <a href="/format/2402.01629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Generalized grammar rules and structure-based  generalization beyond classical equivariance for lexical tasks and  transduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrache%2C+M">Mircea Petrache</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+S">Shubhendu Trivedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Compositional generalization is one of the main properties which
differentiates lexical learning in humans from state-of-art neural networks. We
propose a general framework for building models that can generalize
compositionally using the concept of Generalized Grammar Rules (GGRs), a class
of symmetry-based compositional constraints for transduction tasks, which we
view as a transduction analogue of equivariance constraints in physics-inspired
tasks. Besides formalizing generalized notions of symmetry for language
transduction, our framework is general enough to contain many existing works as
special cases. We present ideas on how GGRs might be implemented, and in the
process draw connections to reinforcement learning and other areas of research.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01632" title="Abstract">arXiv:2402.01632</a> [<a href="/pdf/2402.01632" title="Download PDF">pdf</a>, <a href="/format/2402.01632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown  Hyperparameters Of Any Type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziomek%2C+J">Juliusz Ziomek</a>, 
<a href="/search/cs?searchtype=author&query=Adachi%2C+M">Masaki Adachi</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian optimisation requires fitting a Gaussian process model, which in
turn requires specifying hyperparameters - most of the theoretical literature
assumes those hyperparameters are known. The commonly used maximum likelihood
estimator for hyperparameters of the Gaussian process is consistent only if the
data fills the space uniformly, which does not have to be the case in Bayesian
optimisation. Since no guarantees exist regarding the correctness of
hyperparameter estimation, and those hyperparameters can significantly affect
the Gaussian process fit, theoretical analysis of Bayesian optimisation with
unknown hyperparameters is very challenging. Previously proposed algorithms
with the no-regret property were only able to handle the special case of
unknown lengthscales, reproducing kernel Hilbert space norm and applied only to
the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the
first algorithm enjoying the no-regret property in the case of unknown
hyperparameters of arbitrary form, and which supports both Bayesian and
frequentist settings. Our proof idea is novel and can easily be extended to
other variants of Bayesian optimisation. We show this by extending our
algorithm to the adversarially robust optimisation setting under unknown
hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy
problems and show that it can outperform the maximum likelihood estimator.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Mon,  5 Feb 24</h3>
<dl>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00926" title="Abstract">arXiv:2402.00926</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.00926" title="Download PDF">pdf</a>, <a href="/format/2402.00926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Gene Expression Profiling by Statistical and  Machine Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bontonou%2C+M">Myriam Bontonou</a>, 
<a href="/search/q-bio?searchtype=author&query=Haget%2C+A">Ana&#xef;s Haget</a>, 
<a href="/search/q-bio?searchtype=author&query=Boulougouri%2C+M">Maria Boulougouri</a>, 
<a href="/search/q-bio?searchtype=author&query=Audit%2C+B">Benjamin Audit</a>, 
<a href="/search/q-bio?searchtype=author&query=Borgnat%2C+P">Pierre Borgnat</a>, 
<a href="/search/q-bio?searchtype=author&query=Arbona%2C+J">Jean-Michel Arbona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many machine learning models have been proposed to classify phenotypes from
gene expression data. In addition to their good performance, these models can
potentially provide some understanding of phenotypes by extracting explanations
for their decisions. These explanations often take the form of a list of genes
ranked in order of importance for the predictions, the highest-ranked genes
being interpreted as linked to the phenotype. We discuss the biological and the
methodological limitations of such explanations. Experiments are performed on
several datasets gathering cancer and healthy tissue samples from the TCGA,
GTEx and TARGET databases. A collection of machine learning models including
logistic regression, multilayer perceptron, and graph neural network are
trained to classify samples according to their cancer type. Gene rankings are
obtained from explainability methods adapted to these models, and compared to
the ones from classical statistical feature selection methods such as mutual
information, DESeq2, and EdgeR. Interestingly, on simple tasks, we observe that
the information learned by black-box neural networks is related to the notion
of differential expression. In all cases, a small set containing the
best-ranked genes is sufficient to achieve a good classification. However,
these genes differ significantly between the methods and similar classification
performance can be achieved with numerous lower ranked genes. In conclusion,
although these methods enable the identification of biomarkers characteristic
of certain pathologies, our results question the completeness of the selected
gene sets and thus of explainability by the identification of the underlying
biological processes.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00944" title="Abstract">arXiv:2402.00944</a> (cross-list from hep-th) [<a href="/pdf/2402.00944" title="Download PDF">pdf</a>, <a href="/format/2402.00944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NCoder -- A Quantum Field Theory approach to encoding data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Berman%2C+D+S">David S. Berman</a>, 
<a href="/search/hep-th?searchtype=author&query=Klinger%2C+M+S">Marc S. Klinger</a>, 
<a href="/search/hep-th?searchtype=author&query=Stapleton%2C+A+G">Alexander G. Stapleton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper we present a novel approach to interpretable AI inspired by
Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified
autoencoder neural network whose latent layer is prescribed to be a subset of
$n$-point correlation functions. Regarding images as draws from a lattice field
theory, this architecture mimics the task of perturbatively constructing the
effective action of the theory order by order in an expansion using Feynman
diagrams. Alternatively, the NCoder may be regarded as simulating the procedure
of statistical inference whereby high dimensional data is first summarized in
terms of several lower dimensional summary statistics (here the $n$-point
correlation functions), and subsequent out-of-sample data is generated by
inferring the data generating distribution from these statistics. In this way
the NCoder suggests a fascinating correspondence between perturbative
renormalizability and the sufficiency of models. We demonstrate the efficacy of
the NCoder by applying it to the generation of MNIST images, and find that
generated images can be correctly classified using only information from the
first three $n$-point functions of the image distribution.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00949" title="Abstract">arXiv:2402.00949</a> (cross-list from math.AG) [<a href="/pdf/2402.00949" title="Download PDF">pdf</a>, <a href="/format/2402.00949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry of Polynomial Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kubjas%2C+K">Kaie Kubjas</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jiayi Li</a>, 
<a href="/search/math?searchtype=author&query=Wiesmann%2C+M">Maximilian Wiesmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the expressivity and learning process for polynomial neural networks
(PNNs) with monomial activation functions. The weights of the network
parametrize the neuromanifold. In this paper, we study certain neuromanifolds
using tools from algebraic geometry: we give explicit descriptions as
semialgebraic sets and characterize their Zariski closures, called
neurovarieties. We study their dimension and associate an algebraic degree, the
learning degree, to the neurovariety. The dimension serves as a geometric
measure for the expressivity of the network, the learning degree is a measure
for the complexity of training the network and provides upper bounds on the
number of learnable functions. These theoretical results are accompanied with
experiments.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00993" title="Abstract">arXiv:2402.00993</a> (cross-list from eess.IV) [<a href="/pdf/2402.00993" title="Download PDF">pdf</a>, <a href="/format/2402.00993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed image quality assessment using stacking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hosseini-Benvidi%2C+S+F">S. Farhad Hosseini-Benvidi</a>, 
<a href="/search/eess?searchtype=author&query=Motamednia%2C+H">Hossein Motamednia</a>, 
<a href="/search/eess?searchtype=author&query=Mansouri%2C+A">Azadeh Mansouri</a>, 
<a href="/search/eess?searchtype=author&query=Raei%2C+M">Mohammadreza Raei</a>, 
<a href="/search/eess?searchtype=author&query=Mahmoudi-Aznaveh%2C+A">Ahmad Mahmoudi-Aznaveh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">It is well-known that there is no universal metric for image quality
evaluation. In this case, distortion-specific metrics can be more reliable. The
artifact imposed by image compression can be considered as a combination of
various distortions. Depending on the image context, this combination can be
different. As a result, Generalization can be regarded as the major challenge
in compressed image quality assessment. In this approach, stacking is employed
to provide a reliable method. Both semantic and low-level information are
employed in the presented IQA to predict the human visual system. Moreover, the
results of the Full-Reference (FR) and No-Reference (NR) models are aggregated
to improve the proposed Full-Reference method for compressed image quality
evaluation. The accuracy of the quality benchmark of the clic2024 perceptual
image challenge was achieved 79.6\%, which illustrates the effectiveness of the
proposed fusion-based approach.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01000" title="Abstract">arXiv:2402.01000</a> (cross-list from stat.ML) [<a href="/pdf/2402.01000" title="Download PDF">pdf</a>, <a href="/format/2402.01000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Probabilistic Time Series Forecasting with Correlated  Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zheng%2C+V+Z">Vincent Zhihao Zheng</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+L">Lijun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.17028">arXiv:2305.17028</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modeling the correlations among errors is closely associated with how
accurately the model can quantify predictive uncertainty in probabilistic time
series forecasting. Recent multivariate models have made significant progress
in accounting for contemporaneous correlations among errors, while a common
assumption on these errors is that they are temporally independent for the sake
of statistical simplicity. However, real-world observations often deviate from
this assumption, since errors usually exhibit substantial autocorrelation due
to various factors such as the exclusion of temporally correlated covariates.
In this work, we propose an efficient method, based on a low-rank-plus-diagonal
parameterization of the covariance matrix, which can effectively characterize
the autocorrelation of errors. The proposed method possesses several desirable
properties: the complexity does not scale with the number of time series, the
resulting covariance can be used for calibrating predictions, and it can
seamlessly integrate with any model with Gaussian-distributed errors. We
empirically demonstrate these properties using two distinct neural forecasting
models -- GPVar and Transformer. Our experimental results confirm the
effectiveness of our method in enhancing predictive accuracy and the quality of
uncertainty quantification on multiple real-world datasets.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01031" title="Abstract">arXiv:2402.01031</a> (cross-list from eess.IV) [<a href="/pdf/2402.01031" title="Download PDF">pdf</a>, <a href="/ps/2402.01031" title="Download PostScript">ps</a>, <a href="/format/2402.01031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MRAnnotator: A Multi-Anatomy Deep Learning Model for MRI Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhou%2C+A">Alexander Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zelong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tieu%2C+A">Andrew Tieu</a>, 
<a href="/search/eess?searchtype=author&query=Patel%2C+N">Nikhil Patel</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+S">Sean Sun</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+A">Anthony Yang</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+P">Peter Choi</a>, 
<a href="/search/eess?searchtype=author&query=Fauveau%2C+V">Valentin Fauveau</a>, 
<a href="/search/eess?searchtype=author&query=Soultanidis%2C+G">George Soultanidis</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+M">Mingqian Huang</a>, 
<a href="/search/eess?searchtype=author&query=Doshi%2C+A">Amish Doshi</a>, 
<a href="/search/eess?searchtype=author&query=Fayad%2C+Z+A">Zahi A. Fayad</a>, 
<a href="/search/eess?searchtype=author&query=Deyer%2C+T">Timothy Deyer</a>, 
<a href="/search/eess?searchtype=author&query=Mei%2C+X">Xueyan Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Purpose To develop a deep learning model for multi-anatomy and many-class
segmentation of diverse anatomic structures on MRI imaging.
<br />Materials and Methods In this retrospective study, two datasets were curated
and annotated for model development and evaluation. An internal dataset of 1022
MRI sequences from various clinical sites within a health system and an
external dataset of 264 MRI sequences from an independent imaging center were
collected. In both datasets, 49 anatomic structures were annotated as the
ground truth. The internal dataset was divided into training, validation, and
test sets and used to train and evaluate an nnU-Net model. The external dataset
was used to evaluate nnU-Net model generalizability and performance in all
classes on independent imaging data. Dice scores were calculated to evaluate
model segmentation performance.
<br />Results The model achieved an average Dice score of 0.801 on the internal
test set, and an average score of 0.814 on the complete external dataset across
49 classes.
<br />Conclusion The developed model achieves robust and generalizable segmentation
of 49 anatomic structures on MRI imaging. A future direction is focused on the
incorporation of additional anatomic regions and structures into the datasets
and model.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01034" title="Abstract">arXiv:2402.01034</a> (cross-list from eess.IV) [<a href="/pdf/2402.01034" title="Download PDF">pdf</a>, <a href="/ps/2402.01034" title="Download PostScript">ps</a>, <a href="/format/2402.01034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VISION-MAE: A Foundation Model for Medical Image Segmentation and  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zelong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tieu%2C+A">Andrew Tieu</a>, 
<a href="/search/eess?searchtype=author&query=Patel%2C+N">Nikhil Patel</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+A">Alexander Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Soultanidis%2C+G">George Soultanidis</a>, 
<a href="/search/eess?searchtype=author&query=Fayad%2C+Z+A">Zahi A. Fayad</a>, 
<a href="/search/eess?searchtype=author&query=Deyer%2C+T">Timothy Deyer</a>, 
<a href="/search/eess?searchtype=author&query=Mei%2C+X">Xueyan Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Artificial Intelligence (AI) has the potential to revolutionize diagnosis and
segmentation in medical imaging. However, development and clinical
implementation face multiple challenges including limited data availability,
lack of generalizability, and the necessity to incorporate multi-modal data
effectively. A foundation model, which is a large-scale pre-trained AI model,
offers a versatile base that can be adapted to a variety of specific tasks and
contexts. Here, we present a novel foundation model, VISION-MAE, specifically
designed for medical imaging. Specifically, VISION-MAE is trained on a dataset
of 2.5 million unlabeled images from various modalities (CT, MR, PET, X-rays,
and ultrasound), using self-supervised learning techniques. It is then adapted
to classification and segmentation tasks using explicit labels. VISION-MAE has
high label efficiency, outperforming several benchmark models in both in-domain
and out-of-domain applications, and achieves high performance even with reduced
availability of labeled data. This model represents a significant advancement
in medical imaging AI, offering a generalizable and robust solution for
improving segmentation and classification tasks while reducing the data
annotation workload.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01036" title="Abstract">arXiv:2402.01036</a> (cross-list from math.PR) [<a href="/pdf/2402.01036" title="Download PDF">pdf</a>, <a href="/format/2402.01036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fisher information dissipation for time inhomogeneous stochastic  differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Feng%2C+Q">Qi Feng</a>, 
<a href="/search/math?searchtype=author&query=Zuo%2C+X">Xinzhe Zuo</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wuchen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 figures, 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We provide a Lyapunov convergence analysis for time-inhomogeneous variable
coefficient stochastic differential equations (SDEs). Three typical examples
include overdamped, irreversible drift, and underdamped Langevin dynamics. We
first formula the probability transition equation of Langevin dynamics as a
modified gradient flow of the Kullback-Leibler divergence in the probability
space with respect to time-dependent optimal transport metrics. This
formulation contains both gradient and non-gradient directions depending on a
class of time-dependent target distribution. We then select a time-dependent
relative Fisher information functional as a Lyapunov functional. We develop a
time-dependent Hessian matrix condition, which guarantees the convergence of
the probability density function of the SDE. We verify the proposed conditions
for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin
dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the
simulated annealing dynamics with a strongly convex potential function. For the
irreversible drift Langevin dynamics, we prove an improved convergence towards
the target distribution in an asymptotic regime. We also verify the convergence
condition for the underdamped Langevin dynamics. Numerical examples demonstrate
the convergence results for the time-dependent Langevin dynamics.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01050" title="Abstract">arXiv:2402.01050</a> (cross-list from stat.ML) [<a href="/pdf/2402.01050" title="Download PDF">pdf</a>, <a href="/format/2402.01050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed MCMC inference for Bayesian Non-Parametric Latent Block  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Khoufache%2C+R">Reda Khoufache</a>, 
<a href="/search/stat?searchtype=author&query=Belhadj%2C+A">Anisse Belhadj</a>, 
<a href="/search/stat?searchtype=author&query=Azzag%2C+H">Hanene Azzag</a>, 
<a href="/search/stat?searchtype=author&query=Lebbah%2C+M">Mustapha Lebbah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PaKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">In this paper, we introduce a novel Distributed Markov Chain Monte Carlo
(MCMC) inference method for the Bayesian Non-Parametric Latent Block Model
(DisNPLBM), employing the Master/Worker architecture. Our non-parametric
co-clustering algorithm divides observations and features into partitions using
latent multivariate Gaussian block distributions. The workload on rows is
evenly distributed among workers, who exclusively communicate with the master
and not among themselves. DisNPLBM demonstrates its impact on cluster labeling
accuracy and execution times through experimental results. Moreover, we present
a real-use case applying our approach to co-cluster gene expression data. The
code source is publicly available at
https://github.com/redakhoufache/Distributed-NPLBM.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01052" title="Abstract">arXiv:2402.01052</a> (cross-list from math.OC) [<a href="/pdf/2402.01052" title="Download PDF">pdf</a>, <a href="/format/2402.01052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Convex Regularisers for Inverse Problems: Convergence of Critical  Points and Primal-Dual Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shumaylov%2C+Z">Zakhar Shumaylov</a>, 
<a href="/search/math?searchtype=author&query=Budd%2C+J">Jeremy Budd</a>, 
<a href="/search/math?searchtype=author&query=Mukherjee%2C+S">Subhadip Mukherjee</a>, 
<a href="/search/math?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Variational regularisation is the primary method for solving inverse
problems, and recently there has been considerable work leveraging deeply
learned regularisation for enhanced performance. However, few results exist
addressing the convergence of such regularisation, particularly within the
context of critical points as opposed to global minima. In this paper, we
present a generalised formulation of convergent regularisation in terms of
critical points, and show that this is achieved by a class of weakly convex
regularisers. We prove convergence of the primal-dual hybrid gradient method
for the associated variational problem, and, given a Kurdyka-Lojasiewicz
condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally,
applying this theory to learned regularisation, we prove universal
approximation for input weakly convex neural networks (IWCNN), and show
empirically that IWCNNs can lead to improved performance of learned adversarial
regularisers for computed tomography (CT) reconstruction.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01054" title="Abstract">arXiv:2402.01054</a> (cross-list from eess.IV) [<a href="/pdf/2402.01054" title="Download PDF">pdf</a>, <a href="/format/2402.01054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unconditional Latent Diffusion Models Memorize Patient Imaging Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dar%2C+S+U+H">Salman Ul Hassan Dar</a>, 
<a href="/search/eess?searchtype=author&query=Seyfarth%2C+M">Marvin Seyfarth</a>, 
<a href="/search/eess?searchtype=author&query=Kahmann%2C+J">Jannik Kahmann</a>, 
<a href="/search/eess?searchtype=author&query=Ayx%2C+I">Isabelle Ayx</a>, 
<a href="/search/eess?searchtype=author&query=Papavassiliu%2C+T">Theano Papavassiliu</a>, 
<a href="/search/eess?searchtype=author&query=Schoenberg%2C+S+O">Stefan O. Schoenberg</a>, 
<a href="/search/eess?searchtype=author&query=Engelhardt%2C+S">Sandy Engelhardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative latent diffusion models hold a wide range of applications in the
medical imaging domain. A noteworthy application is privacy-preserved open-data
sharing by proposing synthetic data as surrogates of real patient data. Despite
the promise, these models are susceptible to patient data memorization, where
models generate patient data copies instead of novel synthetic samples. This
undermines the whole purpose of preserving patient data and may even result in
patient re-identification. Considering the importance of the problem,
surprisingly it has received relatively little attention in the medical imaging
community. To this end, we assess memorization in latent diffusion models for
medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR,
and X-ray datasets for synthetic data generation. Afterwards, we examine the
amount of training data memorized utilizing self-supervised models and further
investigate various factors that can possibly lead to memorization by training
models in different settings. We observe a surprisingly large amount of data
memorization among all datasets, with up to 41.7%, 19.6%, and 32.6% of the
training data memorized in CT, MRI, and X-ray datasets respectively. Further
analyses reveal that increasing training data size and using data augmentation
reduce memorization, while over-training enhances it. Overall, our results
suggest a call for memorization-informed evaluation of synthetic data prior to
open-data sharing.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01067" title="Abstract">arXiv:2402.01067</a> (cross-list from eess.IV) [<a href="/pdf/2402.01067" title="Download PDF">pdf</a>, <a href="/format/2402.01067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Patient Eligibility for Inspire Therapy through Machine  Learning and Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chowdhury%2C+M">Mohsena Chowdhury</a>, 
<a href="/search/eess?searchtype=author&query=Vyas%2C+T">Tejas Vyas</a>, 
<a href="/search/eess?searchtype=author&query=Alapati%2C+R">Rahul Alapati</a>, 
<a href="/search/eess?searchtype=author&query=Bur%2C+A+M">Andr&#xe9;s M Bur</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Guanghui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Inspire therapy is an FDA-approved internal neurostimulation treatment for
obstructive sleep apnea. However, not all patients respond to this therapy,
posing a challenge even for experienced otolaryngologists to determine
candidacy. This paper makes the first attempt to leverage both machine learning
and deep learning techniques in discerning patient responsiveness to Inspire
therapy using medical data and videos captured through Drug-Induced Sleep
Endoscopy (DISE), an essential procedure for Inspire therapy. To achieve this,
we gathered and annotated three datasets from 127 patients. Two of these
datasets comprise endoscopic videos focused on the Base of the Tongue and
Velopharynx. The third dataset composes the patient's clinical information. By
utilizing these datasets, we benchmarked and compared the performance of six
deep learning models and five classical machine learning algorithms. The
results demonstrate the potential of employing machine learning and deep
learning techniques to determine a patient's eligibility for Inspire therapy,
paving the way for future advancements in this field.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01089" title="Abstract">arXiv:2402.01089</a> (cross-list from stat.ML) [<a href="/pdf/2402.01089" title="Download PDF">pdf</a>, <a href="/format/2402.01089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Free Prune: Information-Theoretic Barriers to Pruning at  Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kumar%2C+T">Tanishq Kumar</a>, 
<a href="/search/stat?searchtype=author&query=Luo%2C+K">Kevin Luo</a>, 
<a href="/search/stat?searchtype=author&query=Sellke%2C+M">Mark Sellke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The existence of "lottery tickets" <a href="/abs/1803.03635">arXiv:1803.03635</a> at or near initialization
raises the tantalizing question of whether large models are necessary in deep
learning, or whether sparse networks can be quickly identified and trained
without ever training the dense models that contain them. However, efforts to
find these sparse subnetworks without training the dense model ("pruning at
initialization") have been broadly unsuccessful <a href="/abs/2009.08576">arXiv:2009.08576</a>. We put
forward a theoretical explanation for this, based on the model's effective
parameter count, $p_\text{eff}$, given by the sum of the number of non-zero
weights in the final network and the mutual information between the sparsity
mask and the data. We show the Law of Robustness of <a href="/abs/2105.12806">arXiv:2105.12806</a> extends to
sparse networks with the usual parameter count replaced by $p_\text{eff}$,
meaning a sparse neural network which robustly interpolates noisy data requires
a heavily data-dependent mask. We posit that pruning during and after training
outputs masks with higher mutual information than those produced by pruning at
initialization. Thus two networks may have the same sparsities, but differ in
effective parameter count based on how they were trained. This suggests that
pruning near initialization may be infeasible and explains why lottery tickets
exist, but cannot be found fast (i.e. without training the full network).
Experiments on neural networks confirm that information gained during training
may indeed affect model capacity.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01090" title="Abstract">arXiv:2402.01090</a> (cross-list from stat.ML) [<a href="/pdf/2402.01090" title="Download PDF">pdf</a>, <a href="/format/2402.01090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Higher-Order Tensor Product Spline Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=R%C3%BCgamer%2C+D">David R&#xfc;gamer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024. arXiv admin note: substantial text overlap with <a href="/abs/2205.14515">arXiv:2205.14515</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">In the current era of vast data and transparent machine learning, it is
essential for techniques to operate at a large scale while providing a clear
mathematical comprehension of the internal workings of the method. Although
there already exist interpretable semi-parametric regression methods for
large-scale applications that take into account non-linearity in the data, the
complexity of the models is still often limited. One of the main challenges is
the absence of interactions in these models, which are left out for the sake of
better interpretability but also due to impractical computational costs. To
overcome this limitation, we propose a new approach using a factorization
method to derive a highly scalable higher-order tensor product spline model.
Our method allows for the incorporation of all (higher-order) interactions of
non-linear feature effects while having computational costs proportional to a
model without interactions. We further develop a meaningful penalization scheme
and examine the induced optimization problem. We conclude by evaluating the
predictive and estimation performance of our method.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01092" title="Abstract">arXiv:2402.01092</a> (cross-list from stat.ML) [<a href="/pdf/2402.01092" title="Download PDF">pdf</a>, <a href="/format/2402.01092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dynamical Model of Neural Scaling Laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bordelon%2C+B">Blake Bordelon</a>, 
<a href="/search/stat?searchtype=author&query=Atanasov%2C+A">Alexander Atanasov</a>, 
<a href="/search/stat?searchtype=author&query=Pehlevan%2C+C">Cengiz Pehlevan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 9 figures, in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">On a variety of tasks, the performance of neural networks predictably
improves with training time, dataset size and model size across many orders of
magnitude. This phenomenon is known as a neural scaling law. Of fundamental
importance is the compute-optimal scaling law, which reports the performance as
a function of units of compute when choosing model sizes optimally. We analyze
a random feature model trained with gradient descent as a solvable model of
network training and generalization. This reproduces many observations about
neural scaling laws. First, our model makes a prediction about why the scaling
of performance with training time and with model size have different power law
exponents. Consequently, the theory predicts an asymmetric compute-optimal
scaling rule where the number of training steps are increased faster than model
parameters, consistent with recent empirical observations. Second, it has been
observed that early in training, networks converge to their infinite-width
dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate
$\textit{width}^{-c}$, where $c$ depends on the structure of the architecture
and task. We show that our model exhibits this behavior. Lastly, our theory
shows how the gap between training and test loss can gradually build up over
time due to repeated reuse of data.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01137" title="Abstract">arXiv:2402.01137</a> (cross-list from math.PR) [<a href="/pdf/2402.01137" title="Download PDF">pdf</a>, <a href="/ps/2402.01137" title="Download PostScript">ps</a>, <a href="/format/2402.01137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-time dynamics of stochastic wave equation with dissipative damping  and its full discretization: exponential ergodicity and strong law of large  numbers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+M">Meng Cai</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chuchu Chen</a>, 
<a href="/search/math?searchtype=author&query=Hong%2C+J">Jialin Hong</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+T">Tau Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">For stochastic wave equation, when the dissipative damping is a non-globally
Lipschitz function of the velocity, there are few results on the long-time
dynamics, in particular, the exponential ergodicity and strong law of large
numbers, for the equation and its numerical discretization to our knowledge.
Focus on this issue, the main contributions of this paper are as follows.
First, based on constructing novel Lyapunov functionals, we show the unique
invariant measure and exponential ergodicity of the underlying equation and its
full discretization. Second, the error estimates of invariant measures both in
Wasserstein distance and in the weak sense are obtained. Third, the strong laws
of large numbers of the equation and the full discretization are obtained,
which states that the time averages of the exact and numerical solutions are
shown to converge to the ergodic limit almost surely.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01138" title="Abstract">arXiv:2402.01138</a> (cross-list from eess.SP) [<a href="/pdf/2402.01138" title="Download PDF">pdf</a>, <a href="/format/2402.01138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks in EEG-based Emotion Recognition: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chenyu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xinliang Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yihao Wu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+R">Ruizhi Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhai%2C+L">Liming Zhai</a>, 
<a href="/search/eess?searchtype=author&query=Jia%2C+Z">Ziyu Jia</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Compared to other modalities, EEG-based emotion recognition can intuitively
respond to the emotional patterns in the human brain and, therefore, has become
one of the most concerning tasks in the brain-computer interfaces field. Since
dependencies within brain regions are closely related to emotion, a significant
trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion
recognition. However, brain region dependencies in emotional EEG have
physiological bases that distinguish GNNs in this field from those in other
time series fields. Besides, there is neither a comprehensive review nor
guidance for constructing GNNs in EEG-based emotion recognition. In the survey,
our categorization reveals the commonalities and differences of existing
approaches under a unified framework of graph construction. We analyze and
categorize methods from three stages in the framework to provide clear guidance
on constructing GNNs in EEG-based emotion recognition. In addition, we discuss
several open challenges and future directions, such as Temporal full-connected
graph and Graph condensation.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01139" title="Abstract">arXiv:2402.01139</a> (cross-list from stat.ML) [<a href="/pdf/2402.01139" title="Download PDF">pdf</a>, <a href="/format/2402.01139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online conformal prediction with decaying step sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Angelopoulos%2C+A+N">Anastasios N. Angelopoulos</a>, 
<a href="/search/stat?searchtype=author&query=Barber%2C+R+F">Rina Foygel Barber</a>, 
<a href="/search/stat?searchtype=author&query=Bates%2C+S">Stephen Bates</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">We introduce a method for online conformal prediction with decaying step
sizes. Like previous methods, ours possesses a retrospective guarantee of
coverage for arbitrary sequences. However, unlike previous methods, we can
simultaneously estimate a population quantile when it exists. Our theory and
experiments indicate substantially improved practical properties: in
particular, when the distribution is stable, the coverage is close to the
desired level for every time point, not just on average over the observed
sequence.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01148" title="Abstract">arXiv:2402.01148</a> (cross-list from math.ST) [<a href="/pdf/2402.01148" title="Download PDF">pdf</a>, <a href="/format/2402.01148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Optimality of Kernel Classifiers in Sobolev Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lai%2C+J">Jianfa Lai</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Z">Zhifan Li</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+D">Dongming Huang</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+Q">Qian Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Kernel methods are widely used in machine learning, especially for
classification problems. However, the theoretical analysis of kernel
classification is still limited. This paper investigates the statistical
performances of kernel classifiers. With some mild assumptions on the
conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper
bound on the classification excess risk of a kernel classifier using recent
advances in the theory of kernel regression. We also obtain a minimax lower
bound for Sobolev spaces, which shows the optimality of the proposed
classifier. Our theoretical results can be extended to the generalization error
of overparameterized neural network classifiers. To make our theoretical
results more applicable in realistic settings, we also propose a simple method
to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method
to real datasets.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01153" title="Abstract">arXiv:2402.01153</a> (cross-list from physics.optics) [<a href="/pdf/2402.01153" title="Download PDF">pdf</a>, <a href="/format/2402.01153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nanophotonic Phased Array XY Hamiltonian Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chalupnik%2C+M">Michelle Chalupnik</a>, 
<a href="/search/physics?searchtype=author&query=Singh%2C+A">Anshuman Singh</a>, 
<a href="/search/physics?searchtype=author&query=Leatham%2C+J">James Leatham</a>, 
<a href="/search/physics?searchtype=author&query=Loncar%2C+M">Marko Loncar</a>, 
<a href="/search/physics?searchtype=author&query=Soltani%2C+M">Moe Soltani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Solving large-scale computationally hard optimization problems using existing
computers has hit a bottleneck. A promising alternative approach uses
physics-based phenomena to naturally solve optimization problems wherein the
physical phenomena evolves to its minimum energy. In this regard, photonics
devices have shown promise as alternative optimization architectures,
benefiting from high-speed, high-bandwidth and parallelism in the optical
domain. Among photonic devices, programmable spatial light modulators (SLMs)
have shown promise in solving large scale Ising model problems to which many
computationally hard problems can be mapped. Despite much progress, existing
SLMs for solving the Ising model and similar problems suffer from slow update
rates and physical bulkiness. Here, we show that using a compact silicon
photonic integrated circuit optical phased array (PIC-OPA) we can simulate an
XY Hamiltonian, a generalized form of Ising Hamiltonian, where spins can vary
continuously. In this nanophotonic XY Hamiltonian solver, the spins are
implemented using analog phase shifters in the optical phased array. The far
field intensity pattern of the PIC-OPA represents an all-to-all coupled XY
Hamiltonian energy and can be optimized with the tunable phase-shifters
allowing us to solve an all-to-all coupled XY model. Our results show the
utility of PIC-OPAs as compact, low power, and high-speed solvers for
nondeterministic polynomial (NP)-hard problems. The scalability of the silicon
PIC-OPA and its compatibility with monolithic integration with CMOS electronics
further promises the realization of a powerful hybrid photonic/electronic
non-Von Neumann compute engine.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01171" title="Abstract">arXiv:2402.01171</a> (cross-list from eess.IV) [<a href="/pdf/2402.01171" title="Download PDF">pdf</a>, <a href="/format/2402.01171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AmbientCycleGAN for Establishing Interpretable Stochastic Object Models  Based on Mathematical Phantoms and Medical Imaging Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+X">Xichen Xu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wentao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+W">Weimin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical imaging systems that are designed for producing diagnostically
informative images should be objectively assessed via task-based measures of
image quality (IQ). Ideally, computation of task-based measures of IQ needs to
account for all sources of randomness in the measurement data, including the
variability in the ensemble of objects to be imaged. To address this need,
stochastic object models (SOMs) that can generate an ensemble of synthesized
objects or phantoms can be employed. Various mathematical SOMs or phantoms were
developed that can interpretably synthesize objects, such as lumpy object
models and parameterized torso phantoms. However, such SOMs that are purely
mathematically defined may not be able to comprehensively capture realistic
object variations. To establish realistic SOMs, it is desirable to use
experimental data. An augmented generative adversarial network (GAN),
AmbientGAN, was recently proposed for establishing SOMs from medical imaging
measurements. However, it remains unclear to which extent the
AmbientGAN-produced objects can be interpretably controlled. This work
introduces a novel approach called AmbientCycleGAN that translates mathematical
SOMs to realistic SOMs by use of noisy measurement data. Numerical studies that
consider clustered lumpy background (CLB) models and real mammograms are
conducted. It is demonstrated that our proposed method can stably establish
SOMs based on mathematical models and noisy measurement data. Moreover, the
ability of the proposed AmbientCycleGAN to interpretably control image features
in the synthesized objects is investigated.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01186" title="Abstract">arXiv:2402.01186</a> (cross-list from eess.IV) [<a href="/pdf/2402.01186" title="Download PDF">pdf</a>, <a href="/format/2402.01186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ambient-Pix2PixGAN for Translating Medical Images from Noisy Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wentao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+X">Xichen Xu</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+J">Jie Luo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+W">Weimin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image-to-image translation is a common task in computer vision and has been
rapidly increasing the impact on the field of medical imaging. Deep
learning-based methods that employ conditional generative adversarial networks
(cGANs), such as Pix2PixGAN, have been extensively explored to perform
image-to-image translation tasks. However, when noisy medical image data are
considered, such methods cannot be directly applied to produce clean images.
Recently, an augmented GAN architecture named AmbientGAN has been proposed that
can be trained on noisy measurement data to synthesize high-quality clean
medical images. Inspired by AmbientGAN, in this work, we propose a new cGAN
architecture, Ambient-Pix2PixGAN, for performing medical image-to-image
translation tasks by use of noisy measurement data. Numerical studies that
consider MRI-to-PET translation are conducted. Both traditional image quality
metrics and task-based image quality metrics are employed to assess the
proposed Ambient-Pix2PixGAN. It is demonstrated that our proposed
Ambient-Pix2PixGAN can be successfully trained on noisy measurement data to
produce high-quality translated images in target imaging modality.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01209" title="Abstract">arXiv:2402.01209</a> (cross-list from math.OC) [<a href="/pdf/2402.01209" title="Download PDF">pdf</a>, <a href="/format/2402.01209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solution of the Probabilistic Lambert&#x27;s Problem: Optimal Transport  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Teter%2C+A+M+H">Alexis M.H. Teter</a>, 
<a href="/search/math?searchtype=author&query=Nodozi%2C+I">Iman Nodozi</a>, 
<a href="/search/math?searchtype=author&query=Halder%2C+A">Abhishek Halder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The deterministic variant of the Lambert's problem was posed by Lambert in
the 18th century and its solution for conic trajectory has been derived by
many, including Euler, Lambert, Lagrange, Laplace, Gauss and Legendre. The
solution amounts to designing velocity control for steering a spacecraft from a
given initial to a given terminal position subject to gravitational potential
and flight time constraints. In recent years, a probabilistic variant of the
Lambert's problem has received attention in the aerospace community where the
endpoint position constraints are softened to endpoint joint probability
distributions over the respective positions. Such probabilistic specifications
account for the estimation errors, modeling uncertainties, etc. Building on a
deterministic optimal control reformulation via analytical mechanics, we show
that the probabilistic Lambert's problem is a generalized dynamic optimal mass
transport problem where the gravitational potential plays the role of an
additive state cost. This allows us to rigorously prove the
existence-uniqueness of the solution for the probabilistic Lambert problem both
with and without process noise. In the latter case, the problem and its
solution correspond to a generalized Schr\"odinger bridge, much like how
classical Schrodinger bridge can be seen as stochastic regularization of the
optimal mass transport. We deduce the large deviation principle enjoyed by the
Lambertian Schr\"odinger bridge. Leveraging these newfound connections, we
design a computational algorithm to illustrate the nonparametric numerical
solution of the probabilistic Lambert's problem.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01213" title="Abstract">arXiv:2402.01213</a> (cross-list from math.LO) [<a href="/pdf/2402.01213" title="Download PDF">pdf</a>, <a href="/ps/2402.01213" title="Download PostScript">ps</a>, <a href="/format/2402.01213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forcing with Language Fragments, Extending Namba Forcing, and Models of  Theories with Constraints in Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lau%2C+D">Desmond Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 90 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We develop a forcing framework based on the idea of amalgamating language
fragments into a theory with a canonical Henkin model. We then demonstrate the
usefulness of this framework by applying it to both the extended Namba problem
and the analysis of models of certain theories with constraints in
interpretation (TCIs). The foundations for a theory of TCIs and their models
are laid in parallel to the development of our framework, and are of
independent interest.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01258" title="Abstract">arXiv:2402.01258</a> (cross-list from stat.ML) [<a href="/pdf/2402.01258" title="Download PDF">pdf</a>, <a href="/format/2402.01258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field  Dynamics on the Attention Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+J">Juno Kim</a>, 
<a href="/search/stat?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models based on the Transformer architecture have demonstrated
impressive capabilities to learn in context. However, existing theoretical
studies on how this phenomenon arises are limited to the dynamics of a single
layer of attention trained on linear regression tasks. In this paper, we study
the optimization of a Transformer consisting of a fully connected layer
followed by a linear attention layer. The MLP acts as a common nonlinear
representation or feature map, greatly enhancing the power of in-context
learning. We prove in the mean-field and two-timescale limit that the
infinite-dimensional loss landscape for the distribution of parameters, while
highly nonconvex, becomes quite benign. We also analyze the second-order
stability of mean-field dynamics and show that Wasserstein gradient flow almost
always avoids saddle points. Furthermore, we establish novel methods for
obtaining concrete improvement rates both away from and near critical points.
This represents the first saddle point analysis of mean-field dynamics in
general and the techniques are of independent interest.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01271" title="Abstract">arXiv:2402.01271</a> (cross-list from eess.AS) [<a href="/pdf/2402.01271" title="Download PDF">pdf</a>, <a href="/format/2402.01271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Linping Xu</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+J">Jiawei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Dejun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+X">Xianjun Xia</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Y">Yijian Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+P">Piao Ding</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shenyi Song</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+S">Sixing Yin</a>, 
<a href="/search/eess?searchtype=author&query=Sohel%2C+F">Ferdous Sohel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Recently, neural networks have proven to be effective in performing speech
coding task at low bitrates. However, under-utilization of intra-frame
correlations and the error of quantizer specifically degrade the reconstructed
audio quality. To improve the coding quality, we present an end-to-end neural
speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural
Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to
exploit the intra-frame correlations more efficiently. Furthermore, Group-wise
and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the
quantization noise. CBRC encodes audio every 20ms with no additional latency,
which is suitable for real-time communication. Experimental results demonstrate
the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at
12kbps.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01282" title="Abstract">arXiv:2402.01282</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.01282" title="Download PDF">pdf</a>, <a href="/format/2402.01282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable and accelerated wavelet transforms on the sphere and ball
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Price%2C+M+A">Matthew A. Price</a>, 
<a href="/search/astro-ph?searchtype=author&query=Polanska%2C+A">Alicja Polanska</a>, 
<a href="/search/astro-ph?searchtype=author&query=Whitney%2C+J">Jessica Whitney</a>, 
<a href="/search/astro-ph?searchtype=author&query=McEwen%2C+J+D">Jason D. McEwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Directional wavelet dictionaries are hierarchical representations which
efficiently capture and segment information across scale, location and
orientation. Such representations demonstrate a particular affinity to physical
signals, which often exhibit highly anisotropic, localised multiscale
structure. Many physically important signals are observed over spherical
domains, such as the celestial sky in cosmology. Leveraging recent advances in
computational harmonic analysis, we design new highly distributable and
automatically differentiable directional wavelet transforms on the
$2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 =
\mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere
with the radial half-line). We observe up to a $300$-fold and $21800$-fold
acceleration for signals on the sphere and ball, respectively, compared to
existing software, whilst maintaining 64-bit machine precision. Not only do
these algorithms dramatically accelerate existing spherical wavelet transforms,
the gradient information afforded by automatic differentiation unlocks many
data-driven analysis techniques previously not possible for these spaces. We
publicly release both S2WAV and S2BALL, open-sourced JAX libraries for our
transforms that are automatically differentiable and readily deployable both on
and over clusters of hardware accelerators (e.g. GPUs &amp; TPUs).
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01298" title="Abstract">arXiv:2402.01298</a> (cross-list from eess.AS) [<a href="/pdf/2402.01298" title="Download PDF">pdf</a>, <a href="/format/2402.01298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Semantic Information from Raw Audio Signal Using Both  Contextual and Phonetic Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jaeyeon Kim</a>, 
<a href="/search/eess?searchtype=author&query=Hwang%2C+I">Injune Hwang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+K">Kyogu Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD)

</div>
<p class="mathjax">We propose a framework to learn semantics from raw audio signals using two
types of representations, encoding contextual and phonetic information
respectively. Specifically, we introduce a speech-to-unit processing pipeline
that captures two types of representations with different time resolutions. For
the language model, we adopt a dual-channel architecture to incorporate both
types of representation. We also present new training objectives, masked
context reconstruction and masked context prediction, that push models to learn
semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech
Benchmark 2021 and Fluent Speech Command dataset show our framework learns
semantics better than models trained with only one type of representation.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01338" title="Abstract">arXiv:2402.01338</a> (cross-list from cond-mat.stat-mech) [<a href="/pdf/2402.01338" title="Download PDF">pdf</a>, <a href="/format/2402.01338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring the Langevin Equation with Uncertainty via Bayesian Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Bae%2C+Y">Youngkyoung Bae</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ha%2C+S">Seungwoong Ha</a>, 
<a href="/search/cond-mat?searchtype=author&query=Jeong%2C+H">Hawoong Jeong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Soft Condensed Matter (cond-mat.soft); Machine Learning (cs.LG); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">Pervasive across diverse domains, stochastic systems exhibit fluctuations in
processes ranging from molecular dynamics to climate phenomena. The Langevin
equation has served as a common mathematical model for studying such systems,
enabling predictions of their temporal evolution and analyses of thermodynamic
quantities, including absorbed heat, work done on the system, and entropy
production. However, inferring the Langevin equation from observed trajectories
remains challenging, particularly for nonlinear and high-dimensional systems.
In this study, we present a comprehensive framework that employs Bayesian
neural networks for inferring Langevin equations in both overdamped and
underdamped regimes. Our framework first provides the drift force and diffusion
matrix separately and then combines them to construct the Langevin equation. By
providing a distribution of predictions instead of a single value, our approach
allows us to assess prediction uncertainties, which can prevent potential
misunderstandings and erroneous decisions about the system. We demonstrate the
effectiveness of our framework in inferring Langevin equations for various
scenarios including a neuron model and microscopic engine, highlighting its
versatility and potential impact.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01382" title="Abstract">arXiv:2402.01382</a> (cross-list from stat.ML) [<a href="/pdf/2402.01382" title="Download PDF">pdf</a>, <a href="/format/2402.01382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of heavy tails in homogenized stochastic gradient descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jiao%2C+Z">Zhe Jiao</a>, 
<a href="/search/stat?searchtype=author&query=Keller-Ressel%2C+M">Martin Keller-Ressel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It has repeatedly been observed that loss minimization by stochastic gradient
descent (SGD) leads to heavy-tailed distributions of neural network parameters.
Here, we analyze a continuous diffusion approximation of SGD, called
homogenized stochastic gradient descent, show that it behaves asymptotically
heavy-tailed, and give explicit upper and lower bounds on its tail-index. We
validate these bounds in numerical experiments and show that they are typically
close approximations to the empirical tail-index of SGD iterates. In addition,
their explicit form enables us to quantify the interplay between optimization
parameters and the tail-index. Doing so, we contribute to the ongoing
discussion on links between heavy tails and the generalization performance of
neural networks as well as the ability of SGD to avoid suboptimal local minima.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01385" title="Abstract">arXiv:2402.01385</a> (cross-list from eess.AS) [<a href="/pdf/2402.01385" title="Download PDF">pdf</a>, <a href="/ps/2402.01385" title="Download PostScript">ps</a>, <a href="/format/2402.01385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Del Visual al Auditivo: Sonorizaci&#xf3;n de Escenas Guiada por Imagen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=S%C3%A1nchez%2C+M">Mar&#xed;a S&#xe1;nchez</a>, 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez%2C+L">Laura Fern&#xe1;ndez</a>, 
<a href="/search/eess?searchtype=author&query=Arias%2C+J">Juli&#xe1;n Arias</a>, 
<a href="/search/eess?searchtype=author&query=C%C3%A1mara%2C+M">Mateo C&#xe1;mara</a>, 
<a href="/search/eess?searchtype=author&query=Comini%2C+G">Giulia Comini</a>, 
<a href="/search/eess?searchtype=author&query=Gabrys%2C+A">Adam Gabrys</a>, 
<a href="/search/eess?searchtype=author&query=Blanco%2C+J+L">Jos&#xe9; Luis Blanco</a>, 
<a href="/search/eess?searchtype=author&query=Godino%2C+J+I">Juan Ignacio Godino</a>, 
<a href="/search/eess?searchtype=author&query=Hern%C3%A1ndez%2C+L+A">Luis Alfonso Hern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, in Spanish, Tecniac\'ustica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Recent advances in image, video, text and audio generative techniques, and
their use by the general public, are leading to new forms of content
generation. Usually, each modality was approached separately, which poses
limitations. The automatic sound recording of visual sequences is one of the
greatest challenges for the automatic generation of multimodal content. We
present a processing flow that, starting from images extracted from videos, is
able to sound them. We work with pre-trained models that employ complex
encoders, contrastive learning, and multiple modalities, allowing complex
representations of the sequences for their sonorization. The proposed scheme
proposes different possibilities for audio mapping and text guidance. We
evaluated the scheme on a dataset of frames extracted from a commercial video
game and sounds extracted from the Freesound platform. Subjective tests have
evidenced that the proposed scheme is able to generate and assign audios
automatically and conveniently to images. Moreover, it adapts well to user
preferences, and the proposed objective metrics show a high correlation with
the subjective ratings.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01400" title="Abstract">arXiv:2402.01400</a> (cross-list from stat.ML) [<a href="/pdf/2402.01400" title="Download PDF">pdf</a>, <a href="/format/2402.01400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-Efficient Correlation Clustering with Noisy Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kuroki%2C+Y">Yuko Kuroki</a>, 
<a href="/search/stat?searchtype=author&query=Miyauchi%2C+A">Atsushi Miyauchi</a>, 
<a href="/search/stat?searchtype=author&query=Bonchi%2C+F">Francesco Bonchi</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study a general clustering setting in which we have $n$ elements to be
clustered, and we aim to perform as few queries as possible to an oracle that
returns a noisy sample of the similarity between two elements. Our setting
encompasses many application domains in which the similarity function is costly
to compute and inherently noisy. We propose two novel formulations of online
learning problems rooted in the paradigm of Pure Exploration in Combinatorial
Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For
both settings, we design algorithms that combine a sampling strategy with a
classic approximation algorithm for correlation clustering and study their
theoretical guarantees. Our results are the first examples of polynomial-time
algorithms that work for the case of PE-CMAB in which the underlying offline
optimization problem is NP-hard.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01434" title="Abstract">arXiv:2402.01434</a> (cross-list from stat.ML) [<a href="/pdf/2402.01434" title="Download PDF">pdf</a>, <a href="/format/2402.01434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditioning non-linear and infinite-dimensional diffusion processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Baker%2C+E+L">Elizabeth Louise Baker</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+G">Gefan Yang</a>, 
<a href="/search/stat?searchtype=author&query=Severinsen%2C+M+L">Michael L. Severinsen</a>, 
<a href="/search/stat?searchtype=author&query=Hipsley%2C+C+A">Christy Anna Hipsley</a>, 
<a href="/search/stat?searchtype=author&query=Sommer%2C+S">Stefan Sommer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Generative diffusion models and many stochastic models in science and
engineering naturally live in infinite dimensions before discretisation. To
incorporate observed data for statistical and learning tasks, one needs to
condition on observations. While recent work has treated conditioning linear
processes in infinite dimensions, conditioning non-linear processes in infinite
dimensions has not been explored. This paper conditions function valued
stochastic processes without prior discretisation. To do so, we use an
infinite-dimensional version of Girsanov's theorem to condition a
function-valued stochastic process, leading to a stochastic differential
equation (SDE) for the conditioned process involving the score. We apply this
technique to do time series analysis for shapes of organisms in evolutionary
biology, where we discretise via the Fourier basis and then learn the
coefficients of the score function with score matching methods.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01441" title="Abstract">arXiv:2402.01441</a> (cross-list from q-fin.TR) [<a href="/pdf/2402.01441" title="Download PDF">pdf</a>, <a href="/ps/2402.01441" title="Download PostScript">ps</a>, <a href="/format/2402.01441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Market: Sentiment-Based Ensemble Trading Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Ye%2C+A">Andrew Ye</a>, 
<a href="/search/q-fin?searchtype=author&query=Xu%2C+J">James Xu</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Yu%2C+Y">Yifan Yu</a>, 
<a href="/search/q-fin?searchtype=author&query=Yan%2C+D">Daniel Yan</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+R">Ryan Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Dong%2C+B">Bosheng Dong</a>, 
<a href="/search/q-fin?searchtype=author&query=Chaudhary%2C+V">Vipin Chaudhary</a>, 
<a href="/search/q-fin?searchtype=author&query=Xu%2C+S">Shuai Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose the integration of sentiment analysis and deep-reinforcement
learning ensemble algorithms for stock trading, and design a strategy capable
of dynamically altering its employed agent given concurrent market sentiment.
In particular, we create a simple-yet-effective method for extracting news
sentiment and combine this with general improvements upon existing works,
resulting in automated trading agents that effectively consider both
qualitative market factors and quantitative stock data. We show that our
approach results in a strategy that is profitable, robust, and risk-minimal --
outperforming the traditional ensemble strategy as well as single agent
algorithms and market metrics. Our findings determine that the conventional
practice of switching ensemble agents every fixed-number of months is
sub-optimal, and that a dynamic sentiment-based framework greatly unlocks
additional performance within these agents. Furthermore, as we have designed
our algorithm with simplicity and efficiency in mind, we hypothesize that the
transition of our method from historical evaluation towards real-time trading
with live data should be relatively simple.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01445" title="Abstract">arXiv:2402.01445</a> (cross-list from quant-ph) [<a href="/pdf/2402.01445" title="Download PDF">pdf</a>, <a href="/format/2402.01445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All graph state verification protocols are composably secure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Colisson%2C+L">L&#xe9;o Colisson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Markham%2C+D">Damian Markham</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yehia%2C+R">Raja Yehia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Graph state verification protocols allow multiple parties to share a graph
state while checking that the state is honestly prepared, even in the presence
of malicious parties. Since graph states are the starting point of numerous
quantum protocols, it is crucial to ensure that graph state verification
protocols can safely be composed with other protocols, this property being
known as composable security. Previous works [YDK21] conjectured that such a
property could not be proven within the abstract cryptography framework: we
disprove this conjecture by showing that all graph state verification protocols
can be turned into a composably secure protocol with respect to the natural
functionality for graph state preparation. Moreover, we show that any unchanged
graph state verification protocols can also be considered as composably secure
for a slightly different, yet useful, functionality. Finally, we show that
these two results are optimal, in the sense that any such generic result,
considering arbitrary black-box protocols, must either modify the protocol or
consider a different functionality.
<br />Along the way, we show a protocol to generalize entanglement swapping to
arbitrary graph states that might be of independent interest.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01460" title="Abstract">arXiv:2402.01460</a> (cross-list from stat.ML) [<a href="/pdf/2402.01460" title="Download PDF">pdf</a>, <a href="/format/2402.01460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Conditional Generative Learning: Model and Error Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chang%2C+J">Jinyuan Chang</a>, 
<a href="/search/stat?searchtype=author&query=Ding%2C+Z">Zhao Ding</a>, 
<a href="/search/stat?searchtype=author&query=Jiao%2C+Y">Yuling Jiao</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+R">Ruoxuan Li</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+J+Z">Jerry Zhijian Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce an Ordinary Differential Equation (ODE) based deep generative
method for learning a conditional distribution, named the Conditional Follmer
Flow. Starting from a standard Gaussian distribution, the proposed flow could
efficiently transform it into the target conditional distribution at time 1.
For effective implementation, we discretize the flow with Euler's method where
we estimate the velocity field nonparametrically using a deep neural network.
Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein
distance between the distribution of the learned samples and the target
distribution, providing the first comprehensive end-to-end error analysis for
conditional distribution learning via ODE flow. Our numerical experiments
showcase its effectiveness across a range of scenarios, from standard
nonparametric conditional density estimation problems to more intricate
challenges involving image data, illustrating its superiority over various
existing conditional density estimation methods.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01483" title="Abstract">arXiv:2402.01483</a> (cross-list from math.CO) [<a href="/pdf/2402.01483" title="Download PDF">pdf</a>, <a href="/format/2402.01483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorics of rectangulations: Old and new bijections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Asinowski%2C+A">Andrei Asinowski</a>, 
<a href="/search/math?searchtype=author&query=Cardinal%2C+J">Jean Cardinal</a>, 
<a href="/search/math?searchtype=author&query=Felsner%2C+S">Stefan Felsner</a>, 
<a href="/search/math?searchtype=author&query=Fusy%2C+%C3%89">&#xc9;ric Fusy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A rectangulation is a decomposition of a rectangle into finitely many
rectangles. Via natural equivalence relations, rectangulations can be seen as
combinatorial objects with a rich structure, with links to lattice congruences,
flip graphs, polytopes, lattice paths, Hopf algebras, etc. In this paper, we
first revisit the structure of the respective equivalence classes: weak
rectangulations that preserve rectangle-segment adjacencies, and strong
rectangulations that preserve rectangle-rectangle adjacencies. We thoroughly
investigate posets defined by adjacency in rectangulations of both kinds, and
unify and simplify known bijections between rectangulations and permutation
classes. This yields a uniform treatment of mappings between permutations and
rectangulations that unifies the results from earlier contributions, and
emphasizes parallelism and differences between the weak and the strong cases.
Then, we consider the special case of guillotine rectangulations, and prove
that they can be characterized - under all known mappings between permutations
and rectangulations - by avoidance of two mesh patterns that correspond to
"windmills" in rectangulations. This yields new permutation classes in
bijection with weak guillotine rectangulations, and the first known permutation
class in bijection with strong guillotine rectangulations. Finally, we address
enumerative issues and prove asymptotic bounds for several families of strong
rectangulations.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01493" title="Abstract">arXiv:2402.01493</a> (cross-list from stat.ML) [<a href="/pdf/2402.01493" title="Download PDF">pdf</a>, <a href="/format/2402.01493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sliced-Wasserstein Estimation with Spherical Harmonics as Control  Variates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Leluc%2C+R">R&#xe9;mi Leluc</a>, 
<a href="/search/stat?searchtype=author&query=Dieuleveut%2C+A">Aymeric Dieuleveut</a>, 
<a href="/search/stat?searchtype=author&query=Portier%2C+F">Fran&#xe7;ois Portier</a>, 
<a href="/search/stat?searchtype=author&query=Segers%2C+J">Johan Segers</a>, 
<a href="/search/stat?searchtype=author&query=Zhuman%2C+A">Aigerim Zhuman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Sliced-Wasserstein (SW) distance between probability measures is defined
as the average of the Wasserstein distances resulting for the associated
one-dimensional projections. As a consequence, the SW distance can be written
as an integral with respect to the uniform measure on the sphere and the Monte
Carlo framework can be employed for calculating the SW distance. Spherical
harmonics are polynomials on the sphere that form an orthonormal basis of the
set of square-integrable functions on the sphere. Putting these two facts
together, a new Monte Carlo method, hereby referred to as Spherical Harmonics
Control Variates (SHCV), is proposed for approximating the SW distance using
spherical harmonics as control variates. The resulting approach is shown to
have good theoretical properties, e.g., a no-error property for Gaussian
measures under a certain form of linear dependency between the variables.
Moreover, an improved rate of convergence, compared to Monte Carlo, is
established for general measures. The convergence analysis relies on the
Lipschitz property associated to the SW integrand. Several numerical
experiments demonstrate the superior performance of SHCV against
state-of-the-art methods for SW distance computation.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01502" title="Abstract">arXiv:2402.01502</a> (cross-list from stat.ML) [<a href="/pdf/2402.01502" title="Download PDF">pdf</a>, <a href="/format/2402.01502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why do Random Forests Work? Understanding Tree Ensembles as  Self-Regularizing Adaptive Smoothers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Curth%2C+A">Alicia Curth</a>, 
<a href="/search/stat?searchtype=author&query=Jeffares%2C+A">Alan Jeffares</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite their remarkable effectiveness and broad application, the drivers of
success underlying ensembles of trees are still not fully understood. In this
paper, we highlight how interpreting tree ensembles as adaptive and
self-regularizing smoothers can provide new intuition and deeper insight to
this topic. We use this perspective to show that, when studied as smoothers,
randomized tree ensembles not only make predictions that are quantifiably more
smooth than the predictions of the individual trees they consist of, but also
further regulate their smoothness at test-time based on the dissimilarity
between testing and training inputs. First, we use this insight to revisit,
refine and reconcile two recent explanations of forest success by providing a
new way of quantifying the conjectured behaviors of tree ensembles objectively
by measuring the effective degree of smoothing they imply. Then, we move beyond
existing explanations for the mechanisms by which tree ensembles improve upon
individual trees and challenge the popular wisdom that the superior performance
of forests should be understood as a consequence of variance reduction alone.
We argue that the current high-level dichotomy into bias- and
variance-reduction prevalent in statistics is insufficient to understand tree
ensembles -- because the prevailing definition of bias does not capture
differences in the expressivity of the hypothesis classes formed by trees and
forests. Instead, we show that forests can improve upon trees by three distinct
mechanisms that are usually implicitly entangled. In particular, we demonstrate
that the smoothing effect of ensembling can reduce variance in predictions due
to noise in outcome generation, reduce variability in the quality of the
learned function given fixed input data and reduce potential bias in learnable
functions by enriching the available hypothesis space.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01509" title="Abstract">arXiv:2402.01509</a> (cross-list from eess.IV) [<a href="/pdf/2402.01509" title="Download PDF">pdf</a>, <a href="/format/2402.01509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Brain Tumor Inpainting with Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+R">Ruizhi Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xinru Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Pang%2C+H">Haowen Pang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+C">Chundan Xu</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+C">Chuyang Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Synthesizing healthy brain scans from diseased brain scans offers a potential
solution to address the limitations of general-purpose algorithms, such as
tissue segmentation and brain extraction algorithms, which may not effectively
handle diseased images. We consider this a 3D inpainting task and investigate
the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic
resonance imaging(MRI) data. Our contributions encompass potential
modifications tailored to MRI-specific needs, and we conducted evaluations of
multiple inpainting techniques using the BraTS2023 Inpainting datasets to
assess their efficacy and limitations.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01529" title="Abstract">arXiv:2402.01529</a> (cross-list from quant-ph) [<a href="/pdf/2402.01529" title="Download PDF">pdf</a>, <a href="/format/2402.01529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big data applications on small quantum computers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Yogendran%2C+B">Boniface Yogendran</a>, 
<a href="/search/quant-ph?searchtype=author&query=Charlton%2C+D">Daniel Charlton</a>, 
<a href="/search/quant-ph?searchtype=author&query=Beddig%2C+M">Miriam Beddig</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kolotouros%2C+I">Ioannis Kolotouros</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wallden%2C+P">Petros Wallden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Current quantum hardware prohibits any direct use of large classical
datasets. Coresets allow for a succinct description of these large datasets and
their solution in a computational task is competitive with the solution on the
original dataset. The method of combining coresets with small quantum computers
to solve a given task that requires a large number of data points was first
introduced by Harrow [<a href="/abs/2004.00026">arXiv:2004.00026</a>]. In this paper, we apply the coreset
method in three different well-studied classical machine learning problems,
namely Divisive Clustering, 3-means Clustering, and Gaussian Mixture Model
Clustering. We provide a Hamiltonian formulation of the aforementioned problems
for which the number of qubits scales linearly with the size of the coreset.
Then, we evaluate how the variational quantum eigensolver (VQE) performs on
these problems and demonstrate the practical efficiency of coresets when used
along with a small quantum computer. We perform noiseless simulations on
instances of sizes up to 25 qubits on CUDA Quantum and show that our approach
provides comparable performance to classical solvers.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01542" title="Abstract">arXiv:2402.01542</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.01542" title="Download PDF">pdf</a>, <a href="/format/2402.01542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Collective Variables for Protein Folding with Labeled Data  Augmentation through Geodesic Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yang%2C+S">Soojung Yang</a>, 
<a href="/search/physics?searchtype=author&query=Nam%2C+J">Juno Nam</a>, 
<a href="/search/physics?searchtype=author&query=Dietschreit%2C+J+C+B">Johannes C. B. Dietschreit</a>, 
<a href="/search/physics?searchtype=author&query=G%C3%B3mez-Bombarelli%2C+R">Rafael G&#xf3;mez-Bombarelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">In molecular dynamics (MD) simulations, rare events, such as protein folding,
are typically studied by means of enhanced sampling techniques, most of which
rely on the definition of a collective variable (CV) along which the
acceleration occurs. Obtaining an expressive CV is crucial, but often hindered
by the lack of information about the particular event, e.g., the transition
from unfolded to folded conformation. We propose a simulation-free data
augmentation strategy using physics-inspired metrics to generate geodesic
interpolations resembling protein folding transitions, thereby improving
sampling efficiency without true transition state samples. Leveraging
interpolation progress parameters, we introduce a regression-based learning
scheme for CV models, which outperforms classifier-based methods when
transition state data is limited and noisy
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01579" title="Abstract">arXiv:2402.01579</a> (cross-list from eess.AS) [<a href="/pdf/2402.01579" title="Download PDF">pdf</a>, <a href="/ps/2402.01579" title="Download PostScript">ps</a>, <a href="/format/2402.01579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Paralingual are Paralinguistic Representations? A Case Study in  Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
<a href="/search/eess?searchtype=author&query=Kashyap%2C+G+S">Gautam Siddharth Kashyap</a>, 
<a href="/search/eess?searchtype=author&query=Buduru%2C+A+B">Arun Balaji Buduru</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+R">Rajesh Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Pre-trained Models (PTMs) have facilitated substantial progress in the field
of Speech Emotion Recognition (SER). SER is an area with applications ranging
from HumanComputer Interaction to Healthcare. Recent studies have leveraged
various PTM representations as input features for downstream models for SER.
PTM specifically pre-trained for paralinguistic tasks have obtained
state-of-the-art (SOTA) performance for SER. However, such PTM haven't been
evaluated for SER in multilingual settings and experimented only with English.
So, we fill this gap, by performing a comprehensive comparative study of five
PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the
effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages.
Representations from TRILLsson achieved the best performance among all the
PTMs. This demonstrates that TRILLsson is able to effectively capture the
various paralinguistic features from speech data for better SER. We also show
that downstream models using TRILLsson representations achieve SOTA performance
in terms of accuracy across various multi-lingual datasets.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01585" title="Abstract">arXiv:2402.01585</a> (cross-list from math.OC) [<a href="/pdf/2402.01585" title="Download PDF">pdf</a>, <a href="/format/2402.01585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Market proliferation and the impact of locational complexity on network  restructuring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pinar-P%C3%A9rez%2C+J+M">J.M. Pinar-P&#xe9;rez</a>, 
<a href="/search/math?searchtype=author&query=Ruiz-Hern%C3%A1ndez%2C+D">D. Ruiz-Hern&#xe1;ndez</a>, 
<a href="/search/math?searchtype=author&query=Menezes%2C+M+B+C">M.B.C. Menezes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was published in Applied Mathematical modelling (Elsevier). Cite as: Pinar-P\'erez, J.M., Ruiz-Hern\'andez, D. and Menezes, M.B.C. (2022) Market proliferation and the impact of locational complexity on network restructuring. Applied Mathematical Modelling, 104. pp. 315-338. <a href="https://doi.org/10.1016/j.apm.2021.11.031">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Mathematical modelling 104 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">This manuscript investigates the problem of locational complexity, a type of
complexity that emanates from a companys territorial strategy. Using an
entropy-based measure for supply chain structural complexity (
pars-complexity), we develop a theoretical framework for analysing the effects
of locational complexity on the profitability of service/manufacturing
networks. The proposed model is used to shed light on the reasons why network
restructuring strategies may result ineffective at reducing complexity-related
costs. Our contribution is three-fold. First, we develop a novel mathematical
formulation of a facility location problem that integrates the pars-complexity
measure in the decision process. Second, using this model, we propose a
decomposition of the penalties imposed by locational complexity into (a) an
intrinsic cost of structural complexity; and (b) an avoidable cost of ignoring
such complexity in the decision process. Such a decomposition is a valuable
tool for identifying more effective measures for tackling locational
complexity, moreover, it has allowed us to provide an explanation to the
so-called addiction to growth within the locational context. Finally, we
propose three alternative strategies that attempt to mimic different approaches
used in practice by companies that have engaged in network restructuring
processes. The impact of those approaches is evaluated through extensive
numerical experiments. Our experimental results suggest that network
restructuring efforts that are not accompanied by a substantial reduction on
the target market of the company, fail at reducing complexity-related costs
and, therefore, have a limited impact on the companys profitability.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01591" title="Abstract">arXiv:2402.01591</a> (cross-list from eess.AS) [<a href="/pdf/2402.01591" title="Download PDF">pdf</a>, <a href="/format/2402.01591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BAT: Learning to Reason about Spatial Sounds with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Z">Zhisheng Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+P">Puyuan Peng</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xie Chen</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>, 
<a href="/search/eess?searchtype=author&query=Harwath%2C+D">David Harwath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Spatial sound reasoning is a fundamental human skill, enabling us to navigate
and interpret our surroundings based on sound. In this paper we present BAT,
which combines the spatial sound perception ability of a binaural acoustic
scene analysis model with the natural language reasoning capabilities of a
large language model (LLM) to replicate this innate ability. To address the
lack of existing datasets of in-the-wild spatial sounds, we synthesized a
binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed
SpatialSoundQA, a spatial sound-based question-answering dataset, offering a
range of QA tasks that train BAT in various aspects of spatial sound perception
and reasoning. The acoustic front end encoder of BAT is a novel spatial audio
encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by
itself achieves strong performance across sound event detection, spatial
localization, and distance estimation. By integrating Spatial-AST with LLaMA-2
7B model, BAT transcends standard Sound Event Localization and Detection (SELD)
tasks, enabling the model to reason about the relationships between the sounds
in its environment. Our experiments demonstrate BAT's superior performance on
both spatial sound perception and reasoning, showcasing the immense potential
of LLMs in navigating and interpreting complex spatial audio environments.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01596" title="Abstract">arXiv:2402.01596</a> (cross-list from eess.IV) [<a href="/pdf/2402.01596" title="Download PDF">pdf</a>, <a href="/format/2402.01596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immersive Video Compression using Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kwan%2C+H+M">Ho Man Kwan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gower%2C+A">Andrew Gower</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent work on implicit neural representations (INRs) has evidenced their
potential for efficiently representing and encoding conventional video content.
In this paper we, for the first time, extend their application to immersive
(multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video
codec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video
codec, HiNeRV, which was developed for single-view video compression. We have
modified the model to learn a different group of feature grids for each view,
and share the learnt network parameters among all views. This enables the model
to effectively exploit the spatio-temporal and the inter-view redundancy that
exists within multi-view videos. The proposed codec was used to compress
multi-view texture and depth video sequences in the MPEG Immersive Video (MIV)
Common Test Conditions, and tested against the MIV Test model (TMIV) that uses
the VVenC video codec. The results demonstrate the superior performance of
MV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV. The
implementation of MV-HiNeRV will be published for further development and
evaluation.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01598" title="Abstract">arXiv:2402.01598</a> (cross-list from q-bio.PE) [<a href="/pdf/2402.01598" title="Download PDF">pdf</a>, <a href="/format/2402.01598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Two Decades of Blood Pressure Data: Demography-Specific  Patterns Across 75 Million Patient Encounters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Mousavi%2C+S+S">Seyedeh Somayyeh Mousavi</a>, 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+Y">Yuting Guo</a>, 
<a href="/search/q-bio?searchtype=author&query=Sarker%2C+A">Abeed Sarker</a>, 
<a href="/search/q-bio?searchtype=author&query=Sameni%2C+R">Reza Sameni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Applications (stat.AP)

</div>
<p class="mathjax">Hypertension remains a global health concern with a rising prevalence,
necessitating effective monitoring and understanding of blood pressure (BP)
dynamics. This study delves into the wealth of information derived from BP
measurement, a crucial approach in informing our understanding of hypertensive
trends. Numerous studies have reported on the relationship between BP variation
and various factors. In this research, we leveraged an extensive dataset
comprising 75 million records spanning two decades, offering a unique
opportunity to explore and analyze BP variations across demographic features
such as age, race, and gender. Our findings revealed that gender-based BP
variation was not statistically significant, challenging conventional
assumptions. Interestingly, systolic blood pressure (SBP) consistently
increased with age, while diastolic blood pressure (DBP) displayed a
distinctive peak in the forties age group. Moreover, our analysis uncovered
intriguing similarities in the distribution of BP among some of the racial
groups. This comprehensive investigation contributes to the ongoing discourse
on hypertension and underscores the importance of considering diverse
demographic factors in understanding BP variations. Our results provide
valuable insights that may inform personalized healthcare approaches tailored
to specific demographic profiles.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01605" title="Abstract">arXiv:2402.01605</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.01605" title="Download PDF">pdf</a>, <a href="/format/2402.01605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lyapunov theory demonstrating a fundamental limit on the speed of  systems consolidation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Alemi%2C+A">Alireza Alemi</a>, 
<a href="/search/q-bio?searchtype=author&query=Aksay%2C+E+R+F">Emre R. F. Aksay</a>, 
<a href="/search/q-bio?searchtype=author&query=Goldman%2C+M+S">Mark S. Goldman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Systems and Control (eess.SY); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">The nervous system reorganizes memories from an early site to a late site, a
commonly observed feature of learning and memory systems known as systems
consolidation. Previous work has suggested learning rules by which
consolidation may occur. Here, we provide conditions under which such rules are
guaranteed to lead to stable convergence of learning and consolidation. We use
the theory of Lyapunov functions, which enforces stability by requiring
learning rules to decrease an energy-like (Lyapunov) function. We present the
theory in the context of a simple circuit architecture motivated by classic
models of learning in systems consolidation mediated by the cerebellum.
Stability is only guaranteed if the learning rate in the late stage is not
faster than the learning rate in the early stage. Further, the slower the
learning rate at the late stage, the larger the perturbation the system can
tolerate with a guarantee of stability. We provide intuition for this result by
mapping the consolidation model to a damped driven oscillator system, and
showing that the ratio of early- to late-stage learning rates in the
consolidation model can be directly identified with the (square of the)
oscillator's damping ratio. This work suggests the power of the Lyapunov
approach to provide constraints on nervous system function.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01611" title="Abstract">arXiv:2402.01611</a> (cross-list from math.CT) [<a href="/pdf/2402.01611" title="Download PDF">pdf</a>, <a href="/ps/2402.01611" title="Download PostScript">ps</a>, <a href="/format/2402.01611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Duality for weak $&#x3c9;$-categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Benjamin%2C+T">Thibaut Benjamin</a>, 
<a href="/search/math?searchtype=author&query=Markakis%2C+I">Ioannis Markakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We define inductively the opposites of a weak globular $\omega$-category with
respect to a set of dimensions, and we show that the properties of being free
on a globular set of a computad are preserved under forming opposites. We then
provide a new description of hom $\omega$-categories, and show that the
opposites of a hom $\omega$-category are hom $\omega$-category of opposites of
the original $\omega$-category.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01635" title="Abstract">arXiv:2402.01635</a> (cross-list from stat.ME) [<a href="/pdf/2402.01635" title="Download PDF">pdf</a>, <a href="/format/2402.01635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> kNN Algorithm for Conditional Mean and Variance Estimation with  Automated Uncertainty Quantification and Variable Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Matabuena%2C+M">Marcos Matabuena</a>, 
<a href="/search/stat?searchtype=author&query=Vidal%2C+J+C">Juan C. Vidal</a>, 
<a href="/search/stat?searchtype=author&query=Padilla%2C+O+H+M">Oscar Hernan Madrid Padilla</a>, 
<a href="/search/stat?searchtype=author&query=Onnela%2C+J">Jukka-Pekka Onnela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we introduce a kNN-based regression method that synergizes the
scalability and adaptability of traditional non-parametric kNN models with a
novel variable selection technique. This method focuses on accurately
estimating the conditional mean and variance of random response variables,
thereby effectively characterizing conditional distributions across diverse
scenarios.Our approach incorporates a robust uncertainty quantification
mechanism, leveraging our prior estimation work on conditional mean and
variance. The employment of kNN ensures scalable computational efficiency in
predicting intervals and statistical accuracy in line with optimal
non-parametric rates. Additionally, we introduce a new kNN semi-parametric
algorithm for estimating ROC curves, accounting for covariates. For selecting
the smoothing parameter k, we propose an algorithm with theoretical
guarantees.Incorporation of variable selection enhances the performance of the
method significantly over conventional kNN techniques in various modeling
tasks. We validate the approach through simulations in low, moderate, and
high-dimensional covariate spaces. The algorithm's effectiveness is
particularly notable in biomedical applications as demonstrated in two case
studies. Concluding with a theoretical analysis, we highlight the consistency
and convergence rate of our method over traditional kNN models, particularly
when the underlying regression model takes values in a low-dimensional space.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Mon,  5 Feb 24</h3>
<dl>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1902.06307" title="Abstract">arXiv:1902.06307</a> (replaced) [<a href="/pdf/1902.06307" title="Download PDF">pdf</a>, <a href="/format/1902.06307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Braces of Perfect Matching Width 2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Giannopoulou%2C+A+C">Archontia C. Giannopoulou</a>, 
<a href="/search/math?searchtype=author&query=Hatzel%2C+M">Meike Hatzel</a>, 
<a href="/search/math?searchtype=author&query=Wiederrecht%2C+S">Sebastian Wiederrecht</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03158" title="Abstract">arXiv:2104.03158</a> (replaced) [<a href="/pdf/2104.03158" title="Download PDF">pdf</a>, <a href="/format/2104.03158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Imputation Rules for Prediction with Missing Data: Contrasting  Theoretical Guarantees with Empirical Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bertsimas%2C+D">Dimitris Bertsimas</a>, 
<a href="/search/stat?searchtype=author&query=Delarue%2C+A">Arthur Delarue</a>, 
<a href="/search/stat?searchtype=author&query=Pauphilet%2C+J">Jean Pauphilet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04475" title="Abstract">arXiv:2106.04475</a> (replaced) [<a href="/pdf/2106.04475" title="Download PDF">pdf</a>, <a href="/ps/2106.04475" title="Download PostScript">ps</a>, <a href="/format/2106.04475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Globular weak $&#x3c9;$-categories as models of a type theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benjamin%2C+T">Thibaut Benjamin</a>, 
<a href="/search/cs?searchtype=author&query=Finster%2C+E">Eric Finster</a>, 
<a href="/search/cs?searchtype=author&query=Mimram%2C+S">Samuel Mimram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.06545" title="Abstract">arXiv:2108.06545</a> (replaced) [<a href="/pdf/2108.06545" title="Download PDF">pdf</a>, <a href="/format/2108.06545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PICCOLO: Point Cloud-Centric Omnidirectional Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Changwoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hojun Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+M">Young Min Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03155" title="Abstract">arXiv:2110.03155</a> (replaced) [<a href="/pdf/2110.03155" title="Download PDF">pdf</a>, <a href="/format/2110.03155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Benefits of Being Categorical Distributional: Uncertainty-aware  Regularized Exploration in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yingnan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+E">Enze Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yafei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiaodong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Linglong Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14003" title="Abstract">arXiv:2110.14003</a> (replaced) [<a href="/pdf/2110.14003" title="Download PDF">pdf</a>, <a href="/format/2110.14003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connected greedy colourings of perfect graphs and other classes: the  good, the bad and the ugly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beaudou%2C+L">Laurent Beaudou</a>, 
<a href="/search/cs?searchtype=author&query=Brosse%2C+C">Caroline Brosse</a>, 
<a href="/search/cs?searchtype=author&query=Defrain%2C+O">Oscar Defrain</a>, 
<a href="/search/cs?searchtype=author&query=Foucaud%2C+F">Florent Foucaud</a>, 
<a href="/search/cs?searchtype=author&query=Lagoutte%2C+A">Aur&#xe9;lie Lagoutte</a>, 
<a href="/search/cs?searchtype=author&query=Limouzy%2C+V">Vincent Limouzy</a>, 
<a href="/search/cs?searchtype=author&query=Pastor%2C+L">Lucas Pastor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.00769" title="Abstract">arXiv:2202.00769</a> (replaced) [<a href="/pdf/2202.00769" title="Download PDF">pdf</a>, <a href="/format/2202.00769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Reinforcement Learning by Sinkhorn Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yingnan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wulong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Linglong Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2110.03155">arXiv:2110.03155</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07365" title="Abstract">arXiv:2202.07365</a> (replaced) [<a href="/pdf/2202.07365" title="Download PDF">pdf</a>, <a href="/format/2202.07365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Statistical Learning View of Simple Kriging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Siviero%2C+E">Emilia Siviero</a>, 
<a href="/search/stat?searchtype=author&query=Chautru%2C+E">Emilie Chautru</a>, 
<a href="/search/stat?searchtype=author&query=Cl%C3%A9men%C3%A7on%2C+S">Stephan Cl&#xe9;men&#xe7;on</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06343" title="Abstract">arXiv:2204.06343</a> (replaced) [<a href="/pdf/2204.06343" title="Download PDF">pdf</a>, <a href="/format/2204.06343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-grasp deformable object discrimination: the effect of gripper  morphology, sensing modalities, and action parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pliska%2C+M">Michal Pliska</a>, 
<a href="/search/cs?searchtype=author&query=Patni%2C+S">Shubhan Patni</a>, 
<a href="/search/cs?searchtype=author&query=Mares%2C+M">Michal Mares</a>, 
<a href="/search/cs?searchtype=author&query=Stoudek%2C+P">Pavel Stoudek</a>, 
<a href="/search/cs?searchtype=author&query=Straka%2C+Z">Zdenek Straka</a>, 
<a href="/search/cs?searchtype=author&query=Stepanova%2C+K">Karla Stepanova</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+M">Matej Hoffmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10205" title="Abstract">arXiv:2205.10205</a> (replaced) [<a href="/pdf/2205.10205" title="Download PDF">pdf</a>, <a href="/format/2205.10205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of binary time-frequency masks from ambient noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+J+L">Jos&#xe9; Luis Romero</a>, 
<a href="/search/cs?searchtype=author&query=Speckbacher%2C+M">Michael Speckbacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Functional Analysis (math.FA); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02997" title="Abstract">arXiv:2206.02997</a> (replaced) [<a href="/pdf/2206.02997" title="Download PDF">pdf</a>, <a href="/ps/2206.02997" title="Download PostScript">ps</a>, <a href="/format/2206.02997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TadML: A fast temporal action detection with Mechanics-MLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+B">Bowen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongchang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.06854" title="Abstract">arXiv:2206.06854</a> (replaced) [<a href="/pdf/2206.06854" title="Download PDF">pdf</a>, <a href="/format/2206.06854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the explainable properties of 1-Lipschitz Neural Networks: An Optimal  Transport Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serrurier%2C+M">Mathieu Serrurier</a> (IRIT-ADRIA, UT), 
<a href="/search/cs?searchtype=author&query=Mamalet%2C+F">Franck Mamalet</a> (UT), 
<a href="/search/cs?searchtype=author&query=Fel%2C+T">Thomas Fel</a> (UT), 
<a href="/search/cs?searchtype=author&query=B%C3%A9thune%2C+L">Louis B&#xe9;thune</a> (UT3, UT, IRIT-ADRIA), 
<a href="/search/cs?searchtype=author&query=Boissin%2C+T">Thibaut Boissin</a> (UT)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Conference on Neural Information Processing Systems (NeurIPS),
  Neural Information Processing Systems Foundation, Dec 2023, New Orleans
  (Louisiana), United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.11004" title="Abstract">arXiv:2206.11004</a> (replaced) [<a href="/pdf/2206.11004" title="Download PDF">pdf</a>, <a href="/format/2206.11004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-Encoding Adversarial Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaifeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.14051" title="Abstract">arXiv:2206.14051</a> (replaced) [<a href="/pdf/2206.14051" title="Download PDF">pdf</a>, <a href="/format/2206.14051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Business Process Simulation Models with Extraneous Activity  Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chapela-Campa%2C+D">David Chapela-Campa</a>, 
<a href="/search/cs?searchtype=author&query=Dumas%2C+M">Marlon Dumas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of the ICPM 2022 publication (see v1)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Systems (2024), 102346
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05317" title="Abstract">arXiv:2207.05317</a> (replaced) [<a href="/pdf/2207.05317" title="Download PDF">pdf</a>, <a href="/format/2207.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPO: Change Robust Panorama to Point Cloud Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hojun Jang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Changwoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+M">Young Min Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ECCV 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12259" title="Abstract">arXiv:2208.12259</a> (replaced) [<a href="/pdf/2208.12259" title="Download PDF">pdf</a>, <a href="/format/2208.12259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+G">Guocheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Hamdi%2C+A">Abdullah Hamdi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingdi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> camera-ready version at 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02767" title="Abstract">arXiv:2209.02767</a> (replaced) [<a href="/pdf/2209.02767" title="Download PDF">pdf</a>, <a href="/format/2209.02767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separators in Continuous Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blondin%2C+M">Michael Blondin</a>, 
<a href="/search/cs?searchtype=author&query=Esparza%2C+J">Javier Esparza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to LMCS as an extension of the FoSSaCS'22 conference version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03601" title="Abstract">arXiv:2209.03601</a> (replaced) [<a href="/pdf/2209.03601" title="Download PDF">pdf</a>, <a href="/ps/2209.03601" title="Download PostScript">ps</a>, <a href="/format/2209.03601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavenumber-explicit stability and convergence analysis of hp finite  element discretizations of Helmholtz problems in piecewise smooth media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bernkopf%2C+M">M. Bernkopf</a>, 
<a href="/search/math?searchtype=author&query=Chaumont-Frelet%2C+T">T. Chaumont-Frelet</a>, 
<a href="/search/math?searchtype=author&query=Melenk%2C+J+M">J.M. Melenk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04769" title="Abstract">arXiv:2209.04769</a> (replaced) [<a href="/e-print/2209.04769" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation of Algebraic Riccati Equations with Generators of  Noncompact Semigroups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheung%2C+J">James Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Replaced with more recent article titled "On the Approximation of Operator-Valued Riccati Equations in Hilbert Spaces"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.06615" title="Abstract">arXiv:2209.06615</a> (replaced) [<a href="/pdf/2209.06615" title="Download PDF">pdf</a>, <a href="/format/2209.06615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperPUT: Generating Synthetic Faulty Programs to Challenge Bug-Finding  Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felici%2C+R">Riccardo Felici</a>, 
<a href="/search/cs?searchtype=author&query=Pozzi%2C+L">Laura Pozzi</a>, 
<a href="/search/cs?searchtype=author&query=Furia%2C+C+A">Carlo A. Furia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Empirical Software Engineering, and is available online at: <a href="https://doi.org/10.1007/s10664-023-10430-8">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07517" title="Abstract">arXiv:2209.07517</a> (replaced) [<a href="/pdf/2209.07517" title="Download PDF">pdf</a>, <a href="/format/2209.07517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Total-Variation Processing of Shapes: Theory and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brokman%2C+J">Jonathan Brokman</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+M">Martin Burger</a>, 
<a href="/search/cs?searchtype=author&query=Gilboa%2C+G">Guy Gilboa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11812" title="Abstract">arXiv:2209.11812</a> (replaced) [<a href="/pdf/2209.11812" title="Download PDF">pdf</a>, <a href="/format/2209.11812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explanations, Fairness, and Appropriate Reliance in Human-AI  Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=De-Arteaga%2C+M">Maria De-Arteaga</a>, 
<a href="/search/cs?searchtype=author&query=Kuehl%2C+N">Niklas Kuehl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM CHI Conference on Human Factors in Computing Systems (CHI '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01672" title="Abstract">arXiv:2210.01672</a> (replaced) [<a href="/pdf/2210.01672" title="Download PDF">pdf</a>, <a href="/format/2210.01672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic  manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaquier%2C+N">No&#xe9;mie Jaquier</a>, 
<a href="/search/cs?searchtype=author&query=Rozo%2C+L">Leonel Rozo</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Duque%2C+M">Miguel Gonz&#xe1;lez-Duque</a>, 
<a href="/search/cs?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/cs?searchtype=author&query=Asfour%2C+T">Tamim Asfour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08247" title="Abstract">arXiv:2210.08247</a> (replaced) [<a href="/pdf/2210.08247" title="Download PDF">pdf</a>, <a href="/format/2210.08247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A sparse spectral method for fractional differential equations in  one-spatial dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Papadopoulos%2C+I+P+A">Ioannis P. A. Papadopoulos</a>, 
<a href="/search/math?searchtype=author&query=Olver%2C+S">Sheehan Olver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09404" title="Abstract">arXiv:2210.09404</a> (replaced) [<a href="/pdf/2210.09404" title="Download PDF">pdf</a>, <a href="/format/2210.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measures of Information Reflect Memorization Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+R">Rachit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Pruthi%2C+D">Danish Pruthi</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages; NeurIPS 2022. Code and data at <a href="https://rachitbansal.github.io/information-measures">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11049" title="Abstract">arXiv:2210.11049</a> (replaced) [<a href="/pdf/2210.11049" title="Download PDF">pdf</a>, <a href="/format/2210.11049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Does a Deep Learning Model Architecture Impact Its Privacy? A  Comprehensive Study of Privacy Attacks on CNNs and Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guangsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Huan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wanlei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in USENIX Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13954" title="Abstract">arXiv:2210.13954</a> (replaced) [<a href="/pdf/2210.13954" title="Download PDF">pdf</a>, <a href="/format/2210.13954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Prefer not to Say: Protecting User Consent in Models with Optional  Personal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leemann%2C+T">Tobias Leemann</a>, 
<a href="/search/cs?searchtype=author&query=Pawelczyk%2C+M">Martin Pawelczyk</a>, 
<a href="/search/cs?searchtype=author&query=Eberle%2C+C+T">Christian Thomas Eberle</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+G">Gjergji Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v5: AAAI-24 Camera-Ready Version Including Appendices. v1: NeurIPS 2022 Workshop on Algorithmic Fairness through the Lens of Causality and Privacy (AFCP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04411" title="Abstract">arXiv:2211.04411</a> (replaced) [<a href="/pdf/2211.04411" title="Download PDF">pdf</a>, <a href="/format/2211.04411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motif-guided Time Series Counterfactual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Boubrahimi%2C+S+F">Soukaina Filali Boubrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Hamdi%2C+S+M">Shah Muhammad Hamdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, accepted at 2-nd Workshop on Explainable and Ethical AI - ICPR 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00133" title="Abstract">arXiv:2212.00133</a> (replaced) [<a href="/pdf/2212.00133" title="Download PDF">pdf</a>, <a href="/format/2212.00133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Learning of Sinkhorn Algorithm Initializations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geuter%2C+J">Jonathan Geuter</a>, 
<a href="/search/cs?searchtype=author&query=Laschos%2C+V">Vaios Laschos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07286" title="Abstract">arXiv:2212.07286</a> (replaced) [<a href="/pdf/2212.07286" title="Download PDF">pdf</a>, <a href="/format/2212.07286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for improving the accessibility of research papers on  arXiv.org
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brinn%2C+S">Shamsi Brinn</a> (1), 
<a href="/search/cs?searchtype=author&query=Cameron%2C+C">Christopher Cameron</a> (1), 
<a href="/search/cs?searchtype=author&query=Fielding%2C+D">David Fielding</a> (1), 
<a href="/search/cs?searchtype=author&query=Frankston%2C+C">Charles Frankston</a> (1), 
<a href="/search/cs?searchtype=author&query=Fromme%2C+A">Alison Fromme</a> (1), 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Peter Huang</a> (1), 
<a href="/search/cs?searchtype=author&query=Nazzaro%2C+M">Mark Nazzaro</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Orphan%2C+S">Stephanie Orphan</a> (1), 
<a href="/search/cs?searchtype=author&query=Sigurdsson%2C+S">Steinn Sigurdsson</a> (1,3), 
<a href="/search/cs?searchtype=author&query=Tay%2C+R">Ryan Tay</a> (1), 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Miranda Yang</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qianyu Zhou</a> (1) ((1) arXiv, Cornell University, (2) University of Maryland, (3) The Pennsylvania State University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10249" title="Abstract">arXiv:2212.10249</a> (replaced) [<a href="/pdf/2212.10249" title="Download PDF">pdf</a>, <a href="/format/2212.10249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning efficient backprojections across cortical hierarchies in real  time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Max%2C+K">Kevin Max</a>, 
<a href="/search/q-bio?searchtype=author&query=Kriener%2C+L">Laura Kriener</a>, 
<a href="/search/q-bio?searchtype=author&query=Garc%C3%ADa%2C+G+P">Garibaldi Pineda Garc&#xed;a</a>, 
<a href="/search/q-bio?searchtype=author&query=Nowotny%2C+T">Thomas Nowotny</a>, 
<a href="/search/q-bio?searchtype=author&query=Jaras%2C+I">Ismael Jaras</a>, 
<a href="/search/q-bio?searchtype=author&query=Senn%2C+W">Walter Senn</a>, 
<a href="/search/q-bio?searchtype=author&query=Petrovici%2C+M+A">Mihai A. Petrovici</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated with streamlined main part, CIFAR-10 simulations, including DFA and minor fixes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10777" title="Abstract">arXiv:2212.10777</a> (replaced) [<a href="/pdf/2212.10777" title="Download PDF">pdf</a>, <a href="/format/2212.10777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchically branched diffusion models leverage dataset structure for  class-conditional generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseng%2C+A+M">Alex M. Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Max Shen</a>, 
<a href="/search/cs?searchtype=author&query=Biancalani%2C+T">Tommaso Biancalani</a>, 
<a href="/search/cs?searchtype=author&query=Scalia%2C+G">Gabriele Scalia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10923" title="Abstract">arXiv:2212.10923</a> (replaced) [<a href="/pdf/2212.10923" title="Download PDF">pdf</a>, <a href="/format/2212.10923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models as Inductive Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13977" title="Abstract">arXiv:2212.13977</a> (replaced) [<a href="/e-print/2212.13977" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and energy-efficient derivatives risk analysis: Streaming option  Greeks on Xilinx and Intel FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klaisoongnoen%2C+M">Mark Klaisoongnoen</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nick Brown</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+O">Oliver Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work uses a benchmark of STAC, whilst this was approved at the time they have asked we remove the paper as it needs to be made more explicit that these are unofficial ports and are entirely independent from any vendor and don't follow STAC rules. As we are comparing vendor hardware in the paper, it was felt that this could easily be mistaken to be representing something that the paper is not
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Mathematical Software (cs.MS)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04923" title="Abstract">arXiv:2301.04923</a> (replaced) [<a href="/pdf/2301.04923" title="Download PDF">pdf</a>, <a href="/format/2301.04923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Lagrangian Finite-Element Exterior Calculus for Incompressible  Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tonnon%2C+W">Wouter Tonnon</a>, 
<a href="/search/math?searchtype=author&query=Hiptmair%2C+R">Ralf Hiptmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Advances in Computational Mathematics (ACM), Topical Collection on Systematic Exploitation of Structural Properties in Electromagnetism
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11984" title="Abstract">arXiv:2301.11984</a> (replaced) [<a href="/pdf/2301.11984" title="Download PDF">pdf</a>, <a href="/format/2301.11984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Control of Exploration and Exploitation for Auto-Optimisation  Control in Uncertain Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhongguo Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wen-Hua Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+Y">Yunda Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13734" title="Abstract">arXiv:2301.13734</a> (replaced) [<a href="/pdf/2301.13734" title="Download PDF">pdf</a>, <a href="/format/2301.13734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Monte Carlo Evaluation with Offline Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shangtong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04202" title="Abstract">arXiv:2302.04202</a> (replaced) [<a href="/pdf/2302.04202" title="Download PDF">pdf</a>, <a href="/format/2302.04202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element approximation for uniformly elliptic linear PDE of second  order in nondivergence form
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tran%2C+N+T">Ngoc Tien Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08062" title="Abstract">arXiv:2302.08062</a> (replaced) [<a href="/pdf/2302.08062" title="Download PDF">pdf</a>, <a href="/ps/2302.08062" title="Download PostScript">ps</a>, <a href="/format/2302.08062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fossil Image Identification using Deep Learning Ensembles of Data  Augmented Multiviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+C">Chengbin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hanhui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Junxuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yukun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Hairong Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in Methods in Ecology and Evolution
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Methods in Ecology and Evolution, 14, 3020-3034 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12565" title="Abstract">arXiv:2302.12565</a> (replaced) [<a href="/pdf/2302.12565" title="Download PDF">pdf</a>, <a href="/format/2302.12565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Linearized Laplace Approximation for Bayesian Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ortega%2C+L+A">Luis A. Ortega</a>, 
<a href="/search/stat?searchtype=author&query=Santana%2C+S+R">Sim&#xf3;n Rodr&#xed;guez Santana</a>, 
<a href="/search/stat?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+D">Daniel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print, under revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05092" title="Abstract">arXiv:2303.05092</a> (replaced) [<a href="/pdf/2303.05092" title="Download PDF">pdf</a>, <a href="/format/2303.05092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Aware Dreamer for Task Generalization in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+C">Chengyang Ying</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhongkai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinning Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07988" title="Abstract">arXiv:2303.07988</a> (replaced) [<a href="/pdf/2303.07988" title="Download PDF">pdf</a>, <a href="/format/2303.07988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbalanced and Light Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gazdieva%2C+M">Milena Gazdieva</a>, 
<a href="/search/cs?searchtype=author&query=Asadulaev%2C+A">Arip Asadulaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08714" title="Abstract">arXiv:2303.08714</a> (replaced) [<a href="/pdf/2303.08714" title="Download PDF">pdf</a>, <a href="/format/2303.08714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+S">Shuyao Shang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Z">Zhengyang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangxing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">LunQian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">XingHua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zekai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinglin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11435" title="Abstract">arXiv:2303.11435</a> (replaced) [<a href="/pdf/2303.11435" title="Download PDF">pdf</a>, <a href="/format/2303.11435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inversion by Direct Iteration: An Alternative to Denoising Diffusion for  Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Delbracio%2C+M">Mauricio Delbracio</a>, 
<a href="/search/eess?searchtype=author&query=Milanfar%2C+P">Peyman Milanfar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (TMLR), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12050" title="Abstract">arXiv:2303.12050</a> (replaced) [<a href="/pdf/2303.12050" title="Download PDF">pdf</a>, <a href="/format/2303.12050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CurveCloudNet: Processing Point Clouds with 1D Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stearns%2C+C">Colton Stearns</a>, 
<a href="/search/cs?searchtype=author&query=Rempe%2C+D">Davis Rempe</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiateng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+A">Alex Fu</a>, 
<a href="/search/cs?searchtype=author&query=Mascha%2C+S">Sebastien Mascha</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+J">Jeong Joon Park</a>, 
<a href="/search/cs?searchtype=author&query=Paschalidou%2C+D">Despoina Paschalidou</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L+J">Leonidas J. Guibas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14702" title="Abstract">arXiv:2303.14702</a> (replaced) [<a href="/pdf/2303.14702" title="Download PDF">pdf</a>, <a href="/format/2303.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lot of Talk and a Badge: An Exploratory Analysis of Personal  Achievements in GitHub
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calefato%2C+F">Fabio Calefato</a>, 
<a href="/search/cs?searchtype=author&query=Quaranta%2C+L">Luigi Quaranta</a>, 
<a href="/search/cs?searchtype=author&query=Lanubile%2C+F">Filippo Lanubile</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16150" title="Abstract">arXiv:2303.16150</a> (replaced) [<a href="/pdf/2303.16150" title="Download PDF">pdf</a>, <a href="/ps/2303.16150" title="Download PostScript">ps</a>, <a href="/format/2303.16150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal video and IMU kinematic dataset on daily life activities  using affordable devices (VIDIMU)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Zarzuela%2C+M">Mario Mart&#xed;nez-Zarzuela</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Alonso%2C+J">Javier Gonz&#xe1;lez-Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Ant%C3%B3n-Rodr%C3%ADguez%2C+M">M&#xed;riam Ant&#xf3;n-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-Pernas%2C+F+J">Francisco J. D&#xed;az-Pernas</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+H">Henning M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B3n-Mart%C3%ADnez%2C+C">Cristina Sim&#xf3;n-Mart&#xed;nez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sci Data 10, 648 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16515" title="Abstract">arXiv:2303.16515</a> (replaced) [<a href="/pdf/2303.16515" title="Download PDF">pdf</a>, <a href="/format/2303.16515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Hypergraph Supports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Raman%2C+R">Rajiv Raman</a>, 
<a href="/search/math?searchtype=author&query=Singh%2C+K">Karamjeet Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17057" title="Abstract">arXiv:2303.17057</a> (replaced) [<a href="/pdf/2303.17057" title="Download PDF">pdf</a>, <a href="/format/2303.17057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avian-Inspired Claws Enable Robot Perching or Walking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Askari%2C+M">Mohammad Askari</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W+D">Won Dong Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lenherr%2C+D">Damian Lenherr</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+W">William Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Floreano%2C+D">Dario Floreano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18240" title="Abstract">arXiv:2303.18240</a> (replaced) [<a href="/pdf/2303.18240" title="Download PDF">pdf</a>, <a href="/format/2303.18240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where are we in the search for an Artificial Visual Cortex for Embodied  Intelligence?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+A">Arjun Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+K">Karmesh Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Arnaud%2C+S">Sergio Arnaud</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y+J">Yecheng Jason Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Claire Chen</a>, 
<a href="/search/cs?searchtype=author&query=Silwal%2C+S">Sneha Silwal</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Aryan Jain</a>, 
<a href="/search/cs?searchtype=author&query=Berges%2C+V">Vincent-Pierre Berges</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+D">Dhruv Batra</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yixin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Maksymets%2C+O">Oleksandr Maksymets</a>, 
<a href="/search/cs?searchtype=author&query=Rajeswaran%2C+A">Aravind Rajeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Meier%2C+F">Franziska Meier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://eai-vc.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01580" title="Abstract">arXiv:2304.01580</a> (replaced) [<a href="/pdf/2304.01580" title="Download PDF">pdf</a>, <a href="/ps/2304.01580" title="Download PostScript">ps</a>, <a href="/format/2304.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untargeted Near-collision Attacks on Biometrics: Real-world Bounds and  Theoretical Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durbet%2C+A">Axel Durbet</a>, 
<a href="/search/cs?searchtype=author&query=Grollemund%2C+P">Paul-Marie Grollemund</a>, 
<a href="/search/cs?searchtype=author&query=Thiry-Atighehchi%2C+K">Kevin Thiry-Atighehchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Addition of results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03674" title="Abstract">arXiv:2304.03674</a> (replaced) [<a href="/pdf/2304.03674" title="Download PDF">pdf</a>, <a href="/format/2304.03674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning with Requirements: a Manifesto
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giunchiglia%2C+E">Eleonora Giunchiglia</a>, 
<a href="/search/cs?searchtype=author&query=Imrie%2C+F">Fergus Imrie</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05166" title="Abstract">arXiv:2304.05166</a> (replaced) [<a href="/pdf/2304.05166" title="Download PDF">pdf</a>, <a href="/format/2304.05166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Distribution over Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%A9sz%C3%A1ros%2C+A">Anna M&#xe9;sz&#xe1;ros</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+J+F">Julian F. Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>, 
<a href="/search/cs?searchtype=author&query=Zgonnikov%2C+A">Arkady Zgonnikov</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07193" title="Abstract">arXiv:2304.07193</a> (replaced) [<a href="/pdf/2304.07193" title="Download PDF">pdf</a>, <a href="/format/2304.07193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DINOv2: Learning Robust Visual Features without Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oquab%2C+M">Maxime Oquab</a>, 
<a href="/search/cs?searchtype=author&query=Darcet%2C+T">Timoth&#xe9;e Darcet</a>, 
<a href="/search/cs?searchtype=author&query=Moutakanni%2C+T">Th&#xe9;o Moutakanni</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+H">Huy Vo</a>, 
<a href="/search/cs?searchtype=author&query=Szafraniec%2C+M">Marc Szafraniec</a>, 
<a href="/search/cs?searchtype=author&query=Khalidov%2C+V">Vasil Khalidov</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+P">Pierre Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Haziza%2C+D">Daniel Haziza</a>, 
<a href="/search/cs?searchtype=author&query=Massa%2C+F">Francisco Massa</a>, 
<a href="/search/cs?searchtype=author&query=El-Nouby%2C+A">Alaaeldin El-Nouby</a>, 
<a href="/search/cs?searchtype=author&query=Assran%2C+M">Mahmoud Assran</a>, 
<a href="/search/cs?searchtype=author&query=Ballas%2C+N">Nicolas Ballas</a>, 
<a href="/search/cs?searchtype=author&query=Galuba%2C+W">Wojciech Galuba</a>, 
<a href="/search/cs?searchtype=author&query=Howes%2C+R">Russell Howes</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Po-Yao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+I">Ishan Misra</a>, 
<a href="/search/cs?searchtype=author&query=Rabbat%2C+M">Michael Rabbat</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+V">Vasu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Synnaeve%2C+G">Gabriel Synnaeve</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jegou%2C+H">Herv&#xe9; Jegou</a>, 
<a href="/search/cs?searchtype=author&query=Mairal%2C+J">Julien Mairal</a>, 
<a href="/search/cs?searchtype=author&query=Labatut%2C+P">Patrick Labatut</a>, 
<a href="/search/cs?searchtype=author&query=Joulin%2C+A">Armand Joulin</a>, 
<a href="/search/cs?searchtype=author&query=Bojanowski%2C+P">Piotr Bojanowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12241" title="Abstract">arXiv:2304.12241</a> (replaced) [<a href="/pdf/2304.12241" title="Download PDF">pdf</a>, <a href="/format/2304.12241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positive AI: Key Challenges in Designing Artificial Intelligence for  Wellbeing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Maden%2C+W">Willem van der Maden</a>, 
<a href="/search/cs?searchtype=author&query=Lomas%2C+D">Derek Lomas</a>, 
<a href="/search/cs?searchtype=author&query=Sadek%2C+M">Malak Sadek</a>, 
<a href="/search/cs?searchtype=author&query=Hekkert%2C+P">Paul Hekkert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00309" title="Abstract">arXiv:2305.00309</a> (replaced) [<a href="/pdf/2305.00309" title="Download PDF">pdf</a>, <a href="/ps/2305.00309" title="Download PostScript">ps</a>, <a href="/format/2305.00309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patent Mining by Extracting Functional Analysis Information Modelled As  Graph Structure: A Patent Knowledge-base Collaborative Building Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helal%2C+M+E">Manal E. Helal</a>, 
<a href="/search/cs?searchtype=author&query=Helal%2C+M+E">Mohammed E. Helal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures, to be submitted later for peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02330" title="Abstract">arXiv:2305.02330</a> (replaced) [<a href="/pdf/2305.02330" title="Download PDF">pdf</a>, <a href="/format/2305.02330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Goes Fishing: Rapid, High-Resolution Biological Hotspot Mapping in  Coral Reefs with Vision-Guided Autonomous Underwater Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Daniel Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Levi Cai</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+S">Stewart Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Girdhar%2C+Y">Yogesh Girdhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CV4Animals Workshop at CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03610" title="Abstract">arXiv:2305.03610</a> (replaced) [<a href="/pdf/2305.03610" title="Download PDF">pdf</a>, <a href="/format/2305.03610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Data Curation in Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+C">Chen Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+D">Desmond Elliott</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03731" title="Abstract">arXiv:2305.03731</a> (replaced) [<a href="/pdf/2305.03731" title="Download PDF">pdf</a>, <a href="/format/2305.03731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Working Memory Capacity of ChatGPT: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dongyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingmin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 38th AAAI Conference on Artificial Intelligence (AAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07303" title="Abstract">arXiv:2305.07303</a> (replaced) [<a href="/pdf/2305.07303" title="Download PDF">pdf</a>, <a href="/format/2305.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Relational Hyperbolic Word Embeddings from Natural Language  Definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+D+S">Danilo S. Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07722" title="Abstract">arXiv:2305.07722</a> (replaced) [<a href="/pdf/2305.07722" title="Download PDF">pdf</a>, <a href="/format/2305.07722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Search of Verifiability: Explanations Rarely Enable Complementary  Performance in AI-Advised Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fok%2C+R">Raymond Fok</a>, 
<a href="/search/cs?searchtype=author&query=Weld%2C+D+S">Daniel S. Weld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 1 table, working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12534" title="Abstract">arXiv:2305.12534</a> (replaced) [<a href="/pdf/2305.12534" title="Download PDF">pdf</a>, <a href="/format/2305.12534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BertRLFuzzer: A BERT and Reinforcement Learning Based Fuzzer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+P">Piyush Jha</a>, 
<a href="/search/cs?searchtype=author&query=Scott%2C+J">Joseph Scott</a>, 
<a href="/search/cs?searchtype=author&query=Ganeshna%2C+J+S">Jaya Sriram Ganeshna</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mudit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+V">Vijay Ganesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12641" title="Abstract">arXiv:2305.12641</a> (replaced) [<a href="/pdf/2305.12641" title="Download PDF">pdf</a>, <a href="/format/2305.12641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Sentence Representations: From the BERT Epoch  to the ChatGPT Era and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+A+R">Abhinav Ramesh Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thanh-Tung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Schlegel%2C+V">Viktor Schlegel</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+S">Stefan Winkler</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14279" title="Abstract">arXiv:2305.14279</a> (replaced) [<a href="/pdf/2305.14279" title="Download PDF">pdf</a>, <a href="/format/2305.14279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Phang%2C+J">Jason Phang</a>, 
<a href="/search/cs?searchtype=author&query=Parrish%2C+A">Alicia Parrish</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+V">Vishakh Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TMLR: <a href="https://openreview.net/forum?id=5nBqY1y96B">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15577" title="Abstract">arXiv:2305.15577</a> (replaced) [<a href="/pdf/2305.15577" title="Download PDF">pdf</a>, <a href="/format/2305.15577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing $f$-Divergences by Interpolating Velocity Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+S">Song Liu</a>, 
<a href="/search/stat?searchtype=author&query=Yu%2C+J">Jiahao Yu</a>, 
<a href="/search/stat?searchtype=author&query=Simons%2C+J">Jack Simons</a>, 
<a href="/search/stat?searchtype=author&query=Yi%2C+M">Mingxuan Yi</a>, 
<a href="/search/stat?searchtype=author&query=Beaumont%2C+M">Mark Beaumont</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15659" title="Abstract">arXiv:2305.15659</a> (replaced) [<a href="/pdf/2305.15659" title="Download PDF">pdf</a>, <a href="/format/2305.15659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to escape sharp minima with random perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+S">Suvrit Sra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments would be appreciated!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15927" title="Abstract">arXiv:2305.15927</a> (replaced) [<a href="/pdf/2305.15927" title="Download PDF">pdf</a>, <a href="/format/2305.15927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Directed Graphical Models with Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+V">Vy Vo</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+L">Long-Tung Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bonilla%2C+E">Edwin Bonilla</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17026" title="Abstract">arXiv:2305.17026</a> (replaced) [<a href="/pdf/2305.17026" title="Download PDF">pdf</a>, <a href="/format/2305.17026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Powerful are Decoder-Only Transformer Neural Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roberts%2C+J">Jesse Roberts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17410" title="Abstract">arXiv:2305.17410</a> (replaced) [<a href="/pdf/2305.17410" title="Download PDF">pdf</a>, <a href="/format/2305.17410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Game for Coupled Power System with Energy Sharing and  Transportation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yan%2C+D">Dongxiang Yan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+T">Tongxin Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Changhong Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yue Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18453" title="Abstract">arXiv:2305.18453</a> (replaced) [<a href="/pdf/2305.18453" title="Download PDF">pdf</a>, <a href="/format/2305.18453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dorjsembe%2C+Z">Zolnamar Dorjsembe</a>, 
<a href="/search/eess?searchtype=author&query=Pao%2C+H">Hsing-Kuo Pao</a>, 
<a href="/search/eess?searchtype=author&query=Odonchimed%2C+S">Sodtavilan Odonchimed</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+F">Furen Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19075" title="Abstract">arXiv:2305.19075</a> (replaced) [<a href="/pdf/2305.19075" title="Download PDF">pdf</a>, <a href="/format/2305.19075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Conditioned Imitation Learning with Base Skill Priors under  Unstructured Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiangtong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaojie Su</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenguang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19241" title="Abstract">arXiv:2305.19241</a> (replaced) [<a href="/pdf/2305.19241" title="Download PDF">pdf</a>, <a href="/format/2305.19241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accountable authentication with privacy protection: The Larch system for  universal login
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dauterman%2C+E">Emma Dauterman</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Danny Lin</a>, 
<a href="/search/cs?searchtype=author&query=Corrigan-Gibbs%2C+H">Henry Corrigan-Gibbs</a>, 
<a href="/search/cs?searchtype=author&query=Mazi%C3%A8res%2C+D">David Mazi&#xe8;res</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of a paper appearing at OSDI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01387" title="Abstract">arXiv:2306.01387</a> (replaced) [<a href="/pdf/2306.01387" title="Download PDF">pdf</a>, <a href="/format/2306.01387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Augmented Data-EnablEd Predictive Control for Eco-driving of  Mixed Traffic Considering Diverse Human Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+D">Dongjun Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+K">Kaixiang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+H">Haoxuan Dong</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qun Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhaojian Li</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Z">Ziyou Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05185" title="Abstract">arXiv:2306.05185</a> (replaced) [<a href="/pdf/2306.05185" title="Download PDF">pdf</a>, <a href="/ps/2306.05185" title="Download PostScript">ps</a>, <a href="/format/2306.05185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Identification and Optimization of Nonsmooth Superposition  Operators in Semilinear Elliptic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Christof%2C+C">Constantin Christof</a>, 
<a href="/search/math?searchtype=author&query=Kowalczyk%2C+J">Julia Kowalczyk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor revision; to appear in ESAIM COCV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05366" title="Abstract">arXiv:2306.05366</a> (replaced) [<a href="/pdf/2306.05366" title="Download PDF">pdf</a>, <a href="/format/2306.05366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ordinal Potential-based Player Rating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vadori%2C+N">Nelson Vadori</a>, 
<a href="/search/cs?searchtype=author&query=Savani%2C+R">Rahul Savani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05817" title="Abstract">arXiv:2306.05817</a> (replaced) [<a href="/pdf/2306.05817" title="Download PDF">pdf</a>, <a href="/format/2306.05817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Can Recommender Systems Benefit from Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yunjia Xi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Huifeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New version released with 27-page main content; Look-up table in appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06534" title="Abstract">arXiv:2306.06534</a> (replaced) [<a href="/pdf/2306.06534" title="Download PDF">pdf</a>, <a href="/format/2306.06534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-Tensors: Clustering Positive Semi-Definite Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Baoyi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tarpey%2C+T">Thaddeus Tarpey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08879" title="Abstract">arXiv:2306.08879</a> (replaced) [<a href="/pdf/2306.08879" title="Download PDF">pdf</a>, <a href="/format/2306.08879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferenczi%2C+B">Bryce Ferenczi</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M">Michael Burke</a>, 
<a href="/search/cs?searchtype=author&query=Drummond%2C+T">Tom Drummond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, Accepted for publication in IEEE RA-L 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09896" title="Abstract">arXiv:2306.09896</a> (replaced) [<a href="/pdf/2306.09896" title="Download PDF">pdf</a>, <a href="/format/2306.09896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Self-Repair a Silver Bullet for Code Generation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olausson%2C+T+X">Theo X. Olausson</a>, 
<a href="/search/cs?searchtype=author&query=Inala%2C+J+P">Jeevana Priya Inala</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenglong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Solar-Lezama%2C+A">Armando Solar-Lezama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Added additional Code Llama experiments and fixed a data processing error harming Code Llama's reported self-repair performance on HumanEval
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09912" title="Abstract">arXiv:2306.09912</a> (replaced) [<a href="/pdf/2306.09912" title="Download PDF">pdf</a>, <a href="/ps/2306.09912" title="Download PostScript">ps</a>, <a href="/format/2306.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Quantum Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rudai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minrui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huihui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z+Y">Zhao Yang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kwek%2C+L+C">Leong Chuan Kwek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Survey of quantum federated learning (QFL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11313" title="Abstract">arXiv:2306.11313</a> (replaced) [<a href="/pdf/2306.11313" title="Download PDF">pdf</a>, <a href="/format/2306.11313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep graph kernel point processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dong%2C+Z">Zheng Dong</a>, 
<a href="/search/stat?searchtype=author&query=Repasky%2C+M">Matthew Repasky</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01403" title="Abstract">arXiv:2307.01403</a> (replaced) [<a href="/pdf/2307.01403" title="Download PDF">pdf</a>, <a href="/format/2307.01403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Multi-Agent Communication with Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+Y+L">Yat Long Lo</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+B">Biswa Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Noukhovitch%2C+M">Michael Noukhovitch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 12th International Conference on Learning Representations (ICLR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02573" title="Abstract">arXiv:2307.02573</a> (replaced) [<a href="/pdf/2307.02573" title="Download PDF">pdf</a>, <a href="/format/2307.02573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a Programmable Quantum Annealer as a Random Number Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03813" title="Abstract">arXiv:2307.03813</a> (replaced) [<a href="/pdf/2307.03813" title="Download PDF">pdf</a>, <a href="/ps/2307.03813" title="Download PostScript">ps</a>, <a href="/format/2307.03813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Chaotic Maps using Next-Generation Reservoir Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kent%2C+R+M">Robert M. Kent</a>, 
<a href="/search/cs?searchtype=author&query=Barbosa%2C+W+A+S">Wendson A. S. Barbosa</a>, 
<a href="/search/cs?searchtype=author&query=Gauthier%2C+D+J">Daniel J. Gauthier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chaos 34, 023102 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Systems and Control (eess.SY); Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04870" title="Abstract">arXiv:2307.04870</a> (replaced) [<a href="/pdf/2307.04870" title="Download PDF">pdf</a>, <a href="/format/2307.04870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RACH-Space: Reconstructing Adaptive Convex Hull Space with Applications  in Weak Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Na%2C+W">Woojoo Na</a>, 
<a href="/search/cs?searchtype=author&query=Tasissa%2C+A">Abiy Tasissa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05842" title="Abstract">arXiv:2307.05842</a> (replaced) [<a href="/pdf/2307.05842" title="Download PDF">pdf</a>, <a href="/format/2307.05842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Butterfly Effect in Artificial Intelligence Systems: Implications  for AI Bias and Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Cite as: Machine Learning with Applications, Volume 15, 2024, 100525 10.1016/j.mlwa.2024.100525
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning with Applications, Volume 15, 2024, 100525
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06571" title="Abstract">arXiv:2307.06571</a> (replaced) [<a href="/pdf/2307.06571" title="Download PDF">pdf</a>, <a href="/format/2307.06571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpacking polarization: Antagonism and Alignment in Signed Networks of  Online Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraxanet%2C+E">Emma Fraxanet</a>, 
<a href="/search/cs?searchtype=author&query=Pellert%2C+M">Max Pellert</a>, 
<a href="/search/cs?searchtype=author&query=Schweighofer%2C+S">Simon Schweighofer</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+V">Vicen&#xe7; G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+D">David Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07184" title="Abstract">arXiv:2307.07184</a> (replaced) [<a href="/pdf/2307.07184" title="Download PDF">pdf</a>, <a href="/format/2307.07184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TVPR: Text-to-Video Person Retrieval and a New Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fan Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianhui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guan-Nan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Aichun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07564" title="Abstract">arXiv:2307.07564</a> (replaced) [<a href="/pdf/2307.07564" title="Download PDF">pdf</a>, <a href="/format/2307.07564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Euler-Maruyama approximations of the stochastic heat equation on the  sphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lang%2C+A">Annika Lang</a>, 
<a href="/search/math?searchtype=author&query=Motschan-Armen%2C+I">Ioanna Motschan-Armen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated convergence plot for the Monte Carlo forward Euler-Maruyama scheme compared to published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational Dynamics, 2024, 11(1): 23-42
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07788" title="Abstract">arXiv:2307.07788</a> (replaced) [<a href="/pdf/2307.07788" title="Download PDF">pdf</a>, <a href="/ps/2307.07788" title="Download PostScript">ps</a>, <a href="/format/2307.07788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciding One to One property of Boolean maps: Condition and algorithm in  terms of implicants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sule%2C+V">Virendra Sule</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> I have fixed errors in proof of theorem 2 that I noticed. A new section and a theorem on one to one-ness is added. Paper is replaced as version 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07854" title="Abstract">arXiv:2307.07854</a> (replaced) [<a href="/pdf/2307.07854" title="Download PDF">pdf</a>, <a href="/format/2307.07854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvFusion: Multilingual Adapter-based Knowledge Transfer for Code  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saberi%2C+I">Iman Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Fard%2C+F">Fatemeh Fard</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fuxiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13016" title="Abstract">arXiv:2307.13016</a> (replaced) [<a href="/pdf/2307.13016" title="Download PDF">pdf</a>, <a href="/format/2307.13016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relationship Between Several Variants of the Linear Hashing  Conjecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Westover%2C+A">Alek Westover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14247" title="Abstract">arXiv:2307.14247</a> (replaced) [<a href="/pdf/2307.14247" title="Download PDF">pdf</a>, <a href="/ps/2307.14247" title="Download PostScript">ps</a>, <a href="/format/2307.14247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CBGL: Fast Monte Carlo Passive Global Localisation of 2D LIDAR Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filotheou%2C+A">Alexandros Filotheou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, 3 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14898" title="Abstract">arXiv:2307.14898</a> (replaced) [<a href="/pdf/2307.14898" title="Download PDF">pdf</a>, <a href="/ps/2307.14898" title="Download PostScript">ps</a>, <a href="/format/2307.14898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback and Open-Loop Nash Equilibria for LQ Infinite-Horizon  Discrete-Time Dynamic Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Monti%2C+A">A. Monti</a>, 
<a href="/search/math?searchtype=author&query=Nortmann%2C+B">B. Nortmann</a>, 
<a href="/search/math?searchtype=author&query=Mylvaganam%2C+T">T. Mylvaganam</a>, 
<a href="/search/math?searchtype=author&query=Sassano%2C+M">M. Sassano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16304" title="Abstract">arXiv:2307.16304</a> (replaced) [<a href="/pdf/2307.16304" title="Download PDF">pdf</a>, <a href="/format/2307.16304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Shall Pass: Dealing with the Zero-Gradient Problem in Predict and  Optimize for Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veviurko%2C+G">Grigorii Veviurko</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hmer%2C+W">Wendelin B&#xf6;hmer</a>, 
<a href="/search/cs?searchtype=author&query=de+Weerdt%2C+M">Mathijs de Weerdt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16513" title="Abstract">arXiv:2307.16513</a> (replaced) [<a href="/pdf/2307.16513" title="Download PDF">pdf</a>, <a href="/ps/2307.16513" title="Download PostScript">ps</a>, <a href="/format/2307.16513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deception Abilities Emerged in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagendorff%2C+T">Thilo Hagendorff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00985" title="Abstract">arXiv:2308.00985</a> (replaced) [<a href="/pdf/2308.00985" title="Download PDF">pdf</a>, <a href="/format/2308.00985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluate and Guard the Wisdom of Crowds: Zero Knowledge Proofs for  Crowdsourcing Truth Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuanming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinpeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaohu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01286" title="Abstract">arXiv:2308.01286</a> (replaced) [<a href="/pdf/2308.01286" title="Download PDF">pdf</a>, <a href="/ps/2308.01286" title="Download PostScript">ps</a>, <a href="/format/2308.01286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enumeration Kernelizations of Polynomial Size for Cuts of Bounded Degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+D">Diptapriyo Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Ramanujan%2C+M+S">M. S. Ramanujan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There have been major revision in the technicalities and proofs of the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03233" title="Abstract">arXiv:2308.03233</a> (replaced) [<a href="/pdf/2308.03233" title="Download PDF">pdf</a>, <a href="/format/2308.03233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEAPS: Topological-Layout-Adaptable Multi-Die FPGA Placement for Super  Long Line Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+Z">Zhixiong Di</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Runzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+J">Jing Mai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yibo Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06341" title="Abstract">arXiv:2308.06341</a> (replaced) [<a href="/pdf/2308.06341" title="Download PDF">pdf</a>, <a href="/format/2308.06341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Model for Geological CO2 Storage and Its Use in Hierarchical  MCMC History Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yifu Han</a>, 
<a href="/search/cs?searchtype=author&query=Hamon%2C+F+P">Francois P. Hamon</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Su Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Durlofsky%2C+L+J">Louis J. Durlofsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08143" title="Abstract">arXiv:2308.08143</a> (replaced) [<a href="/pdf/2308.08143" title="Download PDF">pdf</a>, <a href="/format/2308.08143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual  Speech Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Runxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaolin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08480" title="Abstract">arXiv:2308.08480</a> (replaced) [<a href="/pdf/2308.08480" title="Download PDF">pdf</a>, <a href="/format/2308.08480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Propagation Techniques for Artifact Detection in Imbalanced  Classes using Photoplethysmogram Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macabiau%2C+C">Clara Macabiau</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thanh-Dung Le</a>, 
<a href="/search/cs?searchtype=author&query=Albert%2C+K">Kevin Albert</a>, 
<a href="/search/cs?searchtype=author&query=Jouvet%2C+P">Philippe Jouvet</a>, 
<a href="/search/cs?searchtype=author&query=Noumeir%2C+R">Rita Noumeir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under preparation to submit to IEEE for possible publications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10130" title="Abstract">arXiv:2308.10130</a> (replaced) [<a href="/pdf/2308.10130" title="Download PDF">pdf</a>, <a href="/format/2308.10130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Approximation of Operator-Valued Riccati Equations in Hilbert  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheung%2C+J">James Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revision 4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11842" title="Abstract">arXiv:2308.11842</a> (replaced) [<a href="/pdf/2308.11842" title="Download PDF">pdf</a>, <a href="/format/2308.11842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative  Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dingyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14005" title="Abstract">arXiv:2308.14005</a> (replaced) [<a href="/pdf/2308.14005" title="Download PDF">pdf</a>, <a href="/format/2308.14005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrating Panoramic Depth Estimation for Practical Localization and  Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+S">Eun Sun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+M">Young Min Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00300" title="Abstract">arXiv:2309.00300</a> (replaced) [<a href="/pdf/2309.00300" title="Download PDF">pdf</a>, <a href="/format/2309.00300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Identifiability and Explainability for Personalized Learner  Modeling: An Inductive Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiatong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiayu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+F">Fangzhou Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linbo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Proceedings of the ACM Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01606" title="Abstract">arXiv:2309.01606</a> (replaced) [<a href="/pdf/2309.01606" title="Download PDF">pdf</a>, <a href="/format/2309.01606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese  Geographic Re-Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ruixue Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengjun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, EACL 2024 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04836" title="Abstract">arXiv:2309.04836</a> (replaced) [<a href="/pdf/2309.04836" title="Download PDF">pdf</a>, <a href="/format/2309.04836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Semantic Surface Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morreale%2C+L">Luca Morreale</a>, 
<a href="/search/cs?searchtype=author&query=Aigerman%2C+N">Noam Aigerman</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+V+G">Vladimir G. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+N+J">Niloy J. Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05388" title="Abstract">arXiv:2309.05388</a> (replaced) [<a href="/pdf/2309.05388" title="Download PDF">pdf</a>, <a href="/format/2309.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Single Rotation Averaging Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">Seong Hun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Civera%2C+J">Javier Civera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added the url to the code (<a href="https://github.com/sunghoon031/SingleRotationAveraging_TLUD">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06030" title="Abstract">arXiv:2309.06030</a> (replaced) [<a href="/pdf/2309.06030" title="Download PDF">pdf</a>, <a href="/format/2309.06030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning for Large-Scale Scene Modeling with Neural Radiance  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Teppei Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06895" title="Abstract">arXiv:2309.06895</a> (replaced) [<a href="/pdf/2309.06895" title="Download PDF">pdf</a>, <a href="/format/2309.06895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MagiCapture: High-Resolution Multi-Concept Portrait Customization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hyung%2C+J">Junha Hyung</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jaeyo Shin</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07929" title="Abstract">arXiv:2309.07929</a> (replaced) [<a href="/pdf/2309.07929" title="Download PDF">pdf</a>, <a href="/format/2309.07929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Segmentation with Sound Is Generalizable Audio-Visual Source  Localizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaoting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weisong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guangyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Di Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10186" title="Abstract">arXiv:2309.10186</a> (replaced) [<a href="/pdf/2309.10186" title="Download PDF">pdf</a>, <a href="/format/2309.10186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-enabled Reinforcement Learning for Time Series Forecasting with  Adaptive Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+J">Jianming Yong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuefeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10283" title="Abstract">arXiv:2309.10283</a> (replaced) [<a href="/pdf/2309.10283" title="Download PDF">pdf</a>, <a href="/format/2309.10283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FRAMU: Attention-based Machine Unlearning using Federated Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Taotao Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10293" title="Abstract">arXiv:2309.10293</a> (replaced) [<a href="/pdf/2309.10293" title="Download PDF">pdf</a>, <a href="/format/2309.10293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QXAI: Explainable AI Framework for Quantitative Analysis in Patient  Monitoring Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Velasquez%2C+J+D">Juan D. Velasquez</a>, 
<a href="/search/cs?searchtype=author&query=Higgins%2C+N">Niall Higgins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the ELSEVIER for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15687" title="Abstract">arXiv:2309.15687</a> (replaced) [<a href="/pdf/2309.15687" title="Download PDF">pdf</a>, <a href="/format/2309.15687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking On-Chip Communication Anonymity using Flow Correlation Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weerasena%2C+H">Hansika Weerasena</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Prabhat Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00241" title="Abstract">arXiv:2310.00241</a> (replaced) [<a href="/pdf/2310.00241" title="Download PDF">pdf</a>, <a href="/format/2310.00241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Distance-$r$ Dominating Set Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+N">Niranka Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+A">Duc A. Hoang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 10 figures, v2: revise some incorrect proofs in v1, add new results for split graphs and planar graphs, add minor modifications. The HTML version of the abstract was shorten to fit the requirement of arXiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00808" title="Abstract">arXiv:2310.00808</a> (replaced) [<a href="/pdf/2310.00808" title="Download PDF">pdf</a>, <a href="/format/2310.00808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Completing Visual Objects via Bridging Generation and Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02063" title="Abstract">arXiv:2310.02063</a> (replaced) [<a href="/pdf/2310.02063" title="Download PDF">pdf</a>, <a href="/format/2310.02063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lessons Learned from EXMOS User Studies: A Technical Report Summarizing  Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Aditya Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Stumpf%2C+S">Simone Stumpf</a>, 
<a href="/search/cs?searchtype=author&query=Gosak%2C+L">Lucija Gosak</a>, 
<a href="/search/cs?searchtype=author&query=Stiglic%2C+G">Gregor Stiglic</a>, 
<a href="/search/cs?searchtype=author&query=Verbert%2C+K">Katrien Verbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It is a technical report only. The contents are not peer-reviewed. Please reach out to the main author for any questions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02258" title="Abstract">arXiv:2310.02258</a> (replaced) [<a href="/pdf/2310.02258" title="Download PDF">pdf</a>, <a href="/format/2310.02258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Scaling Law from Lottery Ticket Ensembling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures. Note from authors: the theory in this paper is questionable; we are trying our best to fix it. Empirical results still stand
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03337" title="Abstract">arXiv:2310.03337</a> (replaced) [<a href="/pdf/2310.03337" title="Download PDF">pdf</a>, <a href="/format/2310.03337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Step-aware Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yukang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Luozhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingcong Chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The International Conference on Learning Representations, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04584" title="Abstract">arXiv:2310.04584</a> (replaced) [<a href="/pdf/2310.04584" title="Download PDF">pdf</a>, <a href="/format/2310.04584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algorithm to Train Unrestricted Sequential Discrete Morphological  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marcondes%2C+D">Diego Marcondes</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Mariana Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Barrera%2C+J">Junior Barrera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04631" title="Abstract">arXiv:2310.04631</a> (replaced) [<a href="/pdf/2310.04631" title="Download PDF">pdf</a>, <a href="/format/2310.04631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust in Generative AI among students: An Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amoozadeh%2C+M">Matin Amoozadeh</a>, 
<a href="/search/cs?searchtype=author&query=Daniels%2C+D">David Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+D">Daye Nam</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aayush Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Stella Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hilton%2C+M">Michael Hilton</a>, 
<a href="/search/cs?searchtype=author&query=Ragavan%2C+S+S">Sruti Srinivasa Ragavan</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+M+A">Mohammad Amin Alipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SIGCSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05495" title="Abstract">arXiv:2310.05495</a> (replaced) [<a href="/pdf/2310.05495" title="Download PDF">pdf</a>, <a href="/format/2310.05495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of Federated Averaging under Partial Participation  for Over-parameterized Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=li%2C+W">Wei li</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">Dazhi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhisong Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06000" title="Abstract">arXiv:2310.06000</a> (replaced) [<a href="/pdf/2310.06000" title="Download PDF">pdf</a>, <a href="/ps/2310.06000" title="Download PostScript">ps</a>, <a href="/format/2310.06000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Replication-Robust Analytics Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Falconer%2C+T">Thomas Falconer</a>, 
<a href="/search/econ?searchtype=author&query=Kazempour%2C+J">Jalal Kazempour</a>, 
<a href="/search/econ?searchtype=author&query=Pinson%2C+P">Pierre Pinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06380" title="Abstract">arXiv:2310.06380</a> (replaced) [<a href="/pdf/2310.06380" title="Download PDF">pdf</a>, <a href="/format/2310.06380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAST: Cluster-Aware Self-Training for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minwook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+B">Ki Beom Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Giltae Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages for main body, and 16 additional pages for reference and appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06959" title="Abstract">arXiv:2310.06959</a> (replaced) [<a href="/pdf/2310.06959" title="Download PDF">pdf</a>, <a href="/ps/2310.06959" title="Download PostScript">ps</a>, <a href="/format/2310.06959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Proof Repair in Cubical Agda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viola%2C+C">Cosmo Viola</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Max Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ringer%2C+T">Talia Ringer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> for associated code, see <a href="https://github.com/InnovativeInventor/proof-repair-cubical">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08204" title="Abstract">arXiv:2310.08204</a> (replaced) [<a href="/pdf/2310.08204" title="Download PDF">pdf</a>, <a href="/format/2310.08204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STELLA: Continual Audio-Video Pre-training with Spatio-Temporal  Localized Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaewoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yunji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09611" title="Abstract">arXiv:2310.09611</a> (replaced) [<a href="/pdf/2310.09611" title="Download PDF">pdf</a>, <a href="/format/2310.09611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VizAbility: Enhancing Chart Accessibility with LLM-based Conversational  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorniak%2C+J">Joshua Gorniak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09935" title="Abstract">arXiv:2310.09935</a> (replaced) [<a href="/pdf/2310.09935" title="Download PDF">pdf</a>, <a href="/format/2310.09935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Passivity and Decentralized Stability Conditions for Grid-Forming  Converters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Power Systems, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10107" title="Abstract">arXiv:2310.10107</a> (replaced) [<a href="/pdf/2310.10107" title="Download PDF">pdf</a>, <a href="/ps/2310.10107" title="Download PostScript">ps</a>, <a href="/format/2310.10107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of the Posterior Sampling-based Learning Algorithm for  Episodic POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Dengwang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Nayyar%2C+A">Ashutosh Nayyar</a>, 
<a href="/search/cs?searchtype=author&query=Nuzzo%2C+P">Pierluigi Nuzzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11085" title="Abstract">arXiv:2310.11085</a> (replaced) [<a href="/pdf/2310.11085" title="Download PDF">pdf</a>, <a href="/format/2310.11085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozyurt%2C+Y">Yilmazcan Ozyurt</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11244" title="Abstract">arXiv:2310.11244</a> (replaced) [<a href="/pdf/2310.11244" title="Download PDF">pdf</a>, <a href="/format/2310.11244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entity Matching using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peeters%2C+R">Ralph Peeters</a>, 
<a href="/search/cs?searchtype=author&query=Bizer%2C+C">Christian Bizer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11531" title="Abstract">arXiv:2310.11531</a> (replaced) [<a href="/pdf/2310.11531" title="Download PDF">pdf</a>, <a href="/ps/2310.11531" title="Download PostScript">ps</a>, <a href="/format/2310.11531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Online Learning with Offline Datasets for Infinite Horizon  MDPs: A Bayesian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Dengwang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+B">Botao Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zheng Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11714" title="Abstract">arXiv:2310.11714</a> (replaced) [<a href="/pdf/2310.11714" title="Download PDF">pdf</a>, <a href="/format/2310.11714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evaluation of Generative Models in Distributed Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhenghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13164" title="Abstract">arXiv:2310.13164</a> (replaced) [<a href="/pdf/2310.13164" title="Download PDF">pdf</a>, <a href="/format/2310.13164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Equivariance via Lie Algebra Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McNeela%2C+D">Daniel McNeela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13836" title="Abstract">arXiv:2310.13836</a> (replaced) [<a href="/pdf/2310.13836" title="Download PDF">pdf</a>, <a href="/format/2310.13836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model&#x27;s Embedded Representations May Detect Distribution  Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vargas%2C+M">Max Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Tsou%2C+A">Adam Tsou</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13886" title="Abstract">arXiv:2310.13886</a> (replaced) [<a href="/pdf/2310.13886" title="Download PDF">pdf</a>, <a href="/format/2310.13886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Filtering with Brenier Optimal Transport Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Jarrah%2C+M">Mohammad Al-Jarrah</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+N">Niyizhen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+B">Bamdad Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Taghvaei%2C+A">Amirhossein Taghvaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 16 figures, 1 Table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14073" title="Abstract">arXiv:2310.14073</a> (replaced) [<a href="/pdf/2310.14073" title="Download PDF">pdf</a>, <a href="/format/2310.14073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Asymptotic Estimation of Unknown Parameters of Perturbed LRE with  Application to State Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Glushchenko%2C+A">Anton Glushchenko</a>, 
<a href="/search/eess?searchtype=author&query=Lastochkin%2C+K">Konstantin Lastochkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14168" title="Abstract">arXiv:2310.14168</a> (replaced) [<a href="/pdf/2310.14168" title="Download PDF">pdf</a>, <a href="/format/2310.14168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Forward Mode of Automatic Differentiation For Optimization  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shukla%2C+K">Khemraj Shukla</a>, 
<a href="/search/math?searchtype=author&query=Shin%2C+Y">Yeonjong Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 Pages, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15351" title="Abstract">arXiv:2310.15351</a> (replaced) [<a href="/pdf/2310.15351" title="Download PDF">pdf</a>, <a href="/format/2310.15351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Exploration in Bayesian Optimization: Order-Optimal Regret and  Computational Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salgia%2C+S">Sudeep Salgia</a>, 
<a href="/search/cs?searchtype=author&query=Vakili%2C+S">Sattar Vakili</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18449" title="Abstract">arXiv:2310.18449</a> (replaced) [<a href="/pdf/2310.18449" title="Download PDF">pdf</a>, <a href="/format/2310.18449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Representation for Black-Box Optimization with  Implicit Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xing%2C+W">Wenqian Xing</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+J">Jungho Lee</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+C">Chong Liu</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00168" title="Abstract">arXiv:2311.00168</a> (replaced) [<a href="/pdf/2311.00168" title="Download PDF">pdf</a>, <a href="/format/2311.00168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from  Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Calandra%2C+R">Roberto Calandra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02516" title="Abstract">arXiv:2311.02516</a> (replaced) [<a href="/pdf/2311.02516" title="Download PDF">pdf</a>, <a href="/format/2311.02516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forward $&#x3c7;^2$ Divergence Based Variational Importance Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yule Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02535" title="Abstract">arXiv:2311.02535</a> (replaced) [<a href="/e-print/2311.02535" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TokenMotion: Motion-Guided Vision Transformer for Video Camouflaged  Object Detection Via Learnable Token Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tavakoli%2C+E+B">Erfan Bank Tavakoli</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meida Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Suya You</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+R">Raghuveer Rao</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Sanjeev Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+F">Fengbo Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revising Needed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02818" title="Abstract">arXiv:2311.02818</a> (replaced) [<a href="/pdf/2311.02818" title="Download PDF">pdf</a>, <a href="/format/2311.02818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal Processing Meets SGD: From Momentum to Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhipeng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dazhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2010.07468">arXiv:2010.07468</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02826" title="Abstract">arXiv:2311.02826</a> (replaced) [<a href="/pdf/2311.02826" title="Download PDF">pdf</a>, <a href="/format/2311.02826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shilong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zidong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaiwen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinghui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianmin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/mybabyyh/InstructPix2NeRF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04843" title="Abstract">arXiv:2311.04843</a> (replaced) [<a href="/pdf/2311.04843" title="Download PDF">pdf</a>, <a href="/format/2311.04843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Dimensions: Confident Reachability for High-Dimensional  Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yuang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Souradeep Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Ruchkin%2C+I">Ivan Ruchkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05112" title="Abstract">arXiv:2311.05112</a> (replaced) [<a href="/pdf/2311.05112" title="Download PDF">pdf</a>, <a href="/format/2311.05112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Large Language Models in Medicine: Principles, Applications,  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongjian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fenglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Boyang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xinyu Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinfa Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinge Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiru Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S+S">Sam S. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yining Hua</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengfeng Mao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chenyu You</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Clifton%2C+L">Lei Clifton</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiebo Luo</a>, 
<a href="/search/cs?searchtype=author&query=Clifton%2C+D+A">David A. Clifton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Version 3. 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06130" title="Abstract">arXiv:2311.06130</a> (replaced) [<a href="/pdf/2311.06130" title="Download PDF">pdf</a>, <a href="/format/2311.06130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional mixed-categorical Gaussian processes with application  to multidisciplinary design optimization for a green aircraft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Saves%2C+P">Paul Saves</a>, 
<a href="/search/math?searchtype=author&query=Diouane%2C+Y">Youssef Diouane</a>, 
<a href="/search/math?searchtype=author&query=Bartoli%2C+N">Nathalie Bartoli</a>, 
<a href="/search/math?searchtype=author&query=Lefebvre%2C+T">Thierry Lefebvre</a>, 
<a href="/search/math?searchtype=author&query=Morlier%2C+J">Joseph Morlier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 - Structural and Multidisciplinary Optimization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06597" title="Abstract">arXiv:2311.06597</a> (replaced) [<a href="/pdf/2311.06597" title="Download PDF">pdf</a>, <a href="/format/2311.06597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Grokking Through A Robustness Viewpoint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07965" title="Abstract">arXiv:2311.07965</a> (replaced) [<a href="/pdf/2311.07965" title="Download PDF">pdf</a>, <a href="/format/2311.07965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengcheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xulong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 13th IEEE International Conference on Big Data and Cloud Computing (IEEE BDCloud 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09200" title="Abstract">arXiv:2311.09200</a> (replaced) [<a href="/pdf/2311.09200" title="Download PDF">pdf</a>, <a href="/format/2311.09200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A  Path through the Accuracy-Privacy Ceiling Constraining Differentially Private  ML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bridges%2C+R+A">Robert A. Bridges</a>, 
<a href="/search/stat?searchtype=author&query=Tombs%2C+V+J">Vandy J. Tombs</a>, 
<a href="/search/stat?searchtype=author&query=Stanley%2C+C+B">Christopher B. Stanley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10189" title="Abstract">arXiv:2311.10189</a> (replaced) [<a href="/pdf/2311.10189" title="Download PDF">pdf</a>, <a href="/format/2311.10189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAPA-CS: Enabling Scalable Accelerator Design on Distributed HBM-FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prakriya%2C+N">Neha Prakriya</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuze Chi</a>, 
<a href="/search/cs?searchtype=author&query=Basalama%2C+S">Suhail Basalama</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linghao Song</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+J">Jason Cong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10359" title="Abstract">arXiv:2311.10359</a> (replaced) [<a href="/pdf/2311.10359" title="Download PDF">pdf</a>, <a href="/format/2311.10359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenqing Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 21 figures. Added a timeline figure to demonstrate low priority tasks JCT stability. Updated all multi-tasking experiments with a newer NVIDIA driver version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11855" title="Abstract">arXiv:2311.11855</a> (replaced) [<a href="/pdf/2311.11855" title="Download PDF">pdf</a>, <a href="/format/2311.11855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evil Geniuses: Delving into the Safety of LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12052" title="Abstract">arXiv:2311.12052</a> (replaced) [<a href="/pdf/2311.12052" title="Download PDF">pdf</a>, <a href="/format/2311.12052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MagicPose: Realistic Human Poses and Facial Expressions Retargeting with  Identity-aware Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+D">Di Chang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Quankai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jessica Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Guoxian Song</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qing Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yizhe Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mohammad Soleymani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page:<a href="https://boese0601.github.io/magicdance/">this https URL</a> Code:<a href="https://github.com/Boese0601/MagicDance">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12696" title="Abstract">arXiv:2311.12696</a> (replaced) [<a href="/pdf/2311.12696" title="Download PDF">pdf</a>, <a href="/ps/2311.12696" title="Download PostScript">ps</a>, <a href="/format/2311.12696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven System Interconnections and a Novel Data-enabled Internal  Model Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pedari%2C+Y">Yasaman Pedari</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Jaeho Lee</a>, 
<a href="/search/eess?searchtype=author&query=Eun%2C+Y">Yongsoon Eun</a>, 
<a href="/search/eess?searchtype=author&query=Ossareh%2C+H">Hamid Ossareh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, submitted to ACC2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13123" title="Abstract">arXiv:2311.13123</a> (replaced) [<a href="/pdf/2311.13123" title="Download PDF">pdf</a>, <a href="/ps/2311.13123" title="Download PostScript">ps</a>, <a href="/format/2311.13123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Parallel Algorithms for Submodular $p$-Superseparable Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cervenjak%2C+P">Philip Cervenjak</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Junhao Gan</a>, 
<a href="/search/cs?searchtype=author&query=Wirth%2C+A">Anthony Wirth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages. To be published in Approximation and Online Algorithms (Proceedings of the 21st International Workshop, WAOA 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Approximation and Online Algorithms, vol. 14297, p. 219. Springer
  Nature, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13870" title="Abstract">arXiv:2311.13870</a> (replaced) [<a href="/pdf/2311.13870" title="Download PDF">pdf</a>, <a href="/format/2311.13870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-intention Inverse Q-learning for Interpretable Behavior  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Crompe%2C+B">Brice De La Crompe</a>, 
<a href="/search/cs?searchtype=author&query=Kalweit%2C+G">Gabriel Kalweit</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+A">Artur Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Kalweit%2C+M">Maria Kalweit</a>, 
<a href="/search/cs?searchtype=author&query=Diester%2C+I">Ilka Diester</a>, 
<a href="/search/cs?searchtype=author&query=Boedecker%2C+J">Joschka Boedecker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14506" title="Abstract">arXiv:2311.14506</a> (replaced) [<a href="/pdf/2311.14506" title="Download PDF">pdf</a>, <a href="/format/2311.14506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Class Anomaly Detection based on Regularized Discriminative  Coupled hypersphere-based Feature Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafiei%2C+M">Mehdi Rafiei</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+A">Alexandros Iosifidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15040" title="Abstract">arXiv:2311.15040</a> (replaced) [<a href="/pdf/2311.15040" title="Download PDF">pdf</a>, <a href="/format/2311.15040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style  Adviser
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P+P">Pei Pei Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages,20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15679" title="Abstract">arXiv:2311.15679</a> (replaced) [<a href="/pdf/2311.15679" title="Download PDF">pdf</a>, <a href="/format/2311.15679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-agnostic Body Part Relevance Assessment for Pedestrian Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCnder%2C+M">Maurice G&#xfc;nder</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Sneha Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Sifa%2C+R">Rafet Sifa</a>, 
<a href="/search/cs?searchtype=author&query=Bauckhage%2C+C">Christian Bauckhage</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17795" title="Abstract">arXiv:2311.17795</a> (replaced) [<a href="/pdf/2311.17795" title="Download PDF">pdf</a>, <a href="/format/2311.17795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marginal Laplacian Score
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hay%2C+G">Guy Hay</a>, 
<a href="/search/cs?searchtype=author&query=Volk%2C+O">Ohad Volk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17929" title="Abstract">arXiv:2311.17929</a> (replaced) [<a href="/pdf/2311.17929" title="Download PDF">pdf</a>, <a href="/format/2311.17929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Online Communities: Graph Deep Learning on Anonymous Voting Networks  to Identify Sybils in Polycentric Governance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DuPont%2C+Q">Quinn DuPont</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17936" title="Abstract">arXiv:2311.17936</a> (replaced) [<a href="/pdf/2311.17936" title="Download PDF">pdf</a>, <a href="/ps/2311.17936" title="Download PostScript">ps</a>, <a href="/format/2311.17936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnostics Using Nuclear Plant Cyber Attack Analysis Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Patel%2C+J+K">Japan K. Patel</a>, 
<a href="/search/eess?searchtype=author&query=Varuttamaseni%2C+A">Athi Varuttamaseni</a>, 
<a href="/search/eess?searchtype=author&query=Youngblood%2C+R+W">Robert W. Youngblood III</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+C">John C. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper has been submitted to ANS for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01627" title="Abstract">arXiv:2312.01627</a> (replaced) [<a href="/pdf/2312.01627" title="Download PDF">pdf</a>, <a href="/format/2312.01627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Explains Teachers&#x27; Trust of AI in Education across Six Countries?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viberg%2C+O">Olga Viberg</a>, 
<a href="/search/cs?searchtype=author&query=Cukurova%2C+M">Mutlu Cukurova</a>, 
<a href="/search/cs?searchtype=author&query=Feldman-Maggor%2C+Y">Yael Feldman-Maggor</a>, 
<a href="/search/cs?searchtype=author&query=Alexandron%2C+G">Giora Alexandron</a>, 
<a href="/search/cs?searchtype=author&query=Shirai%2C+S">Shizuka Shirai</a>, 
<a href="/search/cs?searchtype=author&query=Kanemune%2C+S">Susumu Kanemune</a>, 
<a href="/search/cs?searchtype=author&query=Wasson%2C+B">Barbara Wasson</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B8mte%2C+C">Cathrine T&#xf8;mte</a>, 
<a href="/search/cs?searchtype=author&query=Spikol%2C+D">Daniel Spikol</a>, 
<a href="/search/cs?searchtype=author&query=Milrad%2C+M">Marcelo Milrad</a>, 
<a href="/search/cs?searchtype=author&query=Coelho%2C+R">Raquel Coelho</a>, 
<a href="/search/cs?searchtype=author&query=Kizilcec%2C+R+F">Ren&#xe9; F. Kizilcec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02197" title="Abstract">arXiv:2312.02197</a> (replaced) [<a href="/pdf/2312.02197" title="Download PDF">pdf</a>, <a href="/format/2312.02197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Degradation Adaption for Open-Set Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Y">Yuanbiao Gou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xinyan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xi Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02783" title="Abstract">arXiv:2312.02783</a> (replaced) [<a href="/pdf/2312.02783" title="Download PDF">pdf</a>, <a href="/format/2312.02783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models on Graphs: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05356" title="Abstract">arXiv:2312.05356</a> (replaced) [<a href="/pdf/2312.05356" title="Download PDF">pdf</a>, <a href="/format/2312.05356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Aleti%2C+A">Aldeida Aleti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 6 tables, under peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05632" title="Abstract">arXiv:2312.05632</a> (replaced) [<a href="/pdf/2312.05632" title="Download PDF">pdf</a>, <a href="/format/2312.05632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subject-Based Domain Adaptation for Facial Expression Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeeshan%2C+M+O">Muhammad Osama Zeeshan</a>, 
<a href="/search/cs?searchtype=author&query=Aslam%2C+M+H">Muhammad Haseeb Aslam</a>, 
<a href="/search/cs?searchtype=author&query=Belharbi%2C+S">Soufiane Belharbi</a>, 
<a href="/search/cs?searchtype=author&query=Koerich%2C+A+L">Alessandro L. Koerich</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+S">Simon Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06182" title="Abstract">arXiv:2312.06182</a> (replaced) [<a href="/pdf/2312.06182" title="Download PDF">pdf</a>, <a href="/format/2312.06182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why &quot;classic&quot; Transformers are shallow and how to make them go deep
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yueyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07586" title="Abstract">arXiv:2312.07586</a> (replaced) [<a href="/pdf/2312.07586" title="Download PDF">pdf</a>, <a href="/format/2312.07586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characteristic Guidance: Non-linear Correction for Diffusion Model at  Large Guidance Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Candi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yuan Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07856" title="Abstract">arXiv:2312.07856</a> (replaced) [<a href="/pdf/2312.07856" title="Download PDF">pdf</a>, <a href="/format/2312.07856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DTL: Disentangled Transfer Learning for Visual Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+M">Minghao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Ke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianxin Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07950" title="Abstract">arXiv:2312.07950</a> (replaced) [<a href="/pdf/2312.07950" title="Download PDF">pdf</a>, <a href="/format/2312.07950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CBQ: Cross-Block Quantization for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhijun Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhiwei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baoqun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08594" title="Abstract">arXiv:2312.08594</a> (replaced) [<a href="/pdf/2312.08594" title="Download PDF">pdf</a>, <a href="/format/2312.08594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Lei Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 30th International Conference on Multimedia Modeling(MMM'24 Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09043" title="Abstract">arXiv:2312.09043</a> (replaced) [<a href="/pdf/2312.09043" title="Download PDF">pdf</a>, <a href="/format/2312.09043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic Bias in Emotion Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wegge%2C+M">Maximilian Wegge</a>, 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to W-NUT at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09196" title="Abstract">arXiv:2312.09196</a> (replaced) [<a href="/pdf/2312.09196" title="Download PDF">pdf</a>, <a href="/format/2312.09196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIRECT: Deep Active Learning under Imbalance and Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nuggehalli%2C+S">Shyam Nuggehalli</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+R">Robert Nowak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09901" title="Abstract">arXiv:2312.09901</a> (replaced) [<a href="/pdf/2312.09901" title="Download PDF">pdf</a>, <a href="/format/2312.09901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporally and Distributionally Robust Optimization for Cold-Start  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jujia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10109" title="Abstract">arXiv:2312.10109</a> (replaced) [<a href="/e-print/2312.10109" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zishan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Chaochen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shanying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xinping Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It needs revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12616" title="Abstract">arXiv:2312.12616</a> (replaced) [<a href="/pdf/2312.12616" title="Download PDF">pdf</a>, <a href="/format/2312.12616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Variational Sequential Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mastrototaro%2C+A">Alessandro Mastrototaro</a>, 
<a href="/search/stat?searchtype=author&query=Olsson%2C+J">Jimmy Olsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In this version there are additional simulations in Section 5.1, some added references, and minor typos fixed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13382" title="Abstract">arXiv:2312.13382</a> (replaced) [<a href="/pdf/2312.13382" title="Download PDF">pdf</a>, <a href="/ps/2312.13382" title="Download PostScript">ps</a>, <a href="/format/2312.13382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSPy Assertions: Computational Constraints for Self-Refining Language  Model Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhvi%2C+A">Arnav Singhvi</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+M">Manish Shetty</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shangyin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+K">Koushik Sen</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Khattab%2C+O">Omar Khattab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Arnav*, Manish*, Shangyin* contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13884" title="Abstract">arXiv:2312.13884</a> (replaced) [<a href="/pdf/2312.13884" title="Download PDF">pdf</a>, <a href="/format/2312.13884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measures of Resilience to Cyber Contagion -- An Axiomatic Approach for  Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Svindland%2C+G">Gregor Svindland</a>, 
<a href="/search/q-fin?searchtype=author&query=Vo%C3%9F%2C+A">Alexander Vo&#xdf;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Cryptography and Security (cs.CR); Discrete Mathematics (cs.DM); Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15101" title="Abstract">arXiv:2312.15101</a> (replaced) [<a href="/pdf/2312.15101" title="Download PDF">pdf</a>, <a href="/format/2312.15101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model  Conversions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louloudakis%2C+N">Nikolaos Louloudakis</a>, 
<a href="/search/cs?searchtype=author&query=Gibson%2C+P">Perry Gibson</a>, 
<a href="/search/cs?searchtype=author&query=Cano%2C+J">Jos&#xe9; Cano</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+A">Ajitha Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, 4 tables, 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16243" title="Abstract">arXiv:2312.16243</a> (replaced) [<a href="/pdf/2312.16243" title="Download PDF">pdf</a>, <a href="/format/2312.16243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Mixture in Training Un-assures Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuxiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Haoang Chi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16903" title="Abstract">arXiv:2312.16903</a> (replaced) [<a href="/pdf/2312.16903" title="Download PDF">pdf</a>, <a href="/format/2312.16903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike No More: Stabilizing the Pre-training of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takase%2C+S">Sho Takase</a>, 
<a href="/search/cs?searchtype=author&query=Kiyono%2C+S">Shun Kiyono</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+S">Sosuke Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+J">Jun Suzuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17058" title="Abstract">arXiv:2312.17058</a> (replaced) [<a href="/pdf/2312.17058" title="Download PDF">pdf</a>, <a href="/ps/2312.17058" title="Download PostScript">ps</a>, <a href="/format/2312.17058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Sharing mechanisms for DAOs: On the optimality of Shapley mechanism  for funding public excludable goods under Sybil strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazorra%2C+B">Bruno Mazorra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02524" title="Abstract">arXiv:2401.02524</a> (replaced) [<a href="/pdf/2401.02524" title="Download PDF">pdf</a>, <a href="/format/2401.02524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Exploration of Synthetic Data Generation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bauer%2C+A">Andr&#xe9; Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Trapp%2C+S">Simon Trapp</a>, 
<a href="/search/cs?searchtype=author&query=Stenger%2C+M">Michael Stenger</a>, 
<a href="/search/cs?searchtype=author&query=Leppich%2C+R">Robert Leppich</a>, 
<a href="/search/cs?searchtype=author&query=Kounev%2C+S">Samuel Kounev</a>, 
<a href="/search/cs?searchtype=author&query=Leznik%2C+M">Mark Leznik</a>, 
<a href="/search/cs?searchtype=author&query=Chard%2C+K">Kyle Chard</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed bug in Figure 44
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03462" title="Abstract">arXiv:2401.03462</a> (replaced) [<a href="/pdf/2401.03462" title="Download PDF">pdf</a>, <a href="/format/2401.03462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soaring from 4K to 400K: Extending LLM&#x27;s Context with Activation Beacon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+N">Ninglu Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qiwei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04118" title="Abstract">arXiv:2401.04118</a> (replaced) [<a href="/pdf/2401.04118" title="Download PDF">pdf</a>, <a href="/format/2401.04118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Directive Explanations: Crafting Explainable AI Systems for  Actionable Human-AI Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Aditya Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print version. Please check the published version in ACM CHI 2024 from the related DOI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04592" title="Abstract">arXiv:2401.04592</a> (replaced) [<a href="/pdf/2401.04592" title="Download PDF">pdf</a>, <a href="/ps/2401.04592" title="Download PostScript">ps</a>, <a href="/format/2401.04592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Assessment on Comprehending Mental Health through Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arcan%2C+M">Mihael Arcan</a>, 
<a href="/search/cs?searchtype=author&query=Niland%2C+D">David-Paul Niland</a>, 
<a href="/search/cs?searchtype=author&query=Delahunty%2C+F">Fionn Delahunty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05015" title="Abstract">arXiv:2401.05015</a> (replaced) [<a href="/pdf/2401.05015" title="Download PDF">pdf</a>, <a href="/format/2401.05015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information Theoretic Approach to Interaction-Grounded Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaoyan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+H">Ho-fung Leung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05975" title="Abstract">arXiv:2401.05975</a> (replaced) [<a href="/pdf/2401.05975" title="Download PDF">pdf</a>, <a href="/format/2401.05975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Learnable Clustering for Intent Learning in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yingwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wenliang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kejun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06683" title="Abstract">arXiv:2401.06683</a> (replaced) [<a href="/pdf/2401.06683" title="Download PDF">pdf</a>, <a href="/format/2401.06683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQNC2S: DQN-based Cross-stream Crisis event Summarizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cambrin%2C+D+R">Daniele Rege Cambrin</a>, 
<a href="/search/cs?searchtype=author&query=Cagliero%2C+L">Luca Cagliero</a>, 
<a href="/search/cs?searchtype=author&query=Garza%2C+P">Paolo Garza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at ECIR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07261" title="Abstract">arXiv:2401.07261</a> (replaced) [<a href="/pdf/2401.07261" title="Download PDF">pdf</a>, <a href="/format/2401.07261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LookAhead: Preventing DeFi Attacks via Unveiling Adversarial Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shoupeng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+T">Tianyu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kui Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07305" title="Abstract">arXiv:2401.07305</a> (replaced) [<a href="/pdf/2401.07305" title="Download PDF">pdf</a>, <a href="/format/2401.07305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Service Slowdown using Observational Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kuang%2C+X">Xu Kuang</a>, 
<a href="/search/physics?searchtype=author&query=Mendelson%2C+G">Gal Mendelson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Systems and Control (eess.SY); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07435" title="Abstract">arXiv:2401.07435</a> (replaced) [<a href="/pdf/2401.07435" title="Download PDF">pdf</a>, <a href="/format/2401.07435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manifolds from Partitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Knill%2C+O">Oliver Knill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, added more code and statistics example
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geometric Topology (math.GT)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07519" title="Abstract">arXiv:2401.07519</a> (replaced) [<a href="/pdf/2401.07519" title="Download PDF">pdf</a>, <a href="/format/2401.07519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstantID: Zero-shot Identity-Preserving Generation in Seconds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qixun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haofan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zekui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anthony Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huaxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yao Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, project page available at <a href="https://instantid.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08417" title="Abstract">arXiv:2401.08417</a> (replaced) [<a href="/pdf/2401.08417" title="Download PDF">pdf</a>, <a href="/format/2401.08417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Preference Optimization: Pushing the Boundaries of LLM  Performance in Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sharaf%2C+A">Amr Sharaf</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weiting Tan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lingfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+K">Kenton Murray</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J">Young Jin Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09407" title="Abstract">arXiv:2401.09407</a> (replaced) [<a href="/pdf/2401.09407" title="Download PDF">pdf</a>, <a href="/format/2401.09407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering Textual Authenticity: A Generalized Strategy through the  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated  Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bethany%2C+M">Mazal Bethany</a>, 
<a href="/search/cs?searchtype=author&query=Wherry%2C+B">Brandon Wherry</a>, 
<a href="/search/cs?searchtype=author&query=Bethany%2C+E">Emet Bethany</a>, 
<a href="/search/cs?searchtype=author&query=Vishwamitra%2C+N">Nishant Vishwamitra</a>, 
<a href="/search/cs?searchtype=author&query=Rios%2C+A">Anthony Rios</a>, 
<a href="/search/cs?searchtype=author&query=Najafirad%2C+P">Peyman Najafirad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09714" title="Abstract">arXiv:2401.09714</a> (replaced) [<a href="/pdf/2401.09714" title="Download PDF">pdf</a>, <a href="/format/2401.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust virtual element methods for coupled stress-assisted diffusion  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khot%2C+R">Rekha Khot</a>, 
<a href="/search/math?searchtype=author&query=Rubiano%2C+A+E">Andres E. Rubiano</a>, 
<a href="/search/math?searchtype=author&query=Ruiz-Baier%2C+R">Ricardo Ruiz-Baier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10352" title="Abstract">arXiv:2401.10352</a> (replaced) [<a href="/pdf/2401.10352" title="Download PDF">pdf</a>, <a href="/format/2401.10352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Cultural Nuances in Dialogue Agents through Cultural Value  Surveys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17pages, 7 figures, EACL 2024 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11317" title="Abstract">arXiv:2401.11317</a> (replaced) [<a href="/pdf/2401.11317" title="Download PDF">pdf</a>, <a href="/format/2401.11317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Third-Party Developers and Tool Development For Community Management on  Live Streaming Platform Twitch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jie Cai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Ya-Fang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Carroll%2C+J+M">John M. Carroll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11819" title="Abstract">arXiv:2401.11819</a> (replaced) [<a href="/pdf/2401.11819" title="Download PDF">pdf</a>, <a href="/format/2401.11819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in  Chinese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kangkang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset revised and finalized, results updated with new model; 8 pages, 7 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12425" title="Abstract">arXiv:2401.12425</a> (replaced) [<a href="/pdf/2401.12425" title="Download PDF">pdf</a>, <a href="/format/2401.12425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Neglected Tails of Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parashar%2C+S">Shubham Parashar</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiqiu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+S">Shu Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://shubhamprshr27.github.io/neglected-tails-of-vlms/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12946" title="Abstract">arXiv:2401.12946</a> (replaced) [<a href="/pdf/2401.12946" title="Download PDF">pdf</a>, <a href="/format/2401.12946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coverage Axis++: Efficient Inner Point Selection for 3D Shape  Skeletonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zimeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhiyang Dou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xiaoxiao Long</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+S">Shiqing Xin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Komura%2C+T">Taku Komura</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiaoming Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper needs major revisions in layout/content
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13239" title="Abstract">arXiv:2401.13239</a> (replaced) [<a href="/pdf/2401.13239" title="Download PDF">pdf</a>, <a href="/format/2401.13239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Crowdsourcing Via Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kagrecha%2C+A">Anmol Kagrecha</a>, 
<a href="/search/cs?searchtype=author&query=Marklund%2C+H">Henrik Marklund</a>, 
<a href="/search/cs?searchtype=author&query=Van+Roy%2C+B">Benjamin Van Roy</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H+J">Hong Jun Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Zeckhauser%2C+R">Richard Zeckhauser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14829" title="Abstract">arXiv:2401.14829</a> (replaced) [<a href="/pdf/2401.14829" title="Download PDF">pdf</a>, <a href="/format/2401.14829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMBRELLA: A One-stop Shop Bridging the Gap from Lab to Real-World IoT  Experimentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavromatis%2C+I">Ioannis Mavromatis</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yichao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Stanoev%2C+A">Aleksandar Stanoev</a>, 
<a href="/search/cs?searchtype=author&query=Portelli%2C+A">Anthony Portelli</a>, 
<a href="/search/cs?searchtype=author&query=Weeks%2C+I">Ingram Weeks</a>, 
<a href="/search/cs?searchtype=author&query=Holden%2C+B">Ben Holden</a>, 
<a href="/search/cs?searchtype=author&query=Glasspole%2C+E">Eliot Glasspole</a>, 
<a href="/search/cs?searchtype=author&query=Farnham%2C+T">Tim Farnham</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Aftab Khan</a>, 
<a href="/search/cs?searchtype=author&query=Raza%2C+U">Usman Raza</a>, 
<a href="/search/cs?searchtype=author&query=Aijaz%2C+A">Adnan Aijaz</a>, 
<a href="/search/cs?searchtype=author&query=Bierton%2C+T">Thomas Bierton</a>, 
<a href="/search/cs?searchtype=author&query=Seto%2C+I">Ichiro Seto</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nita Patel</a>, 
<a href="/search/cs?searchtype=author&query=Sooriyabandara%2C+M">Mahesh Sooriyabandara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for publication to IEEE Access
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Software Engineering (cs.SE); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14832" title="Abstract">arXiv:2401.14832</a> (replaced) [<a href="/pdf/2401.14832" title="Download PDF">pdf</a>, <a href="/format/2401.14832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Image Inpainting via Global Structure-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shipeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+P">Pengfei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenjie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zuoyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hui Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15316" title="Abstract">arXiv:2401.15316</a> (replaced) [<a href="/pdf/2401.15316" title="Download PDF">pdf</a>, <a href="/format/2401.15316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNSEE: Unsupervised Non-contrastive Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87a%C4%9Fatan%2C+%C3%96+V">&#xd6;mer Veysel &#xc7;a&#x11f;atan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15378" title="Abstract">arXiv:2401.15378</a> (replaced) [<a href="/pdf/2401.15378" title="Download PDF">pdf</a>, <a href="/ps/2401.15378" title="Download PostScript">ps</a>, <a href="/format/2401.15378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A RAG-based Question Answering System Proposal for Understanding Islam:  MufassirQAS LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alan%2C+A+Y">Ahmet Yusuf Alan</a>, 
<a href="/search/cs?searchtype=author&query=Karaarslan%2C+E">Enis Karaarslan</a>, 
<a href="/search/cs?searchtype=author&query=Aydin%2C+%C3%96">&#xd6;mer Aydin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15443" title="Abstract">arXiv:2401.15443</a> (replaced) [<a href="/pdf/2401.15443" title="Download PDF">pdf</a>, <a href="/format/2401.15443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuserLite: Towards Real-time Diffusion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zibin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yitian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15459" title="Abstract">arXiv:2401.15459</a> (replaced) [<a href="/pdf/2401.15459" title="Download PDF">pdf</a>, <a href="/format/2401.15459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model as Synthesizer: Fusing Diverse Inputs for Better  Automatic Vulnerability Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kisub Kim</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bowen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">DongGyun Han</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the ICSE 2024 Research Track with a slightly different title "Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15877" title="Abstract">arXiv:2401.15877</a> (replaced) [<a href="/pdf/2401.15877" title="Download PDF">pdf</a>, <a href="/format/2401.15877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DPFIX: Improving Remote Novices&#x27; 3D Printing Troubleshooting through  Human-AI Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+N">Nahyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeeeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S+R">Sungsoo Ray Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CSCW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15963" title="Abstract">arXiv:2401.15963</a> (replaced) [<a href="/pdf/2401.15963" title="Download PDF">pdf</a>, <a href="/format/2401.15963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional  Correctness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+M">Manav Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+T">Tushar Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Awasthi%2C+A">Abhijeet Awasthi</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+N">Nagarajan Natarajan</a>, 
<a href="/search/cs?searchtype=author&query=Kanade%2C+A">Aditya Kanade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16184" title="Abstract">arXiv:2401.16184</a> (replaced) [<a href="/pdf/2401.16184" title="Download PDF">pdf</a>, <a href="/format/2401.16184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Semantics of LM Latent Space: A Vocabulary-defined Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Aleti%2C+A">Aldeida Aleti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16405" title="Abstract">arXiv:2401.16405</a> (replaced) [<a href="/pdf/2401.16405" title="Download PDF">pdf</a>, <a href="/format/2401.16405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Sparse Fine-Tuning to Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ansell%2C+A">Alan Ansell</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Sterz%2C+H">Hannah Sterz</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+E+M">Edoardo M. Ponti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16578" title="Abstract">arXiv:2401.16578</a> (replaced) [<a href="/pdf/2401.16578" title="Download PDF">pdf</a>, <a href="/format/2401.16578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Professional Radiologists&#x27; Expertise to Enhance LLMs&#x27;  Evaluation for Radiology Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Benjamin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Mathai%2C+T+S">Tejas Sudharshan Mathai</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+P">Pritam Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+R+M">Ronald M Summers</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16587" title="Abstract">arXiv:2401.16587</a> (replaced) [<a href="/pdf/2401.16587" title="Download PDF">pdf</a>, <a href="/format/2401.16587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Linguistic Comparison between Human and ChatGPT-Generated  Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sandler%2C+M">Morgan Sandler</a>, 
<a href="/search/cs?searchtype=author&query=Choung%2C+H">Hyesun Choung</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+A">Arun Ross</a>, 
<a href="/search/cs?searchtype=author&query=David%2C+P">Prabu David</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Pending review and feedback from ICPRAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16727" title="Abstract">arXiv:2401.16727</a> (replaced) [<a href="/pdf/2401.16727" title="Download PDF">pdf</a>, <a href="/format/2401.16727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advances in Hate Speech Moderation: Multimodality and the Role of  Large Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hee%2C+M+S">Ming Shan Hee</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shivam Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Rui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Nandi%2C+P">Palash Nandi</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Under-Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16775" title="Abstract">arXiv:2401.16775</a> (replaced) [<a href="/pdf/2401.16775" title="Download PDF">pdf</a>, <a href="/format/2401.16775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Activity Detection for Massive Connectivity in Cell-free Networks with  Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity  Probability: A Bayesian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingfeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yik-Chung Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, accepted for publication in IEEE Transactions on Signal Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16942" title="Abstract">arXiv:2401.16942</a> (replaced) [<a href="/pdf/2401.16942" title="Download PDF">pdf</a>, <a href="/format/2401.16942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Price Discrimination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Arieli%2C+I">Itai Arieli</a>, 
<a href="/search/econ?searchtype=author&query=Babichenko%2C+Y">Yakov Babichenko</a>, 
<a href="/search/econ?searchtype=author&query=Madmon%2C+O">Omer Madmon</a>, 
<a href="/search/econ?searchtype=author&query=Tennenholtz%2C+M">Moshe Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17112" title="Abstract">arXiv:2401.17112</a> (replaced) [<a href="/e-print/2401.17112" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Mod-6 Town Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vishwanathan%2C+S">Sundar Vishwanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bug. Lemma 1 is incorrect. The lemma needs the sets to be closed under subtraction which they are not
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17233" title="Abstract">arXiv:2401.17233</a> (replaced) [<a href="/pdf/2401.17233" title="Download PDF">pdf</a>, <a href="/format/2401.17233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inf-Sup neural networks for high-dimensional elliptic PDE problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huo%2C+X">Xiaokai Huo</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hailiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17270" title="Abstract">arXiv:2401.17270</a> (replaced) [<a href="/pdf/2401.17270" title="Download PDF">pdf</a>, <a href="/format/2401.17270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-World: Real-Time Open-Vocabulary Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Tianheng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lin Song</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yixiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work still in progress. Code &amp; models are available at: <a href="https://github.com/AILab-CVC/YOLO-World">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17435" title="Abstract">arXiv:2401.17435</a> (replaced) [<a href="/pdf/2401.17435" title="Download PDF">pdf</a>, <a href="/format/2401.17435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Replace Economic Choice Prediction Labs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapira%2C+E">Eilam Shapira</a>, 
<a href="/search/cs?searchtype=author&query=Madmon%2C+O">Omer Madmon</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+M">Moshe Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17498" title="Abstract">arXiv:2401.17498</a> (replaced) [<a href="/pdf/2401.17498" title="Download PDF">pdf</a>, <a href="/format/2401.17498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving QA Model Performance with Cartographic Inoculation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Allen Chen</a> (UT Austin), 
<a href="/search/cs?searchtype=author&query=Tanrikulu%2C+O">Okan Tanrikulu</a> (UT Austin)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17527" title="Abstract">arXiv:2401.17527</a> (replaced) [<a href="/pdf/2401.17527" title="Download PDF">pdf</a>, <a href="/format/2401.17527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Stop Cut Generation for Efficient Mixed-Integer Linear  Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haotian Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17615" title="Abstract">arXiv:2401.17615</a> (replaced) [<a href="/pdf/2401.17615" title="Download PDF">pdf</a>, <a href="/format/2401.17615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Multi-Similarity Learning for Molecular Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+P">Pengyu Hong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17622" title="Abstract">arXiv:2401.17622</a> (replaced) [<a href="/pdf/2401.17622" title="Download PDF">pdf</a>, <a href="/format/2401.17622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commit Messages in the Age of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopes%2C+C+V">Cristina V. Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Klotzman%2C+V+I">Vanessa I. Klotzman</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+I">Iris Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Iftekar Ahmed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to FSE 23 on Feb 6 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17663" title="Abstract">arXiv:2401.17663</a> (replaced) [<a href="/pdf/2401.17663" title="Download PDF">pdf</a>, <a href="/format/2401.17663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Robot Navigation with Adaptive Proxemics Based on Emotions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilen%2C+B">Baris Bilen</a>, 
<a href="/search/cs?searchtype=author&query=Kivrak%2C+H">Hasan Kivrak</a>, 
<a href="/search/cs?searchtype=author&query=Uluer%2C+P">Pinar Uluer</a>, 
<a href="/search/cs?searchtype=author&query=Kose%2C+H">Hatice Kose</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, Proceeding of Towards Socially Intelligent Robots In Real World Applications: Challenges And Intricacies (SIRRW) Workshop, RO-MAN 2022, 3-7, August 2022, Naples, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17780" title="Abstract">arXiv:2401.17780</a> (replaced) [<a href="/pdf/2401.17780" title="Download PDF">pdf</a>, <a href="/format/2401.17780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with  Uniform PAC Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kitamura%2C+T">Toshinori Kitamura</a>, 
<a href="/search/cs?searchtype=author&query=Kozuno%2C+T">Tadashi Kozuno</a>, 
<a href="/search/cs?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>, 
<a href="/search/cs?searchtype=author&query=Ichihara%2C+Y">Yuki Ichihara</a>, 
<a href="/search/cs?searchtype=author&query=Nishimori%2C+S">Soichiro Nishimori</a>, 
<a href="/search/cs?searchtype=author&query=Sannai%2C+A">Akiyoshi Sannai</a>, 
<a href="/search/cs?searchtype=author&query=Sonoda%2C+S">Sho Sonoda</a>, 
<a href="/search/cs?searchtype=author&query=Kumagai%2C+W">Wataru Kumagai</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17874" title="Abstract">arXiv:2401.17874</a> (replaced) [<a href="/pdf/2401.17874" title="Download PDF">pdf</a>, <a href="/format/2401.17874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VR-based generation of photorealistic synthetic data for training  hand-object tracking models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+R">Rahul Chaudhari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17878" title="Abstract">arXiv:2401.17878</a> (replaced) [<a href="/pdf/2401.17878" title="Download PDF">pdf</a>, <a href="/format/2401.17878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Data-Centric Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+R">Riwei Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17937" title="Abstract">arXiv:2401.17937</a> (replaced) [<a href="/pdf/2401.17937" title="Download PDF">pdf</a>, <a href="/format/2401.17937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branch and Price for the Length-Constrained Cycle Partition Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ghannam%2C+M">Mohammed Ghannam</a>, 
<a href="/search/math?searchtype=author&query=Mexi%2C+G">Gioni Mexi</a>, 
<a href="/search/math?searchtype=author&query=Lam%2C+E">Edward Lam</a>, 
<a href="/search/math?searchtype=author&query=Gleixner%2C+A">Ambros Gleixner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00045" title="Abstract">arXiv:2402.00045</a> (replaced) [<a href="/pdf/2402.00045" title="Download PDF">pdf</a>, <a href="/format/2402.00045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Multimedia Generated by Large AI Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Li Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Neeraj Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hainan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chun-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Feng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Verdoliva%2C+L">Luisa Verdoliva</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00082" title="Abstract">arXiv:2402.00082</a> (replaced) [<a href="/pdf/2402.00082" title="Download PDF">pdf</a>, <a href="/ps/2402.00082" title="Download PostScript">ps</a>, <a href="/format/2402.00082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Grover&#x27;s Search Algorithm: A Modified Approach to Increase the  Probability of Good States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Abdulrahman%2C+I">Ismael Abdulrahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00225" title="Abstract">arXiv:2402.00225</a> (replaced) [<a href="/pdf/2402.00225" title="Download PDF">pdf</a>, <a href="/format/2402.00225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry aware 3D generation from in-the-wild images in ImageNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qijia Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangrun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00281" title="Abstract">arXiv:2402.00281</a> (replaced) [<a href="/pdf/2402.00281" title="Download PDF">pdf</a>, <a href="/format/2402.00281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Interpretable Facial Expression Recognition via Spatial Action  Unit Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belharbi%2C+S">Soufiane Belharbi</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Koerich%2C+A+L">Alessandro Lameiras Koerich</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+S">Simon Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00306" title="Abstract">arXiv:2402.00306</a> (replaced) [<a href="/pdf/2402.00306" title="Download PDF">pdf</a>, <a href="/format/2402.00306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Accurate and Low-Parameter Machine Learning Architecture for Next  Location Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jary%2C+C">Calvin Jary</a>, 
<a href="/search/cs?searchtype=author&query=Kahani%2C+N">Nafiseh Kahani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper was accepted and presented in person at the 2023 IEEE Future Networks World Forum, in Baltimore, Maryland, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00318" title="Abstract">arXiv:2402.00318</a> (replaced) [<a href="/pdf/2402.00318" title="Download PDF">pdf</a>, <a href="/ps/2402.00318" title="Download PostScript">ps</a>, <a href="/format/2402.00318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analog-digital Scheduling for Federated Learning: A  Communication-Efficient Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrar%2C+M+F+U">Muhammad Faraz Ul Abrar</a>, 
<a href="/search/cs?searchtype=author&query=Michelusi%2C+N">Nicol&#xf2; Michelusi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at the 2023 Asilomar Conference on Signals, Systems, and Computers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00321" title="Abstract">arXiv:2402.00321</a> (replaced) [<a href="/pdf/2402.00321" title="Download PDF">pdf</a>, <a href="/format/2402.00321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and  Judger Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+H">Haonan An</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhengru Fang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guowen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuguang Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00331" title="Abstract">arXiv:2402.00331</a> (replaced) [<a href="/pdf/2402.00331" title="Download PDF">pdf</a>, <a href="/ps/2402.00331" title="Download PostScript">ps</a>, <a href="/format/2402.00331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth and Proper Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anel%2C+M">Mathieu Anel</a>, 
<a href="/search/math?searchtype=author&query=Weinberger%2C+J">Jonathan Weinberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dedicated to Andr\'e Joyal to his 80th birthday; 13 pages, 4 tables. v2 simplified Table 3 and corrected the characterization of acyclic/localic maps in the corresponding examples
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO); Algebraic Geometry (math.AG); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00351" title="Abstract">arXiv:2402.00351</a> (replaced) [<a href="/pdf/2402.00351" title="Download PDF">pdf</a>, <a href="/format/2402.00351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Unlearning for Image-to-Image Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guihong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+H">Hsiang Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun-Fu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Marculescu%2C+R">Radu Marculescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00449" title="Abstract">arXiv:2402.00449</a> (replaced) [<a href="/pdf/2402.00449" title="Download PDF">pdf</a>, <a href="/format/2402.00449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Training Spiking Neural Networks with Parallel Spiking Unit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yinqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiang He</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yiting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongcheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00559" title="Abstract">arXiv:2402.00559</a> (replaced) [<a href="/pdf/2402.00559" title="Download PDF">pdf</a>, <a href="/format/2402.00559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for  Verifiers of Reasoning Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacovi%2C+A">Alon Jacovi</a>, 
<a href="/search/cs?searchtype=author&query=Bitton%2C+Y">Yonatan Bitton</a>, 
<a href="/search/cs?searchtype=author&query=Bohnet%2C+B">Bernd Bohnet</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Honovich%2C+O">Or Honovich</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+M">Michael Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+M">Michael Collins</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset at <a href="https://huggingface.co/datasets/google/reveal">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00591" title="Abstract">arXiv:2402.00591</a> (replaced) [<a href="/pdf/2402.00591" title="Download PDF">pdf</a>, <a href="/format/2402.00591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazzari%2C+N">Nicolas Lazzari</a>, 
<a href="/search/cs?searchtype=author&query=De+Giorgis%2C+S">Stefano De Giorgis</a>, 
<a href="/search/cs?searchtype=author&query=Gangemi%2C+A">Aldo Gangemi</a>, 
<a href="/search/cs?searchtype=author&query=Presutti%2C+V">Valentina Presutti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00697" title="Abstract">arXiv:2402.00697</a> (replaced) [<a href="/pdf/2402.00697" title="Download PDF">pdf</a>, <a href="/format/2402.00697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Belief Function Theory and Stochastic Model Predictive Control  for Multi-Modal Uncertainty in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Benciolini%2C+T">Tommaso Benciolini</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+Y">Yuntian Yan</a>, 
<a href="/search/eess?searchtype=author&query=Wollherr%2C+D">Dirk Wollherr</a>, 
<a href="/search/eess?searchtype=author&query=Leibold%2C+M">Marion Leibold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted to the 2024 American Control Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00752" title="Abstract">arXiv:2402.00752</a> (replaced) [<a href="/pdf/2402.00752" title="Download PDF">pdf</a>, <a href="/format/2402.00752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Projection for 3D Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Letian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiayang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jie Guo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanwen Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00786" title="Abstract">arXiv:2402.00786</a> (replaced) [<a href="/pdf/2402.00786" title="Download PDF">pdf</a>, <a href="/format/2402.00786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CroissantLLM: A Truly Bilingual French-English Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faysse%2C+M">Manuel Faysse</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+P">Patrick Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Guerreiro%2C+N+M">Nuno M. Guerreiro</a>, 
<a href="/search/cs?searchtype=author&query=Loison%2C+A">Ant&#xf3;nio Loison</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+D+M">Duarte M. Alves</a>, 
<a href="/search/cs?searchtype=author&query=Corro%2C+C">Caio Corro</a>, 
<a href="/search/cs?searchtype=author&query=Boizard%2C+N">Nicolas Boizard</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jo&#xe3;o Alves</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+R">Ricardo Rei</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+P+H">Pedro H. Martins</a>, 
<a href="/search/cs?searchtype=author&query=Casademunt%2C+A+B">Antoni Bigata Casademunt</a>, 
<a href="/search/cs?searchtype=author&query=Yvon%2C+F">Fran&#xe7;ois Yvon</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F.T. Martins</a>, 
<a href="/search/cs?searchtype=author&query=Viaud%2C+G">Gautier Viaud</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00790" title="Abstract">arXiv:2402.00790</a> (replaced) [<a href="/pdf/2402.00790" title="Download PDF">pdf</a>, <a href="/ps/2402.00790" title="Download PostScript">ps</a>, <a href="/format/2402.00790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Pre-Quantum to Post-Quantum IoT Security: A Survey on  Quantum-Resistant Cryptosystems for the Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez-Carames%2C+T+M">Tiago M. Fernandez-Carames</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> T. M. Fernandez-Carames, "From Pre-Quantum to Post-Quantum IoT
  Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of
  Things," in IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6457-6480,
  July 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00856" title="Abstract">arXiv:2402.00856</a> (replaced) [<a href="/pdf/2402.00856" title="Download PDF">pdf</a>, <a href="/format/2402.00856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient and Exact Optimization of Language Model Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Haozhe Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yilin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00863" title="Abstract">arXiv:2402.00863</a> (replaced) [<a href="/pdf/2402.00863" title="Download PDF">pdf</a>, <a href="/format/2402.00863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry Transfer for Stylizing Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyunyoung Jung</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Seonghyeon Nam</a>, 
<a href="/search/cs?searchtype=author&query=Sarafianos%2C+N">Nikolaos Sarafianos</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Sungjoo Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Sorkine-Hornung%2C+A">Alexander Sorkine-Hornung</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Rakesh Ranjan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> project page: <a href="https://hyblue.github.io/geo-srf/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item372">Cross-lists</a></li>
<li><a href="#item422">Replacements</a></li>
</ul>
<small>[ total of 674 entries:  <b>1-674</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
